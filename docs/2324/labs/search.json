[
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\nwhat is your name?\nwhat is your favourite colour?\n\n\n\n\n Solution \n\n\nsolution\nhello\n\n2+2\n\n[1] 4\n\n\n\n\n\n\n Optional hello my optional friend\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  },
  {
    "objectID": "1_01_function.html",
    "href": "1_01_function.html",
    "title": "Functions and Models",
    "section": "",
    "text": "At the end of this lab, you will:\n\nHave reviewed the main concepts from introductory statistics.\nUnderstand the concept of a function.\nBe able to discuss what a statistical model is.\nUnderstand the link between models and functions.\n\n\n\n\n\nHave attended and/or watched Week 1 lectures.\nHave installed R and RStudio on your own computer (unless you have a Chromebook where you may continue to use the PPLS RStudio Server).\n\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nggExtra\nkableExtra\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/handheight.csv."
  },
  {
    "objectID": "1_01_function.html#functions-and-mathematical-models-plots",
    "href": "1_01_function.html#functions-and-mathematical-models-plots",
    "title": "Functions and Models",
    "section": "Functions and Mathematical Models: Plots",
    "text": "Functions and Mathematical Models: Plots\n\nQuestion 4\n\n\nCreate a data set called squares containing the perimeter of four squares having sides of length \\(0, 2, 5, 9\\) metres, and then plot the squares data as points on a scatterplot.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember that to combine multiple numbers together we use the function c().\n\n\n\n\n\n\n\n Solution \n\n\nFirst, let’s make our squares data:\n\nsquares &lt;- tibble(\n  side = c(0, 2, 5, 9), \n  perimeter = 4 * side\n)\n\nsquares\n\n# A tibble: 4 × 2\n   side perimeter\n  &lt;dbl&gt;     &lt;dbl&gt;\n1     0         0\n2     2         8\n3     5        20\n4     9        36\n\n\nNext, lets plot it:\n\nggplot(data = squares, aes(x = side, y = perimeter)) +\n  geom_point() +   #specify `geom_point' as want to create a scatterplot\n  labs(x = 'Side (m)', y = 'Perimeter (m)')    #label x- and y-axis with new and clearer titles\n\n\n\n\nFigure 1: ?(caption)\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nGenerate one hundred data points, and use them to visualise the relationship between side and perimeter of squares. To do so, you need to complete four steps:\n\nCreate a sequence of one hundred side lengths (x) going from 0 to 3 metres.\nCompute the corresponding perimeters (y).\nPlot the side and perimeter data as points on a graph.\nVisualise the functional relationship between side and perimeter of squares. To do so, use the function geom_line() to connect the computed points with lines.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo create a sequence of numbers, we can use the function seq(). We need to include within the seq() argument our from and to values (i.e., the lowest and highest values in the sequence), as well as length.out to specify the length of the sequence (i.e., how many 0-3’s do we want).\nIf you would like to change the colour of the line (step 4), you can specify geom_line(colour = \"insert_colour\")\n\n\n\n\n\n\n\n Solution \n\n\nSteps 1 & 2 - create the side and perimeter data:\n\nsquares_grid &lt;- tibble(\n  side = seq(0, 3, length.out = 100),\n  perimeter = 4 * side)\n\nStep 3 - plot the individual points:\n\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4*Side')\n\n\n\n\n\n\n\n\nStep 4 - visualise the functional relationship by connecting the individual points with a line:\n\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_point() +\n  geom_line(colour = 'blue') +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4*Side')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\nThe function \\(y = 4 \\ x\\) that you plotted above is an example of a function representing a mathematical model.\nWe typically validate a model using experimental data. However, we all know how squares work and that two squares with the same side will have the same perimeter (more on this later).\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nThe Scottish National Gallery kindly provided us with measurements of side and perimeter (in metres) for a sample of 10 square paintings.\nThe data are provided below:\n\nsng &lt;- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nPlot the mathematical model of the relationship between side and perimeter for squares, and superimpose on top the experimental data from the Scottish National Gallery.\n\n\n\n\n Solution \n\n\n\nsng &lt;- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nggplot() + \n  geom_line(data = squares_grid, aes(x = side, y = perimeter), colour = 'blue') + # add blue line\n  geom_point(data = sng, aes(x = side, y = perimeter), colour = 'black',  # add black points\n             alpha = 0.5, size = 3) + # make the points 0.5 transparency, size 3\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\n\nFigure 2: The exact relationship between side and perimeter of squares\n\n\n\n\nThe above plot shows perfect agreement between the observed data and the model.\n\n\n\n\n\nQuestion 7\n\n\nUse the mathematical model to predict the perimeter of a painting with a side of 1.5 metres.\n\n\n\n\n\n\nHint\n\n\n\n\n\nDon’t forget to always include the measurement units when reporting/writing-up results!\n\n\n\n\n\n\n\n Solution \n\n\nWe do not have a painting with a side of 1.5 metres within the random sample of paintings from the Scottish National Gallery. However, we can predict the perimeter of an unobserved squared painting having a 1.5 metre side using the mathematical model.\nYou can obtain this prediction using either a visual approach or an algebraic one.\n\nVisual ApproachAlgebraic Approach\n\n\n\n\n\n\n\n\n\n\n\nSometimes we can directly read a predicted value from the graph of the functional relationship.\nConsider the plot created in the previous question. First, we need to check where x = 1.5. Then, we draw a vertical dashed line until it meets the blue line. The y value corresponding to x = 1.5 can be read off the y-axis.\nHowever, in this case it is not that easy to read it from the drawing…\n\n\nYou can substitute the x value in the formula and calculate the corresponding y value. \\[\ny = 4 * x = 4 * 1.5 = 6\n\\]\n\n\n\n\n\n\n\n\n\nThe predicted perimeter of squared paintings having a 1.5m side is 6m."
  },
  {
    "objectID": "1_01_function.html#study-overview",
    "href": "1_01_function.html#study-overview",
    "title": "Functions and Models",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Question\nHow does handspan vary as a function of height?\n\nConsider now the association between height (in inches) and handspan (in cm). Utts and Heckard (2015) provided data for a sample of 167 students which reported their height and handspan as part of a class survey.\nUsing the handheight data you already loaded at the start of the lab, your task is to investigate how handspan varies as a function of height for the students in the sample.\n\n Handheight codebook.\n\n\nDescription\nThe data set records the height and handspan reported by a random sample of 167 students as part of a class survey.\nThe variables are:\n\nheight, measured in inches\nhandspan, measured in centimetres\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\nheight\nhandspan\n\n\n\n\n68\n21.5\n\n\n71\n23.5\n\n\n73\n22.5\n\n\n64\n18.0\n\n\n68\n23.5\n\n\n59\n20.0\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nUsing a scatterplot (since the variables are numeric and continuous) to visualise the association between the two numeric variables, comment on any main differences you notice with the association between side and perimeter of squares. Note if you detected outliers or points that do not fit with the pattern in the rest of the data.\n\n\n\n\n Solution \n\n\n\nplt &lt;- ggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\nplt\n\n\n\n\nFigure 3: Simple Scatterplot\n\n\n\n\nWe can also add marginal boxplots for each variable using the package ggExtra.\n\nggMarginal(plt, type = 'boxplot')\n\n\n\n\nFigure 4: The statistical relationship between height and handspan\n\n\n\n\nOutliers are extreme observations that are not possible values of a variable or that do not seem to fit with the rest of the data. This could either be:\n\nmarginally along one axis: points that have an unusual (too high or too low) x-coordinate or y-coordinate;\njointly: observations that do not fit with the rest of the point cloud.\n\nThe boxplots in Figure 4 do not highlight any outliers in the marginal distributions of height and handspan. Furthermore, from the scatterplot we do not notice any extreme observations or points that do not fit with the rest of the point cloud.\nWe notice a moderate, positive linear association between height and handspan.\nRecall Figure 2, displaying the association between side and perimeters of squares. In the plot we notice two points on top of each other, reflecting the fact that two squares having the same side will always have the same perimeter. In fact, the data from the Scottish National Gallery include two squared paintings with a side of 1.1m, both having a measured perimeter of 4.4m.\nFigure 4, instead, displays the association between height and handspan of a sample of students. The first thing that grabs our attention is the fact that students having the same height do not necessarily have the same handspan. Rather, we clearly see a variety of handspan values for students all having a height of, for example, 70in. To be more precise, the seven students who are 70 in. tall all have differing handspans.\n\n\n\n\n\nQuestion 9\n\n\nUsing the following command, superimpose on top of the scatterplot a best-fit line describing how handspan varies as a function of height. For the moment, the argument se = FALSE tells R to not display uncertainty bands.\n\ngeom_smooth(method = lm, se = FALSE)\n\nComment on any differences between the lines representing the linear relationship between (a) the side and perimeter of square and (b) height and handspan.\n\n\n\n\n Solution \n\n\n\nggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_smooth(method = lm, se = FALSE) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\n\n\n\nFigure 5: The best-fit line\n\n\n\n\nThe line representing the relationship between side and perimeter of squares is able to predict the actual perimeter value from the measurement of the side of a square. This is possible because the relationship between side and perimeter is an exact one.\nThat is, any squares having the same side will have the same perimeter, and there will be no variation in those values.\nThe line that best fits the relationship between height and handspan (see Figure 5), instead, is only able to predict the average handspan for a given value of height.\nThis is because there will be a distribution of handspans at each value of height. The line will fit the trend/pattern in the values, but there will be individual-to-individual variability that we must accept around that average pattern.\n\n\n\n\n\nQuestion 10\n\n\nThe line of best-fit is given by:1\n\\[\n\\widehat{Handspan} = -3 + 0.35 \\cdot Height\n\\]\nEstimate (or predict) handspan of a student who is (a) 73in tall, and (b) 5in tall.\n\n\n\n\n Solution \n\n\n\nThe predicted average handspan for students who are 73in tall is:  \\(-3 + (0.35 * 73) = 22.55\\)cm.  \nThe predicted average handspan for students who are 5in tall is:  \\(-3 + (0.35 * 5) = -1.25\\)cm.\n\nBut wait, handspan can not be negative… This does not make any sense! That’s right, we went too far off the range of the available data on heights, which were between 57in and 78in. We extrapolated. This is very dangerous…\n\n\n\n\n\nSource: Randall Munroe, xkcd.com"
  },
  {
    "objectID": "1_01_function.html#footnotes",
    "href": "1_01_function.html#footnotes",
    "title": "Functions and Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, the error term is gone. This is because the line of best-fit gives you the prediction of the average handspan for a given height, and not the individual handspan of a person, which will almost surely be different from the prediction of the line.↩︎"
  },
  {
    "objectID": "1_02_slr.html",
    "href": "1_02_slr.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to specify a simple linear model.\nUnderstand what fitted values and residuals are.\nBe able to interpret the coefficients of a fitted model.\n\n\nBe up to date with lectures from Weeks 1 & 2\nHave completed Week 1 lab exercises\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/riverview.csv."
  },
  {
    "objectID": "1_02_slr.html#data-exploration",
    "href": "1_02_slr.html#data-exploration",
    "title": "Simple Linear Regression",
    "section": "Data Exploration",
    "text": "Data Exploration\nThe common first port of call for almost any statistical analysis is to explore the data, and we can do this visually and/or numerically.\n\n\n\n\n\n\n\n\n\n================ Description\n\n\nMarginal Distributions\nThe distribution of each variable (e.g., employee incomes and education levels) without reference to the values of the other variables\n\n\nBivariate Associations\nDescribing the relationship between two numeric variables\n\n\n\n\n\n\n\n\nVisually \n\n\nPlot each variable individually.  You could use, for example, geom_density() for a density plot or geom_histogram() for a histogram to comment on and/or examine: \n\nThe shape of the distribution. Look at the shape, centre and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\n\n\nIdentify any unusual observations. Do you notice any extreme observations (i.e., outliers)?\n\n\n\nPlot associations among two variables. |  |  | You could use, for example, geom_point() for a scatterplot to comment on and/or examine:  |  |\n\nThe direction of the association indicates whether there is a positive or negative association\n\n\nThe form of association refers to whether the relationship between the variables can be summarized well with a straight line or some more complicated pattern\n\n\nThe strength of association entails how closely the points fall to a recognizable pattern such as a line\n\n\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail\n\n\n\n\n\nNumerically \n\nCompute and report summary statistics e.g., mean, standard deviation, median, min, max, etc.  You could, for example, calculate summary statistics such as the mean (mean()) and standard deviation (sd()), etc. within summarize()\n\nCompute and report the correlation coefficient.  You can use the cor() function to calculate this\n\n\n\n\nMarginal Distributions\n\nQuestion 2\n\n\nVisualise and describe the marginal distributions of employee incomes and education level.\n\n\n\n\n Solution \n\n\n\n\nEmployee Incomes\nEmployee Education Levels\n\n\n\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = income)) +\n  geom_density() +\n  geom_boxplot(width = 1/300) +\n  labs(x = \"Income (in thousands of U.S. dollars)\", \n       y = \"Probability density\")\n\n\n\nFigure 1: Density plot and boxplot of employee incomes\n\n\n\nInitial observations from plot:\n\nThe distribution of employee incomes was unimodal\nMost of the incomes were between roughly $45,000 and $63,000\nThe lowest income in the sample was approximately $25,000 and the highest approximately $83,000. This suggested there was a fair high degree of variation in the data.\nThe boxplot did not highlight any outliers in the data.\n\nDescriptive (or summary) statistics for the employees’ incomes:\n\ndesc_income &lt;- riverview %&gt;% \n  summarize(\n    M = mean(income), \n    SD = sd(income)\n    )\ndesc_income\n\n# A tibble: 1 × 2\n      M    SD\n  &lt;dbl&gt; &lt;dbl&gt;\n1  53.7  14.6\n\n\nFollowing the exploration above, we can describe the income variable as follows:\n\n\n\n\n\n\nThe marginal distribution of income was unimodal with a mean of approximately $53,700. There was variation in employees’ salaries (SD = $14,553).\n\n\n\n\n\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = education)) +\n  geom_density() +\n  geom_boxplot(width = 1/100) +\n  labs(x = \"Education (in years)\", \n       y = \"Probability density\")\n\n\n\nFigure 2: Density plot and boxplot of employee education levels\n\n\n\nInitial observations from plot:\n\nThe distribution of employee education was unimodal\nMost of the employees received formal education for between 12 and 20 years\nThe fewest formal years of education was approximately 8 years, and the highest approximately 25. This suggested there was a fair high degree of variation in the data.\nThe boxplot did not highlight any outliers in the data\n\nDescriptive (or summary) statistics for the employees’ level of education:\n\ndesc_education &lt;- riverview %&gt;%\n  summarize(\n    M = mean(education),\n    SD = sd(education)\n    )\ndesc_education\n\n# A tibble: 1 × 2\n      M    SD\n  &lt;dbl&gt; &lt;dbl&gt;\n1    16  4.36\n\n\nFollowing the exploration above, we can describe the education variable as follows:\n\n\n\n\n\n\nThe marginal distribution of education was unimodal with an average of of 16 years. There was variation in employees’ level of education (SD = 4.4 years).\n\n\n\n\n\n\n\n\n\nAssociations among Variables\n\nQuestion 3\n\n\nCreate a scatterplot of income and education level before calculating the correlation between income and education level.\nMaking reference to both the plot and correlation coefficient, describe the association between income and level of education among the employees in the sample.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe are trying to investigate how income varies when varying years of formal education. Hence, income is the dependent variable (on the y-axis), and education is the independent variable (on the x-axis).\n\n\n\n\n\n\n\n Solution \n\n\nLet’s produce a scatterplot:\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point() +\n  labs(x = \"Education (in years)\", \n       y = \"Income (in thousands of U.S. dollars)\")\n\n\n\nFigure 3: The association between employees’ education level and income\n\n\n\nTo comment on the strength of the linear association we compute the correlation coefficient:\n\ncorr &lt;- riverview %&gt;%\n  select(education, income) %&gt;%\n  cor()\ncorr\n\n          education    income\neducation 1.0000000 0.7947847\nincome    0.7947847 1.0000000\n\n\nthat is, \\[\nr_{\\text({education, income})} = 0.79\n\\] \n\n\n\n\n\n\nThere was a strong positive linear association between education level and income for the employees in the sample. High incomes tended to be observed, on average, with more years of formal education (\\(r\\) = .79)."
  },
  {
    "objectID": "1_02_slr.html#model-specification-and-fitting",
    "href": "1_02_slr.html#model-specification-and-fitting",
    "title": "Simple Linear Regression",
    "section": "Model Specification and Fitting",
    "text": "Model Specification and Fitting\nThe scatterplot highlighted a linear relationship, where the data points were scattered around an underlying linear pattern with a roughly-constant spread as x varied.\nHence, we will try to fit a simple (i.e., one x variable only) linear regression model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\\\\n\\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\n\\]\n\n\n\n\n\n\nWhat does \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\\) mean?\n\n\n\n\n\nLets break the statement down into smaller parts:\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\n\n\\(y_i\\) is our measured outcome variable (our DV)\n\n\\(x_i\\) is our measured predictor variable (our IV)\n\n\\(\\beta_0\\) is the model intercept\n\n\\(\\beta_1\\) is the model slope\n\\(\\epsilon \\sim N(0, \\sigma) \\text{ independently}\\)\n\n\n\\(\\epsilon\\) is the residual error\n\n\\(\\sim\\) means ‘distributed according to’\n\n\\(\\sim N(0, \\sigma) \\text{ independently}\\) means ‘normal distribution with a mean of 0 and a variance of \\(\\sigma\\)’\nTogether, we can say that the errors around the line have a mean of zero and constant spread as x varies.\n\n\n\n\n\n\nQuestion 4\n\n\nFirst, write the equation of the fitted line.\nNext, using the lm() function, fit a simple linear model to predict income (DV) by Education (IV), naming the output mdl.\nLastly, add update your equation of the fitted line to include the estimated coefficients.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe syntax of the lm() function is:\n\n[model name] &lt;- lm([response variable i.e., dependent variable] ~ [explanatory variable i.e., independent variable], data = [dataframe])\n\n\n\n\n\n\n\n\n Solution \n\n\nFirst, lets specify the fitted model, which can be written either as:\n\n\nOption A\nOption B\n\n\n\n\\[\n\\widehat{Income} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Education\n\\]\n\n\n\\[\n\\widehat{Income} = \\hat \\beta_0 \\cdot 1 + \\hat \\beta_1 \\cdot Education\n\\]\n\n\n\nTo fit the model in R, as the variables are in the riverview dataframe, we would write:\n\n\nOption A\nOption B\n\n\n\n\nmdl &lt;- lm(income ~ education, data = riverview)\nmdl\n\n\nCall:\nlm(formula = income ~ education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\n\n\n\nmdl &lt;- lm(income ~ 1 + education, data = riverview)\nmdl\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\n\n\n\n\n\n\n\n\n\nWhy is there a 1 in the Option B’s?\n\n\n\n\n\nWhen we specify the linear model in R, we include after the tilde sign (\\(\\sim\\)), the variables that appear to the right of the \\(\\hat \\beta\\)s. The intercept, or \\(\\beta_0\\), is a constant. That is, we could write it as multiplied by 1.\nIncluding the 1 explicitly is not necessary because it is included by default (you can check this by comparing the outputs of A & B above with and without the 1 included - the estimates are the same!). After a while, you will find you just want to drop the 1 (i.e., Option B) when calling lm() because you know that it’s going to be there, but in these early weeks we tried to keep it explicit to make it clear that you want to the intercept to be estimated.\n\n\n\nNote that by calling the name of the fitted model, mdl, you can see the estimated regression coefficients \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). We can add these values to the fitted line:\n\\[\n\\widehat{Income} = 11.32 + 2.65 \\cdot Education \\\\\n\\]\n\n\n\n\n\nQuestion 5\n\n\nExplore the following equivalent ways to obtain the estimated regression coefficients — that is, \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) — from the fitted model:\n\nmdl\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\n\n\n Solution \n\n\nThe estimated parameters returned by the below methods are all equivalent. However, summary() returns more information.\n\n\nmdl()\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\nSimply invoke the name of the fitted model:\n\nmdl\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\n\n\n\nmdl$coefficients\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\n\ncoef(mdl)\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\n\ncoefficients(mdl)\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\n\n\n\n\n\n\n\n\n\nThe estimated intercept is \\(\\hat \\beta_0 = 11.32\\) and the estimated slope is \\(\\hat \\beta_1 = 2.65\\).\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nExplore the following equivalent ways to obtain the estimated standard deviation of the errors — that is, \\(\\hat \\sigma\\) — from the fitted model mdl:\n\nsigma(mdl)\nsummary(mdl)\n\n\n Huh? What is \\(\\sigma\\)?\n\n\nThe standard deviation of the errors, denoted by \\(\\sigma\\), is an important quantity that our model estimates. It represents how much individual data points tend to deviate above and below the regression line.\nA small \\(\\sigma\\) indicates that the points hug the line closely and we should expect fairly accurate predictions, while a large \\(\\sigma\\) suggests that, even if we estimate the line perfectly, we can expect individual values to deviate from it by substantial amounts.\nThe estimated standard deviation of the errors is denoted \\(\\hat \\sigma\\), and is estimated by essentially averaging squared residuals (giving the variance) and taking the square-root:\n\\[\n\\begin{align}\n& \\hat \\sigma = \\sqrt{\\frac{SS_{Residual}}{n - k - 1}} \\\\\n\\qquad \\\\\n& \\text{where} \\\\\n& SS_{Residual} = \\textrm{Sum of Squared Residuals} = \\sum_{i=1}^n{(\\epsilon_i)^2}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\nThe estimated standard deviation of the errors can be equivalently obtained by the below methods. However, summary() returns more information.\n\n\nsigma(mdl)\nsummary(mdl)\n\n\n\n\nsigma(mdl)\n\n[1] 8.978116\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term “Residual standard error” is a misnomer, as the help page for sigma says (check ?sigma). However, it’s hard to get rid of this bad name as it has been used in too many books showing R output.\n\n\n\n\n\n\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma = 8.98\\).\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nInterpret the estimated intercept, slope, and standard deviation of the errors in the context of the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo interpret the estimated standard deviation of the errors we can use the fact that about 95% of values from a normal distribution fall within two standard deviations of the centre.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nIntercept\nSlope\nStandard deviation of the errors\n\n\n\nThe estimated income associated to zero years of formal education is $11,321.\n\n\nThe estimated increase in income associated to a one year increase in education is $2,651.\n\n\nFor any particular level of education, employee incomes should be distributed above and below the regression line with standard deviation estimated to be \\(\\hat \\sigma = 8.98\\). Since \\(2 \\hat \\sigma = 2 (8.98) = 17.96\\), we expect most (about 95%) of the employee incomes to be within about $18,000 from the regression line.\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nPlot the data and the fitted regression line. To do so:\n\nExtract the estimated regression coefficients e.g., via betas &lt;- coef(mdl)\n\nExtract the first entry of betas (i.e., the intercept) via betas[1]\n\nExtract the second entry of betas (i.e., the slope) via betas[2]\n\nProvide the intercept and slope to the function\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nExtracting values: The function coef(mdl) returns a vector: that is, a sequence of numbers all of the same type. To get the first element of the sequence you append [1], and [2] for the second.\nPlotting: In your ggplot(), you will need to specify geom_abline(). This might help get you started:\n\ngeom_abline(intercept = &lt;intercept&gt;, slope = &lt;slope&gt;)\n\n\n\n\n\n\n\n\n Solution \n\n\nFirst extract the values required:\n\nbetas &lt;- coef(mdl)\nintercept &lt;- betas[1]\nslope &lt;- betas[2]\n\nWe can plot the model as follows:\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point() +\n  geom_abline(intercept = intercept, slope = slope, color = 'blue') + \n  labs(x = \"Education (in years)\", y = \"Income (in thousands of U.S. dollars)\")"
  },
  {
    "objectID": "1_02_slr.html#fitted-and-predicted-values",
    "href": "1_02_slr.html#fitted-and-predicted-values",
    "title": "Intro to Linear Regression",
    "section": "Fitted and Predicted Values",
    "text": "Fitted and Predicted Values\n\nTo compute the model-predicted values for the data in the sample, we can use various funcitons:\n\npredict(&lt;fitted model&gt;)\nfitted(&lt;fitted model&gt;)\nfitted.values(&lt;fitted model&gt;)\nmdl$fitted.values\n\nFor example, this will give us the estimated income (point on our regression line) for each observed value of education level.\n\npredict(mdl)\n\n       1        2        3        4        5        6        7        8 \n32.53175 32.53175 37.83435 37.83435 37.83435 43.13694 43.13694 43.13694 \n       9       10       11       12       13       14       15       16 \n43.13694 48.43953 48.43953 48.43953 51.09083 53.74212 53.74212 53.74212 \n      17       18       19       20       21       22       23       24 \n53.74212 53.74212 56.39342 59.04472 59.04472 61.69601 61.69601 64.34731 \n      25       26       27       28       29       30       31       32 \n64.34731 64.34731 64.34731 66.99861 66.99861 69.64990 69.64990 74.95250 \n\n\nModel-predicted values for other (unobserved) data:\n\npredict(&lt;fitted model&gt;, newdata = &lt;dataframe&gt;)\n\nWe first need to remember that the model predicts income using the independent variable education. Hence, if we want predictions for new (unobserved) data, we first need to create a tibble with a column called education containing the years of education for which we want the prediction, and store this as a dataframe.\n\n#Create dataframe 'newdata' containing education years of 11 and 23\nnewdata &lt;- tibble(education = c(11, 23))\nnewdata\n\n# A tibble: 2 × 1\n  education\n      &lt;dbl&gt;\n1        11\n2        23\n\n\nThen we take newdata and add a new column called income_hat, computed as the prediction from the fitted mdl using the newdata above:\n\nnewdata &lt;- newdata %&gt;%\n  mutate(\n    income_hat = predict(mdl, newdata = newdata)\n  )\nnewdata\n\n# A tibble: 2 × 2\n  education income_hat\n      &lt;dbl&gt;      &lt;dbl&gt;\n1        11       40.5\n2        23       72.3\n\n\nResiduals\nThe residuals represent the deviations between the actual responses and the predicted responses and can be obtained either as\n\nmdl$residuals\nresid(mdl)\nresiduals(mdl)\ncomputing them as the difference between the response and the predicted response\n\n\n\n\nQuestion 9\n\n\nUse predict(mdl) to compute the fitted values and residuals. Mutate the riverview dataframe to include the fitted values and residuals as extra columns.\nAssign to the following symbols the corresponding numerical values:\n\n\n\\(y_{3}\\) = response variable for unit \\(i = 3\\) in the sample data\n\n\\(\\hat y_{3}\\) = fitted value for the third unit\n\n\\(\\hat \\epsilon_{5} = y_{5} - \\hat y_{5}\\) = the residual corresponding to the 5th unit.\n\n\n\n\n\n Solution \n\n\n\nriverview_fitted &lt;- riverview %&gt;%\n  mutate(\n    income_hat = predict(mdl),\n    resid = income - income_hat\n  )\n\nhead(riverview_fitted)\n\n# A tibble: 6 × 8\n  education income seniority gender  male party       income_hat  resid\n      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n1         8   37.4         7 male       1 Democrat          32.5   4.92\n2         8   26.4         9 female     0 Independent       32.5  -6.10\n3        10   47.0        14 male       1 Democrat          37.8   9.20\n4        10   34.2        16 female     0 Independent       37.8  -3.65\n5        10   25.5         1 female     0 Republican        37.8 -12.4 \n6        12   46.5        11 female     0 Democrat          43.1   3.35\n\n\n\n\n\\(y_{3}\\) = 47.03 (see row 3, column 2)\n\n\\(\\hat y_{3}\\) = 37.83 (see row 3, column 7)\n\n\\(\\hat \\epsilon_{5} = y_{5} - \\hat y_{5}\\) = -12.36 (see row 5, columns 2 and 7)\n\n\n\n\n\n\nQuestion 10\n\n\nDescribe the design of the study, and the analyses that you undertook. Interpret your results in the context of the research question and report your model results in full.\nProvide key model results in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package.\nRemember that you can rename your DV and IV labels by specifying dv.labels and pred.labels.\nMake sure to write your results up following APA guidelines\n\n\n\n\n\n\n\n Solution \n\n\nThe riverview dataset contained information on 32 participants who worked in the hypothetical city of Riverview, US. Using a between-subjects design, the researchers collected information on participants’ sex, income, education, and seniority level.\nTo investigate whether there was an association between income and education level, the following simple linear regression model was used:\n\\[\n\\text{Income}= \\beta_0 + \\beta_1 \\cdot \\text{Education}\n\\]\nFull regression results are displayed in Table 1. The estimated average income associated with no formal years of education was $11,321. Each additional year of formal education was associated with an income increase of $2,651.\n\ntab_model(mdl,\n          dv.labels = \"Income (in thousands of US dollars)\",\n          pred.labels = c(\"education\" = \"Education (years)\"),\n          title = \"Regression Table for Income Model\")\n\n\n\nTable 1: Regression Table for Income Model\n\n\n \nIncome (in thousands of US dollars)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n11.32\n-1.18 – 23.83\n0.074\n\n\nEducation (years)\n2.65\n1.90 – 3.41\n&lt;0.001\n\n\nObservations\n32\n\n\nR2 / R2 adjusted\n0.632 / 0.619"
  },
  {
    "objectID": "1_02_slr.html#residuals",
    "href": "1_02_slr.html#residuals",
    "title": "Intro to Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nThe residuals represent the deviations between the actual responses and the predicted responses and can be obtained either as\n\nmdl$residuals\nresid(mdl)\nresiduals(mdl)\ncomputing them as the difference between the response and the predicted response"
  },
  {
    "objectID": "1_02_slr.html#predicted-values-residuals",
    "href": "1_02_slr.html#predicted-values-residuals",
    "title": "Simple Linear Regression",
    "section": "Predicted Values & Residuals",
    "text": "Predicted Values & Residuals\n\nPredicted Values\nModel predicted values for sample data:\nWe can get out the model predicted values for \\(y\\), the “y hats” (\\(\\hat y\\)), for the data in the sample using various functions:\n\npredict(&lt;fitted model&gt;)\nfitted(&lt;fitted model&gt;)\nfitted.values(&lt;fitted model&gt;)\nmdl$fitted.values\n\nFor example, this will give us the estimated income (point on our regression line) for each observed value of education level.\n\npredict(mdl)\n\n       1        2        3        4        5        6        7        8 \n32.53175 32.53175 37.83435 37.83435 37.83435 43.13694 43.13694 43.13694 \n       9       10       11       12       13       14       15       16 \n43.13694 48.43953 48.43953 48.43953 51.09083 53.74212 53.74212 53.74212 \n      17       18       19       20       21       22       23       24 \n53.74212 53.74212 56.39342 59.04472 59.04472 61.69601 61.69601 64.34731 \n      25       26       27       28       29       30       31       32 \n64.34731 64.34731 64.34731 66.99861 66.99861 69.64990 69.64990 74.95250 \n\n\nModel predicted values for other (unobserved) data:\nTo compute the model-predicted values for unobserved data (i.e., data not contained in the sample), we can use the following function:\n\npredict(&lt;fitted model&gt;, newdata = &lt;dataframe&gt;)\n\nFor this example, we first need to remember that the model predicts income using the independent variable education. Hence, if we want predictions for new (unobserved) data, we first need to create a tibble with a column called education containing the years of education for which we want the prediction, and store this as a dataframe.\n\n#Create dataframe 'newdata' containing education years of 7, 11 and 25\nnewdata &lt;- tibble(education = c(7, 11, 25))\nnewdata\n\n# A tibble: 3 × 1\n  education\n      &lt;dbl&gt;\n1         7\n2        11\n3        25\n\n\nThen we take newdata and add a new column called income_hat, computed as the prediction from the fitted mdl using the newdata above:\n\nnewdata &lt;- newdata %&gt;%\n  mutate(\n    income_hat = predict(mdl, newdata = newdata)\n  )\nnewdata\n\n# A tibble: 3 × 2\n  education income_hat\n      &lt;dbl&gt;      &lt;dbl&gt;\n1         7       29.9\n2        11       40.5\n3        25       77.6\n\n\nResiduals\nThe residuals represent the deviations between the actual responses and the predicted responses and can be obtained either as\n\nmdl$residuals\nresid(mdl)\nresiduals(mdl)\ncomputing them as the difference between the response (\\(y_i\\)) and the predicted response (\\(\\hat y_i\\))\n\n\n\n\nQuestion 9\n\n\nUse predict(mdl) to compute the fitted values and residuals. Mutate the riverview dataframe to include the fitted values and residuals as extra columns.\nAssign to the following symbols the corresponding numerical values:\n\n\n\\(y_{3}\\) (response variable for unit \\(i = 3\\) in the sample data)\n\n\\(\\hat y_{3}\\) (fitted value for the third unit)\n\n\\(\\hat \\epsilon_{5}\\) (the residual corresponding to the 5th unit, i.e., \\(y_{5} - \\hat y_{5}\\))\n\n\n\n\n\n Solution \n\n\n\nriverview_fitted &lt;- riverview %&gt;%\n  mutate(\n    income_hat = predict(mdl),\n    resid = income - income_hat\n  )\n\nhead(riverview_fitted)\n\n# A tibble: 6 × 8\n  education income seniority gender  male party       income_hat  resid\n      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n1         8   37.4         7 male       1 Democrat          32.5   4.92\n2         8   26.4         9 female     0 Independent       32.5  -6.10\n3        10   47.0        14 male       1 Democrat          37.8   9.20\n4        10   34.2        16 female     0 Independent       37.8  -3.65\n5        10   25.5         1 female     0 Republican        37.8 -12.4 \n6        12   46.5        11 female     0 Democrat          43.1   3.35\n\n\n\n\n\\(y_{3}\\) = 47.03 (see row 3, column 2)\n\n\\(\\hat y_{3}\\) = 37.83 (see row 3, column 7)\n\n\\(\\hat \\epsilon_{5} = y_{5} - \\hat y_{5}\\) = -12.36 (see row 5, columns 2 and 7)\n\n\n\n\n\n\nQuestion 10\n\n\nDescribe the design of the study, and the analyses that you undertook. Interpret your results in the context of the research question and report your model results in full.\nProvide key model results in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package.\nRemember that you can rename your DV and IV labels by specifying dv.labels and pred.labels.\nMake sure to write your results up following APA guidelines\n\n\n\n\n\n\n\n Solution \n\n\nBefore we write up our analyses & results, lets first create the table of results:\n\ntab_model(mdl,\n          dv.labels = \"Income (in thousands of US dollars)\",\n          pred.labels = c(\"education\" = \"Education (years)\"),\n          title = \"Regression Table for Income Model\")\n\n\n\nTable 1: Regression Table for Income Model\n\n\n \nIncome (in thousands of US dollars)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n11.32\n-1.18 – 23.83\n0.074\n\n\nEducation (years)\n2.65\n1.90 – 3.41\n&lt;0.001\n\n\nObservations\n32\n\n\nR2 / R2 adjusted\n0.632 / 0.619\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe riverview dataset contained information on 32 participants who worked in the hypothetical city of Riverview, US. Using a between-subjects design, the researchers collected information on participants’ sex, income, education, and seniority level.\nTo investigate whether there was an association between income and education level, the following simple linear regression model was used:\n\\[\n\\text{Income}= \\beta_0 + \\beta_1 \\cdot \\text{Education}\n\\]\nFull regression results are displayed in Table 1. The estimated income associated with no formal years of education was $11,321. Each additional year of formal education was associated with an income increase of $2,651."
  },
  {
    "objectID": "1_02_slr.html#predicted-values",
    "href": "1_02_slr.html#predicted-values",
    "title": "Intro to Linear Regression",
    "section": "Predicted Values",
    "text": "Predicted Values\nWe can get out the model predicted values for \\(y\\), the “y hats” (\\(\\hat y\\)), for the data in the sample using various functions:\n\npredict(&lt;fitted model&gt;)\nfitted(&lt;fitted model&gt;)\nfitted.values(&lt;fitted model&gt;)\nmdl$fitted.values\n\nFor example, this will give us the estimated income (point on our regression line) for each observed value of education level.\n\npredict(mdl)\n\n       1        2        3        4        5        6        7        8 \n32.53175 32.53175 37.83435 37.83435 37.83435 43.13694 43.13694 43.13694 \n       9       10       11       12       13       14       15       16 \n43.13694 48.43953 48.43953 48.43953 51.09083 53.74212 53.74212 53.74212 \n      17       18       19       20       21       22       23       24 \n53.74212 53.74212 56.39342 59.04472 59.04472 61.69601 61.69601 64.34731 \n      25       26       27       28       29       30       31       32 \n64.34731 64.34731 64.34731 66.99861 66.99861 69.64990 69.64990 74.95250 \n\n\nModel-predicted values for other (unobserved) data:\nTo compute the model-predicted values for unobserved data (i.e., data not contained in the sample), we can use the following function:\n\npredict(&lt;fitted model&gt;, newdata = &lt;dataframe&gt;)\n\nFor this example, we first need to remember that the model predicts income using the independent variable education. Hence, if we want predictions for new (unobserved) data, we first need to create a tibble with a column called education containing the years of education for which we want the prediction, and store this as a dataframe.\n\n#Create dataframe 'newdata' containing education years of 11 and 23\nnewdata &lt;- tibble(education = c(11, 23))\nnewdata\n\n# A tibble: 2 × 1\n  education\n      &lt;dbl&gt;\n1        11\n2        23\n\n\nThen we take newdata and add a new column called income_hat, computed as the prediction from the fitted mdl using the newdata above:\n\nnewdata &lt;- newdata %&gt;%\n  mutate(\n    income_hat = predict(mdl, newdata = newdata)\n  )\nnewdata\n\n# A tibble: 2 × 2\n  education income_hat\n      &lt;dbl&gt;      &lt;dbl&gt;\n1        11       40.5\n2        23       72.3"
  },
  {
    "objectID": "1_02_slr.html#y_i-beta_0-beta_1-x_i-epsilon_i",
    "href": "1_02_slr.html#y_i-beta_0-beta_1-x_i-epsilon_i",
    "title": "Simple Linear Regression",
    "section": "\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)",
    "text": "\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\n\n\\(y_i\\) is our measured outcome variable (our DV)\n\n\\(x_i\\) is our measured predictor variable (our IV)\n\n\\(\\beta_0\\) is the model intercept\n\n\\(\\beta_1\\) is the model slope"
  },
  {
    "objectID": "1_02_slr.html#epsilon-sim-n0-sigma-textindependently",
    "href": "1_02_slr.html#epsilon-sim-n0-sigma-textindependently",
    "title": "Simple Linear Regression",
    "section": "\\(\\epsilon \\sim N(0, \\sigma) \\text{independently}\\)",
    "text": "\\(\\epsilon \\sim N(0, \\sigma) \\text{independently}\\)\n\n\n\\(\\epsilon\\) is the residual error\n\n\\(\\sim\\) means ‘distributed according to’\n\n\\(\\sim N(0, \\sigma) \\text{ independently}\\) means ‘normal distribution with a mean of 0 and a variance of \\(\\sigma\\)’\n\nTogether, we can say that the errors around the line have a mean of zero and constant spread as x varies."
  },
  {
    "objectID": "1_03_mlr.html",
    "href": "1_03_mlr.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nExtend the ideas of single linear regression to consider regression models with two or more predictors\nUnderstand and interpret the coefficients in multiple linear regression models\n\n\nBe up to date with lectures\nHave completed Week 1 and Week 2 lab exercises\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\nsjPlot\nkableExtra\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing.csv."
  },
  {
    "objectID": "1_03_mlr.html#descriptive-statistics",
    "href": "1_03_mlr.html#descriptive-statistics",
    "title": "Multiple Linear Regression",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nQuestion 2\n\n\nAlongside descriptive statistics, visualize the marginal distributions of the wellbeing, outdoor_time, and social_int variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot interpretation\n- The shape, center and spread of the distribution - Whether the distribution is symmetric or skewed - Whether the distribution is unimodal or bimodal\nPlotting tips\n- Use \\n to wrap text in your titles and or axis labels - The patchwork package allows us to arrange multiple plots in two ways - | arranges the plots adjacent to one another, and / arranges the plots on top of one another\nTable tips\n- The kableExtra package allows us to produce well formatted tables for our descriptive statistics. To do so, you need to specify the kable() and kable_styling() arguments\n\n\n\n\n\n\n\n Solution \n\n\nYou should be familiar now with how to visualise a marginal distribution. You might choose histograms, density curves, or boxplots, or a combination:\n\nwellbeing_plot &lt;- \n  ggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Score on WEMWBS (range 14-70)\", y = \"Probability\\nDensity\")\n\noutdoortime_plot &lt;- \n  ggplot(data = mwdata, aes(x = outdoor_time)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Probability\\nDensity\")\n\nsocial_plot &lt;- \n  ggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Number of social interactions per week\", y = \"Probability\\nDensity\")\n\n# arrange plots vertically \nwellbeing_plot / outdoortime_plot / social_plot\n\n\n\nFigure 1: Marginal distribution plots of wellbeing sores, weekly hours spent outdoors, and social interactions\n\n\n\nSummary statistics for wellbeing, outdoor time, and social interactions:\n\ndescriptives &lt;- mwdata %&gt;% \n  summarize(\n    M_Wellbeing = mean(wellbeing), \n    SD_Wellbeing = sd(wellbeing),\n    M_OutTime = mean(outdoor_time), \n    SD_OutTime = sd(outdoor_time),\n    M_SocInt = mean(social_int), \n    SD_SocInt = sd(social_int)\n    )\ndescriptives\n\n# A tibble: 1 × 6\n  M_Wellbeing SD_Wellbeing M_OutTime SD_OutTime M_SocInt SD_SocInt\n        &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1        36.3         5.39      18.3       7.10     12.1      4.02\n\n\nWe can make this into a well formatted table using kable():\n\nkable(descriptives, caption = \"Wellbeing, Social Interactions, and Outdoor Time Descriptive Statistics\", digits = 2) %&gt;%\n        kable_styling()\n\n\nWellbeing, Social Interactions, and Outdoor Time Descriptive Statistics\n\nM_Wellbeing\nSD_Wellbeing\nM_OutTime\nSD_OutTime\nM_SocInt\nSD_SocInt\n\n\n36.3\n5.39\n18.25\n7.1\n12.06\n4.02\n\n\n\n\n\n\n\n\n\n\n\nThe marginal distribution of scores on the WEMWBS was unimodal with a mean of approximately 36.3. There was variation in WEMWBS scores (SD = 5.39)\n\nThe marginal distribution of weekly hours spent outdoors was unimodal with a mean of approximately 18.25. There was variation in weekly hours spent outdoors (SD = 7.1)\nThe marginal distribution of numbers of social interactions per week was unimodal with a mean of approximately 12.06. There was variation in numbers of social interactions (SD = 4.02)\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nProduce plots of the associations between the outcome variable (wellbeing) and each of the explanatory variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot interpretation\n- Direction of association - Form of association (can it be summarised well with a straight line?)\n- Strength of association (how closely do points fall to a recognizable pattern such as a line?) - Unusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail\nPlot tips\n- use \\n to wrap text in your titles and or axis labels - consider using geom_smooth() to superimpose the best-fitting line describing the association of interest\n\n\n\n\n\n\n\n Solution \n\n\n\nwellbeing_outdoor &lt;- \n  ggplot(data = mwdata, aes(x = outdoor_time, y = wellbeing)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Time spent outdoors \\nper week (hours)\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_social &lt;- \n  ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Number of social interactions \\nper week\", y = \"Wellbeing score (WEMWBS)\")\n\n# place plots adjacent to one another\nwellbeing_outdoor | wellbeing_social\n\n\n\nFigure 2: Scatterplots displaying the relationships between scores on the WEMWBS and a) weekly outdoor time (hours), and b) weekly number of social interactions\n\n\n\nThe scatterplots we created above show weak, positive, and linear associations both between wellbeing and outdoor time, and between wellbeing and the number of weekly social interactions.\n\n\n\n\n\nQuestion 4\n\n\nProduce a correlation matrix of the variables which are to be used in the analysis, and write a short paragraph describing the associations.\n\n\n\n\n\n\nHint\n\n\n\n\n\nCorrelation Matrix\nReview Q2 of your Week 1 lab for guidance on how to produce a correlation matrix.\nAPA Format\nMake sure to round your numbers in-line with APA 7th edition guidelines. The round() function will come in handy here, as might this APA numbers and statistics guide!\n\n\n\n\n\n\n\n Solution \n\n\nWe can either:\n\n\nIndex dataframe ([])\nVariable selection (select())\n\n\n\n\n# correlation matrix of the three columns of interest (check which columns we need - in this case, 2,3, and 5)\nround(cor(mwdata[,c(5,3,2)]), digits = 2)\n\n             wellbeing social_int outdoor_time\nwellbeing         1.00       0.24         0.25\nsocial_int        0.24       1.00        -0.04\noutdoor_time      0.25      -0.04         1.00\n\n\n\n\n\n# select only the columns we want by variable name, and pass this to cor()\nmwdata %&gt;% \n  select(wellbeing, social_int, outdoor_time) %&gt;%\n  cor() %&gt;%\n    round(digits = 2)\n\n             wellbeing social_int outdoor_time\nwellbeing         1.00       0.24         0.25\nsocial_int        0.24       1.00        -0.04\noutdoor_time      0.25      -0.04         1.00\n\n\n\n\n\n\n\n\n\n\n\n\nThere was a weak, positive, linear association between WEMWBS scores and weekly outdoor time for the participants in the sample (\\(r\\) = .25). Higher number of hours spent outdoors each week was associated, on average, with higher wellbeing scores\n\nThere was a weak, positive, linear association between WEMWBS scores and the weekly number of social interactions for the participants in the sample (\\(r\\) = .24). More social interactions were associated, on average, with higher wellbeing scores\nThere was a negligible negative correlation between weekly outdoor time and the weekly number of social interactions (\\(r\\) = -.04)"
  },
  {
    "objectID": "1_03_mlr.html#model-fitting",
    "href": "1_03_mlr.html#model-fitting",
    "title": "Multiple Linear Regression",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nQuestion 5\n\n\nThe scatterplots we created above show moderate, positive, and linear associations both between outdoor time and wellbeing, and between numbers of social interactions and wellbeing.\n\nRecall the model specified in Q1. What are the parameters of the model? How do we denote parameter estimates?\nFit the linear model in using lm(), assigning the output to an object called mdl1.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAs we did for simple linear regression, we can fit our multiple regression model using the lm() function. We can add as many explanatory variables as we like, separating them with a +.\n\n( &lt;response variable&gt; ~ 1 + &lt;explanatory variable 1&gt; + &lt;explanatory variable 2&gt; + ... , data = &lt;dataframe&gt; )\n\n\n\n\n\n\n\n\n Solution \n\n\nA model for the relationship between \\(x_1\\) = weekly outdoor time, \\(x_2\\) = weekly numbers of social interactions and \\(y\\) = scores on the WEMWBS is given by: \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\\\ \\quad \\\\\n\\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nIn the model specified above,\n\n\n\\(\\mu_{y|x_1, x_2} = \\beta_0 + \\beta_1 x + \\beta_2 x_2\\) represents the systematic part of the model giving the mean of \\(y\\) at each combination of values of \\(x_1\\) and \\(x_2\\);\n\n\\(\\epsilon\\) represents the error (deviation) from that mean, and the errors are independent from one another.\n\nThe parameters of our model are:\n\n\n\\(\\beta_0\\) (The intercept);\n\n\\(\\beta_1\\) (The slope across values of \\(x_1\\));\n\n\\(\\beta_2\\) (The slope across values of \\(x_2\\));\n\n\n\\(\\sigma\\) (The standard deviation of the errors).\n\nWhen we estimate these parameters from the available data, we have a fitted model (recall that the h\\(\\hat{\\textrm{a}}\\)ts are used to distinguish our estimates from the true unknown parameters): \\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Social Interactions + \\hat \\beta_2 \\cdot Outdoor Time\n\\] And we have residuals \\(\\hat \\epsilon = y - \\hat y\\) which are the deviations from the observed values and our model-predicted responses.\nFitting the model in R:\n\nmdl1 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\n\n\n\n\n\n\nVisual\nNote that for simple linear regression we talked about our model as a line in 2 dimensions: the systematic part \\(\\beta_0 + \\beta_1 x\\) defined a line for \\(\\mu_y\\) across the possible values of \\(x\\), with \\(\\epsilon\\) as the random deviations from that line. But in multiple regression we have more than two variables making up our model.\nIn this particular case of three variables (one outcome + two explanatory), we can think of our model as a regression surface (see Figure 3). The systematic part of our model defines the surface across a range of possible values of both \\(x_1\\) and \\(x_2\\). Deviations from the surface are determined by the random error component, \\(\\hat \\epsilon\\).\n\n\n\n\nFigure 3: Regression surface for wellbeing ~ social_int + outdoor_time, from two different angles\n\n\n\nDon’t worry about trying to figure out how to visualise it if we had any more explanatory variables! We can only concieve of 3 spatial dimensions. One could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it’s not worth trying!.\n\n\n\nQuestion 6\n\n\nUsing any of:\n\nmdl1\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\nWrite out the estimated parameter values of:\n\n\n\\(\\hat \\beta_0\\), the estimated average wellbeing score associated with zero hours of outdoor time and zero social interactions per week.\n\n\n\\(\\hat \\beta_1\\), the estimated increase in average wellbeing score associated with an additional social interaction per week (an increase of one), holding weekly outdoor time constant.\n\n\n\\(\\hat \\beta_2\\), the estimated increase in average wellbeing score associated with one hour increase in weekly outdoor time, holding the number of social interactions constant\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ: What do we mean by hold constant / controlling for / partialling out / residualizing for?\nA: When the remaining explanatory variables are held at the same value or are fixed.\n\n\n\n\n\n\n Solution \n\n\n\n\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\n\n\n\nmdl1$coefficients\n\n (Intercept)   social_int outdoor_time \n   5.3703775    1.8034489    0.5923673 \n\n\n\n\n\ncoef(mdl1)\n\n (Intercept)   social_int outdoor_time \n   5.3703775    1.8034489    0.5923673 \n\n\n\n\n\ncoefficients(mdl1)\n\n (Intercept)   social_int outdoor_time \n   5.3703775    1.8034489    0.5923673 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\n\n\n\n\n\n\\(\\hat \\beta_0\\) = 5.37\n\n\n\\(\\hat \\beta_1\\) = 1.8\n\n\n\\(\\hat \\beta_2\\) = 0.59\n\n\n\n\n\n\nQuestion 7\n\n\nWithin what distance from the model predicted values (the regression line) would we expect 95% of WEMWBS wellbeing scores to be?\n\n\n\n\n\n\nHint\n\n\n\n\n\nEither sigma() or part of the output from summary() will help you here.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nsigma(mdl1)\nsummary(mdl1)\n\n\n\n\nsigma(mdl1)\n\n[1] 6.148276\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma\\) = 6.15. We would expect 95% of wellbeing scores to be within about 12.3 (\\(2 \\hat \\sigma\\)) from the model fit.\n\n\n\n\n\nQuestion 8\n\n\nBased on the model, predict the wellbeing scores for the following individuals:\n\nLeah: Social Interactions = 24; Outdoor Time = 3\nSean: Social Interactions = 19; Outdoor Time = 26\nMike: Social Interactions = 15; Outdoor Time = 20\nDonna: Social Interactions = 7; Outdoor Time = 2\n\nWho has the highest predicted wellbeing score, and who has the lowest?\n\n\n\n\n Solution \n\n\nFirst we need to pass the data into R:\n\nwellbeing_query &lt;- tibble(social_int = c(24, 19, 15, 7),\n                          outdoor_time = c(3, 26, 20, 2))\n\nAnd next use predict() to get their estimated wellbeing scores:\n\npredict(mdl1, newdata = wellbeing_query)\n\n       1        2        3        4 \n50.43025 55.03746 44.26946 19.17925 \n\n\nSean has the highest predicted wellbeing score (55.04), and Donna the lowest (19.18)."
  },
  {
    "objectID": "1_03_mlr.html#presentation-results",
    "href": "1_03_mlr.html#presentation-results",
    "title": "Multiple Linear Regression",
    "section": "Presentation Results",
    "text": "Presentation Results\n\nQuestion 9\n\n\nProvide key model results in a formatted table.\n\n\n\n\n Solution \n\n\n\n#create table for results\ntab_model(mdl1,\n          dv.labels = \"Wellbeing (WEMWBS Scores)\",\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\",\n                          \"outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression Table for Wellbeing Model\")\n\n\n\nTable 1: Regression Table for Wellbeing Model\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n5.37\n-3.47 – 14.21\n0.224\n\n\nSocial Interactions\n(number per week)\n1.80\n1.25 – 2.35\n&lt;0.001\n\n\nOutdoor Time (hours per\nweek)\n0.59\n0.25 – 0.94\n0.001\n\n\nObservations\n32\n\n\nR2 / R2 adjusted\n0.740 / 0.722\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret your results in the context of the research question.\nMake reference to the your regression table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nMake sure to include a decision in relation to your null hypothesis - based on the evidence, should you reject or fail to reject the null?\n\n\n\n\n\n\n\n Solution \n\n\nA multiple regression model was used to determine if there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. As presented in Table 1, outdoor time was significantly associated with wellbeing scores (\\(\\beta\\) = 0.59, SE = 0.17, \\(p\\) &lt; .001) after controlling for the number of weekly social interactions. Results suggested that for every additional hour spent outdoors each week, wellbeing scores increased by 0.59 points. Therefore, we should reject the null hypothesis since \\(p\\) &lt; .05."
  },
  {
    "objectID": "1_03_mlr.html#model-fitting-interpretation",
    "href": "1_03_mlr.html#model-fitting-interpretation",
    "title": "Multiple Linear Regression",
    "section": "Model Fitting & Interpretation",
    "text": "Model Fitting & Interpretation\n\nQuestion 5\n\n\nRecall the model specified in Q1, and:\n\nState the parameters of the model. How do we denote parameter estimates?\n\nFit the linear model in using lm(), assigning the output to an object called mdl1.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAs we did for simple linear regression, we can fit our multiple regression model using the lm() function. We can add as many explanatory variables as we like, separating them with a +.\n\nmodel_name &lt;- lm(&lt;response variable&gt; ~ 1 + &lt;explanatory variable 1&gt; + &lt;explanatory variable 2&gt; + ... , data = &lt;dataframe&gt;)\n\n\n\n\n\n\n\n\n Solution \n\n\nA model for the association between \\(x_1\\) = weekly numbers of social interactions, \\(x_2\\) = weekly outdoor time, and \\(y\\) = scores on the WEMWBS can be given by: \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\\\ \\quad \\\\\n\\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nIn the model specified above,\n\n\n\\(\\mu_{y|x_1, x_2} = \\beta_0 + \\beta_1 x + \\beta_2 x_2\\) represents the systematic part of the model giving the mean of \\(y\\) at each combination of values of \\(x_1\\) and \\(x_2\\);\n\n\\(\\epsilon\\) represents the error (deviation) from that mean, and the errors are independent from one another.\n\nThe parameters of our model are:\n\n\n\\(\\beta_0\\) (The intercept);\n\n\\(\\beta_1\\) (The slope across values of \\(x_1\\));\n\n\\(\\beta_2\\) (The slope across values of \\(x_2\\));\n\n\n\\(\\sigma\\) (The standard deviation of the errors).\n\nWhen we estimate these parameters from the available data, we have a fitted model (recall that the h\\(\\hat{\\textrm{a}}\\)ts are used to distinguish our estimates from the true unknown parameters): \\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Social Interactions + \\hat \\beta_2 \\cdot Outdoor Time\n\\] And we have residuals \\(\\hat \\epsilon = y - \\hat y\\) which are the deviations from the observed values and our model-predicted responses.\nFitting the model in R:\n\nmdl1 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\n\n\n\n\n\n\nVisual\nNote that for simple linear regression we talked about our model as a line in 2 dimensions: the systematic part \\(\\beta_0 + \\beta_1 x\\) defined a line for \\(\\mu_y\\) across the possible values of \\(x\\), with \\(\\epsilon\\) as the random deviations from that line. But in multiple regression we have more than two variables making up our model.\nIn this particular case of three variables (one outcome + two explanatory), we can think of our model as a regression surface (see Figure 3). The systematic part of our model defines the surface across a range of possible values of both \\(x_1\\) and \\(x_2\\). Deviations from the surface are determined by the random error component, \\(\\hat \\epsilon\\).\n\n\n\n\nFigure 3: Regression surface for wellbeing ~ social_int + outdoor_time, from two different angles\n\n\n\nDon’t worry about trying to figure out how to visualise it if we had any more explanatory variables! We can only concieve of 3 spatial dimensions. One could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it’s not worth trying!.\n\n\n\nQuestion 6\n\n\nUsing any of:\n\nmdl1\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\nWrite out the estimated parameter values of:\n\n\n\\(\\hat \\beta_0\\), the estimated average wellbeing score associated with zero hours of outdoor time and zero social interactions per week.\n\n\n\\(\\hat \\beta_1\\), the estimated increase in average wellbeing score associated with an additional social interaction per week (an increase of one), holding weekly outdoor time constant.\n\n\n\\(\\hat \\beta_2\\), the estimated increase in average wellbeing score associated with one hour increase in weekly outdoor time, holding the number of social interactions constant\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ: What do we mean by hold constant / controlling for / partialling out / residualizing for?\nA: When the remaining explanatory variables are held at the same value or are fixed.\n\n\n\n\n\n\n Solution \n\n\n\n\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\n\n\n\nmdl1$coefficients\n\n (Intercept)   social_int outdoor_time \n  28.6201808    0.3348821    0.1990943 \n\n\n\n\n\ncoef(mdl1)\n\n (Intercept)   social_int outdoor_time \n  28.6201808    0.3348821    0.1990943 \n\n\n\n\n\ncoefficients(mdl1)\n\n (Intercept)   social_int outdoor_time \n  28.6201808    0.3348821    0.1990943 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\n\n\n\\(\\hat \\beta_0\\) = 28.62\n\n\n\\(\\hat \\beta_1\\) = 0.33\n\n\n\\(\\hat \\beta_2\\) = 0.2\n\n\n\n\n\n\nQuestion 7\n\n\nWithin what distance from the model predicted values (the regression line) would we expect 95% of WEMWBS wellbeing scores to be?\n\n\n\n\n\n\nHint\n\n\n\n\n\nEither sigma() or part of the output from summary() will help you here.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nsigma(mdl1)\nsummary(mdl1)\n\n\n\n\nsigma(mdl1)\n\n[1] 5.065003\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma\\) = 5.07. We would expect 95% of wellbeing scores to be within about 10.13 (\\(2 \\hat \\sigma\\)) from the model fit.\n\n\n\n\n\nQuestion 8\n\n\nBased on the model, predict the wellbeing scores for the following individuals:\n\nLeah: Social Interactions = 25; Outdoor Time = 3\nSean: Social Interactions = 19; Outdoor Time = 36\nMike: Social Interactions = 15; Outdoor Time = 20\nDonna: Social Interactions = 7; Outdoor Time = 1\n\nWho has the highest predicted wellbeing score, and who has the lowest?\n\n\n\n\n Solution \n\n\nFirst we need to pass the data into R:\n\nwellbeing_query &lt;- tibble(social_int = c(25, 19, 15, 7),\n                          outdoor_time = c(3, 36, 20, 1))\n\nAnd next use predict() to get their estimated wellbeing scores:\n\npredict(mdl1, newdata = wellbeing_query)\n\n       1        2        3        4 \n37.58952 42.15034 37.62530 31.16345 \n\n\nSean has the highest predicted wellbeing score (42.15), and Donna the lowest (31.16)."
  },
  {
    "objectID": "1_03_mlr.html#writing-up-presenting-results",
    "href": "1_03_mlr.html#writing-up-presenting-results",
    "title": "Multiple Linear Regression",
    "section": "Writing Up & Presenting Results",
    "text": "Writing Up & Presenting Results\n\nQuestion 9\n\n\nProvide key model results in a formatted table.\n\n\n\n\n Solution \n\n\n\ntab_model(mdl1,\n          dv.labels = \"Wellbeing (WEMWBS Scores)\",\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\",\n                          \"outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression Table for Wellbeing Model\")\n\n\n\nTable 1: Regression Table for Wellbeing Model\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n28.62\n25.69 – 31.55\n&lt;0.001\n\n\nSocial Interactions\n(number per week)\n0.33\n0.16 – 0.51\n&lt;0.001\n\n\nOutdoor Time (hours per\nweek)\n0.20\n0.10 – 0.30\n&lt;0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.126 / 0.118\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret your results in the context of the research question.\nMake reference to the your regression table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nMake sure to include a decision in relation to your null hypothesis - based on the evidence, should you reject or fail to reject the null?\n\n\n\n\n\n\n\n Solution \n\n\nA multiple regression model was used to determine if there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. As presented in Table 1, outdoor time was significantly associated with wellbeing scores \\((\\beta = 0.20, SE = 0.05, p &lt; .001)\\) after controlling for the number of weekly social interactions. Results suggested that for every additional hour spent outdoors each week, wellbeing scores increased by 0.20 points. Therefore, we should reject the null hypothesis since \\(p &lt; .05\\)."
  },
  {
    "objectID": "1_02_slr_new.html",
    "href": "1_02_slr_new.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to specify a simple linear model.\nUnderstand what fitted values and residuals are.\nBe able to interpret the coefficients of a fitted model.\n\n\nBe up to date with lectures from Weeks 1 & 2\nHave completed Week 1 lab exercises\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/riverview.csv."
  },
  {
    "objectID": "1_02_slr_new.html#data-exploration",
    "href": "1_02_slr_new.html#data-exploration",
    "title": "Simple Linear Regression",
    "section": "Data Exploration",
    "text": "Data Exploration\nThe common first port of call for almost any statistical analysis is to explore the data, and we can do this visually and/or numerically.\n\n\n\n\n\n\n\n\n\n================ Description\n\nMarginal Distributions | Bivariate Associations | =================================================================================================================================================================+==============================================================================================================================================================================+ The distribution of each variable without reference to the values of the other variables | Describing the relationship between two numeric variables |\n\n\n\n\nVisually \n\n\nPlot each variable individually.  You could use, for example, geom_density() for a density plot or geom_histogram() for a histogram to comment on and/or examine: \n\nThe shape of the distribution. Look at the shape, centre and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\n\n\nIdentify any unusual observations. Do you notice any extreme observations (i.e., outliers)?\n\n\n\nPlot associations among two variables. |  |  | You could use, for example, geom_point() for a scatterplot to comment on and/or examine:  |  |\n\nThe direction of the association indicates whether there is a positive or negative association\n\n\nThe form of association refers to whether the relationship between the variables can be summarized well with a straight line or some more complicated pattern\n\n\nThe strength of association entails how closely the points fall to a recognizable pattern such as a line\n\n\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail\n\n\n\n\n\nNumerically \n\nCompute and report summary statistics e.g., mean, standard deviation, median, min, max, etc.  You could, for example, calculate summary statistics such as the mean (mean()) and standard deviation (sd()), etc. within summarize()\n\nCompute and report the correlation coefficient.  You can use the cor() function to calculate this\n\n\n\n\nMarginal Distributions\n\nQuestion 1\n\n\nVisualise and describe the marginal distributions of wellbeing scores and social interactions.\n\n\n\n\n Solution \n\n\n\n\nWellbeing (WEMWBS) Scores\nSocial Interactions\n\n\n\nVisualisation of distribution:\n\nggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  labs(x = \"Wellbeing (WEMWBS) Scores\", \n       y = \"Probability density\")\n\n\n\nFigure 1: Distribution of Wellbeing (WEMWBS) Scores\n\n\n\nInitial observations from plot:\n\nThe distribution of wellbeing scores was unimodal\nMost of the wellbeing scores were between roughly 30 and 45\nThe lowest wellbeing in the sample was approximately 22 and the highest approximately 59. This suggested there was a fair high degree of variation in the data\nScores were within the range of possible values\n\nDescriptive (or summary) statistics for wellbeing scores:\n\ndesc_wellbeing &lt;- mwdata %&gt;% \n  summarize(\n    M = mean(wellbeing), \n    SD = sd(wellbeing)\n    )\ndesc_wellbeing\n\n# A tibble: 1 × 2\n      M    SD\n  &lt;dbl&gt; &lt;dbl&gt;\n1  36.3  5.39\n\n\nFollowing the exploration above, we can describe the wellbeing variable as follows:\n\n\n\n\n\n\nThe marginal distribution of scores on the WEMWBS was unimodal with a mean of approximately 36.3. There was variation in WEMWBS scores (SD = 5.39)\n\n\n\n\n\nVisualisation of distribution:\n\nggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  labs(x = \"Social Interactions (Number per Week)\", \n       y = \"Probability density\")\n\n\n\nFigure 2: Distribution of Number of Social Interactions\n\n\n\nInitial observations from plot:\n\nThe distribution of social interactions was unimodal\nMost of the participants had between 8 and 15 social interactions per week\nThe fewest social interactions per week was approximately 3, and the highest approximately 24. This suggested there was a fair high degree of variation in the data\n\nDescriptive (or summary) statistics for the number of weekly social interactions per week:\n\ndesc_social &lt;- mwdata %&gt;% \n  summarize(\n    M = mean(social_int), \n    SD = sd(social_int)\n    )\ndesc_social\n\n# A tibble: 1 × 2\n      M    SD\n  &lt;dbl&gt; &lt;dbl&gt;\n1  12.1  4.02\n\n\nFollowing the exploration above, we can describe the social interactions variable as follows:\n\n\n\n\n\n\nThe marginal distribution of numbers of social interactions per week was unimodal with a mean of approximately 12.06. There was variation in numbers of social interactions (SD = 4.02)\n\n\n\n\n\n\n\n\n\nAssociations among Variables\n\nQuestion 2\n\n\nCreate a scatterplot of wellbeing score and social interactions before calculating the correlation between them.\n\nCorrelation Matrix\nA table showing the correlation coefficients - \\(r_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\\) - between variables. Each cell in the table shows the association between two variables. The diagonals show the correlation of a variable with itself (and are therefore always equal to 1).\n\nIn R, we can create a correlation matrix by giving the cor() function a dataframe. However, we only want to give it 2 columns here. Think about how we select specific columns, either giving the column numbers inside [], or using select().\n\n\nMaking reference to both the plot and correlation coefficient, describe the association between wellbeing and social interactions among participants in the Edinburgh & Lothians sample.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot\nWe are trying to investigate how wellbeing varies by varying numbers of weekly social interactions. Hence, wellbeing is the dependent variable (on the y-axis), and social interactions is the independent variable (on the x-axis).\nCorrelation\nMake sure to round your numbers in-line with APA 7th edition guidelines. The round() function will come in handy here, as might this APA numbers and statistics guide!\n\n\n\n\n\n\n\n Solution \n\n\nLet’s produce a scatterplot:\n\nggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  labs(x = \"Social Interactions (Number per Week)\", \n       y = \"Wellbeing (WEMWBS) Scores\")\n\n\n\nFigure 3: Association between Wellbeing and Social Interactions\n\n\n\nTo comment on the strength of the linear association we compute the correlation coefficient in either of the following ways:\n\n\nIndexing ([])\nSelection (select())\n\n\n\n\n# correlation matrix of the two columns of interest - 3 & 5\nround(cor(mwdata[,c(3,5)]), digits = 2)\n\n           social_int wellbeing\nsocial_int       1.00      0.24\nwellbeing        0.24      1.00\n\n\n\n\n\n# select only the columns we want by name, and pass this to cor()\nmwdata %&gt;% \n  select(social_int, wellbeing) %&gt;%\n  cor() %&gt;%\n    round(digits = 2)\n\n           social_int wellbeing\nsocial_int       1.00      0.24\nwellbeing        0.24      1.00\n\n\n\n\n\nAnd we can see that via either method, the correlation is \\[\nr_{\\text({Social~Interactions,~~ Wellbeing})} = .24\n\\] \n\n\n\n\n\n\nThere was a weak, positive, linear association between the weekly number of social interactions and WEMWBS scores for the participants in the sample (\\(r\\) = .24). More social interactions were associated, on average, with higher wellbeing scores."
  },
  {
    "objectID": "1_02_slr_new.html#model-specification-and-fitting",
    "href": "1_02_slr_new.html#model-specification-and-fitting",
    "title": "Simple Linear Regression",
    "section": "Model Specification and Fitting",
    "text": "Model Specification and Fitting\nThe scatterplot highlighted a linear relationship, where the data points were scattered around an underlying linear pattern with a roughly-constant spread as x varied.\nHence, we will try to fit a simple (i.e., one x variable only) linear regression model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\\\\n\\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\n\\]\n\n\n\n\n\n\nWhat does \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\\) mean?\n\n\n\n\n\nLets break the statement down into smaller parts:\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\n\n\\(y_i\\) is our measured outcome variable (our DV)\n\n\\(x_i\\) is our measured predictor variable (our IV)\n\n\\(\\beta_0\\) is the model intercept\n\n\\(\\beta_1\\) is the model slope\n\\(\\epsilon \\sim N(0, \\sigma) \\text{ independently}\\)\n\n\n\\(\\epsilon\\) is the residual error\n\n\\(\\sim\\) means ‘distributed according to’\n\n\\(\\sim N(0, \\sigma) \\text{ independently}\\) means ‘normal distribution with a mean of 0 and a variance of \\(\\sigma\\)’\nTogether, we can say that the errors around the line have a mean of zero and constant spread as x varies.\n\n\n\n\n\n\nQuestion 3\n\n\nFirst, write the equation of the fitted line.\nNext, using the lm() function, fit a simple linear model to predict wellbeing (DV) by social interactions (IV), naming the output mdl.\nLastly, update your equation of the fitted line to include the estimated coefficients.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe syntax of the lm() function is:\n\n[model name] &lt;- lm([response variable i.e., dependent variable] ~ [explanatory variable i.e., independent variable], data = [dataframe])\n\n\n\n\n\n\n\n\n Solution \n\n\nFirst, lets specify the fitted model, which can be written either as:\n\n\nOption A\nOption B\n\n\n\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Social~Interactions\n\\]\n\n\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 \\cdot 1 + \\hat \\beta_1 \\cdot Social~Interactions\n\\]\n\n\n\nTo fit the model in R, as the variables are in the mwdata dataframe, we would write:\n\n\nOption A\nOption B\n\n\n\n\nmdl &lt;- lm(wellbeing ~ social_int, data = mwdata)\nmdl\n\n\nCall:\nlm(formula = wellbeing ~ social_int, data = mwdata)\n\nCoefficients:\n(Intercept)   social_int  \n    32.4077       0.3222  \n\n\n\n\n\nmdl &lt;- lm(wellbeing ~ 1 + social_int, data = mwdata)\nmdl\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nCoefficients:\n(Intercept)   social_int  \n    32.4077       0.3222  \n\n\n\n\n\n\n\n\n\n\n\nWhy is there a 1 in the Option B’s?\n\n\n\n\n\nWhen we specify the linear model in R, we include after the tilde sign (\\(\\sim\\)), the variables that appear to the right of the \\(\\hat \\beta\\)s. The intercept, or \\(\\beta_0\\), is a constant. That is, we could write it as multiplied by 1.\nIncluding the 1 explicitly is not necessary because it is included by default (you can check this by comparing the outputs of A & B above with and without the 1 included - the estimates are the same!). After a while, you will find you just want to drop the 1 (i.e., Option B) when calling lm() because you know that it’s going to be there, but in these early weeks we tried to keep it explicit to make it clear that you want to the intercept to be estimated.\n\n\n\nNote that by calling the name of the fitted model, mdl, you can see the estimated regression coefficients \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). We can add these values to the fitted line:\n\\[\n\\widehat{Wellbeing} = 32.41 + 0.32 \\cdot Social~Interactions \\\\\n\\]\n\n\n\n\n\nQuestion 4\n\n\nExplore the following equivalent ways to obtain the estimated regression coefficients — that is, \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) — from the fitted model:\n\nmdl\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\n\n\n Solution \n\n\nThe estimated parameters returned by the below methods are all equivalent. However, summary() returns more information.\n\n\nmdl()\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\nSimply invoke the name of the fitted model:\n\nmdl\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nCoefficients:\n(Intercept)   social_int  \n    32.4077       0.3222  \n\n\n\n\n\nmdl$coefficients\n\n(Intercept)  social_int \n 32.4077070   0.3221959 \n\n\n\n\n\ncoef(mdl)\n\n(Intercept)  social_int \n 32.4077070   0.3221959 \n\n\n\n\n\ncoefficients(mdl)\n\n(Intercept)  social_int \n 32.4077070   0.3221959 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5628  -3.2741  -0.7908   3.3703  20.4706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 32.40771    1.17532  27.573  &lt; 2e-16 ***\nsocial_int   0.32220    0.09243   3.486 0.000605 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.247 on 198 degrees of freedom\nMultiple R-squared:  0.05781,   Adjusted R-squared:  0.05306 \nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n\n\n\n\n\n\n\n\n\n\n\nThe estimated intercept is \\(\\hat \\beta_0 = 32.41\\) and the estimated slope is \\(\\hat \\beta_1 = 0.32\\).\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nExplore the following equivalent ways to obtain the estimated standard deviation of the errors — that is, \\(\\hat \\sigma\\) — from the fitted model mdl:\n\nsigma(mdl)\nsummary(mdl)\n\n\n Huh? What is \\(\\sigma\\)?\n\n\nThe standard deviation of the errors, denoted by \\(\\sigma\\), is an important quantity that our model estimates. It represents how much individual data points tend to deviate above and below the regression line.\nA small \\(\\sigma\\) indicates that the points hug the line closely and we should expect fairly accurate predictions, while a large \\(\\sigma\\) suggests that, even if we estimate the line perfectly, we can expect individual values to deviate from it by substantial amounts.\nThe estimated standard deviation of the errors is denoted \\(\\hat \\sigma\\), and is estimated by essentially averaging squared residuals (giving the variance) and taking the square-root:\n\\[\n\\begin{align}\n& \\hat \\sigma = \\sqrt{\\frac{SS_{Residual}}{n - k - 1}} \\\\\n\\qquad \\\\\n& \\text{where} \\\\\n& SS_{Residual} = \\textrm{Sum of Squared Residuals} = \\sum_{i=1}^n{(\\epsilon_i)^2}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\nThe estimated standard deviation of the errors can be equivalently obtained by the below methods. However, summary() returns more information.\n\n\nsigma(mdl)\nsummary(mdl)\n\n\n\n\nsigma(mdl)\n\n[1] 5.246982\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5628  -3.2741  -0.7908   3.3703  20.4706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 32.40771    1.17532  27.573  &lt; 2e-16 ***\nsocial_int   0.32220    0.09243   3.486 0.000605 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.247 on 198 degrees of freedom\nMultiple R-squared:  0.05781,   Adjusted R-squared:  0.05306 \nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term “Residual standard error” is a misnomer, as the help page for sigma says (check ?sigma). However, it’s hard to get rid of this bad name as it has been used in too many books showing R output.\n\n\n\n\n\n\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma = 5.25\\).\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nInterpret the estimated intercept, slope, and standard deviation of the errors in the context of the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo interpret the estimated standard deviation of the errors we can use the fact that about 95% of values from a normal distribution fall within two standard deviations of the center.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nIntercept\nSlope\nStandard deviation of the errors\n\n\n\nThe estimated wellbeing score associated to zero weekly social interactions is 32.41.\n\n\nThe estimated increase in wellbeing associated to one additional weekly social interaction is 0.32.\n\n\nFor any particular numnber of weekly social interactions, participants’ wellbeing scores should be distributed above and below the regression line with standard deviation estimated to be \\(\\hat \\sigma = 5.25\\). Since \\(2 \\hat \\sigma = 10.49\\), we expect most (about 95%) of the participants’ wellbeing scores to be within about 11 points from the regression line.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nPlot the data and the fitted regression line. To do so:\n\nExtract the estimated regression coefficients e.g., via betas &lt;- coef(mdl)\n\nExtract the first entry of betas (i.e., the intercept) via betas[1]\n\nExtract the second entry of betas (i.e., the slope) via betas[2]\n\nProvide the intercept and slope to the function\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nExtracting values\nThe function coef(mdl) returns a vector (a sequence of numbers all of the same type). To get the first element of the sequence you append [1], and [2] for the second.\nPlotting\nIn your ggplot(), you will need to specify geom_abline(). This might help get you started:\n\ngeom_abline(intercept = &lt;intercept&gt;, slope = &lt;slope&gt;)\n\n\n\n\n\n\n\n\n Solution \n\n\nFirst extract the values required:\n\nbetas &lt;- coef(mdl)\nintercept &lt;- betas[1]\nslope &lt;- betas[2]\n\nWe can plot the model as follows:\n\nggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_abline(intercept = intercept, slope = slope, color = 'blue') + \n  labs(x = \"Social Interactions (Number per Week)\", y = \"Wellbeing (WEMWBS) Scores\")"
  },
  {
    "objectID": "1_02_slr_new.html#predicted-values-residuals",
    "href": "1_02_slr_new.html#predicted-values-residuals",
    "title": "Simple Linear Regression",
    "section": "Predicted Values & Residuals",
    "text": "Predicted Values & Residuals\n\nPredicted Values\nModel predicted values for sample data:\nWe can get out the model predicted values for \\(y\\), the “y hats” (\\(\\hat y\\)), for the data in the sample using various functions:\n\npredict(&lt;fitted model&gt;)\nfitted(&lt;fitted model&gt;)\nfitted.values(&lt;fitted model&gt;)\nmdl$fitted.values\n\nFor example, this will give us the estimated wellbeing score (point on our regression line) for each observed value of social interactions for each of our 200 participants.\n\npredict(mdl)\n\n       1        2        3        4        5        6        7        8 \n36.59625 37.24064 35.95186 37.24064 38.20723 36.59625 38.52943 36.27406 \n       9       10       11       12       13       14       15       16 \n36.59625 37.24064 34.66308 35.62967 34.66308 36.27406 36.91845 37.88504 \n      17       18       19       20       21       22       23       24 \n37.56284 37.56284 35.62967 35.30747 35.62967 38.20723 36.27406 35.30747 \n      25       26       27       28       29       30       31       32 \n38.52943 38.52943 37.56284 36.27406 37.56284 35.30747 37.56284 36.91845 \n      33       34       35       36       37       38       39       40 \n37.56284 34.34088 37.56284 36.27406 36.27406 36.91845 38.85162 35.30747 \n      41       42       43       44       45       46       47       48 \n38.20723 36.59625 37.56284 36.27406 36.27406 35.95186 34.34088 38.85162 \n      49       50       51       52       53       54       55       56 \n35.62967 37.24064 35.62967 34.98527 37.56284 37.24064 34.01869 35.95186 \n      57       58       59       60       61       62       63       64 \n34.66308 37.88504 36.91845 34.66308 37.88504 35.95186 36.27406 35.95186 \n      65       66       67       68       69       70       71       72 \n36.27406 36.91845 38.20723 34.01869 35.30747 35.30747 34.66308 39.17382 \n      73       74       75       76       77       78       79       80 \n34.01869 35.30747 33.69649 38.52943 35.62967 37.56284 39.17382 36.91845 \n      81       82       83       84       85       86       87       88 \n34.98527 35.95186 36.59625 36.27406 34.66308 35.95186 37.24064 37.88504 \n      89       90       91       92       93       94       95       96 \n35.62967 37.56284 34.66308 34.66308 37.24064 35.95186 34.98527 35.62967 \n      97       98       99      100      101      102      103      104 \n35.95186 34.66308 37.24064 35.95186 34.34088 34.66308 37.56284 34.98527 \n     105      106      107      108      109      110      111      112 \n36.27406 37.88504 39.17382 37.24064 38.52943 35.30747 35.62967 35.95186 \n     113      114      115      116      117      118      119      120 \n37.24064 36.27406 36.27406 36.59625 36.27406 35.95186 36.59625 35.95186 \n     121      122      123      124      125      126      127      128 \n37.56284 36.91845 34.34088 34.66308 34.98527 35.95186 34.01869 35.62967 \n     129      130      131      132      133      134      135      136 \n34.98527 35.62967 33.69649 38.20723 38.20723 35.30747 34.66308 37.56284 \n     137      138      139      140      141      142      143      144 \n36.91845 35.62967 36.91845 38.52943 36.91845 35.30747 35.62967 37.88504 \n     145      146      147      148      149      150      151      152 \n36.27406 34.98527 35.30747 36.91845 36.59625 36.59625 35.95186 34.34088 \n     153      154      155      156      157      158      159      160 \n36.59625 36.59625 34.66308 36.91845 36.27406 35.30747 33.69649 35.62967 \n     161      162      163      164      165      166      167      168 \n36.27406 37.24064 35.95186 36.59625 33.37429 36.59625 38.20723 36.27406 \n     169      170      171      172      173      174      175      176 \n38.85162 34.66308 37.24064 35.62967 38.20723 36.27406 36.59625 40.14041 \n     177      178      179      180      181      182      183      184 \n35.30747 34.98527 34.34088 35.95186 36.59625 34.98527 34.98527 35.62967 \n     185      186      187      188      189      190      191      192 \n34.66308 34.98527 36.27406 37.88504 36.27406 35.95186 35.95186 36.59625 \n     193      194      195      196      197      198      199      200 \n37.56284 36.27406 36.27406 35.95186 34.66308 35.62967 36.27406 37.24064 \n\n\nModel predicted values for other (unobserved) data:\nTo compute the model-predicted values for unobserved data (i.e., data not contained in the sample), we can use the following function:\n\npredict(&lt;fitted model&gt;, newdata = &lt;dataframe&gt;)\n\nFor this example, we first need to remember that the model predicts wellbeing using the independent variable social_int. Hence, if we want predictions for new (unobserved) data, we first need to create a tibble with a column called social_int containing the number of weekly social interactions for which we want the prediction, and store this as a dataframe.\n\n#Create dataframe 'newdata' containing 2, 25, and 28 weekly social interactions\nnewdata &lt;- tibble(social_int = c(2, 25, 28))\nnewdata\n\n# A tibble: 3 × 1\n  social_int\n       &lt;dbl&gt;\n1          2\n2         25\n3         28\n\n\nThen we take newdata and add a new column called wellbeing_hat, computed as the prediction from the fitted mdl using the newdata above:\n\nnewdata &lt;- newdata %&gt;%\n  mutate(\n    wellbeing_hat = predict(mdl, newdata = newdata)\n  )\nnewdata\n\n# A tibble: 3 × 2\n  social_int wellbeing_hat\n       &lt;dbl&gt;         &lt;dbl&gt;\n1          2          33.1\n2         25          40.5\n3         28          41.4\n\n\nResiduals\nThe residuals represent the deviations between the actual responses and the predicted responses and can be obtained either as\n\nmdl$residuals\nresid(mdl)\nresiduals(mdl)\ncomputing them as the difference between the response (\\(y_i\\)) and the predicted response (\\(\\hat y_i\\))\n\n\n\n\nQuestion 8\n\n\nUse predict(mdl) to compute the fitted values and residuals. Mutate the mwdata dataframe to include the fitted values and residuals as extra columns.\nAssign to the following symbols the corresponding numerical values:\n\n\n\\(y_{3}\\) (response variable for unit \\(i = 3\\) in the sample data)\n\n\\(\\hat y_{3}\\) (fitted value for the third unit)\n\n\\(\\hat \\epsilon_{5}\\) (the residual corresponding to the 5th unit, i.e., \\(y_{5} - \\hat y_{5}\\))\n\n\n\n\n\n Solution \n\n\n\nmwdata_fitted &lt;- mwdata %&gt;%\n  mutate(\n    wellbeing_hat = predict(mdl),\n    resid = wellbeing - wellbeing_hat\n  )\n\nhead(mwdata_fitted)\n\n# A tibble: 6 × 9\n    age outdoor_time social_int routine wellbeing location steps_k wellbeing_hat\n  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1    28           12         13       1        36 rural       21.6          36.6\n2    56            5         15       1        41 rural       12.3          37.2\n3    25           19         11       1        35 rural       49.8          36.0\n4    60           25         15       0        35 rural       NA            37.2\n5    19            9         18       1        32 rural       48.1          38.2\n6    34           18         13       1        34 rural       67.3          36.6\n# ℹ 1 more variable: resid &lt;dbl&gt;\n\n\n\n\n\\(y_{3}\\) = 35 (see row 3, column 5)\n\n\\(\\hat y_{3}\\) = 35.95 (see row 3, column 8)\n\n\\(\\hat \\epsilon_{5} = y_{5} - \\hat y_{5}\\) = 32 - 38.21 = -6.21 (see row 5, columns 5 and 8)\n\n\n\n\n\n\nQuestion 9\n\n\nProvide key model results in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package.\nYou can rename your DV and IV labels by specifying dv.labels and pred.labels. To do so, specify your variable name on the left, and what you would like this to be named in the table on the right.\n\n\n\n\n\n\n\n Solution \n\n\n\ntab_model(mdl,\n          dv.labels = \"Wellbeing (WEMWBS) Scores\",\n          pred.labels = c(\"social_int\" = \"Social Interactions (Number per Week)\"),\n          title = \"Regression Table for Wellbeing Model\")\n\n\n\nTable 1: Regression Table for Wellbeing Model\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS) Scores\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n32.41\n30.09 – 34.73\n&lt;0.001\n\n\nSocial Interactions\n(Number per Week)\n0.32\n0.14 – 0.50\n0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.058 / 0.053\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nDescribe the design of the study, and the analyses that you undertook. Interpret your results in the context of the research question and report your model results in full.\nMake reference to your descriptive plots and/or statistics and regression table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nMake sure to write your results up following APA guidelines\n\n\n\n\n\n\n\n Solution \n\n\n\n\n\n\n\n\nThe mwdata dataset contained information on 200 hypothetical participants who lived in Edinburgh & Lothians area. Using a between-subjects design, the researchers collected information on participants’ wellbeing, outdoor time (hours per week), social interactions (number per week), routine, location of residence, average weekly steps, and age.\nTo visualise the marginal distributions of wellbeing and social interactions, density plots were used. To understand the strength of association between the two variables, the correlation coefficient was estimated. To investigate whether there was an association between there was an overall effect of the number of weekly social interactions on wellbeing (WEMWBS) scores, the following simple linear regression model was used:\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social Interactions}\n\\] From Figure 1 and Figure 2, we can see that both wellbeing \\((M = 36.3, SD = 5.39)\\) and social interactions \\((M = 12.06, SD = 4.02)\\) followed unimodal distributions. There was a weak, positive, linear association between WEMWBS scores and the weekly number of social interactions for the participants in the sample (\\(r\\) = .24).\nFull regression results are displayed in Table 1. The estimated wellbeing score with no social interactions per week was 32.41. Each additional social interaction was associated with a 0.32 point increase in wellbeing scores."
  },
  {
    "objectID": "1_03_mlr.html#study-analysis-plan-overview",
    "href": "1_03_mlr.html#study-analysis-plan-overview",
    "title": "Multiple Linear Regression",
    "section": "Study & Analysis Plan Overview",
    "text": "Study & Analysis Plan Overview\n\nQuestion 1\n\n\nProvide a brief overview of the study design and data, before detailing your analysis plan to address the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nGive the reader some background on the context of the study (you might be able to re-use some of the content you wrote for Lab 1 here)\nState what type of analysis you will conduct in order to address the research question\nSpecify the model to be fitted to address the research question\nSpecify your chosen significance (\\(\\alpha\\)) level\nState your hypotheses\n\n\n\n\n\n\n\n\n Solution \n\n\nThe mwdata dataset contained information on 200 hypothetical participants who lived in Edinburgh & Lothians area. Using a between-subjects design, the researchers collected information on participants’ wellbeing (measured via WEMWBS), outdoor time (hours per week), social interactions (number per week), routine (whether or not one was followed), location of residence (City, Suburb, or Rural), average weekly steps (in thousands), and age (in years).\nTo address the research question of whether there is an association between wellbeing and time spent outdoors after taking into account the association between wellbeing and social interactions, we are going to fit the following multiple linear regression model:\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot Social Interactions + \\beta_2 \\cdot Outdoor Time\n\\]\nEffects will be considered statistically significant at \\(\\alpha=.05\\).\nOur hypotheses are:\n\\(H_0: \\beta_2 = 0\\): There is no association between well-being and time spent outdoors after taking into account the association between well-being and social interactions\n\\(H_1: \\beta_2 \\neq 0\\): There is an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions"
  },
  {
    "objectID": "1_02_mlr.html",
    "href": "1_02_mlr.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nExtend the ideas of single linear regression to consider regression models with two or more predictors\nUnderstand and interpret the coefficients in multiple linear regression models\n\n\nBe up to date with lectures\nHave completed Week 1 lab exercises\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\nsjPlot\nkableExtra\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv."
  },
  {
    "objectID": "1_02_mlr.html#study-analysis-plan-overview",
    "href": "1_02_mlr.html#study-analysis-plan-overview",
    "title": "Multiple Linear Regression",
    "section": "Study & Analysis Plan Overview",
    "text": "Study & Analysis Plan Overview\n\nQuestion 1\n\n\nProvide a brief overview of the study design and data, before detailing your analysis plan to address the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nGive the reader some background on the context of the study (you might be able to re-use some of the content you wrote for Lab 1 here)\nState what type of analysis you will conduct in order to address the research question\nSpecify the model to be fitted to address the research question\nSpecify your chosen significance (\\(\\alpha\\)) level\nState your hypotheses\n\n\n\n\n\n\n\n\n Solution \n\n\nThe mwdata dataset contained information on 200 hypothetical participants who lived in Edinburgh & Lothians area. Using a between-subjects design, the researchers collected information on participants’ wellbeing (measured via WEMWBS), outdoor time (hours per week), social interactions (number per week), routine (whether or not one was followed), location of residence (City, Suburb, or Rural), average weekly steps (in thousands), and age (in years).\nDensity plots and boxplots were to be used to visualise the marginal distributions of wellbeing, social interactions, and outdoor time. To understand the strength of association among the variables, we will estimate the the correlation coefficients. To address the research question of whether there is an association between wellbeing and time spent outdoors after taking into account the association between wellbeing and social interactions, we are going to fit the following multiple linear regression model:\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time\n\\]\nEffects will be considered statistically significant at \\(\\alpha=.05\\).\nOur hypotheses are:\n\\(H_0: \\beta_2 = 0\\): There is no association between wellbeing and time spent outdoors after taking into account the association between wellbeing and social interactions\n\\(H_1: \\beta_2 \\neq 0\\): There is an association between wellbeing and time spent outdoors after taking into account the association between wellbeing and social interactions"
  },
  {
    "objectID": "1_02_mlr.html#descriptive-statistics",
    "href": "1_02_mlr.html#descriptive-statistics",
    "title": "Multiple Linear Regression",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nQuestion 2\n\n\nAlongside descriptive statistics, visualize the marginal distributions of the wellbeing, outdoor_time, and social_int variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot interpretation\n- The shape, center and spread of the distribution - Whether the distribution is symmetric or skewed - Whether the distribution is unimodal or bimodal\nPlotting tips\n- Use \\n to wrap text in your titles and or axis labels - The patchwork package allows us to arrange multiple plots in two ways - | arranges the plots adjacent to one another, and / arranges the plots on top of one another\nTable tips\n- The kableExtra package allows us to produce well formatted tables for our descriptive statistics. To do so, you need to specify the kable() and kable_styling() arguments\n\n\n\n\n\n\n\n Solution \n\n\nYou should be familiar now with how to visualise a marginal distribution. You might choose histograms, density curves, or boxplots, or a combination:\n\nwellbeing_plot &lt;- \n  ggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Score on WEMWBS (range 14-70)\", y = \"Probability\\nDensity\")\n\noutdoortime_plot &lt;- \n  ggplot(data = mwdata, aes(x = outdoor_time)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Probability\\nDensity\")\n\nsocial_plot &lt;- \n  ggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Number of social interactions per week\", y = \"Probability\\nDensity\")\n\n# arrange plots vertically \nwellbeing_plot / outdoortime_plot / social_plot\n\n\n\nFigure 1: Marginal distribution plots of wellbeing sores, weekly hours spent outdoors, and social interactions\n\n\n\nSummary statistics for wellbeing, outdoor time, and social interactions:\n\ndescriptives &lt;- mwdata %&gt;% \n  summarize(\n    M_Wellbeing = mean(wellbeing), \n    SD_Wellbeing = sd(wellbeing),\n    M_OutTime = mean(outdoor_time), \n    SD_OutTime = sd(outdoor_time),\n    M_SocInt = mean(social_int), \n    SD_SocInt = sd(social_int)\n    )\ndescriptives\n\n# A tibble: 1 × 6\n  M_Wellbeing SD_Wellbeing M_OutTime SD_OutTime M_SocInt SD_SocInt\n        &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1        36.3         5.39      18.3       7.10     12.1      4.02\n\n\nWe can make this into a well formatted table using kable():\n\nkable(descriptives, caption = \"Wellbeing, Social Interactions, and Outdoor Time Descriptive Statistics\", digits = 2) %&gt;%\n        kable_styling()\n\n\nWellbeing, Social Interactions, and Outdoor Time Descriptive Statistics\n\nM_Wellbeing\nSD_Wellbeing\nM_OutTime\nSD_OutTime\nM_SocInt\nSD_SocInt\n\n\n36.3\n5.39\n18.25\n7.1\n12.06\n4.02\n\n\n\n\n\n\n\n\n\n\n\nThe marginal distribution of scores on the WEMWBS was unimodal with a mean of approximately 36.3. There was variation in WEMWBS scores (SD = 5.39)\n\nThe marginal distribution of weekly hours spent outdoors was unimodal with a mean of approximately 18.25. There was variation in weekly hours spent outdoors (SD = 7.1)\nThe marginal distribution of numbers of social interactions per week was unimodal with a mean of approximately 12.06. There was variation in numbers of social interactions (SD = 4.02)\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nProduce plots of the associations between the outcome variable (wellbeing) and each of the explanatory variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot interpretation\n- Direction of association - Form of association (can it be summarised well with a straight line?)\n- Strength of association (how closely do points fall to a recognizable pattern such as a line?) - Unusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail\nPlot tips\n- use \\n to wrap text in your titles and or axis labels - consider using geom_smooth() to superimpose the best-fitting line describing the association of interest\n\n\n\n\n\n\n\n Solution \n\n\n\nwellbeing_outdoor &lt;- \n  ggplot(data = mwdata, aes(x = outdoor_time, y = wellbeing)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Time spent outdoors \\nper week (hours)\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_social &lt;- \n  ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Number of social interactions \\nper week\", y = \"Wellbeing score (WEMWBS)\")\n\n# place plots adjacent to one another\nwellbeing_outdoor | wellbeing_social\n\n\n\nFigure 2: Scatterplots displaying the relationships between scores on the WEMWBS and a) weekly outdoor time (hours), and b) weekly number of social interactions\n\n\n\nThe scatterplots we created above show weak, positive, and linear associations both between wellbeing and outdoor time, and between wellbeing and the number of weekly social interactions.\n\n\n\n\n\nQuestion 4\n\n\nProduce a correlation matrix of the variables which are to be used in the analysis, and write a short paragraph describing the associations.\n\n\n\n\n\n\nHint\n\n\n\n\n\nCorrelation Matrix\nReview Q2 of your Week 1 lab for guidance on how to produce a correlation matrix.\nAPA Format\nMake sure to round your numbers in-line with APA 7th edition guidelines. The round() function will come in handy here, as might this APA numbers and statistics guide!\n\n\n\n\n\n\n\n Solution \n\n\nWe can either:\n\n\nIndex dataframe ([])\nVariable selection (select())\n\n\n\n\n# correlation matrix of the three columns of interest (check which columns we need - in this case, 2,3, and 5)\nround(cor(mwdata[,c(5,3,2)]), digits = 2)\n\n             wellbeing social_int outdoor_time\nwellbeing         1.00       0.24         0.25\nsocial_int        0.24       1.00        -0.04\noutdoor_time      0.25      -0.04         1.00\n\n\n\n\n\n# select only the columns we want by variable name, and pass this to cor()\nmwdata %&gt;% \n  select(wellbeing, social_int, outdoor_time) %&gt;%\n  cor() %&gt;%\n    round(digits = 2)\n\n             wellbeing social_int outdoor_time\nwellbeing         1.00       0.24         0.25\nsocial_int        0.24       1.00        -0.04\noutdoor_time      0.25      -0.04         1.00\n\n\n\n\n\n\n\n\n\n\n\n\nThere was a weak, positive, linear association between WEMWBS scores and weekly outdoor time for the participants in the sample (\\(r\\) = .25). Higher number of hours spent outdoors each week was associated, on average, with higher wellbeing scores\n\nThere was a weak, positive, linear association between WEMWBS scores and the weekly number of social interactions for the participants in the sample (\\(r\\) = .24). More social interactions were associated, on average, with higher wellbeing scores\nThere was a negligible negative correlation between weekly outdoor time and the weekly number of social interactions (\\(r\\) = -.04)"
  },
  {
    "objectID": "1_02_mlr.html#model-fitting-interpretation",
    "href": "1_02_mlr.html#model-fitting-interpretation",
    "title": "Multiple Linear Regression",
    "section": "Model Fitting & Interpretation",
    "text": "Model Fitting & Interpretation\n\nQuestion 5\n\n\nRecall the model specified in Q1, and:\n\nState the parameters of the model. How do we denote parameter estimates?\n\nFit the linear model in using lm(), assigning the output to an object called mdl1.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAs we did for simple linear regression, we can fit our multiple regression model using the lm() function. We can add as many explanatory variables as we like, separating them with a +.\n\nmodel_name &lt;- lm(&lt;response variable&gt; ~ 1 + &lt;explanatory variable 1&gt; + &lt;explanatory variable 2&gt; + ... , data = &lt;dataframe&gt;)\n\n\n\n\n\n\n\n\n Solution \n\n\nA model for the association between \\(x_1\\) = weekly numbers of social interactions, \\(x_2\\) = weekly outdoor time, and \\(y\\) = scores on the WEMWBS can be given by: \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\\\ \\quad \\\\\n\\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nIn the model specified above,\n\n\n\\(\\mu_{y|x_1, x_2} = \\beta_0 + \\beta_1 x + \\beta_2 x_2\\) represents the systematic part of the model giving the mean of \\(y\\) at each combination of values of \\(x_1\\) and \\(x_2\\);\n\n\\(\\epsilon\\) represents the error (deviation) from that mean, and the errors are independent from one another.\n\nThe parameters of our model are:\n\n\n\\(\\beta_0\\) (The intercept);\n\n\\(\\beta_1\\) (The slope across values of \\(x_1\\));\n\n\\(\\beta_2\\) (The slope across values of \\(x_2\\));\n\n\n\\(\\sigma\\) (The standard deviation of the errors).\n\nWhen we estimate these parameters from the available data, we have a fitted model (recall that the h\\(\\hat{\\textrm{a}}\\)ts are used to distinguish our estimates from the true unknown parameters): \\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Social~Interactions + \\hat \\beta_2 \\cdot Outdoor~Time\n\\] And we have residuals \\(\\hat \\epsilon = y - \\hat y\\) which are the deviations from the observed values and our model-predicted responses.\nFitting the model in R:\n\nmdl1 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\n\n\n\n\n\n\nVisual\nNote that for simple linear regression we talked about our model as a line in 2 dimensions: the systematic part \\(\\beta_0 + \\beta_1 x\\) defined a line for \\(\\mu_y\\) across the possible values of \\(x\\), with \\(\\epsilon\\) as the random deviations from that line. But in multiple regression we have more than two variables making up our model.\nIn this particular case of three variables (one outcome + two explanatory), we can think of our model as a regression surface (see Figure 3). The systematic part of our model defines the surface across a range of possible values of both \\(x_1\\) and \\(x_2\\). Deviations from the surface are determined by the random error component, \\(\\hat \\epsilon\\).\n\n\n\n\nFigure 3: Regression surface for wellbeing ~ social_int + outdoor_time, from two different angles\n\n\n\nDon’t worry about trying to figure out how to visualise it if we had any more explanatory variables! We can only concieve of 3 spatial dimensions. One could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it’s not worth trying!\n\n\n\nQuestion 6\n\n\nUsing any of:\n\nmdl1\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\nWrite out the estimated parameter values of:\n\n\n\\(\\hat \\beta_0\\), the estimated average wellbeing score associated with zero hours of outdoor time and zero social interactions per week.\n\n\n\\(\\hat \\beta_1\\), the estimated increase in average wellbeing score associated with an additional social interaction per week (an increase of one), holding weekly outdoor time constant.\n\n\n\\(\\hat \\beta_2\\), the estimated increase in average wellbeing score associated with one hour increase in weekly outdoor time, holding the number of social interactions constant\n\n\n\n\n\n\n\n\nWhat do we mean by hold constant / controlling for / partialling out / residualizing for?\n\n\n\nWhen the remaining explanatory variables are held at the same value or are fixed.\n\n\n\n\n\n\n Solution \n\n\n\n\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\n\n\n\nmdl1$coefficients\n\n (Intercept)   social_int outdoor_time \n  28.6201808    0.3348821    0.1990943 \n\n\n\n\n\ncoef(mdl1)\n\n (Intercept)   social_int outdoor_time \n  28.6201808    0.3348821    0.1990943 \n\n\n\n\n\ncoefficients(mdl1)\n\n (Intercept)   social_int outdoor_time \n  28.6201808    0.3348821    0.1990943 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\n\n\n\\(\\hat \\beta_0\\) = 28.62\n\n\n\\(\\hat \\beta_1\\) = 0.33\n\n\n\\(\\hat \\beta_2\\) = 0.2\n\n\n\n\n\n\nQuestion 7\n\n\nWithin what distance from the model predicted values (the regression line) would we expect 95% of WEMWBS wellbeing scores to be?\n\n\n\n\n\n\nHint\n\n\n\n\n\nEither sigma() or part of the output from summary() will help you here.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nsigma(mdl1)\nsummary(mdl1)\n\n\n\n\nsigma(mdl1)\n\n[1] 5.065003\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma\\) = 5.07. We would expect 95% of wellbeing scores to be within about 10.13 (\\(2 \\hat \\sigma\\)) from the model fit.\n\n\n\n\n\nQuestion 8\n\n\nBased on the model, predict the wellbeing scores for the following individuals:\n\nLeah: Social Interactions = 25; Outdoor Time = 3\nSean: Social Interactions = 19; Outdoor Time = 36\nMike: Social Interactions = 15; Outdoor Time = 20\nDonna: Social Interactions = 7; Outdoor Time = 1\n\nWho has the highest predicted wellbeing score, and who has the lowest?\n\n\n\n\n Solution \n\n\nFirst we need to pass the data into R:\n\nwellbeing_query &lt;- tibble(social_int = c(25, 19, 15, 7),\n                          outdoor_time = c(3, 36, 20, 1))\n\nAnd next use predict() to get their estimated wellbeing scores:\n\npredict(mdl1, newdata = wellbeing_query)\n\n       1        2        3        4 \n37.58952 42.15034 37.62530 31.16345 \n\n\nSean has the highest predicted wellbeing score (42.15), and Donna the lowest (31.16)."
  },
  {
    "objectID": "1_02_mlr.html#writing-up-presenting-results",
    "href": "1_02_mlr.html#writing-up-presenting-results",
    "title": "Multiple Linear Regression",
    "section": "Writing Up & Presenting Results",
    "text": "Writing Up & Presenting Results\n\nQuestion 9\n\n\nProvide key model results in a formatted table.\n\n\n\n\n Solution \n\n\n\ntab_model(mdl1,\n          dv.labels = \"Wellbeing (WEMWBS Scores)\",\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\",\n                          \"outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression Table for Wellbeing Model\")\n\n\n\nTable 2: Regression Table for Wellbeing Model\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n28.62\n25.69 – 31.55\n&lt;0.001\n\n\nSocial Interactions\n(number per week)\n0.33\n0.16 – 0.51\n&lt;0.001\n\n\nOutdoor Time (hours per\nweek)\n0.20\n0.10 – 0.30\n&lt;0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.126 / 0.118\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret your results in the context of the research question.\nMake reference to the your regression table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nMake sure to include a decision in relation to your null hypothesis - based on the evidence, should you reject or fail to reject the null?\n\n\n\n\n\n\n\n Solution \n\n\nFrom Figure 1, we can see that wellbeing \\((M = 36.3, SD = 5.39)\\), social interactions \\((M = 12.06, SD = 4.02)\\), and outdoor time \\((M = 18.25, SD = 7.1)\\) followed unimodal distributions. There were weak, positive, linear associations between WEMWBS scores and the weekly number of social interactions \\((r = .24)\\), and between WEMWBS scores and outdoor time \\((r = .25)\\) in the sample.\nA multiple regression model was used to determine if there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. As presented in Table 2, outdoor time was significantly associated with wellbeing scores \\((\\beta = 0.20, SE = 0.05, p &lt; .001)\\) after controlling for the number of weekly social interactions. Results suggested that for every additional hour spent outdoors each week, wellbeing scores increased by 0.20 points. Therefore, we should reject the null hypothesis since \\(p &lt; .05\\)."
  },
  {
    "objectID": "1_05_writeup_recap.html",
    "href": "1_05_writeup_recap.html",
    "title": "Write Up & Block 1 Recap",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand how to write-up and provide interpretation of a linear model with multiple predictors.\n\n\nBe up to date with lectures\nHave completed Labs 1 - 4\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\nsjPlot\nkableExtra\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/riverview.csv."
  },
  {
    "objectID": "1_05_writeup_recap.html#study-overview",
    "href": "1_05_writeup_recap.html#study-overview",
    "title": "Write Up & Block 1 Recap",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Question\nDoes education level explain a significant amount of variance in income over seniority?\n\nLet’s imagine a study into income disparity for workers in the city of Riverview, a a hypothetical midwestern city in the US.. We might carry out interviews and find that there is an association between the level of education and an employee’s income. However, we are unsure whether this association explains any more variance than that already accounted for by seniority level.\n\n Riverview data codebook.\n\n\nDescription\nThe riverview data come from Lewis-Beck and Lewis-Beck (2015) and contain five attributes collected from a random sample of employees working in the hypothetical city of Riverview (\\(n=32\\)). The attributes include:\n\n\neducation: Years of formal education\n\nincome: Annual income (in thousands of U.S. dollars)\n\nseniority: Years of seniority\n\ngender: Employee’s gender\n\nmale: Dummy coded gender variable (0 = Female, 1 = Male)\n\nparty: Political party affiliation\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\n\n\neducation\n      income\n      seniority\n      gender\n      male\n      party\n    \n\n\n8\n37.449\n7\nmale\n1\nDemocrat\n\n\n8\n26.430\n9\nfemale\n0\nIndependent\n\n\n10\n47.034\n14\nmale\n1\nDemocrat\n\n\n10\n34.182\n16\nfemale\n0\nIndependent\n\n\n10\n25.479\n1\nfemale\n0\nRepublican\n\n\n12\n46.488\n11\nfemale\n0\nDemocrat\n\n\n\n\n\n\n\n\n\n\n\n\nSetup\n\nSetup\n\n\n\nCreate a new RMarkdown file\nLoad the required package(s)\nRead the riverview dataset into R, assigning it to an object named riverview\n\n\n\n\n\n\n Solution \n\n\n\n#Loading the required package(s)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(sjPlot)\nlibrary(kableExtra)\n\n#Reading in riverview data and storing in object named 'riverview'\nriverview &lt;- read_csv(\"https://uoepsy.github.io/data/riverview.csv\")\n\n#check first six rows\nhead(riverview)\n\n# A tibble: 6 × 6\n  education income seniority gender  male party      \n      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;      \n1         8   37.4         7 male       1 Democrat   \n2         8   26.4         9 female     0 Independent\n3        10   47.0        14 male       1 Democrat   \n4        10   34.2        16 female     0 Independent\n5        10   25.5         1 female     0 Republican \n6        12   46.5        11 female     0 Democrat   \n\n\n\n\n\n\n Provided Analysis Code\n\n\n\nlibrary(tidyverse) # for all things!\nlibrary(psych) # good for descriptive stats\nlibrary(kableExtra) # useful for creating nice tables\nlibrary(sjPlot) #regression tables\n\nriverview &lt;- read_csv(\"https://uoepsy.github.io/data/riverview.csv\")\n\n# standardise scs score\nscs_study &lt;- \n  scs_study %&gt;% \n    mutate(\n      zscs = (scs-mean(scs))/sd(scs)\n    )\n#alternatively, you could do zscs = scale(scs, center = TRUE, scale = TRUE)\n\n# the describe() function is from the psych package, and kable() from kableExtra which is used to make a nice table where the values are rounded to 2 decimal places using digits = 2. \ndescribe(scs_study %&gt;% \n        select(dass, scs, zn))[,c(2:4,8:9)] %&gt;% \n        kable(., caption = \"DASS-21, SCS, and Neuroticism Descriptive Statistics\", digits = 2) %&gt;%\n        kable_styling()\n\n\n# scatterplot matrix of dataset without the zscs variable\npairs.panels(scs_study %&gt;% select(-zscs))\n\n\n#build models\nincome_mdl1 &lt;- lm(income ~ seniority, data = riverview)\nincome_mdl2 &lt;- lm(income ~ seniority + education, data = riverview)\n\n#check summary table of results for each model\nsummary(income_mdl1)\n\n\nCall:\nlm(formula = income ~ seniority, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.007  -7.779  -2.659  10.695  19.845 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   35.690      5.074   7.035 8.07e-08 ***\nseniority      1.219      0.311   3.919 0.000477 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.03 on 30 degrees of freedom\nMultiple R-squared:  0.3386,    Adjusted R-squared:  0.3166 \nF-statistic: 15.36 on 1 and 30 DF,  p-value: 0.0004767\n\nsummary(income_mdl2)\n\n\nCall:\nlm(formula = income ~ seniority + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.339  -6.119  -1.396   7.002  13.928 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.7692     5.3729   1.260  0.21776    \nseniority     0.7388     0.2101   3.516  0.00146 ** \neducation     2.2518     0.3346   6.729  2.2e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.646 on 29 degrees of freedom\nMultiple R-squared:  0.7418,    Adjusted R-squared:  0.724 \nF-statistic: 41.65 on 2 and 29 DF,  p-value: 2.977e-09\n\n\n\n#compare models\nanova(income_mdl1, income_mdl2)\n\nAnalysis of Variance Table\n\nModel 1: income ~ seniority\nModel 2: income ~ seniority + education\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     30 4342.4                                 \n2     29 1695.3  1    2647.1 45.28 2.203e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n#create table for results\ntab_model(income_mdl1, income_mdl2,\n          dv.labels = c(\"Income\", \"Income\"),\n          pred.labels = c(\"seniority\" = \"Seniority (in years)\", \n                          \"education\"=\"Formal Education (in years)\"), \n          title = \"Regression Table for Income Model\")\n\n\nRegression Table for Income Model\n\n\n\n\n\n\n\n\n\n\n\n \nIncome\nIncome\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n35.69\n25.33 – 46.05\n&lt;0.001\n6.77\n-4.22 – 17.76\n0.218\n\n\nSeniority (in years)\n1.22\n0.58 – 1.85\n&lt;0.001\n0.74\n0.31 – 1.17\n0.001\n\n\nFormal Education (in\nyears)\n\n\n\n2.25\n1.57 – 2.94\n&lt;0.001\n\n\nObservations\n32\n32\n\n\nR2 / R2 adjusted\n0.339 / 0.317\n0.742 / 0.724\n\n\n\n\n\n\n\n\nThe 3-Act Structure\nWe need to present our report in three clear sections - think of your sections like the 3 key parts of a play or story - we need to (1) provide some background and scene setting for the reader, (2) present our results in the context of the research question, and (3) present a resolution to our story - relate our findings back to the question we were asked and provide our answer.\nAct I: Analysis Strategy\n\nQuestion 1\n\n\nAttempt to draft a discussion section based on the above research question and analysis provided.\n\n\n\n\n\n\n\n\n\nAnalysis Strategy - What to Include***\n\n\n\n\n\nYour analysis strategy will contain a number of different elements detailing plans and changes to your plan. Remember, your analysis strategy should not contain any results. You may wish to include the following sections:\n\nVery brief data and design description:\n\nGive the reader some background on the context of your write-up. For example, you may wish to describe the data source, data collection strategy, study design, number of observational units.\nSpecify the variables of interest in relation to the research question, including their unit of measurement, the allowed range (for Likert scales), and how they are scored.\n\n\nData management:\n\nDescribe any data cleaning and/or recoding.\nAre there any observations that have been excluded based on pre-defined criteria? How/why, and how many?\n* Describe any transformations performed to aid your interpretation (i.e., mean centering, standardisation, etc.)\n\n\nModel specification:\n\nClearly state your hypotheses and specify your chosen significance level.\nWhat type of statistical analysis do you plan to use to answer the research question? (e.g., simple linear regression, multiple linear regression, binary logistic regression, etc.)\nIn some cases, you may wish to include some visualisations and descriptive tables to motivate your model specification.\nSpecify the model(s) to be fitted to answer your given research question and analysis structure. Clearly specify the response and explanatory variables included in your model(s).\n\n\n\n\n*** This is not yet a completed list – we will add more to this as we progress through the course (e.g., coding of categorical variables, checking assumptions, diagnostics, etc.)\n\nAs noted and encouraged throughout the course, one of the main benefits of using RMarkdown is the ability to include inline R code in your document. Try to incorporate this in your write up so you can automatically pull the specified values from your code. If you need a reminder on how to do this, see Lesson 4 of the Rmd Bootcamp.\n\n\n\n\n Example Write-Up of Analysis Strategy Section\n\n\n\n\n\n\nAct II: Results\n\nQuestion 2\n\n\nAttempt to draft a results section based on your detailed analysis strategy and the analysis provided.\n\n\n\n\n\n\n\n\n\nResults - What To Include***\n\n\n\n\n\nThe results section should follow from your analysis strategy. This is where you would present the evidence and results that will be used to answer the research questions and can support your conclusions. Make sure that you address all aspects of the approach you outlined in the analysis strategy.\nIn this section, it is useful to include tables and plots to clearly present your findings to your reader. It is important, however, to carefully select what is the key information that should be presented. You don’t want to overload the reader with unnecessary information, and you also want to save space in case there is a page limit. Make use of figures with multiple panels where you can.\nAs a broad guideline, you want to start with the results of an exploratory data analysis, presenting tables of summary statistics and exploratory plots. You may also want to visualise associations between/among variables and report covariances or correlations. Then, you should move on to the results from your model.\n\n*** This is not yet a completed list – we will add more to this as we progress through the course (e.g., evaluation of diagnostic plots, etc.)\n\n\n\n\n\n Example Write-Up of Results Section\n\n\n\n\n\n\nAct III: Discussion\n\nQuestion 3\n\n\nAttempt to draft a discussion section based on your results and the analysis provided.\n\n\n\n\n\n\n\n\n\nDiscussion - What To Include\n\n\n\n\n\nIn the discussion section, you should summarise the key findings from the results section and provide the reader with a few take-home sentences drawing the analysis together and relating it back to the original question.\nThe discussion should be relatively brief, and should not include any statistical analysis - instead think of the discussion as a conclusion, providing an answer to the research question(s).\n\n\n\n\n Example Write-Up of Discussion Section"
  },
  {
    "objectID": "1_01_slr.html",
    "href": "1_01_slr.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to specify a simple linear model\n\nUnderstand what fitted values and residuals are\nBe able to interpret the coefficients of a fitted model\n\n\nBe up to date with lectures\n\nHave installed R and RStudio on your own computer (unless you have a Chromebook where you may continue to use the PPLS RStudio Server)\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\nkableExtra\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv."
  },
  {
    "objectID": "1_01_slr.html#data-exploration",
    "href": "1_01_slr.html#data-exploration",
    "title": "Simple Linear Regression",
    "section": "Data Exploration",
    "text": "Data Exploration\nThe common first port of call for almost any statistical analysis is to explore the data, and we can do this visually and/or numerically.\n\n\n\n\n\n\n\n\n\n================ Description\n\nMarginal Distributions | Bivariate Associations | =================================================================================================================================================================+==============================================================================================================================================================================+ The distribution of each variable without reference to the values of the other variables | Describing the relationship between two numeric variables |\n\n\n\n\nVisually \n\n\nPlot each variable individually.  You could use, for example, geom_density() for a density plot or geom_histogram() for a histogram to comment on and/or examine: \n\nThe shape of the distribution. Look at the shape, centre and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\n\n\nIdentify any unusual observations. Do you notice any extreme observations (i.e., outliers)?\n\n\n\nPlot associations among two variables. |  |  | You could use, for example, geom_point() for a scatterplot to comment on and/or examine:  |  |\n\nThe direction of the association indicates whether there is a positive or negative association\n\n\nThe form of association refers to whether the relationship between the variables can be summarized well with a straight line or some more complicated pattern\n\n\nThe strength of association entails how closely the points fall to a recognizable pattern such as a line\n\n\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail\n\n\n\n\n\nNumerically \n\nCompute and report summary statistics e.g., mean, standard deviation, median, min, max, etc.  You could, for example, calculate summary statistics such as the mean (mean()) and standard deviation (sd()), etc. within summarize()\n\nCompute and report the correlation coefficient.  You can use the cor() function to calculate this\n\n\n\n\nMarginal Distributions\n\nQuestion 1\n\n\nVisualise and describe the marginal distributions of wellbeing scores and social interactions.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot interpretation\n- The shape, center and spread of the distribution\n- Whether the distribution is symmetric or skewed\n- Whether the distribution is unimodal or bimodal\nPlotting tips\n- Use \\n to wrap text in your titles and or axis labels\n- The patchwork package allows us to arrange multiple plots in two ways - | arranges the plots adjacent to one another, and / arranges the plots on top of one another\nTable tips\n- The kableExtra package allows us to produce well formatted tables for our descriptive statistics. To do so, you need to specify the kable() and kable_styling() arguments\n- Review the guidance on the rmd bootcamp, particularly Lesson 4\n\n\n\n\n\n\n\n Solution \n\n\n\n\nWellbeing (WEMWBS) Scores\nSocial Interactions\n\n\n\nVisualisation of distribution:\n\nggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  labs(x = \"Wellbeing (WEMWBS) Scores\", \n       y = \"Probability density\")\n\n\n\nFigure 1: Distribution of Wellbeing (WEMWBS) Scores\n\n\n\nInitial observations from plot:\n\nThe distribution of wellbeing scores was unimodal\nMost of the wellbeing scores were between roughly 30 and 45\nThe lowest wellbeing in the sample was approximately 22 and the highest approximately 59. This suggested there was a fair high degree of variation in the data\nScores were within the range of possible values\n\nDescriptive (or summary) statistics for wellbeing scores:\n\nmwdata %&gt;% \n  summarize(\n    M = mean(wellbeing), \n    SD = sd(wellbeing)\n    ) %&gt;%\n    kable(caption = \"Wellbeing Descriptive Statistics\", align = \"c\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 1: Wellbeing Descriptive Statistics\n\nM\nSD\n\n\n36.3\n5.39\n\n\n\n\n\n\n\nFollowing the exploration above, we can describe the wellbeing variable as follows:\n\n\n\n\n\n\nThe marginal distribution of scores on the WEMWBS was unimodal with a mean of approximately 36.3. There was variation in WEMWBS scores (SD = 5.39)\n\n\n\n\n\nVisualisation of distribution:\n\nggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  labs(x = \"Social Interactions (Number per Week)\", \n       y = \"Probability density\")\n\n\n\nFigure 2: Distribution of Number of Social Interactions\n\n\n\nInitial observations from plot:\n\nThe distribution of social interactions was unimodal\nMost of the participants had between 8 and 15 social interactions per week\nThe fewest social interactions per week was approximately 3, and the highest approximately 24. This suggested there was a fair high degree of variation in the data\n\nDescriptive (or summary) statistics for the number of weekly social interactions per week:\n\nmwdata %&gt;% \n  summarize(\n    M = mean(social_int), \n    SD = sd(social_int)\n    ) %&gt;%\n    kable(caption = \"Social Interactions Descriptive Statistics\", align = \"c\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 2: Social Interactions Descriptive Statistics\n\nM\nSD\n\n\n12.06\n4.02\n\n\n\n\n\n\n\nFollowing the exploration above, we can describe the social interactions variable as follows:\n\n\n\n\n\n\nThe marginal distribution of numbers of social interactions per week was unimodal with a mean of approximately 12.06. There was variation in numbers of social interactions (SD = 4.02)\n\n\n\n\n\n\n\n\n\nAssociations among Variables\n\nQuestion 2\n\n\nCreate a scatterplot of wellbeing score and social interactions before calculating the correlation between them.\n\nCorrelation Matrix\nA table showing the correlation coefficients - \\(r_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\\) - between variables. Each cell in the table shows the association between two variables. The diagonals show the correlation of a variable with itself (and are therefore always equal to 1).\n\nIn R, we can create a correlation matrix by giving the cor() function a dataframe. However, we only want to give it 2 columns here. Think about how we select specific columns, either giving the column numbers inside [], or using select().\n\n\nMaking reference to both the plot and correlation coefficient, describe the association between wellbeing and social interactions among participants in the Edinburgh & Lothians sample.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot\nWe are trying to investigate how wellbeing varies by varying numbers of weekly social interactions. Hence, wellbeing is the dependent variable (on the y-axis), and social interactions is the independent variable (on the x-axis).\nCorrelation\nMake sure to round your numbers in-line with APA 7th edition guidelines. The round() function will come in handy here, as might this APA numbers and statistics guide!\n\n\n\n\n\n\n\n Solution \n\n\nLet’s produce a scatterplot:\n\nggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  labs(x = \"Social Interactions (Number per Week)\", \n       y = \"Wellbeing (WEMWBS) Scores\")\n\n\n\nFigure 3: Association between Wellbeing and Social Interactions\n\n\n\nTo comment on the strength of the linear association we compute the correlation coefficient in either of the following ways:\n\n\nIndex dataframe ([])\nVariable selection (select())\n\n\n\n\n# correlation matrix of the two columns of interest - (check with columns we need, in this case 3 & 5)\nround(cor(mwdata[,c(3,5)]), digits = 2)\n\n           social_int wellbeing\nsocial_int       1.00      0.24\nwellbeing        0.24      1.00\n\n\n\n\n\n# select only the columns we want by name, and pass this to cor()\nmwdata %&gt;% \n  select(social_int, wellbeing) %&gt;%\n  cor() %&gt;%\n    round(digits = 2)\n\n           social_int wellbeing\nsocial_int       1.00      0.24\nwellbeing        0.24      1.00\n\n\n\n\n\nAnd we can see that via either method, the correlation is \\[\nr_{\\text({Social~Interactions,~~ Wellbeing})} = .24\n\\] \n\n\n\n\n\n\nThere was a weak, positive, linear association between the weekly number of social interactions and WEMWBS scores for the participants in the sample (\\(r\\) = .24). More social interactions were associated, on average, with higher wellbeing scores."
  },
  {
    "objectID": "1_01_slr.html#model-specification-and-fitting",
    "href": "1_01_slr.html#model-specification-and-fitting",
    "title": "Simple Linear Regression",
    "section": "Model Specification and Fitting",
    "text": "Model Specification and Fitting\nThe scatterplot highlighted a linear relationship, where the data points were scattered around an underlying linear pattern with a roughly-constant spread as x varied.\nHence, we will try to fit a simple (i.e., one x variable only) linear regression model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\\\\n\\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\n\\]\n\n\n\n\n\n\nWhat does \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\\) mean?\n\n\n\n\n\nLets break the statement down into smaller parts:\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\n\n\\(y_i\\) is our measured outcome variable (our DV)\n\n\\(x_i\\) is our measured predictor variable (our IV)\n\n\\(\\beta_0\\) is the model intercept\n\n\\(\\beta_1\\) is the model slope\n\\(\\epsilon \\sim N(0, \\sigma) \\text{ independently}\\)\n\n\n\\(\\epsilon\\) is the residual error\n\n\\(\\sim\\) means ‘distributed according to’\n\n\\(\\sim N(0, \\sigma) \\text{ independently}\\) means ‘normal distribution with a mean of 0 and a variance of \\(\\sigma\\)’\nTogether, we can say that the errors around the line have a mean of zero and constant spread as x varies.\n\n\n\n\n\n\nQuestion 3\n\n\nFirst, write the equation of the fitted line.\nNext, using the lm() function, fit a simple linear model to predict wellbeing (DV) by social interactions (IV), naming the output mdl.\nLastly, update your equation of the fitted line to include the estimated coefficients.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe syntax of the lm() function is:\n\n[model name] &lt;- lm([response variable i.e., dependent variable] ~ [explanatory variable i.e., independent variable], data = [dataframe])\n\n\n\n\n\n\n\n\n Solution \n\n\nFirst, lets specify the fitted model, which can be written either as:\n\n\nOption A\nOption B\n\n\n\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Social~Interactions\n\\]\n\n\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 \\cdot 1 + \\hat \\beta_1 \\cdot Social~Interactions\n\\]\n\n\n\nTo fit the model in R, as the variables are in the mwdata dataframe, we would write:\n\n\nOption A\nOption B\n\n\n\n\nmdl &lt;- lm(wellbeing ~ social_int, data = mwdata)\nmdl\n\n\nCall:\nlm(formula = wellbeing ~ social_int, data = mwdata)\n\nCoefficients:\n(Intercept)   social_int  \n    32.4077       0.3222  \n\n\n\n\n\nmdl &lt;- lm(wellbeing ~ 1 + social_int, data = mwdata)\nmdl\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nCoefficients:\n(Intercept)   social_int  \n    32.4077       0.3222  \n\n\n\n\n\n\n\n\n\n\n\nWhy is there a 1 in the Option B’s?\n\n\n\n\n\nWhen we specify the linear model in R, we include after the tilde sign (\\(\\sim\\)), the variables that appear to the right of the \\(\\hat \\beta\\)s. The intercept, or \\(\\beta_0\\), is a constant. That is, we could write it as multiplied by 1.\nIncluding the 1 explicitly is not necessary because it is included by default (you can check this by comparing the outputs of A & B above with and without the 1 included - the estimates are the same!). After a while, you will find you just want to drop the 1 (i.e., Option B) when calling lm() because you know that it’s going to be there, but in these early weeks we tried to keep it explicit to make it clear that you want to the intercept to be estimated.\n\n\n\nNote that by calling the name of the fitted model, mdl, you can see the estimated regression coefficients \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). We can add these values to the fitted line:\n\\[\n\\widehat{Wellbeing} = 32.41 + 0.32 \\cdot Social~Interactions \\\\\n\\]\n\n\n\n\n\nQuestion 4\n\n\nExplore the following equivalent ways to obtain the estimated regression coefficients — that is, \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) — from the fitted model:\n\nmdl\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\n\n\n Solution \n\n\nThe estimated parameters returned by the below methods are all equivalent. However, summary() returns more information.\n\n\nmdl()\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\nSimply invoke the name of the fitted model:\n\nmdl\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nCoefficients:\n(Intercept)   social_int  \n    32.4077       0.3222  \n\n\n\n\n\nmdl$coefficients\n\n(Intercept)  social_int \n 32.4077070   0.3221959 \n\n\n\n\n\ncoef(mdl)\n\n(Intercept)  social_int \n 32.4077070   0.3221959 \n\n\n\n\n\ncoefficients(mdl)\n\n(Intercept)  social_int \n 32.4077070   0.3221959 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5628  -3.2741  -0.7908   3.3703  20.4706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 32.40771    1.17532  27.573  &lt; 2e-16 ***\nsocial_int   0.32220    0.09243   3.486 0.000605 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.247 on 198 degrees of freedom\nMultiple R-squared:  0.05781,   Adjusted R-squared:  0.05306 \nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n\n\n\n\n\n\n\n\n\n\n\nThe estimated intercept is \\(\\hat \\beta_0 = 32.41\\) and the estimated slope is \\(\\hat \\beta_1 = 0.32\\).\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nExplore the following equivalent ways to obtain the estimated standard deviation of the errors — that is, \\(\\hat \\sigma\\) — from the fitted model mdl:\n\nsigma(mdl)\nsummary(mdl)\n\n\n Huh? What is \\(\\sigma\\)?\n\n\nThe standard deviation of the errors, denoted by \\(\\sigma\\), is an important quantity that our model estimates. It represents how much individual data points tend to deviate above and below the regression line.\nA small \\(\\sigma\\) indicates that the points hug the line closely and we should expect fairly accurate predictions, while a large \\(\\sigma\\) suggests that, even if we estimate the line perfectly, we can expect individual values to deviate from it by substantial amounts.\nThe estimated standard deviation of the errors is denoted \\(\\hat \\sigma\\), and is estimated by essentially averaging squared residuals (giving the variance) and taking the square-root:\n\\[\n\\begin{align}\n& \\hat \\sigma = \\sqrt{\\frac{SS_{Residual}}{n - k - 1}} \\\\\n\\qquad \\\\\n& \\text{where} \\\\\n& SS_{Residual} = \\textrm{Sum of Squared Residuals} = \\sum_{i=1}^n{(\\epsilon_i)^2}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\nThe estimated standard deviation of the errors can be equivalently obtained by the below methods. However, summary() returns more information.\n\n\nsigma(mdl)\nsummary(mdl)\n\n\n\n\nsigma(mdl)\n\n[1] 5.246982\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5628  -3.2741  -0.7908   3.3703  20.4706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 32.40771    1.17532  27.573  &lt; 2e-16 ***\nsocial_int   0.32220    0.09243   3.486 0.000605 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.247 on 198 degrees of freedom\nMultiple R-squared:  0.05781,   Adjusted R-squared:  0.05306 \nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term “Residual standard error” is a misnomer, as the help page for sigma says (check ?sigma). However, it’s hard to get rid of this bad name as it has been used in too many books showing R output.\n\n\n\n\n\n\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma = 5.25\\).\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nInterpret the estimated intercept, slope, and standard deviation of the errors in the context of the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo interpret the estimated standard deviation of the errors we can use the fact that about 95% of values from a normal distribution fall within two standard deviations of the center.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nIntercept\nSlope\nStandard deviation of the errors\n\n\n\nThe estimated wellbeing score associated to zero weekly social interactions is 32.41.\n\n\nThe estimated increase in wellbeing associated to one additional weekly social interaction is 0.32.\n\n\nFor any particular numnber of weekly social interactions, participants’ wellbeing scores should be distributed above and below the regression line with standard deviation estimated to be \\(\\hat \\sigma = 5.25\\). Since \\(2 \\hat \\sigma = 10.49\\), we expect most (about 95%) of the participants’ wellbeing scores to be within about 11 points from the regression line.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nPlot the data and the fitted regression line. To do so:\n\nExtract the estimated regression coefficients e.g., via betas &lt;- coef(mdl)\n\nExtract the first entry of betas (i.e., the intercept) via betas[1]\n\nExtract the second entry of betas (i.e., the slope) via betas[2]\n\nProvide the intercept and slope to the function\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nExtracting values\nThe function coef(mdl) returns a vector (a sequence of numbers all of the same type). To get the first element of the sequence you append [1], and [2] for the second.\nPlotting\nIn your ggplot(), you will need to specify geom_abline(). This might help get you started:\n\ngeom_abline(intercept = &lt;intercept&gt;, slope = &lt;slope&gt;)\n\n\n\n\n\n\n\n\n Solution \n\n\nFirst extract the values required:\n\nbetas &lt;- coef(mdl)\nintercept &lt;- betas[1]\nslope &lt;- betas[2]\n\nWe can plot the model as follows:\n\nggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_abline(intercept = intercept, slope = slope, color = 'blue') + \n  labs(x = \"Social Interactions (Number per Week)\", y = \"Wellbeing (WEMWBS) Scores\")"
  },
  {
    "objectID": "1_01_slr.html#predicted-values-residuals",
    "href": "1_01_slr.html#predicted-values-residuals",
    "title": "Simple Linear Regression",
    "section": "Predicted Values & Residuals",
    "text": "Predicted Values & Residuals\n\nPredicted Values\nModel predicted values for sample data:\nWe can get out the model predicted values for \\(y\\), the “y hats” (\\(\\hat y\\)), for the data in the sample using various functions:\n\npredict(&lt;fitted model&gt;)\nfitted(&lt;fitted model&gt;)\nfitted.values(&lt;fitted model&gt;)\nmdl$fitted.values\n\nFor example, this will give us the estimated wellbeing score (point on our regression line) for each observed value of social interactions for each of our 200 participants.\n\npredict(mdl)\n\n       1        2        3        4        5        6        7        8 \n36.59625 37.24064 35.95186 37.24064 38.20723 36.59625 38.52943 36.27406 \n       9       10       11       12       13       14       15       16 \n36.59625 37.24064 34.66308 35.62967 34.66308 36.27406 36.91845 37.88504 \n      17       18       19       20       21       22       23       24 \n37.56284 37.56284 35.62967 35.30747 35.62967 38.20723 36.27406 35.30747 \n      25       26       27       28       29       30       31       32 \n38.52943 38.52943 37.56284 36.27406 37.56284 35.30747 37.56284 36.91845 \n      33       34       35       36       37       38       39       40 \n37.56284 34.34088 37.56284 36.27406 36.27406 36.91845 38.85162 35.30747 \n      41       42       43       44       45       46       47       48 \n38.20723 36.59625 37.56284 36.27406 36.27406 35.95186 34.34088 38.85162 \n      49       50       51       52       53       54       55       56 \n35.62967 37.24064 35.62967 34.98527 37.56284 37.24064 34.01869 35.95186 \n      57       58       59       60       61       62       63       64 \n34.66308 37.88504 36.91845 34.66308 37.88504 35.95186 36.27406 35.95186 \n      65       66       67       68       69       70       71       72 \n36.27406 36.91845 38.20723 34.01869 35.30747 35.30747 34.66308 39.17382 \n      73       74       75       76       77       78       79       80 \n34.01869 35.30747 33.69649 38.52943 35.62967 37.56284 39.17382 36.91845 \n      81       82       83       84       85       86       87       88 \n34.98527 35.95186 36.59625 36.27406 34.66308 35.95186 37.24064 37.88504 \n      89       90       91       92       93       94       95       96 \n35.62967 37.56284 34.66308 34.66308 37.24064 35.95186 34.98527 35.62967 \n      97       98       99      100      101      102      103      104 \n35.95186 34.66308 37.24064 35.95186 34.34088 34.66308 37.56284 34.98527 \n     105      106      107      108      109      110      111      112 \n36.27406 37.88504 39.17382 37.24064 38.52943 35.30747 35.62967 35.95186 \n     113      114      115      116      117      118      119      120 \n37.24064 36.27406 36.27406 36.59625 36.27406 35.95186 36.59625 35.95186 \n     121      122      123      124      125      126      127      128 \n37.56284 36.91845 34.34088 34.66308 34.98527 35.95186 34.01869 35.62967 \n     129      130      131      132      133      134      135      136 \n34.98527 35.62967 33.69649 38.20723 38.20723 35.30747 34.66308 37.56284 \n     137      138      139      140      141      142      143      144 \n36.91845 35.62967 36.91845 38.52943 36.91845 35.30747 35.62967 37.88504 \n     145      146      147      148      149      150      151      152 \n36.27406 34.98527 35.30747 36.91845 36.59625 36.59625 35.95186 34.34088 \n     153      154      155      156      157      158      159      160 \n36.59625 36.59625 34.66308 36.91845 36.27406 35.30747 33.69649 35.62967 \n     161      162      163      164      165      166      167      168 \n36.27406 37.24064 35.95186 36.59625 33.37429 36.59625 38.20723 36.27406 \n     169      170      171      172      173      174      175      176 \n38.85162 34.66308 37.24064 35.62967 38.20723 36.27406 36.59625 40.14041 \n     177      178      179      180      181      182      183      184 \n35.30747 34.98527 34.34088 35.95186 36.59625 34.98527 34.98527 35.62967 \n     185      186      187      188      189      190      191      192 \n34.66308 34.98527 36.27406 37.88504 36.27406 35.95186 35.95186 36.59625 \n     193      194      195      196      197      198      199      200 \n37.56284 36.27406 36.27406 35.95186 34.66308 35.62967 36.27406 37.24064 \n\n\nModel predicted values for other (unobserved) data:\nTo compute the model-predicted values for unobserved data (i.e., data not contained in the sample), we can use the following function:\n\npredict(&lt;fitted model&gt;, newdata = &lt;dataframe&gt;)\n\nFor this example, we first need to remember that the model predicts wellbeing using the independent variable social_int. Hence, if we want predictions for new (unobserved) data, we first need to create a tibble with a column called social_int containing the number of weekly social interactions for which we want the prediction, and store this as a dataframe.\n\n#Create dataframe 'newdata' containing 2, 25, and 28 weekly social interactions\nnewdata &lt;- tibble(social_int = c(2, 25, 28))\nnewdata\n\n# A tibble: 3 × 1\n  social_int\n       &lt;dbl&gt;\n1          2\n2         25\n3         28\n\n\nThen we take newdata and add a new column called wellbeing_hat, computed as the prediction from the fitted mdl using the newdata above:\n\nnewdata &lt;- newdata %&gt;%\n  mutate(\n    wellbeing_hat = predict(mdl, newdata = newdata)\n  )\nnewdata\n\n# A tibble: 3 × 2\n  social_int wellbeing_hat\n       &lt;dbl&gt;         &lt;dbl&gt;\n1          2          33.1\n2         25          40.5\n3         28          41.4\n\n\nResiduals\nThe residuals represent the deviations between the actual responses and the predicted responses and can be obtained either as\n\nmdl$residuals\nresid(mdl)\nresiduals(mdl)\ncomputing them as the difference between the response (\\(y_i\\)) and the predicted response (\\(\\hat y_i\\))\n\n\n\n\nQuestion 8\n\n\nUse predict(mdl) to compute the fitted values and residuals. Mutate the mwdata dataframe to include the fitted values and residuals as extra columns.\nAssign to the following symbols the corresponding numerical values:\n\n\n\\(y_{3}\\) (response variable for unit \\(i = 3\\) in the sample data)\n\n\\(\\hat y_{3}\\) (fitted value for the third unit)\n\n\\(\\hat \\epsilon_{5}\\) (the residual corresponding to the 5th unit, i.e., \\(y_{5} - \\hat y_{5}\\))\n\n\n\n\n\n Solution \n\n\n\nmwdata_fitted &lt;- mwdata %&gt;%\n  mutate(\n    wellbeing_hat = predict(mdl),\n    resid = wellbeing - wellbeing_hat\n  )\n\nhead(mwdata_fitted)\n\n# A tibble: 6 × 9\n    age outdoor_time social_int routine wellbeing location steps_k wellbeing_hat\n  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1    28           12         13       1        36 rural       21.6          36.6\n2    56            5         15       1        41 rural       12.3          37.2\n3    25           19         11       1        35 rural       49.8          36.0\n4    60           25         15       0        35 rural       NA            37.2\n5    19            9         18       1        32 rural       48.1          38.2\n6    34           18         13       1        34 rural       67.3          36.6\n# ℹ 1 more variable: resid &lt;dbl&gt;\n\n\n\n\n\\(y_{3}\\) = 35 (see row 3, column 5)\n\n\\(\\hat y_{3}\\) = 35.95 (see row 3, column 8)\n\n\\(\\hat \\epsilon_{5} = y_{5} - \\hat y_{5}\\) = 32 - 38.21 = -6.21 (see row 5, columns 5 and 8)"
  },
  {
    "objectID": "1_01_slr.html#writing-up-presenting-results",
    "href": "1_01_slr.html#writing-up-presenting-results",
    "title": "Simple Linear Regression",
    "section": "Writing Up & Presenting Results",
    "text": "Writing Up & Presenting Results\n\nQuestion 9\n\n\nProvide key model results in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package.\nYou can rename your DV and IV labels by specifying dv.labels and pred.labels. To do so, specify your variable name on the left, and what you would like this to be named in the table on the right.\n\n\n\n\n\n\n\n Solution \n\n\n\ntab_model(mdl,\n          dv.labels = \"Wellbeing (WEMWBS) Scores\",\n          pred.labels = c(\"social_int\" = \"Social Interactions (Number per Week)\"),\n          title = \"Regression Table for Wellbeing Model\")\n\n\n\nTable 3: Regression Table for Wellbeing Model\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS) Scores\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n32.41\n30.09 – 34.73\n&lt;0.001\n\n\nSocial Interactions\n(Number per Week)\n0.32\n0.14 – 0.50\n0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.058 / 0.053\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nDescribe the design of the study, and the analyses that you undertook. Interpret your results in the context of the research question and report your model results in full.\nMake reference to your descriptive plots and/or statistics and regression table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nMake sure to write your results up following APA guidelines\n\n\n\n\n\n\n\n Solution \n\n\n\n\n\n\n\n\nThe mwdata dataset contained information on 200 hypothetical participants who lived in Edinburgh & Lothians area. Using a between-subjects design, the researchers collected information on participants’ wellbeing (measured via WEMWBS), outdoor time (hours per week), social interactions (number per week), routine (whether or not one was followed), location of residence (City, Suburb, or Rural), average weekly steps (in thousands), and age (in years).\nTo visualise the marginal distributions of wellbeing and social interactions, density plots were used. To understand the strength of association between the two variables, the correlation coefficient was estimated. To investigate whether the number of weekly social interactions influences wellbeing (WEMWBS) scores, the following simple linear regression model was used:\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social Interactions}\n\\] From Figure 1 and Figure 2, we can see that both wellbeing \\((M = 36.3, SD = 5.39)\\) and social interactions \\((M = 12.06, SD = 4.02)\\) followed unimodal distributions. There was a weak, positive, linear association between WEMWBS scores and the weekly number of social interactions for the participants in the sample \\((r = .24)\\).\nFull regression results are displayed in Table 3. The estimated wellbeing score with no social interactions per week was 32.41. Each additional social interaction was associated with a 0.32 point increase in wellbeing scores."
  },
  {
    "objectID": "1_02_mlr.html#descriptive-statistics-visualisations",
    "href": "1_02_mlr.html#descriptive-statistics-visualisations",
    "title": "Multiple Linear Regression",
    "section": "Descriptive Statistics & Visualisations",
    "text": "Descriptive Statistics & Visualisations\n\nQuestion 2\n\n\nAlongside descriptive statistics, visualize the marginal distributions of the wellbeing, outdoor_time, and social_int variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot interpretation\n- The shape, center and spread of the distribution\n- Whether the distribution is symmetric or skewed\n- Whether the distribution is unimodal or bimodal\nPlotting tips\n- Use \\n to wrap text in your titles and or axis labels\n- The patchwork package allows us to arrange multiple plots in two ways - | arranges the plots adjacent to one another, and / arranges the plots on top of one another\nTable tips\n- The kableExtra package allows us to produce well formatted tables for our descriptive statistics. To do so, you need to specify the kable() and kable_styling() arguments\n- Review the guidance on the rmd bootcamp, particularly Lesson 4\n\n\n\n\n\n\n\n Solution \n\n\nYou should be familiar now with how to visualise a marginal distribution. You might choose histograms, density curves, or boxplots, or a combination:\n\nwellbeing_plot &lt;- \n  ggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Score on WEMWBS (range 14-70)\", y = \"Probability\\nDensity\")\n\noutdoortime_plot &lt;- \n  ggplot(data = mwdata, aes(x = outdoor_time)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Probability\\nDensity\")\n\nsocial_plot &lt;- \n  ggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Number of social interactions per week\", y = \"Probability\\nDensity\")\n\n# arrange plots vertically \nwellbeing_plot / outdoortime_plot / social_plot\n\n\n\nFigure 1: Marginal distribution plots of wellbeing sores, weekly hours spent outdoors, and social interactions\n\n\n\nWe can present oursummary statistics for wellbeing, outdoor time, and social interactions as a well formatted table using kable():\n\nmwdata %&gt;% \n  summarize(\n    M_Wellbeing = mean(wellbeing), \n    SD_Wellbeing = sd(wellbeing),\n    M_OutTime = mean(outdoor_time), \n    SD_OutTime = sd(outdoor_time),\n    M_SocInt = mean(social_int), \n    SD_SocInt = sd(social_int)\n    ) %&gt;%\n    kable(caption = \"Wellbeing, Social Interactions, and Outdoor Time Descriptive Statistics\", align = \"c\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 1: Wellbeing, Social Interactions, and Outdoor Time Descriptive Statistics\n\nM_Wellbeing\nSD_Wellbeing\nM_OutTime\nSD_OutTime\nM_SocInt\nSD_SocInt\n\n\n36.3\n5.39\n18.25\n7.1\n12.06\n4.02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe marginal distribution of scores on the WEMWBS was unimodal with a mean of approximately 36.3. There was variation in WEMWBS scores (SD = 5.39)\n\nThe marginal distribution of weekly hours spent outdoors was unimodal with a mean of approximately 18.25. There was variation in weekly hours spent outdoors (SD = 7.1)\nThe marginal distribution of numbers of social interactions per week was unimodal with a mean of approximately 12.06. There was variation in numbers of social interactions (SD = 4.02)\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nProduce plots of the associations between the outcome variable (wellbeing) and each of the explanatory variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot interpretation\n- Direction of association\n- Form of association (can it be summarised well with a straight line?)\n- Strength of association (how closely do points fall to a recognizable pattern such as a line?)\n- Unusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail\nPlot tips\n- use \\n to wrap text in your titles and or axis labels\n- consider using geom_smooth() to superimpose the best-fitting line describing the association of interest\n\n\n\n\n\n\n\n Solution \n\n\n\nwellbeing_outdoor &lt;- \n  ggplot(data = mwdata, aes(x = outdoor_time, y = wellbeing)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Time spent outdoors \\nper week (hours)\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_social &lt;- \n  ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Number of social interactions \\nper week\", y = \"Wellbeing score (WEMWBS)\")\n\n# place plots adjacent to one another\nwellbeing_outdoor | wellbeing_social\n\n\n\nFigure 2: Scatterplots displaying the relationships between scores on the WEMWBS and a) weekly outdoor time (hours), and b) weekly number of social interactions\n\n\n\nBoth scatterplots indicated weak, positive, and linear associations both between wellbeing and outdoor time, and between wellbeing and the number of weekly social interactions.\n\n\n\n\n\nQuestion 4\n\n\nProduce a correlation matrix of the variables which are to be used in the analysis, and write a short paragraph describing the associations.\n\n\n\n\n\n\nHint\n\n\n\n\n\nCorrelation Matrix\nReview Q2 of your Week 1 lab for guidance on how to produce a correlation matrix.\nAPA Format\nMake sure to round your numbers in-line with APA 7th edition guidelines as noted at the start of the lab (see ‘Presenting Results’). The round() function will come in handy here.\n\n\n\n\n\n\n\n Solution \n\n\nWe can either index the dataframe or select the variables of interest:\n\n\nIndex dataframe ([])\nVariable selection (select())\n\n\n\n\n# correlation matrix of the three columns of interest (check which columns we need - in this case, 2,3, and 5)\nround(cor(mwdata[,c(5,3,2)]), digits = 2)\n\n             wellbeing social_int outdoor_time\nwellbeing         1.00       0.24         0.25\nsocial_int        0.24       1.00        -0.04\noutdoor_time      0.25      -0.04         1.00\n\n\n\n\n\n# select only the columns we want by variable name, and pass this to cor()\nmwdata %&gt;% \n  select(wellbeing, social_int, outdoor_time) %&gt;%\n  cor() %&gt;%\n    round(digits = 2)\n\n             wellbeing social_int outdoor_time\nwellbeing         1.00       0.24         0.25\nsocial_int        0.24       1.00        -0.04\noutdoor_time      0.25      -0.04         1.00\n\n\n\n\n\n\n\n\n\n\n\n\nThere was a weak, positive, linear association between WEMWBS scores and weekly outdoor time for the participants in the sample (\\(r\\) = .25). Higher number of hours spent outdoors each week was associated, on average, with higher wellbeing scores\n\nThere was a weak, positive, linear association between WEMWBS scores and the weekly number of social interactions for the participants in the sample (\\(r\\) = .24). More social interactions were associated, on average, with higher wellbeing scores\nThere was a negligible negative correlation between weekly outdoor time and the weekly number of social interactions (\\(r\\) = -.04)"
  },
  {
    "objectID": "1_03_mlr_stz.html",
    "href": "1_03_mlr_stz.html",
    "title": "Multiple Linear Regression & Standardization",
    "section": "",
    "text": "At the end of this lab, you will:\n\nExtend the ideas of single linear regression to consider regression models with two or more predictors\nUnderstand how to interpret significance tests for \\(\\beta\\) coefficients\nUnderstand how to standardize model coefficients and when this is appropriate to do\nUnderstand how to interpret standardized model coefficients in multiple linear regression models\n\n\nBe up to date with lectures\nHave completed Week 1 and Week 2 lab exercises\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\nsjPlot\nppcor\nkableExtra\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv."
  },
  {
    "objectID": "1_03_mlr_stz.html#lab-2-recap",
    "href": "1_03_mlr_stz.html#lab-2-recap",
    "title": "Multiple Linear Regression & Standardization",
    "section": "Lab 2 Recap",
    "text": "Lab 2 Recap\n\nQuestion 1\n\n\nFit the following multiple linear regression model, and assign the output to an object called mdl, and examine the summary output.\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time\n\\]\n\n\n\n\n Solution \n\n\n\nmdl &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06"
  },
  {
    "objectID": "1_03_mlr_stz.html#writing-up-presenting-results",
    "href": "1_03_mlr_stz.html#writing-up-presenting-results",
    "title": "Multiple Linear Regression & Standardization",
    "section": "Writing Up & Presenting Results",
    "text": "Writing Up & Presenting Results\n\nQuestion 9\n\n\nProvide key model results from the standardized model in a formatted table.\n\n\n\n\n Solution \n\n\n\ntab_model(mdl_z,\n          dv.labels = \"Wellbeing (WEMWBS Scores)\",\n          pred.labels = c(\"z_social_int\" = \"Social Interactions (number per week)\",\n                          \"z_outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression table for Wellbeing model. Outcome variable and predictors are Z-scored\")\n\n\n\nTable 1: Regression Table for Wellbeing Model\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-0.00\n-0.13 – 0.13\n1.000\n\n\nSocial Interactions\n(number per week)\n0.25\n0.12 – 0.38\n&lt;0.001\n\n\nOutdoor Time (hours per\nweek)\n0.26\n0.13 – 0.39\n&lt;0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.126 / 0.118\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret the results from the standardized model the context of the research question.\nMake reference to the your regression table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember to inform the reader of the scale of your variables.\n\n\n\n\n\n\n\n Solution \n\n\nA multiple regression model was used to determine if there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. All variables (wellbeing, social interactions, and outdoor time) were \\(z\\)-scored. As presented in Table 1, outdoor time was significantly associated with wellbeing scores \\((\\beta = 0.26, SE = 0.07, p &lt; .001)\\) after controlling for the number of weekly social interactions. For every standard deviation increase in outdoor time, wellbeing scores increased on average by 0.26 standard deviations. Therefore, we should reject the null hypothesis since \\(p &lt; .05\\)."
  },
  {
    "objectID": "1_04_model_fit_comp.html",
    "href": "1_04_model_fit_comp.html",
    "title": "Model Fit and Comparison",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand how to calculate the interpret \\(R^2\\) and adjusted-\\(R^2\\) as a measure of model quality.\nUnderstand the calculation and interpretation of the \\(F\\)-test of model utility.\nUnderstand measures of model fit using F.\n\nUnderstand the principles of model selection and how to compare models via F tests.\nUnderstand AIC and BIC.\n\n\nBe up to date with lectures\nHave completed previous lab exercises from Week 1, Week 2, and Week 3\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\nkableExtra\n\nAll results should be presented following APA guidelines. If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv."
  },
  {
    "objectID": "1_04_model_fit_comp.html#lab-purpose",
    "href": "1_04_model_fit_comp.html#lab-purpose",
    "title": "Model Fit and Comparison",
    "section": "Lab Purpose",
    "text": "Lab Purpose\nIn this lab (Lab 4), you will focus on the statistics contained within the highlighted sections of the summary() output below. You will be both calculating these by hand and deriving via R code before interpreting these values in the context of the research question following APA guidelines.\n\n\nQuestion 2\n\n\nWhat is the proportion of the total variability in wellbeing scores explained by the model?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe question asks to compute the value of \\(R^2\\). Since the model includes 2 predictors, you should report the Adjusted \\(R^2\\).\n\n\n\n\n\n\n\n Solution \n\n\nThe proportion of the total variability explained is given by R-squared.\nThe R-squared coefficient is defined as: \\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n\\] The Adjusted R-squared coefficient is defined as:\n\\[\n\\hat R^2 = 1 - \\frac{(1 - R^2)(n-1)}{n-k-1}\n\\quad \\\\\n\\begin{align}\n& \\text{Where:} \\\\\n& n = \\text{sample size} \\\\\n& k = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\] ::: {.panel-tabset}\nManually\nIn R we can write:\n\n#R squared & adjusted R squared\n\nwellbeing_fitted &lt;- mwdata %&gt;%\n  mutate(\n    wellbeing_hat = predict(mdl1),\n    resid = wellbeing - wellbeing_hat\n  )\n\n\nwellbeing_fitted %&gt;%\n  summarise(\n    SSModel = sum( (wellbeing_hat - mean(wellbeing))^2 ),\n    SSTotal = sum( (wellbeing - mean(wellbeing))^2 )\n  ) %&gt;%\n  summarise(\n    RSquared = SSModel / SSTotal,\n    AdjRSquared = 1-((1-(RSquared))*(32-1)/(32-2-1))\n  )\n\n# A tibble: 1 × 2\n  RSquared AdjRSquared\n     &lt;dbl&gt;       &lt;dbl&gt;\n1    0.740       0.722\n\n\nR function\n\n#look in second bottom row - Multiple R Squared and Adjusted R Squared both reported here\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\nThe output of summary() displays the Adjusted \\(R\\)-squared value in the following line:\nAdjusted R-squared:  0.7224 \n:::\n\nInterpretation\n\n\n\n\n\n\nApproximately 72% of the total variability in wellbeing scores is explained by associations with social interactions and outdoor time.\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nPerform a model utility test at the 5% significance level and report your results.\nIn other words, conduct an \\(F\\)-test against the null hypothesis that the model is ineffective at predicting wellbeing scores using social interactions and outdoor time by computing the \\(F\\)-statistic using its definition.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe F-ratio is used to test the null hypothesis that all regression slopes are zero.\nIt is called the F-ratio because it is the ratio of the how much of the variation is explained by the model (per paramater) versus how much of the variation is left unexplained in the residuals (per remaining degrees of freedom).\n\\[\nF_{df_{model},df_{residual}} = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\\\\n\\quad \\\\\n\\begin{align}\n& \\text{Where:} \\\\\n& df_{model} = k \\\\\n& df_{residual} = n-k-1 \\\\\n& n = \\text{sample size} \\\\\n& k  = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR function\n\n\n\n\ndf1 &lt;- 2\ndf2 &lt;- nrow(mwdata) - 2 - 1\nf_star &lt;- qf(0.95, df1, df2)\nf_star\n\n[1] 3.327654\n\n\n\nmodel_utility &lt;- wellbeing_fitted %&gt;%\n  summarise(\n    SSModel = sum((wellbeing_hat - mean(wellbeing))^2 ),\n    SSResid = sum( resid^2 ),\n    MSModel = SSModel / df1,\n    MSResid = SSResid / df2,\n    FObs = MSModel / MSResid\n  )\nmodel_utility\n\n# A tibble: 1 × 5\n  SSModel SSResid MSModel MSResid  FObs\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1   3126.   1096.   1563.    37.8  41.3\n\n\nWe can also compute the p-value:\n\npvalue &lt;- 1 - pf(model_utility$FObs, df1, df2)\npvalue\n\n[1] 3.225548e-09\n\n\nThe value 3.225548e-09 simply means \\(3.2 \\times 10^{-9}\\), so it’s a really small number.\n\n\n\n#look in bottom row\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\nThe relevant row is the following:\n\nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\n\n\n\n\n\n\n\n\nWe performed an \\(F\\)-test of model utility at the 5% significance level, where \\(F(2,29) = 41.34, p &lt;.001\\).\nThe large \\(F\\)-statistic and small \\(p\\)-value (\\(&lt;.001\\)) suggested that we have very strong evidence against the null hypothesis that the model is ineffective.\nIn other words, the data provide strong evidence that the number of social interactions and outdoor time are effective predictors of wellbeing scores.\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nProduce a visualisation of the association between wellbeing and outdoor time, after accounting for social interactions.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo visualise just one association, you might need to specify the terms argument in plot_model(). Don’t forget you can look up the documentation by typing ?plot_model in the console.\n\n\n\n\n\n\n\n Solution \n\n\n\nplot_model(mdl1, type = \"eff\",\n           terms = c(\"outdoor_time\"), \n           show.data = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nInterpret the standardized variables presented in the above table.\n\n\n\n\n Solution \n\n\n\nFor every standard deviation increase in social interactions, wellbeing scores increased on average by 0.67 standard deviations.\nFor every standard deviation increase in outdoor time, wellbeing scores increased on average by 0.35 standard deviations.\n\n\n\n\n\n\nQuestion 9\n\n\nCompare the two following models, each looking at the associations of Wellbeing scores and two different predictor variables.\n\\(\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social Interactions} + \\beta_2 \\cdot \\text{Age} + \\epsilon\\)\n\\(\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Outdoor Time} + \\beta_2 \\cdot \\text{Routine} + \\epsilon\\)\nIn APA format, report which model you think best fits the data.\n\n\n\n\n\n\nHint\n\n\n\n\n\nCompare using AIC() and BIC() since the models are non-nested.\n\n\n\n\n\n\n\n Solution \n\n\n\n#fit models\nwb_socint_age &lt;- lm(wellbeing ~ social_int + age, data = mwdata)\nwb_outdoor_routine &lt;- lm(wellbeing ~ outdoor_time + routine, data = mwdata)\n\n\n#AIC values\nAIC(wb_socint_age, wb_outdoor_routine)\n\n#BIC values\nBIC(wb_socint_age, wb_outdoor_routine)\n\n\n\n\n\n\n\nWe used AIC and BIC model selection to distinguish between two possible models describing the association between several personal factors and wellbeing scores. The model with outdoor time and routine included as predictors was better fitting (AIC = 1220.91) the alternative model with weekly number of social interactions and age (AIC = 1236.58). Based on the BIC value of the former model (BIC = 1234.11) we concluded that it was substantively better fitting than the alternative, latter model (BIC = 1249.77).\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nThe code below fits 5 different models based on our wrdata:\n\nmodel1 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nmodel2 &lt;- lm(wellbeing ~ social_int + outdoor_time + age, data = mwdata)\nmodel3 &lt;- lm(wellbeing ~ social_int + outdoor_time + routine, data = mwdata)\nmodel4 &lt;- lm(wellbeing ~ social_int + outdoor_time + routine + age, data = mwdata)\nmodel5 &lt;- lm(wellbeing ~ social_int + outdoor_time + routine + steps_k, data = mwdata)\n\nFor each of the below pairs of models, what methods are/are not available for us to use for comparison and why?\n\n\nmodel1 vs model2\n\n\nmodel2 vs model3\n\n\nmodel1 vs model4\n\n\nmodel3 vs model5\n\n\nThis flowchart might help you to reach your decision:\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou may need to examine the dataset, and check for accuracy (e.g., are there any impossible / out of range values?) and completeness (e.g., are there any missing values?).\n\n\n\n\n\n\n\n Solution \n\n\n\nmodel1 vs model2\nThese models are nested - model2 contains all the variables of model1 and they are fitted on the same dataset.\nWe can therefore use an \\(F\\)-test or AIC and BIC.\nmodel2 vs model3\nThese models are not nested, but they are fitted on the same dataset.\nWe can therefore use AIC or BIC, but we cannot use an \\(F\\)-test.\nmodel1 vs model4 These models are nested - model4 contains all the variables of model1 and they are fitted on the same dataset.\nWe can therefore use an \\(F\\)-test or AIC and BIC.\nmodel3 vs model5\nThese models are not nested, and they are not fitted on the same dataset. The “steps_k” variable contains missing values (over 30% of the data is missing for this variable), and so these whole rows are excluded from model5 (but they are included in model3). We cannot compare these models."
  },
  {
    "objectID": "1_04_model_fit_comp.html#manually",
    "href": "1_04_model_fit_comp.html#manually",
    "title": "Model Fit and Comparison",
    "section": "Manually",
    "text": "Manually\nIn R we can write:\n\n#R squared & adjusted R squared\n\nwellbeing_fitted &lt;- mwdata %&gt;%\n  mutate(\n    wellbeing_hat = predict(mdl),\n    resid = wellbeing - wellbeing_hat\n  )\n\n\nwellbeing_fitted %&gt;%\n  summarise(\n    SSModel = sum( (wellbeing_hat - mean(wellbeing))^2 ),\n    SSTotal = sum( (wellbeing - mean(wellbeing))^2 )\n  ) %&gt;%\n  summarise(\n    RSquared = SSModel / SSTotal,\n    AdjRSquared = 1-((1-(RSquared))*(200-1)/(200-2-1))\n  )\n\n# A tibble: 1 × 2\n  RSquared AdjRSquared\n     &lt;dbl&gt;       &lt;dbl&gt;\n1    0.126       0.118\n\n\nThe output displays the Adjusted \\(R\\)-squared value in the following column:\nAdjRSquared\n &lt;dbl&gt;\n 0.118"
  },
  {
    "objectID": "1_04_model_fit_comp.html#r-function",
    "href": "1_04_model_fit_comp.html#r-function",
    "title": "Model Fit and Comparison",
    "section": "R function",
    "text": "R function\n\n#look in second bottom row - Multiple R Squared and Adjusted R Squared both reported here\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe output of summary() displays the Adjusted \\(R\\)-squared value in the following line:\nAdjusted R-squared:  0.1176 \n:::\n\nInterpretation\n\n\n\n\n\n\nApproximately 12% of the total variability in wellbeing scores is accounted for by social interactions and outdoor time."
  },
  {
    "objectID": "1_03_mlr_stz.html#standardization",
    "href": "1_03_mlr_stz.html#standardization",
    "title": "Multiple Linear Regression & Standardization",
    "section": "Standardization",
    "text": "Standardization\n\nQuestion 4\n\n\nFit two regression models using the standardized response and explanatory variables. Fit one using z-scored variables, and the other using the scale() function.\n\n\n\n\n\n\nHint\n\n\n\n\n\nZ-Score\nAdd to the “mwdata” dataset three variables called z_wellbeing, z_social_int, and z_outdoor_time representing the standardized welllbeing, social interactions and outdoor time variables, respectively.\nRecall the formula for the \\(z\\)-score: \\[\nz_x = \\frac{x - \\bar{x}}{s_x}, \\qquad z_y = \\frac{y - \\bar{y}}{s_y}\n\\]\nscale()\nUse the scale() function when specifying your lm() statement.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nZ-Score\nscale() function\n\n\n\nz-score variables:\n\nmwdata &lt;- mwdata %&gt;%\n  mutate(\n    z_wellbeing = (wellbeing - mean(wellbeing)) / sd(wellbeing),\n    z_social_int = (social_int - mean(social_int)) / sd(social_int),\n    z_outdoor_time = (outdoor_time - mean(outdoor_time)) / sd(outdoor_time)\n  )\n\nCheck that they are standardized:\n\nmwdata %&gt;%\n  summarise(\n    M_z_wellbeing = round(mean(z_wellbeing),2), SD_z_wellbeing = sd(z_wellbeing), \n    M_z_social_int = round(mean(z_social_int),2), SD_z_social_int = sd(z_social_int),\n    M_z_outdoor_time = round(mean(z_outdoor_time),2), SD_z_outdoor_time = sd(z_outdoor_time)\n  )\n\n# A tibble: 1 × 6\n  M_z_wellbeing SD_z_wellbeing M_z_social_int SD_z_social_int M_z_outdoor_time\n          &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1             0              1              0               1                0\n# ℹ 1 more variable: SD_z_outdoor_time &lt;dbl&gt;\n\n#mean of 0, SD of 1 - all good to go\n\nRun model:\n\nmdl_z &lt;- lm(z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\n\n\n\n\nmdl_s &lt;- lm(scale(wellbeing) ~ scale(social_int) + scale(outdoor_time), data = mwdata)\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nExamine the estimates from both standardized models - what do you notice?\n\n\n\n\n Solution \n\n\n\n\nZ-Score\nscale() function\n\n\n\n\nsummary(mdl_z)\n\n\nCall:\nlm(formula = z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9231 -0.5806 -0.0781  0.6144  3.4942 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -4.168e-16  6.642e-02   0.000 1.000000    \nz_social_int    2.499e-01  6.663e-02   3.751 0.000232 ***\nz_outdoor_time  2.622e-01  6.663e-02   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9394 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\nround(summary(mdl_z)$coefficients, 2)\n\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)        0.00       0.07    0.00        1\nz_social_int       0.25       0.07    3.75        0\nz_outdoor_time     0.26       0.07    3.93        0\n\n\n\n\n\nsummary(mdl_s)\n\n\nCall:\nlm(formula = scale(wellbeing) ~ scale(social_int) + scale(outdoor_time), \n    data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9231 -0.5806 -0.0781  0.6144  3.4942 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -4.106e-16  6.642e-02   0.000 1.000000    \nscale(social_int)    2.499e-01  6.663e-02   3.751 0.000232 ***\nscale(outdoor_time)  2.622e-01  6.663e-02   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9394 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\nround(summary(mdl_s)$coefficients, 2)\n\n                    Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)             0.00       0.07    0.00        1\nscale(social_int)       0.25       0.07    3.75        0\nscale(outdoor_time)     0.26       0.07    3.93        0\n\n\n\n\n\nFrom comparing either the summary() or rounded output, we can see that the estimates are the same under both approaches. That means you can use either approach to standardize the variables in your model.\n\n\n\n\n\nQuestion 6\n\n\nExamine the ‘Coefficients’ section of the summary() output from the standardized and unstandardized models - what do you notice? In other words, what is the same / different?\n\n\n\n\n Solution \n\n\n\n\nUnstandardized\nStandardized\n\n\n\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\nround(summary(mdl)$coefficients, 2)\n\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     28.62       1.49   19.24        0\nsocial_int       0.33       0.09    3.75        0\noutdoor_time     0.20       0.05    3.93        0\n\n\n\n\n\nsummary(mdl_z)\n\n\nCall:\nlm(formula = z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9231 -0.5806 -0.0781  0.6144  3.4942 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -4.168e-16  6.642e-02   0.000 1.000000    \nz_social_int    2.499e-01  6.663e-02   3.751 0.000232 ***\nz_outdoor_time  2.622e-01  6.663e-02   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9394 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\nround(summary(mdl_z)$coefficients, 2)\n\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)        0.00       0.07    0.00        1\nz_social_int       0.25       0.07    3.75        0\nz_outdoor_time     0.26       0.07    3.93        0\n\n\n\n\n\nSimilarities\n+ The \\(t\\) and \\(p\\)-values for the two predictor variables in both models are the same. This is because the significance of these values remains the same for the standardized coefficients as for unstandardised coefficients\nDifferences\n+ The estimates and standard errors for the intercept and both predictor variables are different under the unstandardized and standardized models\n+ The \\(t\\) and \\(p\\)-values are different in each model for the intercept. This is because in the standardized model, the intercept is not significantly different from 0 (it is 0!), and hence has a \\(p\\)-value of 1. In the unstandardized model, the intercept is significantly different from 0 (it is 28.62), and hence has a very small \\(p\\)-value (&lt; .001)\n\n\n\n\n\nQuestion 7\n\n\nHow do these standardized estimates relate to the semi-partial (part) correlation coefficients?\nProduce a visualisation of the association between wellbeing and outdoor time, after accounting for social interactions.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSemi-partial (part) correlation coefficient\nTo calculate semi-partial correlation coefficients, you will need to use the spcor.test() from the ppcor package.\nRecall that you can look at the estimates from either either ‘mdl_s’ or ‘mdl_z’ - they are contain the same standardized model estimates.\nPlotting\nTo visualise just one association, you need to specify the terms argument in plot_model(). Don’t forget you can look up the documentation by typing ?plot_model in the console.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nSemi-partial (part) correlation coefficient\nVisualisation\n\n\n\nFirst, lets recall the estimates from our standardized model (rounding to 2 decimal places):\n\nround(mdl_z$coefficients, 2)\n\n   (Intercept)   z_social_int z_outdoor_time \n          0.00           0.25           0.26 \n\n\nNext, lets calculate the semi-partial correlation coefficients:\n\n#semi-partial (part) correlation between wellbeing & social interactions\nwb_soc &lt;- spcor.test(mwdata$wellbeing, mwdata$social_int, mwdata$outdoor_time,  method=\"pearson\")\n#round correlation coefficient estimate to 2 decimal places\nround(wb_soc$estimate, 2)\n\n[1] 0.25\n\n#semi-partial (part) correlation between wellbeing & outdoor time\nwb_out &lt;- spcor.test(mwdata$wellbeing, mwdata$outdoor_time, mwdata$social_int, method=\"pearson\")\n#round correlation coefficient estimate to 2 decimal places\nround(wb_out$estimate, 2)\n\n[1] 0.26\n\n\nWe can see that the slope estimates from the standardized model are equivalent to the semi-partial (part) correlation coefficients.\n\n\n\nplot_model(mdl_z, type = \"eff\",\n           terms = c(\"z_outdoor_time\"), \n           show.data = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nPlot the data and the fitted regression line from both the unstandardized and standardized models. To do so, for each model:\n\nExtract the estimated regression coefficients e.g., via betas &lt;- coef(mdl)\n\nExtract the first entry of betas (i.e., the intercept) via betas[1]\n\nExtract the second entry of betas (i.e., the slope) via betas[2]\n\nProvide the intercept and slope to the function\n\nNote down what you observe from the plots - what is the same / different?\n\n\n\n\n\n\nHint\n\n\n\n\n\nExtracting values\nThe function coef() returns a vector (a sequence of numbers all of the same type). To get the first element of the sequence you append [1], and [2] for the second.\nPlotting\nIn your ggplot(), you will need to specify geom_abline(). This might help get you started:\n\ngeom_abline(intercept = &lt;intercept&gt;, slope = &lt;slope&gt;)\n\nYou may also want to plot these side by side to more easily compare, so consider using | from patchwork.\n\n\n\n\n\n\n\n Solution \n\n\nFirst extract the values required for both non-standardized and standardized models:\n\n#non-standardized (from 'mdl')\nbetas &lt;- coef(mdl)\nintercept &lt;- betas[1]\nslope &lt;- betas[2]\n\n#standardized (from 'mdl_z')\nbetas_z &lt;- coef(mdl_z)\nintercept_z &lt;- betas_z[1]\nslope_z &lt;- betas_z[2]\n\nWe can plot the models as follows:\n\np1 &lt;- ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_abline(intercept = intercept, slope = slope, color = 'blue') + \n  labs(x = \"Social Interactions \\n(Number per Week)\", y = \"Wellbeing (WEMWBS) Scores\")\n\np2 &lt;- ggplot(data = mwdata, aes(x = z_social_int, y = z_wellbeing)) +\n  geom_point() +\n  geom_abline(intercept = intercept_z, slope = slope_z, color = 'red') + \n  labs(x = \"Social Interactions \\n(Number per Week; Z-Scored)\", y = \"Wellbeing (WEMWBS) Scores; Z-Scored\")\n\np1 | p2\n\n\n\n\n\n\n\nSimilarities\n- The data points are distributed in the same pattern\n- The slope of the line follows the same gradient\nDifferences\n- The x- and y-axis scales are different for each plot:\n- The unstandardized is in the original units where we interpret the slope as the change in \\(y\\) units for a unit change in \\(x\\)\n- The standardized is in SD units where we interpret the slope as the SD change in \\(y\\) for 1 SD change in \\(x\\)"
  },
  {
    "objectID": "1_03_mlr_stz.html#significance-tests-for-beta-coefficients",
    "href": "1_03_mlr_stz.html#significance-tests-for-beta-coefficients",
    "title": "Multiple Linear Regression & Standardization",
    "section": "Significance Tests for \\(\\beta\\) Coefficients",
    "text": "Significance Tests for \\(\\beta\\) Coefficients\n\nQuestion 2\n\n\nTest the hypothesis that the population slope for outdoor time is zero — that is, that there is no linear association between wellbeing and outdoor time (after controlling for the number of social interactions) in the population.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRecall the formula for obtaining a test statistic:\nA test statistic for the null hypothesis \\(H_0: \\beta_j = 0\\) is \\[\nt = \\frac{\\hat \\beta_j - 0}{SE(\\hat \\beta_j)}\n\\] which follows a \\(t\\)-distribution with \\(n-k-1\\) degrees of freedom.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR function\n\n\n\nWe calculate the test statistic for \\(\\beta_2\\) \\[\nt = \\frac{\\hat \\beta_2 - 0}{SE(\\hat \\beta_2)} = \\frac{0.19909 - 0}{0.05060} = 3.934585\n\\]\nand compare it with the 5% critical value from a \\(t\\)-distribution with \\(n-3\\) degrees of freedom (since \\(k = 2\\)), which is:\n\nn &lt;- nrow(mwdata)\nk &lt;- 2\ntstar &lt;- qt(0.975, df = n - k - 1)\ntstar\n\n[1] 1.972079\n\n#tstar = 1.972079\n\nAs \\(|t|\\) (\\(|t|\\) = 3.93) is much larger than \\(t^*\\) (\\(t^*\\) = 1.97), we can reject then null hypothesis as we have strong evidence against it.\nThe \\(p\\)-value, shown below, also confirms this conclusion.\n\n2 * (1 - pt(3.934585, n - 3))\n\n[1] 0.0001154709\n\n\n\n\nPlease note that the same information was already contained in the row corresponding to the variable “outdoor_time” in the output of summary(mdl), which reported the \\(t\\)-statistic under t value and the \\(p\\)-value under Pr(&gt;|t|):\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe result is exactly the same (up to rounding errors) as calculating manually.\nBefore we interpret the results, note that sometimes \\(p\\)-values will be reported to \\(e^X\\). For example, look in the Pr(&gt;|t|) column for “(Intercept)”. The value \\(2e^{-16}\\) simply means \\(2 \\times 10^{-16}\\). This is a very small value (i.e., 0.0000000000000002), hence we would simply report it as &lt;.001 following the APA guidelines.\n\n\n\n\n\n\n\n\n\nWe performed a \\(t\\)-test against the null hypothesis that outdoor time was not associated with wellbeing scores after controlling for social interactions. A significant association was found between outdoor time (hours per week) and wellbeing (WEMWBS scores) \\(t(197) = 3.94,\\ p &lt; .001\\), two-sided. Thus, we have evidence to reject the null hypothesis.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nObtain 95% confidence intervals for the regression coefficients, and write a sentence about each one.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRecall the formula for obtaining a confidence interval:\nA confidence interval for the population slope is \\[\n\\hat \\beta_j \\pm t^* \\cdot SE(\\hat \\beta_j)\n\\] where \\(t^*\\) denotes the critical value chosen from t-distribution with \\(n-k-1\\) degrees of freedom for a desired \\(\\alpha\\) level of confidence.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR function\n\n\n\nFor 95% confidence we have \\(t^* = 1.97\\):\n\nn &lt;- nrow(mwdata)\nk &lt;- 2\ntstar &lt;- qt(0.975, df = n - k - 1)\ntstar\n\n[1] 1.972079\n\n\nThe confidence intervals are:\n\ntibble(\n  b0_LowerCI = round(28.62018 - (qt(0.975, n-3) * 1.48786), 3),\n  b0_UpperCI = round(28.62018 + (qt(0.975, n-3)* 1.48786), 3),\n  b1_LowerCI = round(0.33488 - (qt(0.975, n-3) * 0.08929), 3),\n  b1_UpperCI = round(0.33488 + (qt(0.975, n-3)* 0.08929), 3),\n  b2_LowerCI = round(0.19909 - (qt(0.975, n-3) * 0.05060), 3),\n  b2_UpperCI = round(0.19909 + (qt(0.975, n-3)* 0.05060), 3)\n      )\n\n# A tibble: 1 × 6\n  b0_LowerCI b0_UpperCI b1_LowerCI b1_UpperCI b2_LowerCI b2_UpperCI\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1       25.7       31.6      0.159      0.511      0.099      0.299\n\n\n\n\nWe can much more easily obtain the confidence intervals for the regression coefficients using the command confint():\n\nconfint(mdl, level = 0.95)\n\n                   2.5 %     97.5 %\n(Intercept)  25.68600170 31.5543598\nsocial_int    0.15880045  0.5109638\noutdoor_time  0.09931273  0.2988759\n\n\nThe result is exactly the same (up to rounding errors) as calculating manually.\n\n\n\n\n\n\n\n\n\n\nThe average wellbeing score for all those with zero hours of outdoor time and zero social interactions per week was between 25.69 and 31.55.\n\nWhen holding weekly outdoor time constant, each increase of one social interaction per week was associated with a difference in wellbeing scores between 0.16 and 0.51, on average.\n\nWhen holding the number of social interactions per week constant, each one hour increase in weekly outdoor time was associated with a difference in wellbeing scores between 0.1 and 0.3, on average."
  },
  {
    "objectID": "1_04_model_fit_comp.html#lab-2-recap",
    "href": "1_04_model_fit_comp.html#lab-2-recap",
    "title": "Model Fit and Comparison",
    "section": "Lab 2 Recap",
    "text": "Lab 2 Recap\n\nQuestion 1\n\n\nFit the following multiple linear regression model, and assign the output to an object called mdl, and examine the summary output.\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot Social Interactions + \\beta_2 \\cdot Outdoor Time\n\\]\n\n\n\n\n Solution \n\n\n\nmdl &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06"
  },
  {
    "objectID": "1_04_model_fit_comp.html#model-fit",
    "href": "1_04_model_fit_comp.html#model-fit",
    "title": "Model Fit and Comparison",
    "section": "Model Fit",
    "text": "Model Fit\n\nQuestion 2\n\n\nWhat is the proportion of the total variability in wellbeing scores explained by the model?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe question asks to compute the value of \\(R^2\\). Since the model includes 2 predictors, you should report the Adjusted \\(R^2\\).\n\n\n\n\n\n\n\n Solution \n\n\nThe proportion of the total variability explained is given by R-squared.\nThe R-squared coefficient is defined as: \\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n\\] The Adjusted R-squared coefficient is defined as:\n\\[\n\\hat R^2 = 1 - \\frac{(1 - R^2)(n-1)}{n-k-1}\n\\quad \\\\\n\\begin{align}\n& \\text{Where:} \\\\\n& n = \\text{sample size} \\\\\n& k = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\] \n\n\nManually\nR function\n\n\n\nIn R we can write:\n\n#R squared & adjusted R squared\n\nwellbeing_fitted &lt;- mwdata %&gt;%\n  mutate(\n    wellbeing_hat = predict(mdl),\n    resid = wellbeing - wellbeing_hat\n  )\n\n\nwellbeing_fitted %&gt;%\n  summarise(\n    SSModel = sum( (wellbeing_hat - mean(wellbeing))^2 ),\n    SSTotal = sum( (wellbeing - mean(wellbeing))^2 )\n  ) %&gt;%\n  summarise(\n    RSquared = SSModel / SSTotal,\n    AdjRSquared = 1-((1-(RSquared))*(200-1)/(200-2-1))\n  )\n\n# A tibble: 1 × 2\n  RSquared AdjRSquared\n     &lt;dbl&gt;       &lt;dbl&gt;\n1    0.126       0.118\n\n\nThe output displays the Adjusted \\(R\\)-squared value in the following column:\nAdjRSquared\n &lt;dbl&gt;\n 0.118\n\n\n\n#look in second bottom row - Multiple R Squared and Adjusted R Squared both reported here\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe output of summary() displays the Adjusted \\(R\\)-squared value in the following line:\nAdjusted R-squared:  0.1176 \n\n\n\n\n\n\n\n\n\nApproximately 12% of the total variability in wellbeing scores is accounted for by social interactions and outdoor time.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nWhat do you notice about the unadjusted and adjusted R-squared values?\n\n\n\n\n\n\nHint\n\n\n\n\n\nAre they similar or quite different? Why might this be?\n\n\n\n\n\n\n\n Solution \n\n\nThe values of the unadjusted (0.1265) and adjusted R-squared (0.1176) values are quite similar. This is because the sample size is quite large \\((n = 200)\\), and the number of predictors \\((k = 2)\\) is small.\n\n\n\n\n\nQuestion 4\n\n\nPerform a model utility test at the 5% significance level and report your results.\nIn other words, conduct an \\(F\\)-test against the null hypothesis that the model is ineffective at predicting wellbeing scores using social interactions and outdoor time by computing the \\(F\\)-statistic using its definition.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe F-ratio is used to test the null hypothesis that all regression slopes are zero.\nIt is called the F-ratio because it is the ratio of the how much of the variation is explained by the model (per paramater) versus how much of the variation is left unexplained in the residuals (per remaining degrees of freedom).\n\\[\nF_{df_{model},df_{residual}} = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\\\\n\\quad \\\\\n\\begin{align}\n& \\text{Where:} \\\\\n& df_{model} = k \\\\\n& df_{residual} = n-k-1 \\\\\n& n = \\text{sample size} \\\\\n& k  = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR function\n\n\n\n\ndf1 &lt;- 2\ndf2 &lt;- nrow(mwdata) - 2 - 1\nf_star &lt;- qf(0.95, df1, df2)\nf_star\n\n[1] 3.041753\n\n\n\nmodel_utility &lt;- wellbeing_fitted %&gt;%\n  summarise(\n    SSModel = sum((wellbeing_hat - mean(wellbeing))^2 ),\n    SSResid = sum( resid^2 ),\n    MSModel = SSModel / df1,\n    MSResid = SSResid / df2,\n    FObs = MSModel / MSResid\n  )\nmodel_utility\n\n# A tibble: 1 × 5\n  SSModel SSResid MSModel MSResid  FObs\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    732.   5054.    366.    25.7  14.3\n\n\nWe can also compute the p-value:\n\npvalue &lt;- 1 - pf(model_utility$FObs, df1, df2)\npvalue\n\n[1] 1.643779e-06\n\n\nThe value 1.643779e-06 simply means \\(1.6 \\times 10^{-6}\\), so it’s a really small number.\n\n\n\n#look in bottom row\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe relevant row is the following:\n\nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\n\n\n\n\n\nWe performed an \\(F\\)-test of model utility at the 5% significance level, where \\(F(2,197) = 14.26, p &lt;.001\\).\nThe large \\(F\\)-statistic and small \\(p\\)-value (\\(&lt;.001\\)) suggested that we have very strong evidence against the null hypothesis that the model is ineffective.\nIn other words, the data provide strong evidence that the number of social interactions and outdoor time are effective predictors of wellbeing scores."
  },
  {
    "objectID": "1_04_model_fit_comp.html#model-comparisons",
    "href": "1_04_model_fit_comp.html#model-comparisons",
    "title": "Model Fit and Comparison",
    "section": "Model Comparisons",
    "text": "Model Comparisons\nIn this second section of this lab, you will focus on model comparison where you will formally test a number of research questions:\n\n\nRQ1: Is there an overall effect of the number of social interactions on wellbeing scores?\nRQ2: Does weekly outdoor time explain a significant amount of variance in wellbeing scores over and above social interactions?\n\n\n\nQuestion 5\n\n\nFit the below 3 models required to address the three research questions stated above. Note down which model(s) will be used to address each research question, and examine the results of each model.\nName the models as follows: “wb_mdl0”, “wb_mdl1”, “wb_mdl2”\n\\[\n\\text{Wellbeing} = \\beta_0  + \\epsilon\n\\]\n\n\\[\n\\text{Wellbeing} = \\beta_0  + \\beta_1 \\cdot Social~Interactions + \\epsilon\n\\] \\[\n\\text{Wellbeing} = \\beta_0  + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time + \\epsilon\n\\] \n\n\n\n\n\n\nHint\n\n\n\n\n\nThe summary() function will be useful to examine the model output.\n\n\n\n\n\n\n\n Solution \n\n\n\n#null/intercept only model\nwb_mdl0 &lt;- lm(wellbeing ~ 1, data = mwdata)\nsummary(wb_mdl0)\n\n\nCall:\nlm(formula = wellbeing ~ 1, data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.295  -3.295  -1.295   3.705  22.705 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.2950     0.3813   95.19   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 199 degrees of freedom\n\n#model with social interactions\nwb_mdl1 &lt;- lm(wellbeing ~ social_int, data = mwdata)\nsummary(wb_mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5628  -3.2741  -0.7908   3.3703  20.4706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 32.40771    1.17532  27.573  &lt; 2e-16 ***\nsocial_int   0.32220    0.09243   3.486 0.000605 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.247 on 198 degrees of freedom\nMultiple R-squared:  0.05781,   Adjusted R-squared:  0.05306 \nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n\n#model with social interactions and outdoor time\nwb_mdl2 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(wb_mdl2)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe models required to address each research question (RQ) are as follows:\n\nRQ1: Models wb_mdl0 and wb_mdl1\nRQ2: Models wb_mdl1 and wb_mdl2\n\n\n\n\n\n\nQuestion 6\n\n\nIs there a main effect of the number of weekly social interactions? Check that the \\(F\\)-statistic and the \\(p\\)-value are the the same as that which is given at the bottom of summary(wb_mdl1).\nProvide the key model results from the two models in a single formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the anova() function to perform a model comparison between your model with social interactions (wb_mdl1) to the null model (wb_mdl0).\nRemember that the null model tests the null hypothesis that all beta coefficients are zero. By comparing wb_mdl0 to wb_mdl1, we can test whether we should include the IV of ‘social_int’.\n\n\n\n\n\n\n\n Solution \n\n\n\n# model comparison wb_mdl0 vs wb_mdl1\nanova(wb_mdl0, wb_mdl1)\n\nAnalysis of Variance Table\n\nModel 1: wellbeing ~ 1\nModel 2: wellbeing ~ social_int\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    199 5785.6                                 \n2    198 5451.1  1    334.49 12.15 0.0006045 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# extract f statistic from summary of wb_mdl1\nsummary(wb_mdl1)$fstatistic\n\n    value     numdf     dendf \n 12.14975   1.00000 198.00000 \n\n# we can retrieve the p-value:\nfstat = summary(wb_mdl1)$fstatistic[1]\ndf_1 = summary(wb_mdl1)$fstatistic[2]\ndf_2 = summary(wb_mdl1)$fstatistic[3]\npf(fstat, df_1, df_2, lower.tail = FALSE)\n\n       value \n0.0006045334 \n\n\nPresent results from both models:\n\ntab_model(wb_mdl0, wb_mdl1,\n          dv.labels = c(\"Wellbeing (WEMWBS Scores)\", \"Wellbeing (WEMWBS Scores)\"),\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\"),\n          title = \"Regression Table for Wellbeing Models\")\n\n\n\nTable 1: Regression Table for Wellbeing Models\n\n\n\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n36.29\n35.54 – 37.05\n&lt;0.001\n32.41\n30.09 – 34.73\n&lt;0.001\n\n\nSocial Interactions\n(number per week)\n\n\n\n0.32\n0.14 – 0.50\n0.001\n\n\nObservations\n200\n200\n\n\nR2 / R2 adjusted\n0.000 / 0.000\n0.058 / 0.053\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of social interactions was found to explain a significant amount of variance in wellbeing scores (\\(F\\)(1 )=12.15, \\(p\\)&lt;.001). There was a main effect of social interactions. Full results can be seen in Table 1.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nLook at the amount of variation in wellbeing scores explained by models “wb_mdl1” and “wb_mdl2”.\nFrom this, can we answer the second research question of whether weekly outdoor time explains a significant amount of variance in wellbeing scores over and above social interactions?\nProvide justification/rationale for your answer.\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou will need to review the R-Squared and Adjusted R-Squared values.\n\n\n\n\n\n\n\n Solution \n\n\nLet’s look at the amount of variance explained by each model:\n\nsummary(wb_mdl1)$r.squared\n\n[1] 0.0578147\n\nsummary(wb_mdl2)$adj.r.squared\n\n[1] 0.1176021\n\n\nThe model with weekly outdoor time as a predictor explains 12% of the variance, and the model without explains 6%. But, from only looking at the proportion of variance accounted for in each model, we cannot determine which model is statistically a better fit. To answer the question ‘Does including weekly outdoor time as a predictor provide a significantly better fit of the data?’ we need to statistically compare wb_mdl1 to wb_mdl2. To do so, we could use an incremental F-test to compare the models since the models are nested and from the same dataset. We can compare the models and present in a table as below:\n\n\n\n\n\nQuestion 8\n\n\nDoes weekly outdoor time explain a significant amount of variance in wellbeing scores over and above social interactions?\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo address RQ2, you need to statistically compare “wb_mdl1” and “wb_mdl2”\n\n\n\n\n\n\n\n Solution \n\n\nTo statistically compare models, we could use an incremental F-test to compare the models since the models are nested and from the same dataset:\n\nanova(wb_mdl1, wb_mdl2)\n\nAnalysis of Variance Table\n\nModel 1: wellbeing ~ social_int\nModel 2: wellbeing ~ social_int + outdoor_time\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    198 5451.1                                  \n2    197 5053.9  1    397.21 15.483 0.0001153 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPresent results from both models:\n\ntab_model(wb_mdl1, wb_mdl2,\n          dv.labels = c(\"Wellbeing (WEMWBS Scores)\", \"Wellbeing (WEMWBS Scores)\"),\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\",\n                          \"outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression table for Wellbeing model\")\n\n\n\nTable 2: Regression Table for Wellbeing Models\n\n\n\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n32.41\n30.09 – 34.73\n&lt;0.001\n28.62\n25.69 – 31.55\n&lt;0.001\n\n\nSocial Interactions\n(number per week)\n0.32\n0.14 – 0.50\n0.001\n0.33\n0.16 – 0.51\n&lt;0.001\n\n\nOutdoor Time (hours per\nweek)\n\n\n\n0.20\n0.10 – 0.30\n&lt;0.001\n\n\nObservations\n200\n200\n\n\nR2 / R2 adjusted\n0.058 / 0.053\n0.126 / 0.118\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly outdoor time was found to explain a significant amount of variance in wellbeing scores over and above weekly social interactions and location (\\(F\\)(1 )=15.48, \\(p\\)&lt;.001). Full results can be seen in Table 2.\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nCompare the two following models, each looking at the associations of Wellbeing scores and different predictor variables.\n\\(\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social Interactions} + \\beta_2 \\cdot \\text{Age} + \\epsilon\\)\n\\(\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Outdoor Time} + \\epsilon\\)\nIn APA format, report which model you think best fits the data.\n\n\n\n\n\n\nHint\n\n\n\n\n\nCompare using AIC() and BIC() since the models are non-nested.\n\n\n\n\n\n\n\n Solution \n\n\n\n#fit models\nwb_socint_age &lt;- lm(wellbeing ~ social_int + age, data = mwdata)\nwb_outdoor &lt;- lm(wellbeing ~ outdoor_time, data = mwdata)\n\n\n#AIC values\nAIC(wb_socint_age, wb_outdoor)\n\n#BIC values\nBIC(wb_socint_age, wb_outdoor)\n\n\n\n\n\n\n\nWe used AIC and BIC model selection to distinguish between two possible models describing the association between several personal factors and wellbeing scores. The model with outdoor time included as a predictor was better fitting (AIC = 1233.29) the alternative model with weekly number of social interactions and age (AIC = 1236.58). Based on the BIC value of the former model (BIC = 1243.18) we concluded that it was substantively better fitting than the alternative, latter model (BIC = 1249.77).\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nThe code below fits 6 different models based on our wrdata:\n\nmodel1 &lt;- lm(wellbeing ~ social_int, data = mwdata)\nmodel2 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nmodel3 &lt;- lm(wellbeing ~ social_int + age, data = mwdata)\nmodel4 &lt;- lm(wellbeing ~ social_int + outdoor_time + age, data = mwdata)\nmodel5 &lt;- lm(wellbeing ~ social_int + outdoor_time + age + steps_k, data = mwdata)\nmodel6 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = test_data)\n\nFor each of the below pairs of models, what methods are/are not available for us to use for comparison and why?\n\n\nmodel1 vs model2\n\n\nmodel2 vs model3\n\n\nmodel1 vs model4\n\n\nmodel3 vs model5\n\n\nmodel2 vs model6\n\n\nThis flowchart might help you to reach your decision:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou may need to examine the dataset, and check for accuracy (e.g., are there any impossible / out of range values?) and completeness (e.g., are there any missing values?).\n\n\n\n\n\n\n\n Solution \n\n\n\nmodel1 vs model2\nThese models are nested - model2 contains all the variables of model1 and they are fitted on the same dataset.\nWe can therefore use an \\(F\\)-test or AIC and BIC.\nmodel2 vs model3\nThese models are not nested, but they are fitted on the same dataset.\nWe can therefore use AIC or BIC, but we cannot use an \\(F\\)-test.\nmodel1 vs model4 These models are nested - model4 contains all the variables of model1 and they are fitted on the same dataset.\nWe can therefore use an \\(F\\)-test or AIC and BIC.\nmodel3 vs model5\nThese models are not nested, and they are not fitted on the same dataset. The “steps_k” variable contains missing values (over 30% of the data is missing for this variable), and so these whole rows are excluded from model5 (but they are included in model3). We cannot compare these models.\nmodel2 vs model6\nThese models are nested, but they are not fitted on the same dataset. model2 uses the ‘mwdata’ datset, whilst model6 uses the ‘test_data’ dataset. We cannot compare these models."
  },
  {
    "objectID": "1_04_model_fit_comp.html#section-i-model-fit",
    "href": "1_04_model_fit_comp.html#section-i-model-fit",
    "title": "Model Fit and Comparison",
    "section": "Section I: Model Fit",
    "text": "Section I: Model Fit\nIn the first section of this lab, you will focus on the statistics contained within the highlighted sections of the summary() output below. You will be both calculating these by hand and deriving via R code before interpreting these values in the context of the research question.\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nFit the following multiple linear regression model, and assign the output to an object called mdl, and examine the summary output.\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time\n\\]\n\n\n\n\n Solution \n\n\n\nmdl &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\n\n\nQuestion 2\n\n\nWhat is the proportion of the total variability in wellbeing scores explained by the model?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe proportion of the total variability explained is given by \\(R^2\\). Since the model includes 2 predictors, you should report the Adjusted-\\(R^2\\).\nThe \\(R^2\\) coefficient is defined as:\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n\\]\nThe Adjusted-\\(R^2\\) coefficient is defined as:\n\\[\n\\hat R^2 = 1 - \\frac{(1 - R^2)(n-1)}{n-k-1}\n\\quad \\\\\n\\begin{align}\n& \\text{Where:} \\\\\n& n = \\text{sample size} \\\\\n& k = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR function\n\n\n\nIn R we can write:\n\n#R squared & adjusted R squared\n\nwellbeing_fitted &lt;- mwdata %&gt;%\n  mutate(\n    wellbeing_hat = predict(mdl),\n    resid = wellbeing - wellbeing_hat\n  )\n\n\nwellbeing_fitted %&gt;%\n  summarise(\n    SSModel = sum( (wellbeing_hat - mean(wellbeing))^2 ),\n    SSTotal = sum( (wellbeing - mean(wellbeing))^2 )\n  ) %&gt;%\n  summarise(\n    RSquared = SSModel / SSTotal,\n    AdjRSquared = 1-((1-(RSquared))*(200-1)/(200-2-1))\n  )\n\n# A tibble: 1 × 2\n  RSquared AdjRSquared\n     &lt;dbl&gt;       &lt;dbl&gt;\n1    0.126       0.118\n\n\nThe output displays the Adjusted \\(R\\)-squared value in the following column:\nAdjRSquared\n &lt;dbl&gt;\n 0.118\n\n\n\n#look in second bottom row - Multiple R Squared and Adjusted R Squared both reported here\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe output of summary() displays the Adjusted \\(R\\)-squared value in the following line:\nAdjusted R-squared:  0.1176 \n\n\n\n\n\n\n\n\n\nApproximately 12% of the total variability in wellbeing scores is accounted for by social interactions and outdoor time.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nWhat do you notice about the unadjusted and adjusted R-squared values?\n\n\n\n\n\n\nHint\n\n\n\n\n\nAre they similar or quite different? Why might this be?\n\n\n\n\n\n\n\n Solution \n\n\nThe values of the unadjusted (0.1265) and adjusted R-squared (0.1176) values are quite similar. This is because the sample size is quite large \\((n = 200)\\), and the number of predictors \\((k = 2)\\) is small.\n\n\n\n\n\nQuestion 4\n\n\nPerform a model utility test at the 5% significance level and report your results.\nIn other words, conduct an \\(F\\)-test against the null hypothesis that the model is ineffective at predicting wellbeing scores using social interactions and outdoor time by computing the \\(F\\)-statistic using its definition.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe F-ratio is used to test the null hypothesis that all regression slopes are zero.\nIt is called the F-ratio because it is the ratio of the how much of the variation is explained by the model (per paramater) versus how much of the variation is left unexplained in the residuals (per remaining degrees of freedom).\n\\[\nF_{df_{model},df_{residual}} = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\\\\n\\quad \\\\\n\\begin{align}\n& \\text{Where:} \\\\\n& df_{model} = k \\\\\n& df_{residual} = n-k-1 \\\\\n& n = \\text{sample size} \\\\\n& k  = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR function\n\n\n\n\ndf1 &lt;- 2\ndf2 &lt;- nrow(mwdata) - 2 - 1\nf_star &lt;- qf(0.95, df1, df2)\nf_star\n\n[1] 3.041753\n\n\n\nmodel_utility &lt;- wellbeing_fitted %&gt;%\n  summarise(\n    SSModel = sum((wellbeing_hat - mean(wellbeing))^2 ),\n    SSResid = sum( resid^2 ),\n    MSModel = SSModel / df1,\n    MSResid = SSResid / df2,\n    FObs = MSModel / MSResid\n  )\nmodel_utility\n\n# A tibble: 1 × 5\n  SSModel SSResid MSModel MSResid  FObs\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    732.   5054.    366.    25.7  14.3\n\n\nWe can also compute the p-value:\n\npvalue &lt;- 1 - pf(model_utility$FObs, df1, df2)\npvalue\n\n[1] 1.643779e-06\n\n\nThe value 1.643779e-06 simply means \\(1.6 \\times 10^{-6}\\), so it’s a really small number.\n\n\n\n#look in bottom row\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe relevant row is the following:\n\nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\n\n\n\n\n\nWe performed an \\(F\\)-test of model utility at the 5% significance level, where \\(F(2,197) = 14.26, p &lt;.001\\).\nThe large \\(F\\)-statistic and small \\(p\\)-value (\\(&lt;.001\\)) suggested that we have very strong evidence against the null hypothesis that the model is ineffective.\nIn other words, the data provide strong evidence that the number of social interactions and outdoor time are effective predictors of wellbeing scores."
  },
  {
    "objectID": "1_04_model_fit_comp.html#section-ii-model-comparisons",
    "href": "1_04_model_fit_comp.html#section-ii-model-comparisons",
    "title": "Model Fit and Comparison",
    "section": "Section II: Model Comparisons",
    "text": "Section II: Model Comparisons\nIn the second section of this lab, you will focus on model comparison where you will formally test a number of research questions:\n\n\nRQ1: Is there an overall effect of the number of social interactions on wellbeing scores?\nRQ2: Does weekly outdoor time explain a significant amount of variance in wellbeing scores over and above social interactions?\n\n\n\n\nQuestion 5\n\n\nFit the below 3 models required to address the three research questions stated above. Note down which model(s) will be used to address each research question, and examine the results of each model.\nName the models as follows: “wb_mdl0”, “wb_mdl1”, “wb_mdl2”\n\\[\n\\text{Wellbeing} = \\beta_0  + \\epsilon\n\\]\n\n\\[\n\\text{Wellbeing} = \\beta_0  + \\beta_1 \\cdot Social~Interactions + \\epsilon\n\\] \\[\n\\text{Wellbeing} = \\beta_0  + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time + \\epsilon\n\\] \n\n\n\n\n\n\nHint\n\n\n\n\n\nThe summary() function will be useful to examine the model output.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nwb_mdl0\nwb_mdl1\nwb_mdl2\n\n\n\n\n#null/intercept only model\nwb_mdl0 &lt;- lm(wellbeing ~ 1, data = mwdata)\nsummary(wb_mdl0)\n\n\nCall:\nlm(formula = wellbeing ~ 1, data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.295  -3.295  -1.295   3.705  22.705 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.2950     0.3813   95.19   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 199 degrees of freedom\n\n\n\n\n\n#model with social interactions\nwb_mdl1 &lt;- lm(wellbeing ~ social_int, data = mwdata)\nsummary(wb_mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5628  -3.2741  -0.7908   3.3703  20.4706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 32.40771    1.17532  27.573  &lt; 2e-16 ***\nsocial_int   0.32220    0.09243   3.486 0.000605 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.247 on 198 degrees of freedom\nMultiple R-squared:  0.05781,   Adjusted R-squared:  0.05306 \nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n\n\n\n\n\n#model with social interactions and outdoor time\nwb_mdl2 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(wb_mdl2)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\nThe models required to address each research question (RQ) are:\n\nRQ1: Models wb_mdl0 and wb_mdl1\nRQ2: Models wb_mdl1 and wb_mdl2\n\n\n\n\n\n\nQuestion 6\n\n\nIs there a main effect of the number of weekly social interactions? Check that the \\(F\\)-statistic and the \\(p\\)-value are the the same as that which is given at the bottom of summary(wb_mdl1).\nProvide the key model results from the two models in a single formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the anova() function to perform a model comparison between your model with social interactions (wb_mdl1) to the null model (wb_mdl0). Remember that the null model tests the null hypothesis that all beta coefficients are zero. By comparing wb_mdl0 to wb_mdl1, we can test whether we should include the IV of ‘social_int’.\nYou can use KableExtra to present your model comparison results in a well formatted table.\n\n\n\n\n\n\n\n Solution \n\n\nRun model comparison via anova(), and present results in well formatted table:\n\nanova(wb_mdl0, wb_mdl1) %&gt;%\n    kable(caption = \"Model Comparison - wb_mdl0 vs wb_mdl1\", align = \"c\", digits = c(2,2,2,2,2,4)) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 1: Model Comparison - wb_mdl0 vs wb_mdl1\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n199\n5785.6\nNA\nNA\nNA\nNA\n\n\n198\n5451.1\n1\n334.49\n12.15\n6e-04\n\n\n\n\n\n\n\n\nCheck that the \\(F\\)-statistic and the \\(p\\)-value are the the same as that which is given at the bottom of summary(wb_mdl1):\n\n# extract f statistic from summary of wb_mdl1\nsummary(wb_mdl1)$fstatistic\n\n    value     numdf     dendf \n 12.14975   1.00000 198.00000 \n\n# we can retrieve the p-value:\nfstat = summary(wb_mdl1)$fstatistic[1]\ndf_1 = summary(wb_mdl1)$fstatistic[2]\ndf_2 = summary(wb_mdl1)$fstatistic[3]\npf(fstat, df_1, df_2, lower.tail = FALSE)\n\n       value \n0.0006045334 \n\n\nPresent results from both models:\n\ntab_model(wb_mdl0, wb_mdl1,\n          dv.labels = c(\"Wellbeing (WEMWBS Scores)\", \"Wellbeing (WEMWBS Scores)\"),\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\"),\n          title = \"Regression Table for Wellbeing Models\")\n\n\n\nTable 2: Regression Table for Wellbeing Models\n\n\n\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n36.29\n35.54 – 37.05\n&lt;0.001\n32.41\n30.09 – 34.73\n&lt;0.001\n\n\nSocial Interactions\n(number per week)\n\n\n\n0.32\n0.14 – 0.50\n0.001\n\n\nObservations\n200\n200\n\n\nR2 / R2 adjusted\n0.000 / 0.000\n0.058 / 0.053\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of social interactions was found to explain a significant amount of variance in wellbeing scores (\\(F\\)(1 ,198) = 12.15, \\(p\\)&lt;.001). There was a main effect of social interactions. Full regression results are presented in Table 2.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nLook at the amount of variation in wellbeing scores explained by models “wb_mdl1” and “wb_mdl2”.\nFrom this, can we answer the second research question of whether weekly outdoor time explains a significant amount of variance in wellbeing scores over and above social interactions?\nProvide justification/rationale for your answer.\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou will need to review the R-Squared and Adjusted R-Squared values.\n\n\n\n\n\n\n\n Solution \n\n\nLet’s look at the amount of variance explained by each model:\n\nsummary(wb_mdl1)$r.squared\n\n[1] 0.0578147\n\nsummary(wb_mdl2)$adj.r.squared\n\n[1] 0.1176021\n\n\nThe model with weekly outdoor time as a predictor explains 12% of the variance, and the model without explains 6%. But, from only looking at the proportion of variance accounted for in each model, we cannot determine which model is statistically a better fit.\nTo answer the question ‘Does including weekly outdoor time as a predictor provide a significantly better fit of the data?’ we need to statistically compare wb_mdl1 to wb_mdl2.\n\n\n\n\n\nQuestion 8\n\n\nDoes weekly outdoor time explain a significant amount of variance in wellbeing scores over and above social interactions?\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo address RQ2, you need to statistically compare “wb_mdl1” and “wb_mdl2”.\nYou can use KableExtra to present your model comparison results in a well formatted table.\n\n\n\n\n\n\n\n Solution \n\n\nTo statistically compare models, we could use an incremental F-test to compare the models since the models are nested and from the same dataset:\n\nanova(wb_mdl1, wb_mdl2) %&gt;%\n    kable(caption = \"Model Comparison - wb_mdl1 vs wb_mdl2\", align = \"c\", digits = c(2,2,2,2,2,4)) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 3: Model Comparison - wb_mdl1 vs wb_mdl2\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n198\n5451.10\nNA\nNA\nNA\nNA\n\n\n197\n5053.89\n1\n397.21\n15.48\n1e-04\n\n\n\n\n\n\n\n\nPresent results from both models:\n\ntab_model(wb_mdl1, wb_mdl2,\n          dv.labels = c(\"Wellbeing (WEMWBS Scores)\", \"Wellbeing (WEMWBS Scores)\"),\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\",\n                          \"outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression Table for Wellbeing Models wb1 and wb2\")\n\n\n\nTable 4: Regression Table for Wellbeing Models wb1 and wb2\n\n\n\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n32.41\n30.09 – 34.73\n&lt;0.001\n28.62\n25.69 – 31.55\n&lt;0.001\n\n\nSocial Interactions\n(number per week)\n0.32\n0.14 – 0.50\n0.001\n0.33\n0.16 – 0.51\n&lt;0.001\n\n\nOutdoor Time (hours per\nweek)\n\n\n\n0.20\n0.10 – 0.30\n&lt;0.001\n\n\nObservations\n200\n200\n\n\nR2 / R2 adjusted\n0.058 / 0.053\n0.126 / 0.118\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs presented in Table 3, weekly outdoor time was found to explain a significant amount of variance in wellbeing scores over and above weekly social interactions and location (\\(F\\)(1 ,197) = 15.48, \\(p\\)&lt;.001).\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nCompare the two following models, each looking at the associations of Wellbeing scores and different predictor variables.\n\\(\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social~Interactions} + \\beta_2 \\cdot \\text{Age} + \\epsilon\\)\n\\(\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Outdoor~Time} + \\epsilon\\)\nReport which model you think best fits the data, and justify your answer.\n\n\n\n\n\n\nHint\n\n\n\n\n\nCompare using AIC() and BIC() since the models are non-nested.\n\n\n\n\n\n\n\n Solution \n\n\n\n#fit models\nwb_socint_age &lt;- lm(wellbeing ~ social_int + age, data = mwdata)\nwb_outdoor &lt;- lm(wellbeing ~ outdoor_time, data = mwdata)\n\n\n#AIC values\nAIC(wb_socint_age, wb_outdoor)\n\n#BIC values\nBIC(wb_socint_age, wb_outdoor)\n\n\n\n\n\n\n\nWe used AIC and BIC model selection to distinguish between two possible models describing the association between several personal factors and wellbeing scores. The model with outdoor time included as a single predictor was better fitting (AIC = 1233.29) the alternative model with weekly number of social interactions and age (AIC = 1236.58). Based on the BIC value of the former model (BIC = 1243.18) we concluded that it was substantively better fitting than the alternative, latter model (BIC = 1249.77).\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nThe code below fits 6 different models based on our wrdata:\n\nmodel1 &lt;- lm(wellbeing ~ social_int, data = mwdata)\nmodel2 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nmodel3 &lt;- lm(wellbeing ~ social_int + age, data = mwdata)\nmodel4 &lt;- lm(wellbeing ~ social_int + outdoor_time + age, data = mwdata)\nmodel5 &lt;- lm(wellbeing ~ social_int + outdoor_time + age + steps_k, data = mwdata)\nmodel6 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = test_data)\n\nFor each of the below pairs of models, what methods are/are not available for us to use for comparison and why?\n\n\nmodel1 vs model2\n\n\nmodel2 vs model3\n\n\nmodel1 vs model4\n\n\nmodel3 vs model5\n\n\nmodel2 vs model6\n\n\nThis flowchart might help you to reach your decision:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou may need to examine the dataset, and check for accuracy (e.g., are there any impossible / out of range values?) and completeness (e.g., are there any missing values?).\n\n\n\n\n\n\n\n Solution \n\n\n\nmodel1 vs model2\nThese models are nested - model2 contains all the variables of model1 and they are fitted on the same dataset.\nWe can therefore use an \\(F\\)-test or AIC and BIC.\nmodel2 vs model3\nThese models are not nested, but they are fitted on the same dataset.\nWe can therefore use AIC or BIC, but we cannot use an \\(F\\)-test.\nmodel1 vs model4 These models are nested - model4 contains all the variables of model1 and they are fitted on the same dataset.\nWe can therefore use an \\(F\\)-test or AIC and BIC.\nmodel3 vs model5\nThese models are not nested, and they are not fitted on the same dataset. The “steps_k” variable contains missing values (over 30% of the data is missing for this variable), and so these whole rows are excluded from model5 (but they are included in model3). We cannot compare these models.\nmodel2 vs model6\nThese models are nested, but they are not fitted on the same dataset. model2 uses the ‘mwdata’ datset, whilst model6 uses the ‘test_data’ dataset. We cannot compare these models."
  },
  {
    "objectID": "0_01_function.html",
    "href": "0_01_function.html",
    "title": "Functions and Models",
    "section": "",
    "text": "At the end of this lab, you will:\n\nHave reviewed the main concepts from introductory statistics.\nUnderstand the concept of a function.\nBe able to discuss what a statistical model is.\nUnderstand the link between models and functions.\n\n\n\n\n\nHave attended and/or watched Week 1 lectures.\nHave installed R and RStudio on your own computer (unless you have a Chromebook where you may continue to use the PPLS RStudio Server).\n\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nggExtra\nkableExtra\n\n\n\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/handheight.csv."
  },
  {
    "objectID": "0_01_function.html#functions-and-mathematical-models-plots",
    "href": "0_01_function.html#functions-and-mathematical-models-plots",
    "title": "Functions and Models",
    "section": "Functions and Mathematical Models: Plots",
    "text": "Functions and Mathematical Models: Plots\n\nQuestion 4\n\n\nCreate a data set called squares containing the perimeter of four squares having sides of length \\(0, 2, 5, 9\\) metres, and then plot the squares data as points on a scatterplot.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember that to combine multiple numbers together we use the function c().\n\n\n\n\n\n\n\n Solution \n\n\nFirst, let’s make our squares data:\n\nsquares &lt;- tibble(\n  side = c(0, 2, 5, 9), \n  perimeter = 4 * side\n)\n\nsquares\n\n# A tibble: 4 × 2\n   side perimeter\n  &lt;dbl&gt;     &lt;dbl&gt;\n1     0         0\n2     2         8\n3     5        20\n4     9        36\n\n\nNext, lets plot it:\n\nggplot(data = squares, aes(x = side, y = perimeter)) +\n  geom_point() +   #specify `geom_point' as want to create a scatterplot\n  labs(x = 'Side (m)', y = 'Perimeter (m)')    #label x- and y-axis with new and clearer titles\n\n\n\n\nFigure 1: ?(caption)\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nGenerate one hundred data points, and use them to visualise the relationship between side and perimeter of squares. To do so, you need to complete four steps:\n\nCreate a sequence of one hundred side lengths (x) going from 0 to 3 metres.\nCompute the corresponding perimeters (y).\nPlot the side and perimeter data as points on a graph.\nVisualise the functional relationship between side and perimeter of squares. To do so, use the function geom_line() to connect the computed points with lines.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo create a sequence of numbers, we can use the function seq(). We need to include within the seq() argument our from and to values (i.e., the lowest and highest values in the sequence), as well as length.out to specify the length of the sequence (i.e., how many 0-3’s do we want).\nIf you would like to change the colour of the line (step 4), you can specify geom_line(colour = \"insert_colour\")\n\n\n\n\n\n\n\n Solution \n\n\nSteps 1 & 2 - create the side and perimeter data:\n\nsquares_grid &lt;- tibble(\n  side = seq(0, 3, length.out = 100),\n  perimeter = 4 * side)\n\nStep 3 - plot the individual points:\n\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4*Side')\n\n\n\n\n\n\n\n\nStep 4 - visualise the functional relationship by connecting the individual points with a line:\n\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_point() +\n  geom_line(colour = 'blue') +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4*Side')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\nThe function \\(y = 4 \\ x\\) that you plotted above is an example of a function representing a mathematical model.\nWe typically validate a model using experimental data. However, we all know how squares work and that two squares with the same side will have the same perimeter (more on this later).\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nThe Scottish National Gallery kindly provided us with measurements of side and perimeter (in metres) for a sample of 10 square paintings.\nThe data are provided below:\n\nsng &lt;- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nPlot the mathematical model of the relationship between side and perimeter for squares, and superimpose on top the experimental data from the Scottish National Gallery.\n\n\n\n\n Solution \n\n\n\nsng &lt;- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nggplot() + \n  geom_line(data = squares_grid, aes(x = side, y = perimeter), colour = 'blue') + # add blue line\n  geom_point(data = sng, aes(x = side, y = perimeter), colour = 'black',  # add black points\n             alpha = 0.5, size = 3) + # make the points 0.5 transparency, size 3\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\n\nFigure 2: The exact relationship between side and perimeter of squares\n\n\n\n\nThe above plot shows perfect agreement between the observed data and the model.\n\n\n\n\n\nQuestion 7\n\n\nUse the mathematical model to predict the perimeter of a painting with a side of 1.5 metres.\n\n\n\n\n\n\nHint\n\n\n\n\n\nDon’t forget to always include the measurement units when reporting/writing-up results!\n\n\n\n\n\n\n\n Solution \n\n\nWe do not have a painting with a side of 1.5 metres within the random sample of paintings from the Scottish National Gallery. However, we can predict the perimeter of an unobserved squared painting having a 1.5 metre side using the mathematical model.\nYou can obtain this prediction using either a visual approach or an algebraic one.\n\nVisual ApproachAlgebraic Approach\n\n\n\n\n\n\n\n\n\n\n\nSometimes we can directly read a predicted value from the graph of the functional relationship.\nConsider the plot created in the previous question. First, we need to check where x = 1.5. Then, we draw a vertical dashed line until it meets the blue line. The y value corresponding to x = 1.5 can be read off the y-axis.\nHowever, in this case it is not that easy to read it from the drawing…\n\n\nYou can substitute the x value in the formula and calculate the corresponding y value. \\[\ny = 4 * x = 4 * 1.5 = 6\n\\]\n\n\n\n\n\n\n\n\n\nThe predicted perimeter of squared paintings having a 1.5m side is 6m."
  },
  {
    "objectID": "0_01_function.html#study-overview",
    "href": "0_01_function.html#study-overview",
    "title": "Functions and Models",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Question\nHow does handspan vary as a function of height?\n\nConsider now the association between height (in inches) and handspan (in cm). Utts and Heckard (2015) provided data for a sample of 167 students which reported their height and handspan as part of a class survey.\nUsing the handheight data you already loaded at the start of the lab, your task is to investigate how handspan varies as a function of height for the students in the sample.\n\n Handheight codebook.\n\n\nDescription\nThe data set records the height and handspan reported by a random sample of 167 students as part of a class survey.\nThe variables are:\n\nheight, measured in inches\nhandspan, measured in centimetres\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\nheight\nhandspan\n\n\n\n\n68\n21.5\n\n\n71\n23.5\n\n\n73\n22.5\n\n\n64\n18.0\n\n\n68\n23.5\n\n\n59\n20.0\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nUsing a scatterplot (since the variables are numeric and continuous) to visualise the association between the two numeric variables, comment on any main differences you notice with the association between side and perimeter of squares. Note if you detected outliers or points that do not fit with the pattern in the rest of the data.\n\n\n\n\n Solution \n\n\n\nplt &lt;- ggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\nplt\n\n\n\n\nFigure 3: Simple Scatterplot\n\n\n\n\nWe can also add marginal boxplots for each variable using the package ggExtra.\n\nggMarginal(plt, type = 'boxplot')\n\n\n\n\nFigure 4: The statistical relationship between height and handspan\n\n\n\n\nOutliers are extreme observations that are not possible values of a variable or that do not seem to fit with the rest of the data. This could either be:\n\nmarginally along one axis: points that have an unusual (too high or too low) x-coordinate or y-coordinate;\njointly: observations that do not fit with the rest of the point cloud.\n\nThe boxplots in Figure 4 do not highlight any outliers in the marginal distributions of height and handspan. Furthermore, from the scatterplot we do not notice any extreme observations or points that do not fit with the rest of the point cloud.\nWe notice a moderate, positive linear association between height and handspan.\nRecall Figure 2, displaying the association between side and perimeters of squares. In the plot we notice two points on top of each other, reflecting the fact that two squares having the same side will always have the same perimeter. In fact, the data from the Scottish National Gallery include two squared paintings with a side of 1.1m, both having a measured perimeter of 4.4m.\nFigure 4, instead, displays the association between height and handspan of a sample of students. The first thing that grabs our attention is the fact that students having the same height do not necessarily have the same handspan. Rather, we clearly see a variety of handspan values for students all having a height of, for example, 70in. To be more precise, the seven students who are 70 in. tall all have differing handspans.\n\n\n\n\n\nQuestion 9\n\n\nUsing the following command, superimpose on top of the scatterplot a best-fit line describing how handspan varies as a function of height. For the moment, the argument se = FALSE tells R to not display uncertainty bands.\n\ngeom_smooth(method = lm, se = FALSE)\n\nComment on any differences between the lines representing the linear relationship between (a) the side and perimeter of square and (b) height and handspan.\n\n\n\n\n Solution \n\n\n\nggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_smooth(method = lm, se = FALSE) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\n\n\n\nFigure 5: The best-fit line\n\n\n\n\nThe line representing the relationship between side and perimeter of squares is able to predict the actual perimeter value from the measurement of the side of a square. This is possible because the relationship between side and perimeter is an exact one.\nThat is, any squares having the same side will have the same perimeter, and there will be no variation in those values.\nThe line that best fits the relationship between height and handspan (see Figure 5), instead, is only able to predict the average handspan for a given value of height.\nThis is because there will be a distribution of handspans at each value of height. The line will fit the trend/pattern in the values, but there will be individual-to-individual variability that we must accept around that average pattern.\n\n\n\n\n\nQuestion 10\n\n\nThe line of best-fit is given by:1\n\\[\n\\widehat{Handspan} = -3 + 0.35 \\cdot Height\n\\]\nEstimate (or predict) handspan of a student who is (a) 73in tall, and (b) 5in tall.\n\n\n\n\n Solution \n\n\n\nThe predicted average handspan for students who are 73in tall is:  \\(-3 + (0.35 * 73) = 22.55\\)cm.  \nThe predicted average handspan for students who are 5in tall is:  \\(-3 + (0.35 * 5) = -1.25\\)cm.\n\nBut wait, handspan can not be negative… This does not make any sense! That’s right, we went too far off the range of the available data on heights, which were between 57in and 78in. We extrapolated. This is very dangerous…\n\n\n\n\n\nSource: Randall Munroe, xkcd.com"
  },
  {
    "objectID": "0_01_function.html#footnotes",
    "href": "0_01_function.html#footnotes",
    "title": "Functions and Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, the error term is gone. This is because the line of best-fit gives you the prediction of the average handspan for a given height, and not the individual handspan of a person, which will almost surely be different from the prediction of the line.↩︎"
  },
  {
    "objectID": "0_02_slr.html",
    "href": "0_02_slr.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to specify a simple linear model.\nUnderstand what fitted values and residuals are.\nBe able to interpret the coefficients of a fitted model.\n\n\nBe up to date with lectures from Weeks 1 & 2\nHave completed Week 1 lab exercises\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/riverview.csv."
  },
  {
    "objectID": "0_02_slr.html#data-exploration",
    "href": "0_02_slr.html#data-exploration",
    "title": "Simple Linear Regression",
    "section": "Data Exploration",
    "text": "Data Exploration\nThe common first port of call for almost any statistical analysis is to explore the data, and we can do this visually and/or numerically.\n\n\n\n\n\n\n\n\n\n================ Description\n\n\nMarginal Distributions\nThe distribution of each variable (e.g., employee incomes and education levels) without reference to the values of the other variables\n\n\nBivariate Associations\nDescribing the relationship between two numeric variables\n\n\n\n\n\n\n\n\nVisually \n\n\nPlot each variable individually.  You could use, for example, geom_density() for a density plot or geom_histogram() for a histogram to comment on and/or examine: \n\nThe shape of the distribution. Look at the shape, centre and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\n\n\nIdentify any unusual observations. Do you notice any extreme observations (i.e., outliers)?\n\n\n\nPlot associations among two variables. |  |  | You could use, for example, geom_point() for a scatterplot to comment on and/or examine:  |  |\n\nThe direction of the association indicates whether there is a positive or negative association\n\n\nThe form of association refers to whether the relationship between the variables can be summarized well with a straight line or some more complicated pattern\n\n\nThe strength of association entails how closely the points fall to a recognizable pattern such as a line\n\n\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail\n\n\n\n\n\nNumerically \n\nCompute and report summary statistics e.g., mean, standard deviation, median, min, max, etc.  You could, for example, calculate summary statistics such as the mean (mean()) and standard deviation (sd()), etc. within summarize()\n\nCompute and report the correlation coefficient.  You can use the cor() function to calculate this\n\n\n\n\nMarginal Distributions\n\nQuestion 2\n\n\nVisualise and describe the marginal distributions of employee incomes and education level.\n\n\n\n\n Solution \n\n\n\n\nEmployee Incomes\nEmployee Education Levels\n\n\n\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = income)) +\n  geom_density() +\n  geom_boxplot(width = 1/300) +\n  labs(x = \"Income (in thousands of U.S. dollars)\", \n       y = \"Probability density\")\n\n\n\nFigure 1: Density plot and boxplot of employee incomes\n\n\n\nInitial observations from plot:\n\nThe distribution of employee incomes was unimodal\nMost of the incomes were between roughly $45,000 and $63,000\nThe lowest income in the sample was approximately $25,000 and the highest approximately $83,000. This suggested there was a fair high degree of variation in the data.\nThe boxplot did not highlight any outliers in the data.\n\nDescriptive (or summary) statistics for the employees’ incomes:\n\ndesc_income &lt;- riverview %&gt;% \n  summarize(\n    M = mean(income), \n    SD = sd(income)\n    )\ndesc_income\n\n# A tibble: 1 × 2\n      M    SD\n  &lt;dbl&gt; &lt;dbl&gt;\n1  53.7  14.6\n\n\nFollowing the exploration above, we can describe the income variable as follows:\n\n\n\n\n\n\nThe marginal distribution of income was unimodal with a mean of approximately $53,700. There was variation in employees’ salaries (SD = $14,553).\n\n\n\n\n\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = education)) +\n  geom_density() +\n  geom_boxplot(width = 1/100) +\n  labs(x = \"Education (in years)\", \n       y = \"Probability density\")\n\n\n\nFigure 2: Density plot and boxplot of employee education levels\n\n\n\nInitial observations from plot:\n\nThe distribution of employee education was unimodal\nMost of the employees received formal education for between 12 and 20 years\nThe fewest formal years of education was approximately 8 years, and the highest approximately 25. This suggested there was a fair high degree of variation in the data.\nThe boxplot did not highlight any outliers in the data\n\nDescriptive (or summary) statistics for the employees’ level of education:\n\ndesc_education &lt;- riverview %&gt;%\n  summarize(\n    M = mean(education),\n    SD = sd(education)\n    )\ndesc_education\n\n# A tibble: 1 × 2\n      M    SD\n  &lt;dbl&gt; &lt;dbl&gt;\n1    16  4.36\n\n\nFollowing the exploration above, we can describe the education variable as follows:\n\n\n\n\n\n\nThe marginal distribution of education was unimodal with an average of of 16 years. There was variation in employees’ level of education (SD = 4.4 years).\n\n\n\n\n\n\n\n\n\nAssociations among Variables\n\nQuestion 3\n\n\nCreate a scatterplot of income and education level before calculating the correlation between income and education level.\nMaking reference to both the plot and correlation coefficient, describe the association between income and level of education among the employees in the sample.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe are trying to investigate how income varies when varying years of formal education. Hence, income is the dependent variable (on the y-axis), and education is the independent variable (on the x-axis).\n\n\n\n\n\n\n\n Solution \n\n\nLet’s produce a scatterplot:\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point() +\n  labs(x = \"Education (in years)\", \n       y = \"Income (in thousands of U.S. dollars)\")\n\n\n\nFigure 3: The association between employees’ education level and income\n\n\n\nTo comment on the strength of the linear association we compute the correlation coefficient:\n\ncorr &lt;- riverview %&gt;%\n  select(education, income) %&gt;%\n  cor()\ncorr\n\n          education    income\neducation 1.0000000 0.7947847\nincome    0.7947847 1.0000000\n\n\nthat is, \\[\nr_{\\text({education, income})} = 0.79\n\\] \n\n\n\n\n\n\nThere was a strong positive linear association between education level and income for the employees in the sample. High incomes tended to be observed, on average, with more years of formal education (\\(r\\) = .79)."
  },
  {
    "objectID": "0_02_slr.html#model-specification-and-fitting",
    "href": "0_02_slr.html#model-specification-and-fitting",
    "title": "Simple Linear Regression",
    "section": "Model Specification and Fitting",
    "text": "Model Specification and Fitting\nThe scatterplot highlighted a linear relationship, where the data points were scattered around an underlying linear pattern with a roughly-constant spread as x varied.\nHence, we will try to fit a simple (i.e., one x variable only) linear regression model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\\\\n\\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\n\\]\n\n\n\n\n\n\nWhat does \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\\) mean?\n\n\n\n\n\nLets break the statement down into smaller parts:\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\n\n\n\\(y_i\\) is our measured outcome variable (our DV)\n\n\\(x_i\\) is our measured predictor variable (our IV)\n\n\\(\\beta_0\\) is the model intercept\n\n\\(\\beta_1\\) is the model slope\n\\(\\epsilon \\sim N(0, \\sigma) \\text{ independently}\\)\n\n\n\\(\\epsilon\\) is the residual error\n\n\\(\\sim\\) means ‘distributed according to’\n\n\\(\\sim N(0, \\sigma) \\text{ independently}\\) means ‘normal distribution with a mean of 0 and a variance of \\(\\sigma\\)’\nTogether, we can say that the errors around the line have a mean of zero and constant spread as x varies.\n\n\n\n\n\n\nQuestion 4\n\n\nFirst, write the equation of the fitted line.\nNext, using the lm() function, fit a simple linear model to predict income (DV) by Education (IV), naming the output mdl.\nLastly, add update your equation of the fitted line to include the estimated coefficients.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe syntax of the lm() function is:\n\n[model name] &lt;- lm([response variable i.e., dependent variable] ~ [explanatory variable i.e., independent variable], data = [dataframe])\n\n\n\n\n\n\n\n\n Solution \n\n\nFirst, lets specify the fitted model, which can be written either as:\n\n\nOption A\nOption B\n\n\n\n\\[\n\\widehat{Income} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Education\n\\]\n\n\n\\[\n\\widehat{Income} = \\hat \\beta_0 \\cdot 1 + \\hat \\beta_1 \\cdot Education\n\\]\n\n\n\nTo fit the model in R, as the variables are in the riverview dataframe, we would write:\n\n\nOption A\nOption B\n\n\n\n\nmdl &lt;- lm(income ~ education, data = riverview)\nmdl\n\n\nCall:\nlm(formula = income ~ education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\n\n\n\nmdl &lt;- lm(income ~ 1 + education, data = riverview)\nmdl\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\n\n\n\n\n\n\n\n\n\nWhy is there a 1 in the Option B’s?\n\n\n\n\n\nWhen we specify the linear model in R, we include after the tilde sign (\\(\\sim\\)), the variables that appear to the right of the \\(\\hat \\beta\\)s. The intercept, or \\(\\beta_0\\), is a constant. That is, we could write it as multiplied by 1.\nIncluding the 1 explicitly is not necessary because it is included by default (you can check this by comparing the outputs of A & B above with and without the 1 included - the estimates are the same!). After a while, you will find you just want to drop the 1 (i.e., Option B) when calling lm() because you know that it’s going to be there, but in these early weeks we tried to keep it explicit to make it clear that you want to the intercept to be estimated.\n\n\n\nNote that by calling the name of the fitted model, mdl, you can see the estimated regression coefficients \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). We can add these values to the fitted line:\n\\[\n\\widehat{Income} = 11.32 + 2.65 \\cdot Education \\\\\n\\]\n\n\n\n\n\nQuestion 5\n\n\nExplore the following equivalent ways to obtain the estimated regression coefficients — that is, \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) — from the fitted model:\n\nmdl\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\n\n\n Solution \n\n\nThe estimated parameters returned by the below methods are all equivalent. However, summary() returns more information.\n\n\nmdl()\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\nSimply invoke the name of the fitted model:\n\nmdl\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\n\n\n\nmdl$coefficients\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\n\ncoef(mdl)\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\n\ncoefficients(mdl)\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\n\n\n\n\n\n\n\n\n\nThe estimated intercept is \\(\\hat \\beta_0 = 11.32\\) and the estimated slope is \\(\\hat \\beta_1 = 2.65\\).\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nExplore the following equivalent ways to obtain the estimated standard deviation of the errors — that is, \\(\\hat \\sigma\\) — from the fitted model mdl:\n\nsigma(mdl)\nsummary(mdl)\n\n\n Huh? What is \\(\\sigma\\)?\n\n\nThe standard deviation of the errors, denoted by \\(\\sigma\\), is an important quantity that our model estimates. It represents how much individual data points tend to deviate above and below the regression line.\nA small \\(\\sigma\\) indicates that the points hug the line closely and we should expect fairly accurate predictions, while a large \\(\\sigma\\) suggests that, even if we estimate the line perfectly, we can expect individual values to deviate from it by substantial amounts.\nThe estimated standard deviation of the errors is denoted \\(\\hat \\sigma\\), and is estimated by essentially averaging squared residuals (giving the variance) and taking the square-root:\n\\[\n\\begin{align}\n& \\hat \\sigma = \\sqrt{\\frac{SS_{Residual}}{n - k - 1}} \\\\\n\\qquad \\\\\n& \\text{where} \\\\\n& SS_{Residual} = \\textrm{Sum of Squared Residuals} = \\sum_{i=1}^n{(\\epsilon_i)^2}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\nThe estimated standard deviation of the errors can be equivalently obtained by the below methods. However, summary() returns more information.\n\n\nsigma(mdl)\nsummary(mdl)\n\n\n\n\nsigma(mdl)\n\n[1] 8.978116\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term “Residual standard error” is a misnomer, as the help page for sigma says (check ?sigma). However, it’s hard to get rid of this bad name as it has been used in too many books showing R output.\n\n\n\n\n\n\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma = 8.98\\).\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nInterpret the estimated intercept, slope, and standard deviation of the errors in the context of the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo interpret the estimated standard deviation of the errors we can use the fact that about 95% of values from a normal distribution fall within two standard deviations of the centre.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nIntercept\nSlope\nStandard deviation of the errors\n\n\n\nThe estimated income associated to zero years of formal education is $11,321.\n\n\nThe estimated increase in income associated to a one year increase in education is $2,651.\n\n\nFor any particular level of education, employee incomes should be distributed above and below the regression line with standard deviation estimated to be \\(\\hat \\sigma = 8.98\\). Since \\(2 \\hat \\sigma = 2 (8.98) = 17.96\\), we expect most (about 95%) of the employee incomes to be within about $18,000 from the regression line.\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nPlot the data and the fitted regression line. To do so:\n\nExtract the estimated regression coefficients e.g., via betas &lt;- coef(mdl)\n\nExtract the first entry of betas (i.e., the intercept) via betas[1]\n\nExtract the second entry of betas (i.e., the slope) via betas[2]\n\nProvide the intercept and slope to the function\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nExtracting values: The function coef(mdl) returns a vector: that is, a sequence of numbers all of the same type. To get the first element of the sequence you append [1], and [2] for the second.\nPlotting: In your ggplot(), you will need to specify geom_abline(). This might help get you started:\n\ngeom_abline(intercept = &lt;intercept&gt;, slope = &lt;slope&gt;)\n\n\n\n\n\n\n\n\n Solution \n\n\nFirst extract the values required:\n\nbetas &lt;- coef(mdl)\nintercept &lt;- betas[1]\nslope &lt;- betas[2]\n\nWe can plot the model as follows:\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point() +\n  geom_abline(intercept = intercept, slope = slope, color = 'blue') + \n  labs(x = \"Education (in years)\", y = \"Income (in thousands of U.S. dollars)\")"
  },
  {
    "objectID": "0_02_slr.html#predicted-values-residuals",
    "href": "0_02_slr.html#predicted-values-residuals",
    "title": "Simple Linear Regression",
    "section": "Predicted Values & Residuals",
    "text": "Predicted Values & Residuals\n\nPredicted Values\nModel predicted values for sample data:\nWe can get out the model predicted values for \\(y\\), the “y hats” (\\(\\hat y\\)), for the data in the sample using various functions:\n\npredict(&lt;fitted model&gt;)\nfitted(&lt;fitted model&gt;)\nfitted.values(&lt;fitted model&gt;)\nmdl$fitted.values\n\nFor example, this will give us the estimated income (point on our regression line) for each observed value of education level.\n\npredict(mdl)\n\n       1        2        3        4        5        6        7        8 \n32.53175 32.53175 37.83435 37.83435 37.83435 43.13694 43.13694 43.13694 \n       9       10       11       12       13       14       15       16 \n43.13694 48.43953 48.43953 48.43953 51.09083 53.74212 53.74212 53.74212 \n      17       18       19       20       21       22       23       24 \n53.74212 53.74212 56.39342 59.04472 59.04472 61.69601 61.69601 64.34731 \n      25       26       27       28       29       30       31       32 \n64.34731 64.34731 64.34731 66.99861 66.99861 69.64990 69.64990 74.95250 \n\n\nModel predicted values for other (unobserved) data:\nTo compute the model-predicted values for unobserved data (i.e., data not contained in the sample), we can use the following function:\n\npredict(&lt;fitted model&gt;, newdata = &lt;dataframe&gt;)\n\nFor this example, we first need to remember that the model predicts income using the independent variable education. Hence, if we want predictions for new (unobserved) data, we first need to create a tibble with a column called education containing the years of education for which we want the prediction, and store this as a dataframe.\n\n#Create dataframe 'newdata' containing education years of 7, 11 and 25\nnewdata &lt;- tibble(education = c(7, 11, 25))\nnewdata\n\n# A tibble: 3 × 1\n  education\n      &lt;dbl&gt;\n1         7\n2        11\n3        25\n\n\nThen we take newdata and add a new column called income_hat, computed as the prediction from the fitted mdl using the newdata above:\n\nnewdata &lt;- newdata %&gt;%\n  mutate(\n    income_hat = predict(mdl, newdata = newdata)\n  )\nnewdata\n\n# A tibble: 3 × 2\n  education income_hat\n      &lt;dbl&gt;      &lt;dbl&gt;\n1         7       29.9\n2        11       40.5\n3        25       77.6\n\n\nResiduals\nThe residuals represent the deviations between the actual responses and the predicted responses and can be obtained either as\n\nmdl$residuals\nresid(mdl)\nresiduals(mdl)\ncomputing them as the difference between the response (\\(y_i\\)) and the predicted response (\\(\\hat y_i\\))\n\n\n\n\nQuestion 9\n\n\nUse predict(mdl) to compute the fitted values and residuals. Mutate the riverview dataframe to include the fitted values and residuals as extra columns.\nAssign to the following symbols the corresponding numerical values:\n\n\n\\(y_{3}\\) (response variable for unit \\(i = 3\\) in the sample data)\n\n\\(\\hat y_{3}\\) (fitted value for the third unit)\n\n\\(\\hat \\epsilon_{5}\\) (the residual corresponding to the 5th unit, i.e., \\(y_{5} - \\hat y_{5}\\))\n\n\n\n\n\n Solution \n\n\n\nriverview_fitted &lt;- riverview %&gt;%\n  mutate(\n    income_hat = predict(mdl),\n    resid = income - income_hat\n  )\n\nhead(riverview_fitted)\n\n# A tibble: 6 × 8\n  education income seniority gender  male party       income_hat  resid\n      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n1         8   37.4         7 male       1 Democrat          32.5   4.92\n2         8   26.4         9 female     0 Independent       32.5  -6.10\n3        10   47.0        14 male       1 Democrat          37.8   9.20\n4        10   34.2        16 female     0 Independent       37.8  -3.65\n5        10   25.5         1 female     0 Republican        37.8 -12.4 \n6        12   46.5        11 female     0 Democrat          43.1   3.35\n\n\n\n\n\\(y_{3}\\) = 47.03 (see row 3, column 2)\n\n\\(\\hat y_{3}\\) = 37.83 (see row 3, column 7)\n\n\\(\\hat \\epsilon_{5} = y_{5} - \\hat y_{5}\\) = -12.36 (see row 5, columns 2 and 7)\n\n\n\n\n\n\nQuestion 10\n\n\nDescribe the design of the study, and the analyses that you undertook. Interpret your results in the context of the research question and report your model results in full.\nProvide key model results in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package.\nRemember that you can rename your DV and IV labels by specifying dv.labels and pred.labels.\nMake sure to write your results up following APA guidelines\n\n\n\n\n\n\n\n Solution \n\n\nBefore we write up our analyses & results, lets first create the table of results:\n\ntab_model(mdl,\n          dv.labels = \"Income (in thousands of US dollars)\",\n          pred.labels = c(\"education\" = \"Education (years)\"),\n          title = \"Regression Table for Income Model\")\n\n\n\nTable 1: Regression Table for Income Model\n\n\n \nIncome (in thousands of US dollars)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n11.32\n-1.18 – 23.83\n0.074\n\n\nEducation (years)\n2.65\n1.90 – 3.41\n&lt;0.001\n\n\nObservations\n32\n\n\nR2 / R2 adjusted\n0.632 / 0.619\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe riverview dataset contained information on 32 participants who worked in the hypothetical city of Riverview, US. Using a between-subjects design, the researchers collected information on participants’ sex, income, education, and seniority level.\nTo investigate whether there was an association between income and education level, the following simple linear regression model was used:\n\\[\n\\text{Income}= \\beta_0 + \\beta_1 \\cdot \\text{Education}\n\\]\nFull regression results are displayed in Table 1. The estimated income associated with no formal years of education was $11,321. Each additional year of formal education was associated with an income increase of $2,651."
  },
  {
    "objectID": "2_09_power.html",
    "href": "2_09_power.html",
    "title": "Sample Size and Power Analysis",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand how varying factors can influence power\nBe able to conduct power analyses using the pwr package\n\n\nBe up to date with lectures\nHave completed previous lab exercises from Week X and Week X\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nkableExtra\npwr\n\nAll results should be presented following APA guidelines. If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/recall_med_coast.csv."
  },
  {
    "objectID": "2_09_power.html#data-management",
    "href": "2_09_power.html#data-management",
    "title": "Sample Size and Power Analysis",
    "section": "Data Management",
    "text": "Data Management\n\nQuestion 1\n\n\nFirst, provide a brief overview of the study design and data.\nNext examine the dataset, and perform any necessary and appropriate data management steps.\n\n\n\n\n Solution \n\n\nSummary of study design and data:\n\n\n\n\n\n\nThe recdata dataset contained information on 100 hypothetical participants who participated in a between-subjects experiment exploring the associations among recall, age, and intervention type. Participants were aged 18-75 years old, and were randomly allocated to one of two intervention groups (exciting - 1-hour long roller-coaster session; or relaxing - 1-hour long meditation session) before completing a free-recall test (% correct).\n\n\n\nNext, data checks & management:\n\n#look at structure of data:\nstr(recdata)\n\nspc_tbl_ [100 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ perc_recall: num [1:100] 47.4 61.4 50.1 56.4 57 ...\n $ group      : num [1:100] 0 1 0 1 1 0 1 1 1 0 ...\n $ age        : num [1:100] 52 37 46 72 46 69 70 53 41 26 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   perc_recall = col_double(),\n  ..   group = col_double(),\n  ..   age = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n#check for NAs - there are none - all FALSE:\ntable(is.na(recdata))\n\n\nFALSE \n  300 \n\n#Group should be a factor:\nrecdata$group &lt;- factor(recdata$group, \n                        levels = c(0, 1), \n                        labels = c('rollercoaster', 'meditation'))\n\n\n\n\n\n\n\nThe ‘group’ variable denoting which intervention type participants were allocated to was coded as a factor with two levels - ‘rollercoaster’ and ‘meditation’, where ‘rollercoaster’ was designated as the reference group. There were no NAs contained within the dataset, and recall scores were within range (i.e., within possible values of 0-100), as were ages (i.e., all ages ranged from 18-75)."
  },
  {
    "objectID": "2_09_power.html#descriptive-statistics",
    "href": "2_09_power.html#descriptive-statistics",
    "title": "Sample Size and Power Analysis",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nQuestion 2\n\n\nProvide a table of descriptive statistics and visualise your data.\nRemember to interpret your plot in the context of the study.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nFor your table of descriptive statistics, both the group_by() and summarise() functions will come in handy here.\nRecall that when visualising a continuous outcome across groups, geom_boxplot() may be most appropriate to use.\nMake sure to comment on any observed differences among the sample means of the four treatment conditions.\n\n\n\n\n\n\n\n\n Solution \n\n\nLet’s first produce a descriptive statistics table:\n\nrecall_stats &lt;- recdata %&gt;%\n    group_by(group) %&gt;%\n    summarise(\n       n = n(),\n       Avg_Age = mean(age),\n       Avg_Recall = mean(perc_recall)) %&gt;%\n    kable(caption = \"Descriptive Statistics\", digits = 2) %&gt;%\n    kable_styling()\n\nrecall_stats\n\n\n\nTable 1: Descriptive Statistics\n\ngroup\nn\nAvg_Age\nAvg_Recall\n\n\n\nrollercoaster\n53\n47.85\n50.11\n\n\nmeditation\n47\n46.57\n59.46\n\n\n\n\n\n\n\n\nWe can visually explore the association between Recall and the two predictor variables as follows:\n\nrecall_plt1 &lt;- ggplot(data = recdata, aes(x = group, y = perc_recall, fill = group)) +\n    geom_boxplot() + \n    labs(x = \"Intervention Group\", y = \"Recall (%)\", title = \"Association between Recall and Intervention\")\nrecall_plt1\n\n\n\nFigure 1: Association between Recall and Intervention Group\n\n\n\n\nrecall_plt2 &lt;- ggplot(data = recdata, aes(x = age, y = perc_recall)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE) + \n    labs(x = \"Age (in years)\", y = \"Recall (%)\", title = \"Association between Recall and Age\")\nrecall_plt2\n\n\n\nFigure 2: Association between Recall and Age\n\n\n\nFrom Table 1, Figure 1, and Figure 2 we can see:\n\nthere were more participants in the rollercoaster condition than meditation\nparticipants in the meditation condition had higher recall scores than those in the rollercoaster condition\nthere was less variability in scores in the meditation condition in comparison to the rollercoaster condition\nolder age appeared to be associated with lower recall scores\n\n\n\n\n\n\nQuestion 3\n\n\nUse a scatterplot to visualise the association between recall and age by group.\nIs there any evidence of an interaction between age and group?\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nIt might be useful to specify the color = argument for your grouping variable\nConsider using geom_smooth() to superimpose the best-fitting line describing the association of interest for each intervention group.\n\n\n\n\n\n\n\n\n Solution \n\n\n\nrecall_plt3 &lt;- ggplot(data = recdata, aes(x = age, y = perc_recall, color = group)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE) + \n    labs(x = \"Age (in years)\", y = \"Recall (%)\", title = \"Associations among Recall Score, \\nAge, and Intervention Group\")\nrecall_plt3\n\n\n\nFigure 3: Scatterplot displaying the association between age, intervention group, and recall\n\n\n\nThe slope in Figure 3 appears to be stepper in the roller coaster intervention group than the meditation group - this suggests that there could be an interaction."
  },
  {
    "objectID": "2_09_power.html#sample-size-power",
    "href": "2_09_power.html#sample-size-power",
    "title": "Sample Size and Power Analysis",
    "section": "Sample Size & Power",
    "text": "Sample Size & Power\n\nQuestion 4\n\n\nUsing a significance level (\\(\\alpha\\)) of .05, what sample size (\\(n\\)) would you require to check whether any of the predictors (and interaction) influenced recall scores with a 90% chance?\nBecause you do not know the effect size, assume Cohen’s guideline for linear regression and, to be on the safe side, consider the ‘small’ value.\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn linear regression, the relevant function in R is:\n\npwr.f2.test(u = , v = , f2 = , sig.level = , power = )\n\nWhere:\n\n\nu = numerator degrees of freedom = number predictors in the model (\\(k\\))\n\nv = denominator degrees of freedom = \\(v = n-k-1\\)\n\n\nf2 = effect size. Cohen suggests effect size cut-off values of \\(.02\\) (small), \\(.15\\) (moderate), and \\(.35\\) (large)\n\nsig.level = significance level\n\npower = level of power\n\n\n\n\n\n\n\n\n Solution \n\n\n\nk &lt;- 3\nf2 &lt;- .02\npwr.f2.test(u = k, f2 = f2, sig.level = 0.05, power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 3\n              v = 708.495\n             f2 = 0.02\n      sig.level = 0.05\n          power = 0.9\n\n\n\n\n\n\n\n\nA power analysis for a multiple regression model \\((k = 3)\\) was conducted (via the pwr package) to determine the minimum sample size using an \\(\\alpha\\) = .05, power = .90, and small effect size \\((D = .02)\\). The required sample size is \\(n = \\text v + k + 1 = 709 + 3 + 1 = 713\\).\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nUsing the same \\(\\alpha\\) (.05) and power, what would be the sample size if you assumed effect size to be ‘medium’?\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn linear regression, the relevant function in R is:\n\npwr.f2.test(u = , v = , f2 = , sig.level = , power = )\n\nWhere:\n\n\nu = numerator degrees of freedom = number predictors in the model (\\(k\\))\n\nv = denominator degrees of freedom = \\(v = n-k-1\\)\n\n\nf2 = effect size. Cohen suggests effect size cut-off values of \\(.02\\) (small), \\(.15\\) (moderate), and \\(.35\\) (large)\n\nsig.level = significance level\n\npower = level of power\n\n\n\n\n\n\n\n\n Solution \n\n\n\nk &lt;- 3\nf2 &lt;- .15\npwr.f2.test(u = k, f2 = f2, sig.level = 0.05, power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 3\n              v = 94.48157\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.9\n\n\n\n\n\n\n\n\nA power analysis for a multiple regression model \\((k = 3)\\) was conducted (via the pwr package) to determine the minimum sample size using an \\(\\alpha\\) = .05, power = .90, and moderate effect size \\((D = .15)\\). The required sample size is \\(n = \\text v + k + 1 = 95 + 3 + 1 = 99\\).\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nUsing the same \\(\\alpha\\) and power, what would be the sample size if you assumed effect size to be ‘large’?\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn linear regression, the relevant function in R is:\n\npwr.f2.test(u = , v = , f2 = , sig.level = , power = )\n\nWhere:\n\n\nu = numerator degrees of freedom = number predictors in the model (\\(k\\))\n\nv = denominator degrees of freedom = \\(v = n-k-1\\)\n\n\nf2 = effect size. Cohen suggests effect size cut-off values of \\(.02\\) (small), \\(.15\\) (moderate), and \\(.35\\) (large)\n\nsig.level = significance level\n\npower = level of power\n\n\n\n\n\n\n\n\n Solution \n\n\n\nk &lt;- 3\nf2 &lt;- .35\npwr.f2.test(u = k, f2 = f2, sig.level = 0.05, power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 3\n              v = 40.61744\n             f2 = 0.35\n      sig.level = 0.05\n          power = 0.9\n\n\n\n\n\n\n\n\nA power analysis for a multiple regression model \\((k = 3)\\) was conducted (via the pwr package) to determine the minimum sample size using an \\(\\alpha\\) = .05, power = .90, and large effect size \\((D = .35)\\). The required sample size is \\(n = \\text v + k + 1 = 41 + 3 + 1 = 45\\).\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nFit the following model using lm(), and assign it as an object with the name “recall_mdl1”.\n\\[\n\\text{Recall} = \\beta_0 + \\beta_1 \\cdot Age  + \\epsilon \\\\\n\\]\nHow much variance in recall scores does the model explain?\n\n\n\n\n Solution \n\n\n\nrecall_mdl1 &lt;- lm(perc_recall ~ age, data = recdata)\nsummary(recall_mdl1)\n\n\nCall:\nlm(formula = perc_recall ~ age, data = recdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.0850  -4.7474   0.7331   4.6758  10.1733 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 62.16336    1.81984  34.159  &lt; 2e-16 ***\nage         -0.16206    0.03672  -4.414 2.61e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.494 on 98 degrees of freedom\nMultiple R-squared:  0.1658,    Adjusted R-squared:  0.1573 \nF-statistic: 19.48 on 1 and 98 DF,  p-value: 2.614e-05\n\n\nWe can see both the R-squared or Adjusted R-squared from the model summary() output. We can use either since we only have a single predictor. To be conservative, we might want to use the adjusted R-squared (0.16).\nThe model, with Age as a single predictor, explained approximately 16% of the variance in recall scores.\n\n\n\n\n\nQuestion 8\n\n\nImagine you found the R-squared that you computed above (Q6) in a paper, and you are using that to base your next study.\nA researcher believes that the inclusion of intervention group and its interaction with age should explain an extra 50% of the variation in recall scores.\nUsing a significance level of 5%, what sample size should you use for your next data collection in order to discover that effect with a power of 0.90?\n\n\n\n\n Solution \n\n\n\n# restricted model m - number of predictors & R-squared\nk &lt;- 1\nR2m &lt;- 0.16\n\n# full model M - number of predictors & R-squared\nK &lt;- 3\nR2M &lt;- 0.16 + 0.5\n\n# effect size - calculate f2\nf2 &lt;- (R2M - R2m) / (1 - R2M)\n\n#run test\npwr.f2.test(u = K - k, f2 = f2, sig.level = 0.05, power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 2\n              v = 9.211914\n             f2 = 1.470588\n      sig.level = 0.05\n          power = 0.9\n\n\nThe sample size should be \\(n = \\text v + K + 1 = 10 + 3 + 1 = 14\\).\nWith such a big effect size, don’t be surprised it’s so small. When the effect size is much smaller, that will be harder to detect and you will require a bigger sample size.\n\n\n\n\n\nQuestion 9\n\n\nSuppose that the aforementioned researcher made a mistake, and issues a corrected statement in which they state that the inclusion of intervention group and its interaction with age should explain an extra 5% of the variation in recall scores.\nUsing a significance level of 5%, what sample size should you use for your next data collection in order to discover that effect with a power of 0.90?\n\n\n\n\n Solution \n\n\n\n# restricted model m - number of predictors & R-squared\nk &lt;- 1\nR2m &lt;- 0.16\n\n# full model M - number of predictors & R-squared\nK &lt;- 3\nR2M &lt;- 0.16 + 0.05\n\n# effect size - calculate f2\nf2 &lt;- (R2M - R2m) / (1 - R2M)\n\n# run test\npwr.f2.test(u = K - k, f2 = f2, sig.level = 0.05, power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 2\n              v = 199.9608\n             f2 = 0.06329114\n      sig.level = 0.05\n          power = 0.9\n\n\nThe sample size should be \\(n = \\text v + K + 1 = 200 + 3 + 1 = 204\\).\nWith such a small effect size, we need a bigger sample size for us to detect it with high confidence.\n\n\n\n\n\nQuestion 10\n\n\nA colleague produces a visualisation of the joint relationship between sample size and effect size via a power curve (with coloured lines representing large, medium, and small effect sizes).\nBased on this, what feedback/comments might you share with them regarding sample size for their prospective study, and its relation to effect size?\n\n\n\n\nFigure 4: Linear Regression with power = 0.90 and alpha = 0.05\n\n\n\n\n\n\n\n Solution \n\n\nFrom Figure 4, to detect a large effect size (red line), they should aim to recruit ~50 participants, for a medium effect size ~100 (blue line). It is impossible to judge how many participants would be required to detect a small effect size (green line) - we would suggest that they conduct their own power calculation as it is too difficult to judge based on the figure alone.\nGenerally, if they want to be able to detect a medium-large effect with 90% power using \\(\\alpha = .05\\), it appears that there is little gain in recruiting a sample size &gt; ~110. However, if they want to be able to detect a small effect with 90% power using \\(\\alpha = .05\\), they are likely going to require a very large sample size.\nTo summarise the association between effect size and sample size, it appears that the smaller the effect you are trying to detect, the larger the sample size you will require."
  },
  {
    "objectID": "2_09_power.html#study-overview-data-management",
    "href": "2_09_power.html#study-overview-data-management",
    "title": "Sample Size and Power Analysis",
    "section": "Study Overview & Data Management",
    "text": "Study Overview & Data Management\n\nQuestion 1\n\n\nFirst, provide a brief overview of the study design and data.\nNext examine the dataset, and perform any necessary and appropriate data management steps.\n\n\n\n\n Solution \n\n\nSummary of study design and data:\n\n\n\n\n\n\nThe recdata dataset contained information on 100 hypothetical participants who participated in a between-subjects experiment exploring the associations among recall, age, and intervention type. Participants were aged 18-75 years old, and were randomly allocated to one of two intervention groups (exciting - 1-hour long roller-coaster session; or relaxing - 1-hour long meditation session) before completing a free-recall test (% correct).\n\n\n\nNext, data checks & management:\n\n#look at structure of data:\nstr(recdata)\n\nspc_tbl_ [100 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ perc_recall: num [1:100] 47.4 61.4 50.1 56.4 57 ...\n $ group      : num [1:100] 0 1 0 1 1 0 1 1 1 0 ...\n $ age        : num [1:100] 52 37 46 72 46 69 70 53 41 26 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   perc_recall = col_double(),\n  ..   group = col_double(),\n  ..   age = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n#check for NAs - there are none - all FALSE:\ntable(is.na(recdata))\n\n\nFALSE \n  300 \n\n#Group should be a factor:\nrecdata$group &lt;- factor(recdata$group, \n                        levels = c(0, 1), \n                        labels = c('rollercoaster', 'meditation'))\n\n\n\n\n\n\n\nThe ‘group’ variable denoting which intervention type participants were allocated to was coded as a factor with two levels - ‘rollercoaster’ and ‘meditation’, where ‘rollercoaster’ was designated as the reference group. There were no NAs contained within the dataset, and recall scores were within range (i.e., within possible values of 0-100), as were ages (i.e., all ages ranged from 18-75)."
  },
  {
    "objectID": "2_09_power.html#descriptive-statistics-visualisations",
    "href": "2_09_power.html#descriptive-statistics-visualisations",
    "title": "Sample Size and Power Analysis",
    "section": "Descriptive Statistics & Visualisations",
    "text": "Descriptive Statistics & Visualisations\n\nQuestion 2\n\n\nProvide a table of descriptive statistics and visualise your data.\nRemember to interpret your plot in the context of the study.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nFor your table of descriptive statistics, both the group_by() and summarise() functions will come in handy here.\nRecall that when visualising a continuous outcome across groups, geom_boxplot() may be most appropriate to use.\nMake sure to comment on any observed differences among the sample means of the four treatment conditions.\n\n\n\n\n\n\n\n\n Solution \n\n\nLet’s first produce a descriptive statistics table:\n\nrecall_stats &lt;- recdata %&gt;%\n    group_by(group) %&gt;%\n    summarise(\n       n = n(),\n       Avg_Age = mean(age),\n       Avg_Recall = mean(perc_recall)) %&gt;%\n    kable(caption = \"Descriptive Statistics\", digits = 2) %&gt;%\n    kable_styling()\n\nrecall_stats\n\n\n\nTable 1: Descriptive Statistics\n\ngroup\nn\nAvg_Age\nAvg_Recall\n\n\n\nrollercoaster\n53\n47.85\n50.11\n\n\nmeditation\n47\n46.57\n59.46\n\n\n\n\n\n\n\n\nWe can visually explore the association between Recall and the two predictor variables as follows:\n\nrecall_plt1 &lt;- ggplot(data = recdata, aes(x = group, y = perc_recall, fill = group)) +\n    geom_boxplot() + \n    labs(x = \"Intervention Group\", y = \"Recall (%)\", title = \"Association between Recall and Intervention\")\nrecall_plt1\n\n\n\nFigure 1: Association between Recall and Intervention Group\n\n\n\n\nrecall_plt2 &lt;- ggplot(data = recdata, aes(x = age, y = perc_recall)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE) + \n    labs(x = \"Age (in years)\", y = \"Recall (%)\", title = \"Association between Recall and Age\")\nrecall_plt2\n\n\n\nFigure 2: Association between Recall and Age\n\n\n\nFrom Table 1, Figure 1, and Figure 2 we can see:\n\nthere were more participants in the rollercoaster condition than meditation\nparticipants in the meditation condition had higher recall scores than those in the rollercoaster condition\nthere was less variability in scores in the meditation condition in comparison to the rollercoaster condition\nolder age appeared to be associated with lower recall scores\n\n\n\n\n\n\nQuestion 3\n\n\nUse a scatterplot to visualise the association between recall and age by group.\nIs there any evidence of an interaction between age and group?\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nIt might be useful to specify the color = argument for your grouping variable\nConsider using geom_smooth() to superimpose the best-fitting line describing the association of interest for each intervention group.\n\n\n\n\n\n\n\n\n Solution \n\n\n\nrecall_plt3 &lt;- ggplot(data = recdata, aes(x = age, y = perc_recall, color = group)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE) + \n    labs(x = \"Age (in years)\", y = \"Recall (%)\", title = \"Associations among Recall Score, \\nAge, and Intervention Group\")\nrecall_plt3\n\n\n\nFigure 3: Scatterplot displaying the association between age, intervention group, and recall\n\n\n\nThe slope in Figure 3 appears to be stepper in the roller coaster intervention group than the meditation group - this suggests that there could be an interaction."
  }
]