[
  {
    "objectID": "1_01_function.html",
    "href": "1_01_function.html",
    "title": "Functions and Models",
    "section": "",
    "text": "At the end of this lab, you will:\n\nHave reviewed the main concepts from introductory statistics.\nUnderstand the concept of a function.\nBe able to discuss what a statistical model is.\nUnderstand the link between models and functions.\n\n\nHave attended and/or watched Week 1 lectures.\nHave installed R and RStudio on your own computer (unless you have a Chromebook where you may continue to use the PPLS RStudio Server).\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nggExtra\nkableExtra\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/handheight.csv."
  },
  {
    "objectID": "1_01_function.html#functions-and-mathematical-models-plots",
    "href": "1_01_function.html#functions-and-mathematical-models-plots",
    "title": "Functions and Models",
    "section": "Functions and Mathematical Models: Plots",
    "text": "Functions and Mathematical Models: Plots\n\nQuestion 4\n\n\nCreate a data set called squares containing the perimeter of four squares having sides of length \\(0, 2, 5, 9\\) metres, and then plot the squares data as points\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember that to combine multiple numbers together we use the function c().\n\n\n\n\n\n\n\n Solution \n\n\n\n#Create 'squares' dataset using tibble()\nsquares <- tibble(\n  side = c(0, 2, 5, 9),\n  perimeter = 4 * side\n)\n\nsquares\n\n# A tibble: 4 × 2\n   side perimeter\n  <dbl>     <dbl>\n1     0         0\n2     2         8\n3     5        20\n4     9        36\n\n\n\n#Create Plot\nggplot(data = squares, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\nFigure 1: ?(caption)\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nGenerate one hundred data points, and use them to visualise the relationship between side and perimeter of squares. To do so, you need to complete four steps:\n\nCreate a sequence of one hundred side lengths (x) going from 0 to 3 metres.\nCompute the corresponding perimeters (y).\nPlot the side and perimeter data as points on a graph.\nVisualise the functional relationship between side and perimeter of squares. To do so, use the function geom_line() to connect the computed points with lines.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember that to create a sequence of numbers, we can use the function seq().\n\n\n\n\n\n\n\n Solution \n\n\nLet’s start by creating the side and perimeter data:\n\n#step 1 & 2\nsquares_grid <- tibble(\n  side = seq(0, 3, length.out = 100),\n  perimeter = 4 * side\n)\n\nPlot the individual points:\n\n# step 3\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4*Side')\n\n\n\n\nVisualise the functional relationship by connecting the individual points with lines:\n\n#step 4\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_line(colour = 'blue') +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4*Side')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function \\(y = 4 \\ x\\) that you plotted above is an example of a function representing a mathematical model.\nWe typically validate a model using experimental data. However, we all know how squares work and that two squares with the same side will have the same perimeter (more on this later).\n\n\n\n\nQuestion 6\n\n\nThe Scottish National Gallery kindly provided us with measurements of side and perimeter (in metres) for a sample of 10 square paintings.\nThe data are provided below:\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nPlot the mathematical model of the relationship between side and perimeter for squares, and superimpose on top the experimental data from the Scottish National Gallery.\n\n\n\n\n Solution \n\n\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nggplot() +\n  geom_line(data = squares_grid, aes(x = side, y = perimeter), colour = 'blue') +\n  geom_point(data = sng, aes(x = side, y = perimeter), colour = 'black', \n             alpha = 0.5, size = 3) +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\nFigure 2: The exact relationship between side and perimeter of squares\n\n\n\n\nThe above plot shows perfect agreement between the observed data and the model.\n\n\n\n\n\nQuestion 7\n\n\nUse the mathematical model to predict the perimeter of a painting with a side of 1.5 metres.\n\n\n\n\n\n\nHint\n\n\n\n\n\nDon’t forget to always include the measurement units when reporting/writing-up results!\n\n\n\n\n\n\n\n Solution \n\n\nWe do not have a painting with a side of 1.5 metres within the random sample of paintings from the Scottish National Gallery. However, we can predict the perimeter of an unobserved squared painting having a 1.5 metre side using the mathematical model.\nYou can obtain this prediction using either a visual approach or an algebraic one.\n\n\nVisual Approach\nAlgebraic Approach\n\n\n\n\n\n\n\n\nSometimes we can directly read a predicted value from the graph of the functional relationship.\nConsider the plot created in the previous question. First, we need to check where x = 1.5. Then, we draw a vertical dashed line until it meets the blue line. The y value corresponding to x = 1.5 can be read off the y-axis.\nHowever, in this case it is not that easy to read it from the drawing…\n\n\nYou can substitute the x value in the formula and calculate the corresponding y value.\n\\[\ny = 4 * x = 4 * 1.5 = 6\n\\]\n\n\n\n\n\n\n\n\n\nThe predicted perimeter of squared paintings having a 1.5m side is 6m."
  },
  {
    "objectID": "1_01_function.html#study-overview",
    "href": "1_01_function.html#study-overview",
    "title": "Functions and Models",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Question\nHow does handspan vary as a function of height?\n\nConsider now the relationship between height (in inches) and handspan (in cm). Utts and Heckard (2015) provided data for a sample of 167 students which reported their height and handspan as part of a class survey.\nUsing the handheight data you already loaded at the start of the lab, your task is to investigate how handspan varies as a function of height for the students in the sample.\n\n Handheight codebook.\n\n\nDescription\nThe data set records the height and handspan reported by a random sample of 167 students as part of a class survey.\nThe variables are:\n\n\nheight, measured in inches\n\nhandspan, measured in centimetres\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\n height \n    handspan \n  \n\n\n 68 \n    21.5 \n  \n\n 71 \n    23.5 \n  \n\n 73 \n    22.5 \n  \n\n 64 \n    18.0 \n  \n\n 68 \n    23.5 \n  \n\n 59 \n    20.0 \n  \n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nUsing a scatterplot (since the variables are numeric and continuous) to visualise the relationship between the two numeric variables, comment on any main differences you notice with the relationship between side and perimeter of squares. Note if you detected outliers or points that do not fit with the pattern in the rest of the data.\n\n\n\n\n Solution \n\n\n\nplt <- ggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\nplt\n\n\n\nFigure 3: Simple Scatterplot\n\n\n\n\nWe can also add marginal boxplots for each variable using the package ggExtra.\n\nggMarginal(plt, type = 'boxplot')\n\n\n\nFigure 4: The statistical relationship between height and handspan\n\n\n\n\nOutliers are extreme observations that are not possible values of a variable or that do not seem to fit with the rest of the data. This could either be:\n\n\nmarginally along one axis: points that have an unusual (too high or too low) x-coordinate or y-coordinate;\n\njointly: observations that do not fit with the rest of the point cloud.\n\nThe boxplots in Figure 4 do not highlight any outliers in the marginal distributions of height and handspan. Furthermore, from the scatterplot we do not notice any extreme observations or points that do not fit with the rest of the point cloud.\nWe notice a moderate, positive linear relationship between height and handspan.\nRecall Figure 2, displaying the relationship between side and perimeters of squares. In the plot we notice two points on top of each other, reflecting the fact that two squares having the same side will always have the same perimeter. In fact, the data from the Scottish National Gallery include two squared paintings with a side of 1.1m, both having a measured perimeter of 4.4m.\nFigure 4, instead, displays the relationship between height and handspan of a sample of students. The first thing that grabs our attention is the fact that students having the same height do not necessarily have the same handspan. Rather, we clearly see a variety of handspan values for students all having a height of, for example, 70in. To be more precise, the seven students who are 70 in. tall all have differing handspans.\n\n\n\n\n\nQuestion 9\n\n\nUsing the following command, superimpose on top of the scatterplot a best-fit line describing how handspan varies as a function of height. For the moment, the argument se = FALSE tells R to not display uncertainty bands.\n\ngeom_smooth(method = lm, se = FALSE)\n\nComment on any differences between the lines representing the linear relationship between (a) the side and perimeter of square and (b) height and handspan.\n\n\n\n\n Solution \n\n\n\nggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_smooth(method = lm, se = FALSE) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\n\n\nFigure 5: The best-fit line\n\n\n\n\nThe line representing the relationship between side and perimeter of squares is able to predict the actual perimeter value from the measurement of the side of a square. This is possible because the relationship between side and perimeter is an exact one.\nThat is, any squares having the same side will have the same perimeter, and there will be no variation in those values.\nThe line that best fits the relationship between height and handspan (see Figure 5), instead, is only able to predict the average handspan for a given value of height.\nThis is because there will be a distribution of handspans at each value of height. The line will fit the trend/pattern in the values, but there will be individual-to-individual variability that we must accept around that average pattern.\n\n\n\n\n\nQuestion 10\n\n\n\n\n\nThe line of best-fit is given by:1\n\\[\n\\widehat{Handspan} = -3 + 0.35 \\ Height\n\\]\nCalculate the predicted handspan of a student who is (a) 73in tall, and (b) 5in tall.\n\n\n\n\n Solution \n\n\n\nThe predicted average handspan for students who are 73in tall is: \\(-3 + 0.35 * 73 = 22.55\\)cm. \n\nThe predicted average handspan for students who are 5in tall is: \\(-3 + 0.35 * 5 = -1.25\\)cm.\n\nBut wait, handspan can not be negative… This does not make any sense! That’s right, we went too far off the range of the available data on heights, which were between 57in and 78in. We extrapolated. This is very dangerous…\n\n\n\n\nSource: Randall Munroe, xkcd.com"
  },
  {
    "objectID": "1_02_slr.html",
    "href": "1_02_slr.html",
    "title": "Intro to Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to specify a simple linear model.\nUnderstand what fitted values and residuals are.\nBe able to interpret the coefficients of a fitted model.\nBe able to test hypotheses and construct confidence intervals for the regression coefficients.\n\n\nBe up to date with lectures from Weeks 1 & 2\nHave completed Week 1 lab exercises\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/riverview.csv."
  },
  {
    "objectID": "1_02_slr.html#data-exploration",
    "href": "1_02_slr.html#data-exploration",
    "title": "Intro to Linear Regression",
    "section": "Data Exploration",
    "text": "Data Exploration\nThe common first port of call for almost any statistical analysis is to explore the data, and we can do this visually and/or numerically.\n\n\n\n\n\n\n\n\nMarginal Distributions\nBivariate Associations\n\n\n\nDescription\nThe distribution of each variable (e.g., employee incomes and education levels) without reference to the values of the other variables\nDescribing the relationship between two numeric variables\n\n\n\nVisually \n\n\nPlot each variable individually.  You could use, for example, geom_density() for a density plot or geom_histogram() for a histogram to comment on and/or examine: \n\nThe shape of the distribution. Look at the shape, centre and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\n\n\nIdentify any unusual observations. Do you notice any extreme observations (i.e., outliers)?\n\n\n\nPlot associations among two variables.  You could use, for example, a scatterplot to comment on and/or examine: \nThe direction of the association indicates whether there is a positive or negative association\n\n\nThe form of association refers to whether the relationship between the variables can be summarized well with a straight line or some more complicated pattern\n\n\nThe strength of association entails how closely the points fall to a recognizable pattern such as a line\n\n\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail\n\n\n\n\n\nNumerically \n\nCompute and report summary statistics e.g., mean, standard deviation, median, min, max, etc.  You could, for example, calculate summary statistics such as the mean (mean()) and standard deviation (sd()), etc. within summarize()\n\nCompute and report the correlation coefficient.  You can use the cor() function to calculate this\n\n\n\nMarginal Distributions\n\nQuestion 1\n\n\nVisualise and describe the marginal distribution of employee incomes.\n\n\n\n\n Solution \n\n\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = income)) +\n  geom_density() +\n  geom_boxplot(width = 1/300) +\n  labs(x = \"Income (in thousands of U.S. dollars)\", \n       y = \"Probability density\")\n\n\n\nFigure 1: Density plot and boxplot of employee incomes\n\n\n\n\nThe plot suggested that the distribution of employee incomes was unimodal, and most of the incomes were between roughly $45,000 and $70,000. The lowest income in the sample was approximately $25,000 and the highest over $80,000. This suggested there was a fair high degree of variation in the data. Furthermore, the boxplot did not highlight any outliers in the data.\nSummary statistics for the employees’ incomes:\n\ndesc_income <- riverview %>% \n  summarize(\n    M = mean(income), \n    SD = sd(income)\n    )\ndesc_income\n\n# A tibble: 1 × 2\n      M    SD\n  <dbl> <dbl>\n1  53.7  14.6\n\n\nFollowing the exploration above, we can describe the income variable as follows:\n\n\n\n\n\n\nThe marginal distribution of income was unimodal with a mean of approximately $53,700. There was variation in employees’ salaries (SD = $14,553).\n\n\n\n\n\n\n\nQuestion 2\n\n\nVisualise and describe the marginal distribution of education level.\n\n\n\n\n Solution \n\n\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = education)) +\n  geom_density() +\n  geom_boxplot(width = 1/100) +\n  labs(x = \"Education (in years)\", \n       y = \"Probability density\")\n\n\n\nFigure 2: Density plot and boxplot of employee education levels\n\n\n\n\nSummary statistics for the employees’ level of education:\n\ndesc_education <- riverview %>%\n  summarize(\n    M = mean(education),\n    SD = sd(education)\n    )\ndesc_education\n\n# A tibble: 1 × 2\n      M    SD\n  <dbl> <dbl>\n1    16  4.36\n\n\n\n\n\n\n\n\n\nThe marginal distribution of education was unimodal with an average of of 16 years. There was variation in employees’ level of education (SD = 4.4 years).\n\n\n\n\n\n\nAssociations among Variables\n\nQuestion 3\n\n\nCreate a scatterplot of income and education level before calculating the correlation between income and education level.\nMaking reference to both the plot and correlation coefficient, describe the association between income and level of education among the employees in the sample.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe are trying to investigate how income varies when varying years of formal education. Hence, income is the dependent variable (on the y-axis), and education is the independent variable (on the x-axis).\n\n\n\n\n\n\n\n Solution \n\n\nLet’s produce a scatterplot:\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Education (in years)\", \n       y = \"Income (in thousands of U.S. dollars)\")\n\n\n\nFigure 3: The association between employees’ education level and income\n\n\n\n\nTo comment on the strength of the linear association we compute the correlation coefficient:\n\ncorr <- riverview %>%\n  select(education, income) %>%\n  cor()\ncorr\n\n          education    income\neducation 1.0000000 0.7947847\nincome    0.7947847 1.0000000\n\n\nthat is,\n\\[\nr_{\\text{education, income}} = 0.79\n\\]\n\n\n\n\n\n\n\nThere was a strong positive linear association between education level and income for the employees in the sample. High incomes tended to be observed, on average, with more years of formal education (\\(r\\) = .79).\nThe scatterplot did not highlight any outliers."
  },
  {
    "objectID": "1_02_slr.html#model-specification-and-fitting",
    "href": "1_02_slr.html#model-specification-and-fitting",
    "title": "Intro to Linear Regression",
    "section": "Model Specification and Fitting",
    "text": "Model Specification and Fitting\nThe scatterplot highlighted a linear relationship, where the data points were scattered around an underlying linear pattern with a roughly-constant spread as x varied.\nHence, we will try to fit a simple (i.e., one x variable only) linear regression model:\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\\\\n\\quad \\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nwhere “\\(\\epsilon \\sim N(0, \\sigma) \\text{ independently}\\)” means that the errors around the line have mean zero and constant spread as x varies.\n\nQuestion 4\n\n\nUsing the lm() function, fit a simple linear model to predict income (DV) by Education (IV), naming the output mdl.\nWrite down the equation of the fitted line.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe syntax of the lm() function is:\n\n[model name] <- lm([response variable i.e., dependent variable] ~ [explanatory variable i.e., independent variable], data = [dataframe])\n\n\n\n\n\n\n\n\n Solution \n\n\nThe fitted model can be written as\n\\[\n\\widehat{Income} = \\hat \\beta_0 + \\hat \\beta_1 \\ Education\n\\]\nor\n\\[\n\\widehat{Income} = \\hat \\beta_0 \\cdot 1 + \\hat \\beta_1 \\cdot Education\n\\]\nWhen we specify the linear model in R, we include after the tilde sign, ~, the variables that appear to the right of the \\(\\hat \\beta\\)s. That’s why the 1 is included.\nAs the variables are in the riverview dataframe, we would write:\n\nmdl <- lm(income ~ 1 + education, data = riverview)\nmdl\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\nNote that by calling the name of the fitted model, mdl, you can see the estimated regression coefficients \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). The fitted line is\n\n\n\n\n\n\n\\[\n\\widehat{Income} = 11.32 + 2.65 \\ Education \\\\\n\\]\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nExplore the following equivalent ways to obtain the estimated regression coefficients — that is, \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) — from the fitted model:\n\nmdl\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\n\n\n Solution \n\n\nThe estimated parameters returned by the below methods are all equivalent. However, summary() returns more information.\n\n\nmdl()\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\nSimply invoke the name of the fitted model:\n\nmdl\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\n\n\n\nmdl$coefficients\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\n\ncoef(mdl)\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\n\ncoefficients(mdl)\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\n\n\n\n\n\n\n\n\n\nThe estimated intercept is \\(\\hat \\beta_0 = 11.32\\) and the estimated slope is \\(\\hat \\beta_1 = 2.65\\).\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nExplore the following equivalent ways to obtain the estimated standard deviation of the errors — that is, \\(\\hat \\sigma\\) — from the fitted model mdl:\n\nsigma(mdl)\nsummary(mdl)\n\n\n Huh? What is \\(\\sigma\\)?\n\n\nThe standard deviation of the errors, denoted by \\(\\sigma\\) is an important quantity to estimate because it measures how much individual data points tend to deviate above and below the regression line.\nA small \\(\\sigma\\) indicates that the points hug the line closely and we should expect fairly accurate predictions, while a large \\(\\sigma\\) suggests that, even if we estimate the line perfectly, we can expect individual values to deviate from it by substantial amounts.\nThe estimated standard deviation of the errors is (surprisingly) denoted \\(\\hat \\sigma\\) and is equal to\n\\[\n\\hat \\sigma = \\sqrt{\\frac{SS_{Residual}}{n - 2}}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\nThe estimated standard deviation of the errors can be equivalently obtained by the below methods. However, summary() returns more information.\n\n\nsigma(mdl)\nsummary(mdl)\n\n\n\n\nsigma(mdl)\n\n[1] 8.978116\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term “Residual standard error” is a misnomer, as the help page for sigma says (check ?sigma). However, it’s hard to get rid of this bad name as it has been used in too many books showing R output.\n\n\n\n\n\n\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma = 8.98\\).\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nInterpret the estimated intercept and slope in the context of the research question.\n\n\n\n\n Solution \n\n\nWe can interpret the estimated intercept as follows:\n\n\n\n\n\n\nThe estimated average income associated to zero years of formal education is $11,321.\n\n\n\nFor the estimated slope we might write:\n\n\n\n\n\n\nThe estimated increase in average income associated to a one year increase in education is $2,651.\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nInterpret the estimated standard deviation of the errors in the context of the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo interpret the estimated standard deviation of the errors we can use the fact that about 95% of values from a normal distribution fall within two standard deviations of the centre.\n\n\n\n\n\n\n\n Solution \n\n\nWe can interpret the estimated standard deviation of the errors as follows:\n\n\n\n\n\n\nFor any particular level of education, employee incomes should be distributed above and below the regression line with standard deviation estimated to be \\(\\hat \\sigma = 8.98\\). Since \\(2 \\hat \\sigma = 2 (8.98) = 17.96\\), we expect most (about 95%) of the employee incomes to be within about $18,000 from the regression line.\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nPlot the data and the fitted regression line. To do so:\n\nExtract the estimated regression coefficients e.g., via betas <- coef(mdl)\n\nExtract the first entry of betas (i.e., the intercept) via betas[1]\n\nExtract the second entry of betas (i.e., the slope) via betas[2]\n\nProvide the intercept and slope to the function\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn your ggplot(), you will need to specify geom_abline(). This might help get you started:\n\ngeom_abline(intercept = <intercept>, slope = <slope>)\n\n\n\n\n\n\n\n\n Solution \n\n\nThe function coef(mdl) returns a vector: that is, a sequence of numbers all of the same type. To get the first element of the sequence you append [1], and [2] for the second.\nWe can plot the model as follows:\n\nbetas <- coef(mdl)\nintercept <- betas[1]\nslope <- betas[2]\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = intercept, slope = slope, \n              color = 'blue', size = 1) + \n  labs(x = \"Education (in years)\", \n       y = \"Income (in thousands of U.S. dollars)\")"
  },
  {
    "objectID": "1_02_slr.html#fitted-and-predicted-values",
    "href": "1_02_slr.html#fitted-and-predicted-values",
    "title": "Intro to Linear Regression",
    "section": "Fitted and Predicted Values",
    "text": "Fitted and Predicted Values\nTo compute the model-predicted values for the data in the sample, we can use various funcitons:\n\npredict(<fitted model>)\nfitted(<fitted model>)\nfitted.values(<fitted model>)\nmdl$fitted.values\n\nFor example, this will give us the estimated income (point on our regression line) for each observed value of education level.\n\npredict(mdl)\n\n       1        2        3        4        5        6        7        8 \n32.53175 32.53175 37.83435 37.83435 37.83435 43.13694 43.13694 43.13694 \n       9       10       11       12       13       14       15       16 \n43.13694 48.43953 48.43953 48.43953 51.09083 53.74212 53.74212 53.74212 \n      17       18       19       20       21       22       23       24 \n53.74212 53.74212 56.39342 59.04472 59.04472 61.69601 61.69601 64.34731 \n      25       26       27       28       29       30       31       32 \n64.34731 64.34731 64.34731 66.99861 66.99861 69.64990 69.64990 74.95250 \n\n\nWe can also compute model-predicted values for other (unobserved) data:\n\npredict(<fitted model>, newdata = <dataframe>)\n\nWe first need to remember that the model predicts income using the independent variable education. Hence, if we want predictions for new data, we first need to create a tibble with a column called education containing the years of education for which we want the prediction.\n\nnewdata <- tibble(education = c(11, 23))\nnewdata\n\n# A tibble: 2 × 1\n  education\n      <dbl>\n1        11\n2        23\n\n\nThen we take newdata and add a new column called income_hat, computed as the prediction from the fitted mdl using the newdata above:\n\nnewdata <- newdata %>%\n  mutate(\n    income_hat = predict(mdl, newdata = newdata)\n  )\nnewdata\n\n# A tibble: 2 × 2\n  education income_hat\n      <dbl>      <dbl>\n1        11       40.5\n2        23       72.3"
  },
  {
    "objectID": "1_02_slr.html#residuals",
    "href": "1_02_slr.html#residuals",
    "title": "Intro to Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nThe residuals represent the deviations between the actual responses and the predicted responses and can be obtained either as\n\n\nmdl$residuals;\n\nresid(mdl);\n\nresiduals(mdl);\ncomputing them as the difference between the response and the predicted response."
  },
  {
    "objectID": "1_03_mlr.html",
    "href": "1_03_mlr.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Setup\n\nSetup\n\n\n\nCreate a new RMarkdown file\nLoad the required package(s)\nRead the wellbeing dataset into R, assigning it to an object named mwdata\n\n\n\n\n\n\n Solution \n\n\n\n#Loading the required package(s)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(sjPlot)\n\n# Reading in data and storing to an object named 'mwdata'\nmwdata <- read_csv(\"https://uoepsy.github.io/data/wellbeing.csv\")\n\n\n\n\nStudy Overview\n\nResearch Question\nIs there an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions?\n\n\n Wellbeing data codebook.\n\n\nDescription\nResearchers interviewed 32 participants, selected at random from the population of residents of Edinburgh & Lothians. They used the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\nThe researchers also asked participants to estimate the average number of hours they spend outdoors each week, the average number of social interactions they have each week (whether on-line or in-person), and whether they believe that they stick to a routine throughout the week (Yes/No).\nThe data in wellbeing.csv contain five attributes collected from a random sample of \\(n=32\\) hypothetical residents over Edinburgh & Lothians, and include:\n\n\nwellbeing: Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n\n\noutdoor_time: Self report estimated number of hours per week spent outdoors\n\n\nsocial_int: Self report estimated number of social interactions per week (both online and in-person)\n\nroutine: Binary Yes/No response to the question “Do you follow a daily routine throughout the week?”\n\nlocation: Location of primary residence (City, Suburb, Rural)\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\n\n\nwellbeing\n      outdoor_time\n      social_int\n      location\n      routine\n    \n\n\n30\n7\n8\nSuburb\nRoutine\n\n\n21\n9\n8\nCity\nNo Routine\n\n\n38\n14\n10\nSuburb\nRoutine\n\n\n27\n16\n10\nCity\nNo Routine\n\n\n20\n1\n10\nRural\nNo Routine\n\n\n37\n11\n12\nSuburb\nNo Routine\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nProduce plots of the marginal distributions (the distributions of each variable in the analysis without reference to the other variables) of the wellbeing, outdoor_time, and social_int variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nYou could use, for example, geom_density() for a density plot or geom_histogram() for a histogram.\nLook at the shape, center and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\n\n\n\n\n\n\n\n\n Solution \n\n\nWe should be familiar now with how to visualise a marginal distribution. You might choose histograms, density curves, or boxplots, or a combination:\n\nwellbeing_plot <- \n  ggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  geom_boxplot(width = 1/250) +\n  labs(x = \"Score on WEMWBS (range 14-70)\", y = \"Probability\\ndensity\")\n\noutdoortime_plot <- \n  ggplot(data = mwdata, aes(x = outdoor_time)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Probability\\ndensity\")\n\nsocial_plot <- \n  ggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  geom_boxplot(width = 1/150) +\n  labs(x = \"Number of social interactions per week\", y = \"Probability\\ndensity\")\n\n# the \"patchwork\" library allows us to arrange multiple plots\nwellbeing_plot / outdoortime_plot / social_plot\n\n\n\nFigure 1: Marginal distribution plots of wellbeing sores, weekly hours spent outdoors, and social interactions\n\n\n\n\nSummary statistics for wellbeing, outdoor time, and social interactions:\n\ndescriptives <- mwdata %>% \n  summarize(\n    M_Wellbeing = mean(wellbeing), \n    SD_Wellbeing = sd(wellbeing),\n    M_OutTime = mean(outdoor_time), \n    SD_OutTime = sd(outdoor_time),\n    M_SocInt = mean(social_int), \n    SD_SocInt = sd(social_int)\n    )\ndescriptives\n\n# A tibble: 1 × 6\n  M_Wellbeing SD_Wellbeing M_OutTime SD_OutTime M_SocInt SD_SocInt\n        <dbl>        <dbl>     <dbl>      <dbl>    <dbl>     <dbl>\n1          43         11.7      14.8       6.95       16      4.36\n\n\n\n\nThe marginal distribution of scores on the WEMWBS is unimodal with a mean of approximately 43. There is variation in WEMWBS scores (SD = 11.7).\n\nThe marginal distribution of weekly hours spent outdoors is unimodal with a mean of approximately 14.8. There is variation in weekly hours spent outdoors (SD = 6.9).\n\nThe marginal distribution of numbers of social interactions per week is unimodal with a mean of approximately 16. There is variation in numbers of social interactions (SD = 4.4).\n\n\n\n\n\n\n\nQuestion 2\n\n\nProduce plots of the associations between the outcome variable (wellbeing) and each of the explanatory variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThink about:\n\n\nDirection of association\n\nForm of association (can it be summarised well with a straight line?)\n\n\nStrength of association (how closely do points fall to a recognizable pattern such as a line?)\n\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail.\n\nPlot tips:\n\nuse \\n to wrap text in your titles and or axis labels\nconsider using geom_smooth() to superimpose the best-fitting line describing the association of interest\n\n\n\n\n\n\n\n\n Solution \n\n\n\nwellbeing_outdoor <- \n  ggplot(data = mwdata, aes(x = outdoor_time, y = wellbeing)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Time spent outdoors \\nper week (hours)\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_social <- \n  ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Number of social interactions \\nper week\", y = \"Wellbeing score (WEMWBS)\")\n\n# place plots adjacent to one another\nwellbeing_outdoor | wellbeing_social\n\n\n\nFigure 2: Scatterplots displaying the relationships between scores on the WEMWBS and a) weekly outdoor time (hours), and b) weekly number of social interactions\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nProduce a correlation matrix of the variables which are to be used in the analysis, and write a short paragraph describing the associations.\n\nCorrelation matrix\nA table showing the correlation coefficients - \\(r_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\\) - between variables. Each cell in the table shows the relationship between two variables. The diagonals show the correlation of a variable with itself (and are therefore always equal to 1).\n\nIn R, we can create a correlation matrix by giving the cor() function a dataframe. However, we only want to give it 3 columns here. Think about how we select specific columns, either using select(), or giving the column numbers inside [].\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nMake sure to round your numbers in-line with APA 7th edition guidelines. The round() function will come in handy here, as might this APA numbers and statistics guide!\n\n\n\n\n\n\n\n Solution \n\n\nWe can either use:\n\n# correlation matrix of the first 3 columns\nround(cor(mwdata[,1:3]), digits = 2)\n\nor:\n\n# select only the columns we want by name, and pass this to cor()\nmwdata %>% \n  select(wellbeing, outdoor_time, social_int) %>%\n  cor() %>%\n    round(digits = 2)\n\n             wellbeing outdoor_time social_int\nwellbeing         1.00         0.58       0.79\noutdoor_time      0.58         1.00       0.34\nsocial_int        0.79         0.34       1.00\n\n\n\n\n\n\n\n\n\nThere was a moderate, positive, linear association between weekly outdoor time and WEMWBS scores for the participants in the sample (\\(r\\) = .58). Higher number of hours spent outdoors each week was associated, on average, with higher wellbeing scores,\n\nThere was a moderate, positive, linear association between the weekly number of social interactions and WEMWBS scores for the participants in the sample (\\(r\\) = .79). More social interactions were associated, on average, with higher wellbeing scores.\nThere was a weak positive correlation between weekly outdoor time and the weekly number of social interactions (\\(r\\) = .34).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there is a weak correlation between our two explanatory variables (outdoor_time and social_int). We will return to how this might affect our model when later on we look at the assumptions of multiple regression.\n\n\n\n\n\n\n\nQuestion 4\n\n\nThe scatterplots we created above show moderate, positive, and linear relationships both between outdoor time and wellbeing, and between numbers of social interactions and wellbeing.\n\nSpecify the form of your model, where \\(y\\) = scores on the WEMWBS, \\(x_1\\) = weekly number of social interactions, and \\(x_2\\) = weekly outdoor time.\nWhat are the parameters of the model. How do we denote parameter estimates?\nFit the linear model in using lm(), assigning the output to an object called mdl1.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAs we did for simple linear regression, we can fit our multiple regression model using the lm() function. We can add as many explanatory variables as we like, separating them with a +.\n\n( <response variable> ~ 1 + <explanatory variable 1> + <explanatory variable 2> + ... , data = <dataframe> )\n\n\n\n\n\n\n\n\n Solution \n\n\nA model for the relationship between \\(x_1\\) = weekly outdoor time, \\(x_2\\) = weekly numbers of social interactions and \\(y\\) = scores on the WEMWBS is given by:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\\\ \\quad \\\\\n\\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nIn the model specified above,\n\n\n\\(\\mu_{y|x_1, x_2} = \\beta_0 + \\beta_1 x + \\beta_2 x_2\\) represents the systematic part of the model giving the mean of \\(y\\) at each combination of values of \\(x_1\\) and \\(x_2\\);\n\n\\(\\epsilon\\) represents the error (deviation) from that mean, and the errors are independent from one another.\n\nThe parameters of our model are:\n\n\n\\(\\beta_0\\) (The intercept);\n\n\\(\\beta_1\\) (The slope across values of \\(x_1\\));\n\n\\(\\beta_2\\) (The slope across values of \\(x_2\\));\n\n\n\\(\\sigma\\) (The standard deviation of the errors).\n\nWhen we estimate these parameters from the available data, we have a fitted model (recall that the h\\(\\hat{\\textrm{a}}\\)ts are used to distinguish our estimates from the true unknown parameters):\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Social Interactions + \\hat \\beta_2 \\cdot Outdoor Time\n\\]\nAnd we have residuals \\(\\hat \\epsilon = y - \\hat y\\) which are the deviations from the observed values and our model-predicted responses.\nFitting the model in R:\n\nmdl1 <- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\n\n\n\n\n\n\nVisual\nNote that for simple linear regression we talked about our model as a line in 2 dimensions: the systematic part \\(\\beta_0 + \\beta_1 x\\) defined a line for \\(\\mu_y\\) across the possible values of \\(x\\), with \\(\\epsilon\\) as the random deviations from that line. But in multiple regression we have more than two variables making up our model.\nIn this particular case of three variables (one outcome + two explanatory), we can think of our model as a regression surface (see Figure 3). The systematic part of our model defines the surface across a range of possible values of both \\(x_1\\) and \\(x_2\\). Deviations from the surface are determined by the random error component, \\(\\hat \\epsilon\\).\n\n\n\n\nFigure 3: Regression surface for wellbeing ~ social_int + outdoor_time, from two different angles\n\n\n\n\nDon’t worry about trying to figure out how to visualise it if we had any more explanatory variables! We can only concieve of 3 spatial dimensions. One could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it’s not worth trying!.\n\n\n\nQuestion 5\n\n\nState the research question in the form of a testable hypothesis.\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou must define both a null (\\(H_0\\)) and alternative hypothesis (\\(H_1\\)).\n\n\n\n\n\n\n\n Solution \n\n\nIn words:\n\\(H_0\\): There is no association between well-being and time spent outdoors after taking into account the relationship between well-being and social interactions\n\\(H_1\\): There is an association between well-being and time spent outdoors after taking into account the relationship between well-being and social interactions\nIn symbols:\n\\(H_0: \\beta_2 = 0\\)\n\\(H_1: \\beta_2 \\neq 0\\)\n\n\n\n\n\nQuestion 6\n\n\nUsing any of:\n\nmdl1\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\nWrite out the estimated parameter values of:\n\n\n\\(\\hat \\beta_0\\), the estimated average wellbeing score associated with zero hours of outdoor time and zero social interactions per week.\n\n\n\\(\\hat \\beta_1\\), the estimated increase in average wellbeing score associated with an additional social interaction per week (an increase of one), holding weekly outdoor time constant.\n\n\n\\(\\hat \\beta_2\\), the estimated increase in average wellbeing score associated with one hour increase in weekly outdoor time, holding the number of social interactions constant\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ: What do we mean by hold constant / controlling for / partialling out / residualizing for?\nA: When the remaining explanatory variables are held at the same value or are fixed.\n\n\n\n\n\n\n Solution \n\n\n\n\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\n\n\n\nmdl1$coefficients\n\n (Intercept)   social_int outdoor_time \n   5.3703775    1.8034489    0.5923673 \n\n\n\n\n\ncoef(mdl1)\n\n (Intercept)   social_int outdoor_time \n   5.3703775    1.8034489    0.5923673 \n\n\n\n\n\ncoefficients(mdl1)\n\n (Intercept)   social_int outdoor_time \n   5.3703775    1.8034489    0.5923673 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\n\n\n\n\n\n\\(\\hat \\beta_0\\) = 5.37\n\n\n\\(\\hat \\beta_1\\) = 1.8\n\n\n\\(\\hat \\beta_2\\) = 0.59\n\n\n\n\n\n\nQuestion 7\n\n\nWithin what distance from the model predicted values (the regression surface) would we expect 95% of wEMWBS wellbeing scores to be?\n\n\n\n\n\n\nHint\n\n\n\n\n\nEither sigma() or part of the output from summary() will help you here.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nsigma(mdl1)\nsummary(mdl1)\n\n\n\n\nsigma(mdl1)\n\n[1] 6.148276\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma\\) = 6.15. We would expect 95% of wellbeing scores to be within about 12.3 (\\(2 \\hat \\sigma\\)) from the model fit.\n\n\n\n\n\nQuestion 8\n\n\nBased on the model, predict the wellbeing scores for the following individuals:\n\nLeah: Social Interactions = 24; Outdoor Time = 3\nSean: Social Interactions = 19; Outdoor Time = 26\nMike: Social Interactions = 15; Outdoor Time = 20\nDonna: Social Interactions = 7; Outdoor Time = 2\n\nWho has the highest predicted wellbeing score, and who has the lowest?\n\n\n\n\n Solution \n\n\nFirst we need to pass the data into R:\n\nwellbeing_query <- tibble(social_int = c(24, 19, 15, 7),\n                          outdoor_time = c(3, 26, 20, 2))\n\nAnd next use predict() to get their estimated wellbeing scores:\n\npredict(mdl1, newdata = wellbeing_query)\n\n       1        2        3        4 \n50.43025 55.03746 44.26946 19.17925 \n\n\nSean has the highest predicted wellbeing score (55.04), and Donna the lowest (19.18).\n\n\n\n\n\nQuestion 9\n\n\nShould we reject or fail to reject \\(H_0\\)? Why?\n\n\n\n\n Solution \n\n\nThe research question asked whether there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. This was equivalent to testing the following null hypothesis:\n\\(H_0: \\beta_2 = 0\\)\nBased on the model output (if we considered effects to be significant at \\(\\alpha\\) = .05), we should reject the null hypothesis since our \\(p\\)-value smaller than this (\\(p\\) = .0015). In short, we reject the null since \\(p\\) < .05.\n\n\n\n\n\nQuestion 10\n\n\nInterpret the outdoor time coefficient in the context of the research question.\n\n\n\n\n Solution \n\n\nA multiple regression model was used to determine if there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. Outdoor time was significantly associated with wellbeing scores (\\(\\beta\\) = 0.59, SE = 0.17, \\(p\\) < .001) after controlling for the number of weekly social interactions. Results suggested that for every additional hour spent outdoors each week, wellbeing scores increased by 0.59 points."
  },
  {
    "objectID": "1_04_model_fit.html",
    "href": "1_04_model_fit.html",
    "title": "Model Fit and Standardization",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand how to interpret significance tests for \\(\\beta\\) coefficients\nUnderstand how to calculate the interpret \\(R^2\\) and adjusted-\\(R^2\\) as a measure of model quality.\nUnderstand the calculation and interpretation of the \\(F\\)-test of model utility.\nUnderstand how to standardize model coefficients and when this is appropriate to do.\n\n\nBe up to date with lectures\nHave completed previous lab exercises from Week 1, Week 2, and Week 3\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\nsjPlot\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing.csv.\nNote: this is the same data as Lab 3."
  },
  {
    "objectID": "1_04_model_fit.html#lab-purpose",
    "href": "1_04_model_fit.html#lab-purpose",
    "title": "Model Fit and Standardization",
    "section": "Lab Purpose",
    "text": "Lab Purpose\nIn this lab (Lab 4), you will focus on the statistics contained within the highlighted sections of the summary() output below. You will be both calculating these by hand and deriving via R code before interpreting these values in the context of the research question following APA guidelines.\n\n\n\n\n\n\n\nQuestion 3\n\n\nTest the hypothesis that the population slope for outdoor time is zero — that is, that there is no linear association between wellbeing and outdoor time (after controlling for the number of social interactions) in the population.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRecall the formula for obtaining a test statistic:\nA test statistic for the null hypothesis \\(H_0: \\beta_j = 0\\) is\n\\[\nt = \\frac{\\hat \\beta_j - 0}{SE(\\hat \\beta_j)}\n\\]\nwhich follows a \\(t\\)-distribution with \\(n-k-1 = n - 2 - 1 = n - 3\\) degrees of freedom.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR function\n\n\n\nWe calculate the test statistic for \\(\\beta_2\\)\n\\[\nt = \\frac{\\hat \\beta_2 - 0}{SE(\\hat \\beta_2)} = \\frac{0.5924 - 0}{0.1689} = 3.5074\n\\]\nand compare it with the 5% critical value from a \\(t\\)-distribution with \\(n-3\\) degrees of freedom, which is:\n\nn <- nrow(mwdata)\ntstar <- qt(0.975, df = n - 3)\ntstar\n\n[1] 2.04523\n\n#tstar = 2.04523\n\nAs \\(|t|\\) (\\(|t|\\) = 3.51) is much larger than \\(t^*\\) (\\(t^*\\) = 2.05), we can reject then null hypothesis as we have strong evidence against it.\nThe \\(p\\)-value, shown below, also confirms this conclusion.\n\n2 * (1 - pt(3.506, n - 3))\n\n[1] 0.001500588\n\n\n\n\nPlease note that the same information was already contained in the row corresponding to the variable “outdoor_time” in the output of summary(mdl), which reported the \\(t\\)-statistic under t value and the \\(p\\)-value under Pr(>|t|):\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\nThe result is exactly the same (up to rounding errors) as calculating manually.\nBefore we interpret the results, note that sometimes \\(p\\)-values will be reported to \\(e^X\\). For example, look in the Pr(>|t|) column for “social_int”. The value \\(2.37e^{-07}\\) simply means \\(2.37 \\times 10^{-7}\\). This is a very small value (i.e., 0.000000237), hence we will report it as <.001 following the APA guidelines.\n\n\n\n\n\n\n\n\n\nWe performed a \\(t\\)-test against the null hypothesis that outdoor time was not associated with wellbeing scores after controlling for social interactions. A significant association was found between outdoor time (in hours per week) and wellbeing (WEMWBS scores) \\(t(29) = 3.51,\\ p = 002\\), two-sided. Thus, we have evidence to reject the null hypothesis.\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nObtain 95% confidence intervals for the regression coefficients, and write a sentence about each one.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRecall the formula for obtaining a confidence interval:\nA confidence interval for the population slope is\n\\[\n\\hat \\beta_j \\pm t^* \\cdot SE(\\hat \\beta_j)\n\\]\nwhere \\(t^*\\) denotes the critical value chosen from t-distribution with \\(n-k-1 = n - 2 - 1 = n - 3\\) degrees of freedom for a desired \\(\\alpha\\) level of confidence.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR function\n\n\n\nFor 95% confidence we have \\(t^* = 2.05\\):\n\nn <- nrow(mwdata)\ntstar <- qt(0.975, df = n - 3)\ntstar\n\n[1] 2.04523\n\n\nThe confidence intervals are:\n\ntibble(\n  b0_LowerCI = round(5.3704 - (qt(0.975, n-3) * 4.3205), 3),\n  b0_UpperCI = round(5.3704 + (qt(0.975, n-3)* 4.3205), 3),\n  b1_LowerCI = round(1.8034 - (qt(0.975, n-3) * 0.2691), 3),\n  b1_UpperCI = round(1.8034 + (qt(0.975, n-3)* 0.2691), 3),\n  b2_LowerCI = round(0.5924 - (qt(0.975, n-3) * 0.1689), 3),\n  b2_UpperCI = round(0.5924 + (qt(0.975, n-3)* 0.1689), 3)\n      )\n\n# A tibble: 1 × 6\n  b0_LowerCI b0_UpperCI b1_LowerCI b1_UpperCI b2_LowerCI b2_UpperCI\n       <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>\n1      -3.47       14.2       1.25       2.35      0.247      0.938\n\n\n\n\nWe can easily obtain the confidence intervals for the regression coefficients using the command confint():\n\nconfint(mdl1, level = 0.95)\n\n                  2.5 %     97.5 %\n(Intercept)  -3.4660660 14.2068209\nsocial_int    1.2530813  2.3538164\noutdoor_time  0.2468371  0.9378975\n\n\nThe result is exactly the same (up to rounding errors) as calculating manually.\n\n\n\n\n\n\n\n\n\n\nThe average wellbeing score for all those with zero hours of outdoor time and zero social interactions per week was between -3.47 and 14.21.\n\nWhen holding weekly outdoor time constant, each increase of one social interaction per week was associated with a difference in wellbeing scores between 1.25 and 2.35, on average.\n\nWhen holding the number of social interactions per week constant, each one hour increase in weekly outdoor time was associated with a difference in wellbeing scores between 0.25 and 0.94, on average.\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nWhat is the proportion of the total variability in wellbeing scores explained by the model?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe question asks to compute the value of \\(R^2\\). Since the model includes 2 predictors, you should report the Adjusted \\(R^2\\).\n\n\n\n\n\n\n\n Solution \n\n\nThe proportion of the total variability explained is given by R-squared.\nThe R-squared coefficient is defined as:\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n\\]\nThe Adjusted R-squared coefficient is defined as:\n\\[\n\\hat R^2 = 1 - \\frac{(1 - R^2)(n-1)}{n-k-1}\n\\quad \\\\\n\\begin{align}\n& \\text{Where:} \\\\\n& n = \\text{sample size} \\\\\n& k = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\]\n\n\nManually\nR function\n\n\n\nIn R we can write:\n\n#R squared & adjusted R squared\n\nwellbeing_fitted <- mwdata %>%\n  mutate(\n    wellbeing_hat = predict(mdl1),\n    resid = wellbeing - wellbeing_hat\n  )\n\n\nwellbeing_fitted %>%\n  summarise(\n    SSModel = sum( (wellbeing_hat - mean(wellbeing))^2 ),\n    SSTotal = sum( (wellbeing - mean(wellbeing))^2 )\n  ) %>%\n  summarise(\n    RSquared = SSModel / SSTotal,\n    AdjRSquared = 1-((1-(RSquared))*(32-1)/(32-2-1))\n  )\n\n# A tibble: 1 × 2\n  RSquared AdjRSquared\n     <dbl>       <dbl>\n1    0.740       0.722\n\n\n\n\n\n#look in second bottom row - Multiple R Squared and Adjusted R Squared both reported here\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\nThe output of summary() displays the Adjusted \\(R\\)-squared value in the following line:\nAdjusted R-squared:  0.7224 \n\n\n\n\nInterpretation\n\n\n\n\n\n\nApproximately 72% of the total variability in wellbeing scores is explained by associations with social interactions and outdoor time.\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nPerform a model utility test at the 5% significance level and report your results.\nIn other words, conduct an \\(F\\)-test against the null hypothesis that the model is ineffective at predicting wellbeing scores using social interactions and outdoor time by computing the \\(F\\)-statistic using its definition.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe F-ratio is used to test the null hypothesis that all regression slopes are zero.\nIt is called the F-ratio because it is the ratio of the how much of the variation is explained by the model (per paramater) versus how much of the variation is left unexplained in the residuals (per remaining degrees of freedom).\n\\[\nF_{df_{model},df_{residual}} = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\\\\n\\quad \\\\\n\\begin{align}\n& \\text{Where:} \\\\\n& df_{model} = k \\\\\n& df_{residual} = n-k-1 \\\\\n& n = \\text{sample size} \\\\\n& k  = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR function\n\n\n\n\ndf1 <- 2\ndf2 <- nrow(mwdata) - 2 - 1\nf_star <- qf(0.95, df1, df2)\nf_star\n\n[1] 3.327654\n\n\n\nmodel_utility <- wellbeing_fitted %>%\n  summarise(\n    SSModel = sum((wellbeing_hat - mean(wellbeing))^2 ),\n    SSResid = sum( resid^2 ),\n    MSModel = SSModel / df1,\n    MSResid = SSResid / df2,\n    FObs = MSModel / MSResid\n  )\nmodel_utility\n\n# A tibble: 1 × 5\n  SSModel SSResid MSModel MSResid  FObs\n    <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n1   3126.   1096.   1563.    37.8  41.3\n\n\nWe can also compute the p-value:\n\npvalue <- 1 - pf(model_utility$FObs, df1, df2)\npvalue\n\n[1] 3.225548e-09\n\n\nThe value 3.225548e-09 simply means \\(3.2 \\times 10^{-9}\\), so it’s a really small number.\n\n\n\n#look in bottom row\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\nThe relevant row is the following:\n\nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\n\n\n\n\n\n\n\n\nWe performed an \\(F\\)-test of model utility at the 5% significance level, where \\(F(2,29) = 41.34, p <.001\\).\nThe large \\(F\\)-statistic and small \\(p\\)-value (\\(<.001\\)) suggested that we have very strong evidence against the null hypothesis that the model is ineffective.\nIn other words, the data provide strong evidence that the number of social interactions and outdoor time are effective predictors of wellbeing scores.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nProduce a visualisation of the association between wellbeing and outdoor time, after accounting for social interactions.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo visualise just one association, you might need to specify the terms argument in plot_model(). Don’t forget you can look up the documentation by typing ?plot_model in the console.\n\n\n\n\n\n\n\n Solution \n\n\n\nplot_model(mdl1, type = \"eff\",\n           terms = c(\"outdoor_time\"), \n           show.data = TRUE)"
  },
  {
    "objectID": "1_04_model_fit.html#standardization",
    "href": "1_04_model_fit.html#standardization",
    "title": "Model Fit and Standardization",
    "section": "Standardization",
    "text": "Standardization\n\nQuestion 8\n\n\nFit the regression model using the standardized response and explanatory variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can either:\n\nAdd to the “mwdata” dataset three variables called z_wellbeing, z_social_int, and z_outdoor_time representing the standardized welllbeing, social interactions and outdoor time variables, respectively.\n\nRecall the formula for the \\(z\\)-score:\n\\[\nz_x = \\frac{x - \\bar{x}}{s_x}, \\qquad z_y = \\frac{y - \\bar{y}}{s_y}\n\\]\nOR\n\nUse the scale() function when specifying your lm() statement.\n\n\n\n\n\n\n\n\n Solution \n\n\n\n\nZ-Score\nscale function\n\n\n\nz score variables\n\nmwdata <- mwdata %>%\n  mutate(\n    z_wellbeing = (wellbeing - mean(wellbeing)) / sd(wellbeing),\n    z_social_int = (social_int - mean(social_int)) / sd(social_int),\n    z_outdoor_time = (outdoor_time - mean(outdoor_time)) / sd(outdoor_time)\n  )\n\nCheck that they are standardized\n\nmwdata %>%\n  summarise(\n    M_z_wellbeing = round(mean(z_wellbeing),2), SD_z_wellbeing = sd(z_wellbeing), \n    M_z_social_int = round(mean(z_social_int),2), SD_z_social_int = sd(z_social_int),\n    M_z_outdoor_time = round(mean(z_outdoor_time),2), SD_z_outdoor_time = sd(z_outdoor_time)\n  )\n\n# A tibble: 1 × 6\n  M_z_wellbeing SD_z_wellbeing M_z_social_int SD_z_social_int M_z_outd…¹ SD_z_…²\n          <dbl>          <dbl>          <dbl>           <dbl>      <dbl>   <dbl>\n1             0              1              0               1          0       1\n# … with abbreviated variable names ¹​M_z_outdoor_time, ²​SD_z_outdoor_time\n\n#mean of 0, SD of 1 - all good to go\n\nRun model\n\n#with z scoring\nmdl_z <- lm(z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\nsummary(mdl_z)\n\n\nCall:\nlm(formula = z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8347 -0.4212 -0.1075  0.4822  0.9371 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    1.327e-16  9.313e-02   0.000   1.0000    \nz_social_int   6.742e-01  1.006e-01   6.702 2.37e-07 ***\nz_outdoor_time 3.527e-01  1.006e-01   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5268 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\nround(summary(mdl_z)$coefficients,3)\n\n               Estimate Std. Error t value Pr(>|t|)\n(Intercept)       0.000      0.093   0.000    1.000\nz_social_int      0.674      0.101   6.702    0.000\nz_outdoor_time    0.353      0.101   3.506    0.001\n\n\n\n\n\nmdl_s <- lm(scale(wellbeing) ~ scale(social_int) + scale(outdoor_time), data = mwdata)\nsummary(mdl_s)\n\n\nCall:\nlm(formula = scale(wellbeing) ~ scale(social_int) + scale(outdoor_time), \n    data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8347 -0.4212 -0.1075  0.4822  0.9371 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         1.327e-16  9.313e-02   0.000   1.0000    \nscale(social_int)   6.742e-01  1.006e-01   6.702 2.37e-07 ***\nscale(outdoor_time) 3.527e-01  1.006e-01   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5268 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\nround(summary(mdl_s)$coefficients,3)\n\n                    Estimate Std. Error t value Pr(>|t|)\n(Intercept)            0.000      0.093   0.000    1.000\nscale(social_int)      0.674      0.101   6.702    0.000\nscale(outdoor_time)    0.353      0.101   3.506    0.001\n\n\n\n\n\nFrom comparing either the summary() or rounded output, you should see that the estimates are the same under both approaches.\n\n\n\n\n\nQuestion 9\n\n\nCreate a table to present your results from the standardized model.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package.\nRemember that you can rename your DV and IV labels by specifying dv.labels and pred.labels.\n\n\n\n\n\n\n\n Solution \n\n\n\ntab_model(mdl_z,\n          dv.labels = \"Wellbeing (WEMWBS Scores)\",\n          pred.labels = c(\"z_social_int\" = \"Social Interactions (number per week)\",\n                          \"z_outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression table for Wellbeing model. Outcome variable and predictors are Z-scored\")\n\n\n\nRegression table for Wellbeing model. Outcome variable and predictors are Z-scored\n\n \nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n0.00\n-0.19 – 0.19\n1.000\n\n\nSocial Interactions(number per week)\n0.67\n0.47 – 0.88\n<0.001\n\n\nOutdoor Time (hours perweek)\n0.35\n0.15 – 0.56\n0.001\n\n\nObservations\n32\n\n\nR2 / R2 adjusted\n0.740 / 0.722\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret the standardized variables presented in the above table.\n\n\n\n\n Solution \n\n\n\nFor every standard deviation increase in social interactions, wellbeing scores increased on average by 0.67 standard deviations.\nFor every standard deviation increase in outdoor time, wellbeing scores increased on average by 0.35 standard deviations."
  },
  {
    "objectID": "1_05_cat_recap.html",
    "href": "1_05_cat_recap.html",
    "title": "Categorical Predictors & Block 1 Recap",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand the meaning of (and how to interpret) a multiple regression model with a binary predictor\nUnderstand how to specify a new baseline/reference level for categorical variables\n\n\nBe up to date with lectures\nHave completed Labs 1 - 4\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\nsjPlot\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing.csv.\nNote: this is the same data as Lab 3 & 4."
  },
  {
    "objectID": "1_05_cat_recap.html#study-overview",
    "href": "1_05_cat_recap.html#study-overview",
    "title": "Categorical Predictors & Block 1 Recap",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Question\nIs there an assocation between well-being and time spent outdoors after taking into account the assocation between well-being and having a routine?\n\n\n Wellbeing data codebook.\n\n\nDescription\nResearchers interviewed 32 participants, selected at random from the population of residents of Edinburgh & Lothians. They used the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\nThe researchers also asked participants to estimate the average number of hours they spend outdoors each week, the average number of social interactions they have each week (whether on-line or in-person), and whether they believe that they stick to a routine throughout the week (Yes/No).\nThe data in wellbeing.csv contain five attributes collected from a random sample of \\(n=32\\) hypothetical residents over Edinburgh & Lothians, and include:\n\n\nwellbeing: Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n\n\noutdoor_time: Self report estimated number of hours per week spent outdoors\n\n\nsocial_int: Self report estimated number of social interactions per week (both online and in-person)\n\nroutine: Binary Yes/No response to the question “Do you follow a daily routine throughout the week?”\n\nlocation: Location of primary residence (City, Suburb, Rural)\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\n\n\nwellbeing\n      outdoor_time\n      social_int\n      location\n      routine\n    \n\n\n30\n7\n8\nSuburb\nRoutine\n\n\n21\n9\n8\nCity\nNo Routine\n\n\n38\n14\n10\nSuburb\nRoutine\n\n\n27\n16\n10\nCity\nNo Routine\n\n\n20\n1\n10\nRural\nNo Routine\n\n\n37\n11\n12\nSuburb\nNo Routine"
  },
  {
    "objectID": "1_07_int1_nc.html",
    "href": "1_07_int1_nc.html",
    "title": "Interactions I: Num x Cat",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand the concept of an interaction.\nBe able to interpret the meaning of a numeric \\(\\times\\) categorical interaction.\nVisualize and probe interactions.\n\n\nBe up to date with lectures\nHave completed all labs from Semester 1 Block 1 (Weeks 1 - 5)\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npsych\npatchwork\nsandwich\ninteractions\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv."
  },
  {
    "objectID": "1_07_int1_nc.html#exercises",
    "href": "1_07_int1_nc.html#exercises",
    "title": "Interactions I: Num x Cat",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion 1\n\n\nFormally state:\n\na linear model to investigate if the association between wellbeing and social interactions differs among rural and non-rural residents\nyour chosen significance level\nthe null and alternative hypotheses\n\n\n\n“Except in special circumstances, a model including a product term for interaction between two explanatory variables should also include terms with each of the explanatory variables individually, even though their coefficients may not be significantly different from zero. Following this rule avoids the logical inconsistency of saying that the effect of \\(X_1\\) depends on the level of \\(X_2\\) but that there is no effect of \\(X_1\\).”\n— Ramsey and Schafer (2012)\n\n\n\n\n\n\n Solution \n\n\nTo address the research question, we are going to fit the following model, where \\(y\\) = wellbeing; \\(x_1\\) = social interactions; and \\(x_2\\) = whether or not the respondent lives in a rural location.\n\\[\ny = \\beta_0 + \\beta_1  x_1 + \\beta_2  x_2 + \\beta_3 (x_1 \\cdot x_2) + \\epsilon \\\\\n\\quad \\\\ \\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\quad \\text{independently}\n\\]\nor\n\\[\n\\begin{split}\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot Social Interactions + \\beta_2 \\cdot Location_{Rural} \\\\+ \\beta_3 \\cdot (Social Interactions \\cdot Location_{Rural}) + \\epsilon \\\\\n\\end{split}\n\\]\nEffects will be considered statistically significant at \\(\\alpha=.05\\)\nOur hypotheses are:\n\\(H_0: \\beta_3 = 0\\)\nThe association between wellbeing and social interactions is not moderated by whether or not a person lives in a rural area.\n\\(H_1: \\beta_3 \\neq 0\\)\nThe association between wellbeing and social interactions is moderated by whether or not a person lives in a rural area.\n\n\n\n\n\nQuestion 2\n\n\nCheck coding of variables (e.g., that categorical variables are coded as factors).\nNote that the “location” variable currently has three levels (Rural/Suburb/City). In order to address the research question, we only want two (Rural/Not Rural) locations - you will need to fix this.\nSpecify ‘not rural’ as your reference group.\n\n\n\n\n\n\nHint\n\n\n\n\n\nOne way to do this would be to use ifelse() to define a variable which takes one value (“Rural”) if the observation meets from some condition, or another value (“Not Rural”) if it does not. Type ?ifelse in the console if you want to see the help function. You can use it to add a new variable either inside mutate(), or using data$new_variable_name <- ifelse(test, x, y) syntax.\n\n\n\n\n\n\n\n Solution \n\n\nCreate a new variable for Rural/Not Rural:\n\n#if location is rural, assign name 'rural'. If another value (i.e., city or suburb) assign name 'not rural'.\n# In other words, if location = rural assign name rural; if location != rural then assign name not rural.\nwrdata <- wrdata %>% \n  mutate(\n    isRural = ifelse(location == \"rural\", \"rural\", \"not rural\")\n  )\n\nCheck coding of variables within wrdata and ensure isRural is a factor with two levels, ‘rural’ and ‘not rural’:\n\nstr(wrdata) #returns overall 'structure' of data. Or could run is.factor() for specific variable of interest\n\ntibble [200 × 8] (S3: tbl_df/tbl/data.frame)\n $ age         : num [1:200] 28 56 25 60 19 34 41 41 35 53 ...\n $ outdoor_time: num [1:200] 12 5 19 25 9 18 17 11 12 13 ...\n $ social_int  : num [1:200] 13 15 11 15 18 13 19 12 13 15 ...\n $ routine     : num [1:200] 1 1 1 0 1 1 1 1 0 1 ...\n $ wellbeing   : num [1:200] 36 41 35 35 32 34 39 43 35 37 ...\n $ location    : chr [1:200] \"rural\" \"rural\" \"rural\" \"rural\" ...\n $ steps_k     : num [1:200] 21.6 12.3 49.8 NA 48.1 67.3 1.9 50.9 NA 35.5 ...\n $ isRural     : chr [1:200] \"rural\" \"rural\" \"rural\" \"rural\" ...\n\nwrdata$isRural <- as_factor(wrdata$isRural)\nis.factor(wrdata$isRural) #check that isRural is now a factor\n\n[1] TRUE\n\n\n\n#specify 'not rural' as reference group\nwrdata$isRural <- relevel(wrdata$isRural, 'not rural')\n\n\n\n\n\n\nQuestion 3\n\n\nVisualise your data, and interpret your plots.\nIn particular:\n\nExplore the associations among the variables included in your analysis\nProduce a visualisation of the association between weekly number of social interactions and well-being, with separate facets for rural vs non-rural respondents OR with different colours for each level of the isRural variable.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nThe pairs.panels() function from the psych package will can plot all variables in a dataset against one another. This will save you the time you would have spent creating individual plots, but is only useful for continuous variables.\nTo include facets, Within your ggplot() argument you will need to specify + facet_wrap() in order to produce facets for each location. It would also be useful to specify geom_smooth(method=\"lm\")\n\n\n\n\n\n\n\n\n\n Solution \n\n\nLet’s first plot the continuous variables included within our model (note that we could use this for the whole dataset, but we don’t want to include irrelevant variables):\n\nwrdata %>% \n  select(wellbeing, social_int) %>%\n  pairs.panels()\n\n\n\n\n\n\n\n\n\n\nWellbeing and social interactions appear to follow unimodal distributions. There was a weak, positive association between wellbeing and social interactions (\\(r\\) = .24).\n\n\n\nNow lets look at wellbeing scores by location:\n\nggplot(data = wrdata, aes(x = isRural, y = wellbeing)) +\n  geom_boxplot() + \n  labs(x = \"Location\", y = \"Wellbeing (WEMWBS Scores)\")\n\n\n\n\n\n\n\n\n\n\nThose in rural locations appear to have lower wellbeing scores in comparison to those in non-rural locations.\n\n\n\nNext, lets produce our plots with a facet for rural vs non-rural residents:\n\nggplot(data = wrdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  facet_wrap(~isRural) + \n  labs(x = \"Social Interactions (number per week)\", y = \"Wellbeing (WEMWBS Scores)\")\n\n\n\n\nOr instead of facets, we could use different colours for each location (rural vs non-rural):\n\nggplot(data = wrdata, aes(x = social_int, y = wellbeing, colour = isRural)) +\n  geom_point() + \n  geom_smooth(method=\"lm\", se=FALSE) +\n    scale_colour_discrete(\n    name =\"Location\",\n    labels=c(\"Not Rural\", \"Rural\")) + \n    labs(x = \"Social Interactions (number per week)\", y = \"Wellbeing (WEMWBS Scores)\")\n\n\n\n\n\n\n\n\n\n\nThose in non-rural locations appear to have higher wellbeing scores across almost all levels of social interactions. The slopes appear to be different for each location, where the greatest difference in wellbeing scores by location is most visible the highest number of social interactions.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHow can we tell that there is an interaction?\nThe lines in the two plots above are not running in parallel - this suggests the presence of an interaction. Specifically in our example, the non-parallel lines suggest an interaction effect based on location, as the number of social interactions does not appear to have the same influence on rural and non-rural residents’ wellbeing scores.\n\n\n\n\n\n\n\nQuestion 4\n\n\nFit your model using lm(), and assign it as an object with the name “rural_mod”.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhen fitting a regression model in R with two explanatory variables A and B, and their interaction, these three are equivalent:\n\ny ~ A + B + A:B\ny ~ A + B + A*B\ny ~ A*B\n\n\n\n\n\n\n\n\n Solution \n\n\n\n#fit model including interaction between social_int and isRural\nrural_mod <- lm(wellbeing ~  social_int * isRural, data = wrdata)\n\n#check model output\nsummary(rural_mod)\n\n\nCall:\nlm(formula = wellbeing ~ social_int * isRural, data = wrdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.4845  -2.7975   0.0155   2.4539  15.6743 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              30.9986     1.4284  21.702  < 2e-16 ***\nsocial_int                0.6488     0.1160   5.593 7.42e-08 ***\nisRuralrural              1.3866     2.0510   0.676  0.49981    \nsocial_int:isRuralrural  -0.5176     0.1615  -3.206  0.00157 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.558 on 196 degrees of freedom\nMultiple R-squared:  0.2962,    Adjusted R-squared:  0.2854 \nF-statistic: 27.49 on 3 and 196 DF,  p-value: 6.97e-15\n\n\n\n\n\n\n\nQuestion 5\n\n\nLook at the parameter estimates from your model, and write a description of what each one corresponds to on the plot shown in Figure 1 (it may help to sketch out the plot yourself and annotate it).\n\n\n\n\nFigure 1: Multiple regression model: Wellbeing ~ Social Interactions * is Rural\n\n\n\n\n\n Options\n\n\nHere are some options to choose from:\n\nThe point at which the red line cuts the y-axis (where social_int = 0)\nThe point at which the blue line cuts the y-axis (where social_int = 0)\nThe vertical distance from the red to the blue line at the y-axis (where social_int = 0)\nThe vertical distance from the blue to the red line at the y-axis (where social_int = 0)\nThe vertical distance from the blue to the red line at the center of the plot\n\nThe vertical distance from the red to the blue line at the center of the plot\n\nThe slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the red line\nThe slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line\nHow the slope of the line changes when you move from the red to the blue line\n\n\n\n\n\n\n\n\n Solution \n\n\nRecall that we can obtain our parameter estimates using various functions such as summary(),coef(), coefficients(), etc.\n\ncoefficients(rural_mod)\n\n            (Intercept)              social_int            isRuralrural \n             30.9985688               0.6487945               1.3865688 \nsocial_int:isRuralrural \n             -0.5175856 \n\n\n\n\n\\(\\beta_0\\) = (Intercept) = 31\n\nOn plot: The point at which the red line cuts the y-axis\nInterpretation: The intercept, or predicted wellbeing score when the number of social interactions per week is 0, and when location is not rural.\n\n\n\n\\(\\beta_1\\) = social_int = 0.65\n\nOn plot: The slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the red line.\nInterpretation: The simple slope of social interactions (number per week) for location reference group (not rural).\n\n\n\n\\(\\beta_2\\) = isRuralrural = 1.39\n\nOn plot: The vertical distance from the red to the blue line at the y-axis (where social_int = 0).\n\nInterpretation: The simple effect of location (or the difference in wellbeing scores between rural and non rural residents) when number of social interactions is 0.\n\n\n\n\\(\\beta_3\\) social_int:isRuralrural = -0.52\n\nOn plot: How the slope of the line changes when you move from the red to the blue line.\nInterpretation: The interaction between social interactions (number per week) and location (rural/not rural) - the difference in the simple slopes of social interactions for rural vs non-rural residents.\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nMean center the continuous IV(s), and re-run your model with mean centered variable(s).\n\n\n\n\n Solution \n\n\nCreate mean centered variable for ‘social_int’:\n\nwrdata <-\n wrdata %>%\n  mutate(\n   mc_social_int = social_int - mean(social_int)\n    )\n\nRe-run model:\n\n#fit model including interaction between social_int and isRural\nrural_mod1 <- lm(wellbeing ~  mc_social_int * isRural, data = wrdata)\n\n#check model output\nsummary(rural_mod1)\n\n\nCall:\nlm(formula = wellbeing ~ mc_social_int * isRural, data = wrdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.4845  -2.7975   0.0155   2.4539  15.6743 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 38.8263     0.4581  84.754  < 2e-16 ***\nmc_social_int                0.6488     0.1160   5.593 7.42e-08 ***\nisRuralrural                -4.8581     0.6478  -7.500 2.17e-12 ***\nmc_social_int:isRuralrural  -0.5176     0.1615  -3.206  0.00157 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.558 on 196 degrees of freedom\nMultiple R-squared:  0.2962,    Adjusted R-squared:  0.2854 \nF-statistic: 27.49 on 3 and 196 DF,  p-value: 6.97e-15\n\n\n\n\n\n\n\nQuestion 7\n\n\nNote any differences between the summary() output between the “rural_mod” and “rural_mod1” models. Pay particular attention to your coefficients and their significance values. Why do you think these differences have been observed?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis plot illustrates the difference between the “rural_mod” and “rural_mod1” models.\n\n\n\n\nFigure 2: Difference when social interactions is not vs is mean centered.Note that the lines without SE intervals on the left plot represent predicted values below the minimum observed number of social interactions, to ensure that zero on the x-axis is visible\n\n\n\n\n\n\n\n\n\n\n\n Solution \n\n\nRecall that when there is an interaction A\\(\\times\\)B, the coefficients A and B are no longer main effects. Instead, they are conditional effects upon the other being zero.\nIn our “rural_mod”, the isRural coefficient is the difference in rural vs non-rural when social interactions is 0. In our “rural_mod1”, this difference is when social interactions is the mean (12.06).\nWhilst the difference in rural vs non-rural may not be significantly different when social interactions is zero, there is a significant difference at the average number of social interactions (as you can see from the plot below - note that this is the same plot as in the hint).\n\n\n\n\nFigure 3: Difference when social interactions is not vs is mean centered\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nUsing the probe_interaction() function from the interactions package, visualise the interaction effects from your model.\nTry to summarise the interaction effects in a short and concise sentence.\n\n\n\n\n\n\nHint\n\n\n\n\n\nMake sure to give your plot informative titles/labels. You, for example, likely want to give your plot:\n\na clear and concise title (specify main.title =)\naxis labels with units or scale included (specify x.label = and y.label =)\na legend title (specify legend.main =)\n\n\n\n\n\n\n\n\n Solution \n\n\n\nplt_rural_mod <- probe_interaction(model = rural_mod1, \n                  pred = mc_social_int, \n                  modx = isRural, \n                  interval = T,\n                  main.title = \"Predicted Wellbeing Scores across \\n Social Interactions by Location\",\n                  x.label = \"Number of Social Interactions per Week (mean centred)\",\n                  y.label = \"Wellbeing (WEMWBS Scores)\",\n                  legend.main = \"Location\")\n\nLet’s look at our plot:\n\nplt_rural_mod$interactplot\n\n\n\nFigure 4: Predicted Wellbeing Scores across Social Interactions by Location\n\n\n\n\n\n\n\n\n\n\nThis suggested that for individuals living in non-rural locations, wellbeing scores increased at a steeper rate across the number of social interactions in comparison to those in rural locations.\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nProvide key model results in a formatted table.\n\n\n\n\n Solution \n\n\n\n#create table for results\ntab_model(rural_mod1,\n          dv.labels = \"Wellbeing (WEMWBS Scores)\",\n          pred.labels = c(\"mc_social_int\" = \"Social Interactions (number per week)\",\n                          \"isRuralrural\" = \"Location - Rural\",\n                          \"mc_social_int:isRuralrural\" = \"Social Interactions * Location - Rural\"),\n          title = \"Regression Table for Wellbeing Model\")\n\n\n\n\nTable 1:  Regression Table for Wellbeing Model \n\n \nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n38.83\n37.92 – 39.73\n<0.001\n\n\nSocial Interactions(number per week)\n0.65\n0.42 – 0.88\n<0.001\n\n\nLocation - Rural\n-4.86\n-6.14 – -3.58\n<0.001\n\n\nSocial Interactions *Location - Rural\n-0.52\n-0.84 – -0.20\n0.002\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.296 / 0.285\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret your results in the context of the research question and report your model in full.\nMake reference to the interaction plot and regression table.\n\n\n“The best method of communicating findings about the presence of a significant interaction may be to present a table or graph of the estimated means at various combinations of the interacting variables.”\n— Ramsey and Schafer (2012)\n\n\n\n\n\n\n Solution \n\n\nMake sure to write your results up following APA guidelines:\n\n\n\n\n\n\nFull regression results including 95% Confidence Intervals are shown in Table 1. The \\(F\\)-test for model utility was significant \\((F(3,196) = 27.49, p<.001)\\), and the model explained approximately 28.54% of the variability in wellbeing scores.\nThere was a significant conditional association between wellbeing (WEMWBS Scores) and social interactions (\\(\\beta\\) = 0.65, \\(SE\\) = 0.12, \\(p\\) < .001), which suggested that for those living in non-rural locations, wellbeing scores increased by 0.65 for every additional social interaction per week. A significant conditional association was also evident between wellbeing and location (\\(\\beta\\) = -4.86, \\(SE\\) = 0.65, \\(p\\) < .001), which suggested that for those with the average number of social interactions per week (\\(M\\) = 12.06), wellbeing scores were 4.86 points lower for those in rural areas in comparison to those in non-rurual.\nThe association between wellbeing (WEMWBS Scores) and social interactions was found to be dependent upon location (rural/non-rural), where the slope was less steep for those in rural locations (\\(\\beta\\) = -0.52, \\(SE\\) = 0.16, \\(p\\) = .002). This interaction is visually presented in Figure 4. The effect of every additional social interaction per week on wellbeing (WEMWBS Scores) was 0.52 less for those living in rural locations in comparison to those in non-rural locations. Therefore, we have evidence to reject the null hypothesis (that the association between wellbeing and social interactions is not moderated by whether or not a person lives in a rural area)."
  },
  {
    "objectID": "1_08_int2_nn.html",
    "href": "1_08_int2_nn.html",
    "title": "Interactions II: Num x Num",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand the concept of an interaction.\nBe able to interpret the meaning of a numeric \\(\\times\\) numeric interaction.\nUnderstand the principle of marginality and why this impacts modelling choices with interactions.\nVisualize and probe interactions.\n\n\nBe up to date with lectures\nHave completed previous lab exercises from Week 7\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npsych\nsjPlot\nkableExtra\nsandwich\ninteractions\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/scs_study.csv."
  },
  {
    "objectID": "1_08_int2_nn.html#exercises",
    "href": "1_08_int2_nn.html#exercises",
    "title": "Interactions II: Num x Num",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion 1\n\n\nFormally state:\n\na linear model to investigate whether the effect of social comparison on symptoms of depression, anxiety and stress varies depending on level of Neuroticism\nyour chosen significance level\nthe null and alternative hypotheses\n\n\n\n\n\n Solution \n\n\n\\[\n{DASS-21 Score} = \\beta_0 + \\beta_1 \\cdot SCS Score + \\beta_2 \\cdot Neuroticism  + \\beta_3 \\cdot (SCS Score \\cdot Neuroticism)\n\\]\nEffects will be considered statistically significant at \\(\\alpha=.05\\)\nOur hypotheses are:\n\\(H_0: \\beta_3 = 0\\)\nThe effect of social comparison on symptoms of depression, anxiety and stress does not vary depending on level of Neuroticism\n\\(H_1: \\beta_3 \\neq 0\\)\nThe effect of social comparison on symptoms of depression, anxiety and stress does vary depending on level of Neuroticism.\n\n\n\n\n\nQuestion 2\n\n\nProvide a table of descriptive statistics and visualise your data.\nRemember to interpret these plots in the context of the study.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nThe describe() function is from the psych package; and kable() and kable_styling() (which are used to make a nice table) from kableExtra would be useful to present your descriptive statistics.\nThe pairs.panels() function from the psych package will plot all variables in a dataset against one another. This will save you the time you would have spent creating individual plots.\n\n\n\n\n\n\n\n\n Solution \n\n\nFirst, lets look at our descriptive statistics and present in a well formatted table:\n\n# note that we are selecting our three variables of interest (dass, scs, zn)\n# we are then passing these to the kable() and kable_styling() functions so that we end up with a nice looking table, and specify digits = 2 so that output is rounded to 2 decimal places\n\ndescribe(scs_study %>% \n             select(dass, scs, zn)) %>% \n             kable(caption = \"Descriptive Statistics - DASS-21, SCS, and Neuroticism (Z-Scored)\", digits = 2) %>%\n             kable_styling()\n\n\n\n\nTable 1:  Descriptive Statistics - DASS-21, SCS, and Neuroticism (Z-Scored) \n \n   \n    vars \n    n \n    mean \n    sd \n    median \n    trimmed \n    mad \n    min \n    max \n    range \n    skew \n    kurtosis \n    se \n  \n\n\n dass \n    1 \n    656 \n    44.72 \n    6.76 \n    44.00 \n    44.62 \n    5.93 \n    23.00 \n    68.00 \n    45.0 \n    0.18 \n    0.33 \n    0.26 \n  \n\n scs \n    2 \n    656 \n    35.77 \n    3.53 \n    35.00 \n    35.59 \n    2.97 \n    27.00 \n    54.00 \n    27.0 \n    0.60 \n    0.96 \n    0.14 \n  \n\n zn \n    3 \n    656 \n    0.00 \n    1.00 \n    -0.21 \n    -0.10 \n    1.00 \n    -1.45 \n    3.35 \n    4.8 \n    0.80 \n    0.04 \n    0.04 \n  \n\n\n\n\n\n Next, look at associations among variables of interest:\n\nscs_study %>% \n  select(dass, scs, zn) %>%\n  pairs.panels()\n\n\n\n\nDescription of individual variables:\n\n\n\n\n\n\n\nThe marginal distribution of scores on the Depression, Anxiety and Stress Scale (DASS-21) was unimodal with a mean of 44.72 and a standard deviation of 6.76.\nThe marginal distribution of score on the Social Comparison Scale (SCS) was unimodal with a mean of 35.77 and a standard deviation of 3.53.\nThe marginal distribution of Neuroticism (Z-scored) was positively skewed.\n\n\n\n\nDescription of correlations:\n\n\n\n\n\n\n\nThere was a weak, negative association between scores on the Social Comparison Scale and scores on the Depression Anxiety and Stress Scale for the participants in the sample (\\(r\\) = -.23). Severity of symptoms measured on the DASS-21 were lower, on average, for those who more favorably perceived their social rank.\n\nThere was a weak, positive association between the levels of Neuroticism and scores on the DASS-21 (\\(r\\) = .20). Participants who are more neurotic tend to, on average, display a higher severity of symptoms of depression, anxiety and stress.\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nRun the two code chunks below. It takes the dataset, and uses the cut() function to add a new variable called “zn_group”, which is the “zn” variable split into 4 groups.\n\nscs_study <-\n  scs_study %>%\n  mutate(\n    zn_group = cut(zn, 4)\n  )\n\nWe can see how it has split the “zn” variable by plotting the two against one another (note that the levels of the new variable are named according to the cut-points):\n\nggplot(data = scs_study, aes(x = zn_group, y = zn)) + \n  geom_point()\n\n\n\n\nPlot the association between scores on the SCS and scores on the DASS-21, for each group of the variable we just created.\nHow does the pattern differ across groups? Does it suggest an interaction?\n\n\n\n\n\n\nHint\n\n\n\n\n\nRather than creating four separate plots, you might want to map some feature of the plot to the variable we created in the data, or make use of facet_wrap()/facet_grid().\nRemember that you can specify geom_smooth() to add a trend line\n\n\n\n\n\n\n\n Solution \n\n\n\nggplot(data = scs_study, aes(x = scs, y = dass, col = zn_group)) + \n  geom_point() + \n  geom_smooth(method='lm', se = FALSE) +\n  facet_grid(~zn_group) +\n  labs(x = \"SCS Scores \", y = \"DASS-21 Scores\") +\n  theme(legend.position = \"none\") # removes the legend\n\n\n\n\nThe association between SCS scores and DASS-21 scores appears to be different across these groups. For those with a relatively high Neuroticism score, the association seems stronger, while for those with a low Neuroticism score there is almost no discernible association.\nThis does suggest an interaction - the association of DASS-21 ~ SCS differs across the values of Neuroticism.\n\n\n\n\n\nVisualising Interaction Terms\nCutting one of the explanatory variables up into groups essentially turns a numeric variable into a categorical one. We did this just to make it easier to visualise how an association differs across the values of another variable, because we can imagine a separate line for the association between SCS and DASS-21 scores for each of the groups of Neuroticism. However, in grouping a numeric variable like this we lose information. Neuroticism is measured on a continuous scale, and we want to capture how the association between SCS and DASS-21 differs across that continuum (rather than cutting it into chunks).\nWe could imagine cutting it into more and more chunks (see Figure 1), until what we end up with is an infinite number of lines - i.e., a three-dimensional plane/surface (recall that in for a multiple regression model with 2 explanatory variables, we can think of the model as having three-dimensions). The inclusion of the interaction term simply results in this surface no longer being necessarily flat. You can see this in Figure 2).\n\n\n\n\nFigure 1: Separate regression lines DASS ~ SCS for Neuroticism when cut into 4 (left) or 6 (center) or 12 (right) groups.\n\n\n\n\n\n\n\nFigure 2: 3D plot of regression surface with interaction. You can explore the plot in the figure below from different angles by moving it around with your mouse.\n\n\n\n\n\n\nQuestion 4\n\n\nConsider that Neuroticism has already been \\(z\\)-scored, but scs has not. To ensure that we can compare the effects of our estimates (and so they are both on meaningful scales), standardize the scs variable.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRecall the formula for the \\(z\\)-score:\n\\[\nz_x = \\frac{x - \\bar{x}}{s_x}, \\qquad z_y = \\frac{y - \\bar{y}}{s_y}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\n\n# standardize scs score\nscs_study <- \n  scs_study %>% \n    mutate(\n      zscs = (scs-mean(scs))/sd(scs)\n    )\n\n\n\n\n\n\nQuestion 5\n\n\nFit your model (including the standardized variables) using lm(), and assign it as an object with the name “dass_mdl”.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhen fitting a regression model in R with two explanatory variables A and B, and their interaction, these three are equivalent:\n\ny ~ A + B + A:B\ny ~ A + B + A*B\ny ~ A*B\n\n\n\n\n\n\n\n\n Solution \n\n\n\n#fit interaction model\ndass_mdl <- lm(dass ~  zscs*zn, data = scs_study)\n\n#check model output\nsummary(dass_mdl)\n\n\nCall:\nlm(formula = dass ~ zscs * zn, data = scs_study)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.301  -3.825  -0.173   3.733  45.777 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  44.9324     0.2405 186.807  < 2e-16 ***\nzscs         -1.5691     0.2416  -6.495 1.64e-10 ***\nzn            1.5798     0.2409   6.559 1.11e-10 ***\nzscs:zn      -1.8332     0.2316  -7.915 1.06e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.123 on 652 degrees of freedom\nMultiple R-squared:  0.1825,    Adjusted R-squared:  0.1787 \nF-statistic:  48.5 on 3 and 652 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n\nQuestion 6\n\n\nInterpret your coefficients in the context of the study.\n\n\n\n\n Solution \n\n\nRecall that we can obtain our parameter estimates using various functions such as summary(),coef(), coefficients(), etc.\n\ncoefficients(dass_mdl)\n\n(Intercept)        zscs          zn     zscs:zn \n  44.932448   -1.569097    1.579769   -1.833169 \n\n\n\n\n\\(\\beta_0\\) = (Intercept) = 44.93\n\nThe intercept, or predicted DASS-21 score for average SCS score (Mean = 0) and average Neuroticism score (Mean = 0).\n\n\n\n\\(\\beta_1\\) = zscs = -1.57\n\nThe simple slope of SCS scores for average Nueroticism scores (Mean = 0).\nFor an individual with average Neuroticism levels, DASS-21 scores decreased by 1.57 standard deviations for every 1 standard deviation increase in SCS scores.\n\n\n\n\\(\\beta_2\\) = zn = 1.58\n\nThe simple slope of Neuroticism for average SCS scores (Mean = 0).\nFor those with average SCS scores, DASS-21 scored increased by 1.58 for every 1 standard deviation increase in Neuroticism.\n\n\n\n\\(\\beta_3\\) zscs:zn = -1.83\n\nThe interaction between SCS score and Neuroticism on DASS-21 Scores\nFor every 1 standard deviation increase in SCS scores, the association between Neuroticism and DASS-21 scores decreases by an additional 1.83 standard deviations.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nUsing the probe_interaction() function from the interactions package, visualise the interaction effects from your model.\nTry to summarise the interaction effects in a short and concise sentence.\n\n\n\n\n\n\nHint\n\n\n\n\n\nBecause we are looking at a numeric x numeric interaction, we want to specify jnplot = T (see this weeks lecture, slides 17-21 for a worked example).\nRemember to give your plot informative titles/labels. You, for example, likely want to give your plot:\n\na clear and concise title (specify main.title =)\naxis labels with units or scale included (specify x.label = and y.label =)\na legend title (specify legend.main =)\n\n\n\n\n\n\n\n\n Solution \n\n\n\nplt_dass_mdl <- probe_interaction(model = dass_mdl, \n                  pred = zscs, \n                  modx = zn, \n                  cond.int = T,\n                  interval = T, \n                  jnplot = T,\n                  main.title = \"Neuroticism moderating the effect of\\nsocial comparison on depression and anxiety\",\n                  x.label = \"Social Comparison Scale (Z-scored)\",\n                  y.label = \"DASS-21 Scores\",\n                  legend.main = \"Neuroticism (Z-scored)\")\n\nLet’s look at the plot - to do so you need to call interactplot from your object plt_dass_mdl:\n\nplt_dass_mdl$interactplot\n\n\n\nFigure 3: Simple Sloes for +/- 1 SD and Mean Neuroticism Scores\n\n\n\n\nThe effect of SCS scores on DASS-21 scores was more negatively pronounced for those with higher Neuroticism scores.\n\n\n\n\n\nQuestion 8\n\n\nConduct a simple slopes analysis.\n\n\n\n\n\n\nHint\n\n\n\n\n\nIf you wanted to see only the simple slopes or only the Johnson-Neyman plot, you could call $simslopes$slopes or simslopes$jnplot respectively from your object plt_dass_mdl.\n\n\n\n\n\n\n\n Solution \n\n\n\nplt_dass_mdl$simslopes\n\nJOHNSON-NEYMAN INTERVAL \n\nWhen zn is OUTSIDE the interval [-1.28, -0.55], the slope of zscs is p <\n.05.\n\nNote: The range of observed values of zn is [-1.45, 3.35]\n\n\nSIMPLE SLOPES ANALYSIS \n\nWhen zn = -1.000000e+00 (- 1 SD): \n\n                               Est.   S.E.   t val.      p\n--------------------------- ------- ------ -------- ------\nSlope of zscs                  0.26   0.35     0.76   0.45\nConditional intercept         43.35   0.34   127.47   0.00\n\nWhen zn = -8.610271e-16 (Mean): \n\n                               Est.   S.E.   t val.      p\n--------------------------- ------- ------ -------- ------\nSlope of zscs                 -1.57   0.24    -6.50   0.00\nConditional intercept         44.93   0.24   186.81   0.00\n\nWhen zn =  1.000000e+00 (+ 1 SD): \n\n                               Est.   S.E.   t val.      p\n--------------------------- ------- ------ -------- ------\nSlope of zscs                 -3.40   0.32   -10.59   0.00\nConditional intercept         46.51   0.34   136.52   0.00\n\n\n\n\nFigure 4: Johnson-Neyman Plot\n\n\n\n\nThe Johnson-Neyman technique indicated that the association between DASS-21 scores and SCS was significant when Neuroticism scores were less than 1.28 standard deviations below the mean or more than -0.55 standard deviations above the mean.\n\n\n\n\n\nQuestion 9\n\n\nProvide key model results in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package.\nRemember that you can rename your DV and IV labels by specifying dv.labels and pred.labels.\n\n\n\n\n\n\n\n Solution \n\n\n\n#create table for results\ntab_model(dass_mdl,\n          dv.labels = \"DASS-21 Scores\",\n          pred.labels = c(\"zscs\" = \"Social Comparison Scale (Z-scored)\",\n                          \"zn\" = \"Neuroticism (Z-scored)\",\n                          \"zscs:zn\" = \"Social Comparison Scale (Z-scored): Neutoricism (Z-scored)\"),\n          title = \"Regression table for DASS-21 model\")\n\n\n\n\nTable 2:  Regression table for DASS-21 model \n\n \nDASS-21 Scores\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n44.93\n44.46 – 45.40\n<0.001\n\n\nSocial Comparison Scale(Z-scored)\n-1.57\n-2.04 – -1.09\n<0.001\n\n\nNeuroticism (Z-scored)\n1.58\n1.11 – 2.05\n<0.001\n\n\nSocial Comparison Scale(Z-scored): Neutoricism(Z-scored)\n-1.83\n-2.29 – -1.38\n<0.001\n\n\nObservations\n656\n\n\nR2 / R2 adjusted\n0.182 / 0.179\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret your results in the context of the research question and report your model in full.\nMake reference to the interaction plot and regression table.\n\n\n\n\n Solution \n\n\nMake sure to write your results up following APA guidelines:\n\n\n\n\n\n\nFull regression results including 95% Confidence Intervals are shown in Table 2. The \\(F\\)-test for model utility was significant \\((F(3,652) = 48.50, p<.001)\\), and the model explained approximately 17.87% of the variability in DASS-21 scores.\nThere was a significant conditional association between DASS-21 Scores and SCS scores (Z-scored) (\\(\\beta\\) = -1.57, SE = 0.24, \\(p\\) < .001), suggesting that for those with average Neuroticism scores (\\(M\\) = 0),DASS-21 scores decreased by 1.57 for every 1 standard deviation increase in SCS scores.\nA significant conditional association was also evident between DASS-21 Scores and Neuroticism (Z-scored) (\\(\\beta\\) = 1.58, SE = 0.24, \\(p\\) <.001), suggesting that for those with average SCS scores (\\(M\\) = 0), DASS-21 scores increased by 1.58 for every 1 standard deviation increase in Neuroticism.\nThe association between symptoms of depression and anxiety (DASS-21 scores) and social comparison was found to be dependent upon the level of Neuroticism, with a greater negative association between the two for those with high levels of Neuroticism (\\(\\beta\\) = -1.83, SE = 0.23, \\(p\\) <.001). This buffering interaction suggested that for every standard deviation increase in SCS Scores, the negative influence of Neuroticism on DASS-21 scores was reduced by 1.83 standard deviations (see Figure 3). We further used the Johnson-Neyman technique to probe the interaction, and to identify regions of significance. We identified that Neuroticism values (z-scored) outside the range of -1.28 to -0.55 were significant (see Figure 4).\nTherefore, we have evidence to reject the null hypothesis (that the effect of social comparison on symptoms of depression, anxiety and stress does not vary depending on level of Neuroticism)."
  },
  {
    "objectID": "1_09_int3_cc.html",
    "href": "1_09_int3_cc.html",
    "title": "Interactions III: Cat x Cat",
    "section": "",
    "text": "Study Overview\n\nResearch Question\nAre there differences in types of memory deficits for those experiencing different cognitive impairment(s)?\n\nA group of researchers wants to test a hypothesised theory according to which the difference in performance between explicit and implicit memory tasks will be greatest for Huntington patients in comparison to controls. On the other hand, the difference in performance between explicit and implicit memory tasks will not significantly differ between patients with amnesia in comparison to controls.\n\n Cognitive Exp 3x2 data codebook.\n\n\nDescription\nThe researchers designed a study yielding a 3 by 2 factorial design to test this theory. The first factor, “Diagnosis”, classifies the three types of individuals:\n\n1 denotes amnesic patients;\n2 denotes Huntingtons patients; and\n3 denotes a control group of individuals with no known neurological disorder.\n\nThe second factor, “Task”, tells us to which of two tasks each study participant was randomly assigned to:\n\n1 = grammar task, which consists of classifying letter sequences as either following or not following grammatical rules; and\n2 = recognition task, which consists of recognising particular stimuli as stimuli that have previously been presented during the task.\n\nKeep in mind that each person has been randomly assigned to one of the two tasks, so there are five observations per cell of the design.1\nThe tasks chosen by the researchers have been picked to map onto the theoretical differences between the three types of research participants. The first task (grammar) is known to reflect implicit memory processes, whereas the recognition task is known to reflect explicit memory processes. If the theory is correct, we would expect the difference in scores between the recognition and grammar tasks to be relatively similar for the control and amnesiac groups, but relatively larger for the Huntingtons group compared to controls.\nPreview\nThe first ten rows of the data are:\n\n\n\n\n\n\n\nDiagnosis\n      Task\n      Y\n    \n\n\n1\n1\n44\n\n\n1\n1\n63\n\n\n1\n1\n76\n\n\n1\n1\n72\n\n\n1\n1\n45\n\n\n1\n2\n70\n\n\n\n\n\n\n\n\n\nSetup\n\nSetup\n\n\n\nCreate a new RMarkdown file\nLoad the required package(s)\nRead the cognitive_experiment_3_by_2 dataset into R, assigning it to an object named cog\n\n\n\n\n\n\n Solution \n\n\n\n#Loading the required package(s)\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(sjPlot)\nlibrary(kableExtra) \nlibrary(sandwich)\nlibrary(interactions)\n\n#Reading in data and storing in object named 'cog'\ncog <- read_csv(\"https://uoepsy.github.io/data/cognitive_experiment_3_by_2.csv\")\n\n\n\n\nExercises\n\nQuestion 1\n\n\nExamine the dataset, and perform any necessary and appropriate data management steps.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nConvert categorical variables to factors\nLabel appropriately factors to aid with your model interpretations\nIf needed, provide better variable names\n\nNote that all of these steps can be done in combination - the mutate() and factor() functions will likely be useful here.\n\n\n\n\n\n\n\n Solution \n\n\nLet’s have a look at the data to see what we’re working with:\n\n#first look at dataset structure\nstr(cog)\n\nspec_tbl_df [30 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Diagnosis: num [1:30] 1 1 1 1 1 1 1 1 1 1 ...\n $ Task     : num [1:30] 1 1 1 1 1 2 2 2 2 2 ...\n $ Y        : num [1:30] 44 63 76 72 45 70 51 82 66 56 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Diagnosis = col_double(),\n  ..   Task = col_double(),\n  ..   Y = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n#now lets look at top 6 rows (or the head) of the dataset\nhead(cog)\n\n# A tibble: 6 × 3\n  Diagnosis  Task     Y\n      <dbl> <dbl> <dbl>\n1         1     1    44\n2         1     1    63\n3         1     1    76\n4         1     1    72\n5         1     1    45\n6         1     2    70\n\n\nThe columns Diagnosis and Task should be coded into factors with better labels, as currently, without making reference to the codebook, it is not clear what “1” and “2” represent. It is also unclear what the Y column represents - this should be renamed.\n\n#We can make all of the changes noted above in one (long) command. \n#First we can use the function `factor()` by specifying the current levels and what labels each level should map to. \n#We can also simply rename the Y column to score. \n\ncog <- cog %>%\n    mutate(\n        Diagnosis = factor(Diagnosis, \n                           levels = c(1, 2, 3),\n                           labels = c('Amnesic', 'Huntingtons', 'Control')),\n        Task = factor(Task, \n                      levels = c(1, 2),\n                      labels = c('Grammar', 'Recognition'))) %>%\n    rename(Score = Y)\n\n#Use head() function to check renaming\nhead(cog)\n\n# A tibble: 6 × 3\n  Diagnosis Task        Score\n  <fct>     <fct>       <dbl>\n1 Amnesic   Grammar        44\n2 Amnesic   Grammar        63\n3 Amnesic   Grammar        76\n4 Amnesic   Grammar        72\n5 Amnesic   Grammar        45\n6 Amnesic   Recognition    70\n\n\n\n\n\n\n\nQuestion 2\n\n\nChoose appropriate reference levels for the Diagnosis and Task variables.\n\n\n\n\n Solution \n\n\nThe Diagnosis factor has a group coded ‘Control’ which lends itself naturally to be the reference category.\n\ncog$Diagnosis <- relevel(cog$Diagnosis, 'Control')\n\nlevels(cog$Diagnosis)\n\n[1] \"Control\"     \"Amnesic\"     \"Huntingtons\"\n\n\nThere is no natural reference category for the Task factor, so we will leave it unaltered. However, if you are of a different opinion, please note that there is no absolute correct answer. As long as you interpret the model correctly, you will reach to the same conclusions as someone that has chosen a different baseline category.\n\n\n\n\n\nQuestion 3\n\n\nFormally state:\n\na linear model to investigate whether there are differences in types of memory deficits for those experiencing different cognitive impairment(s)\nyour chosen significance level\nthe null and alternative hypotheses\n\n\n\n\n\n Solution \n\n\nBefore stating the model, you first need to define the dummy variables for Diagnosis:\n\\[\nD_\\text{Amnesic} = \\begin{cases}\n1 & \\text{if Diagnosis is Amnesic} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\quad\nD_\\text{Huntingtons} = \\begin{cases}\n1 & \\text{if Diagnosis is Huntingtons} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\quad\n(\\text{Control is base level})\n\\]\nAnd for Task:\n\\[\nT_\\text{Recognition} = \\begin{cases}\n1 & \\text{if Task is Recognition} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\quad\n(\\text{Grammar is base level})\n\\]\nThe model becomes:\n\\[\n\\begin{aligned}\nScore &= \\beta_0 \\\\\n      &+ \\beta_1 D_\\text{Amnesic} + \\beta_2 D_\\text{Huntingtons}  \\\\\n      &+ \\beta_3 T_\\text{Recognition}  \\\\\n      &+ \\beta_4 (D_\\text{Amnesic} * T_\\text{Recognition}) + \\beta_5 (D_\\text{Huntingtons} * T_\\text{Recognition})  \\\\\n      &+ \\epsilon\n\\end{aligned}\n\\]\nEffects will be considered statistically significant at \\(\\alpha=.05\\)\nOur hypotheses are:\n\\(H_0: \\beta_5 = 0\\)\nThe difference in performance between explicit and implicit memory tasks does not significantly differ between patients with Huntingtons in comparison to Controls.\n\\(H_1: \\beta_5 \\neq 0\\)\nThe difference in performance between explicit and implicit memory tasks does significantly differ between patients with Huntingtons in comparison to Controls.\n\n\n\n\n\nQuestion 4\n\n\nProvide a table of descriptive statistics and visualise your data.\nRemember to interpret your plot in the context of the study.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nFor your table of descriptive statistics, both the group_by() and summarise() functions will come in handy here.\nRecall that when visualising categorical variables, geom_boxplot() may be most appropriate to use.\n\n\n\n\n\n\n\n\n Solution \n\n\nFirst, lets look at our descriptive statistics and present in a well formatted table:\n\ncog_desc <- cog %>% \n            group_by(Diagnosis, Task) %>%\n            summarise(Mean = mean(Score),\n                      SD = sd(Score),\n                      Min = min(Score),\n                      Max = max(Score)) %>% \n            kable(caption = \"Descriptive Statistics\", digits = 2) %>%\n            kable_styling()\n\ncog_desc\n\n\n\n\nTable 1:  Descriptive Statistics \n \n Diagnosis \n    Task \n    Mean \n    SD \n    Min \n    Max \n  \n\n\n Control \n    Grammar \n    80 \n    11.68 \n    70 \n    98 \n  \n\n Control \n    Recognition \n    95 \n    12.98 \n    80 \n    107 \n  \n\n Amnesic \n    Grammar \n    60 \n    14.92 \n    44 \n    76 \n  \n\n Amnesic \n    Recognition \n    65 \n    12.17 \n    51 \n    82 \n  \n\n Huntingtons \n    Grammar \n    40 \n    13.25 \n    24 \n    55 \n  \n\n Huntingtons \n    Recognition \n    95 \n    13.38 \n    80 \n    108 \n  \n\n\n\n\n\nNext, look at associations among variables of interest:\n\ncog_plt <- ggplot(cog, aes(x = Diagnosis, y = Score, fill = Task)) + \n  geom_boxplot() \ncog_plt\n\n\n\nFigure 1: Associations among Score, Diagnosis, and Task\n\n\n\n\n\nParticipants with Amnesia do not appear to differ in Score for Recognition or Grammar tasks. In comparison to Controls, Amnesic patients score lower on both tasks, but not considerably so.\nParticipants with Huntingtons do differ in Score for Recognition and Grammar tasks, with higher scores on Recognition tasks. In comparison to Controls, Huntingtons patients score similarly on Recognition tasks, but considerably lower on Grammar tasks.\n\n\n\n\n\n\nQuestion 5\n\n\nFit the specified model using lm(), and store the model in an object named “cog_mdl”.\n\n\n\n\n\n\nNote\n\n\n\nFortunately, R computes the dummy variables for us! Thus. each row in the summary() output of the model will correspond to one of the estimated \\(\\beta\\)’s in the equation above.\n\n\n\n\n\n\n Solution \n\n\n\n#fit interaction model\ncog_mdl <- lm(Score ~ Diagnosis * Task, data = cog)\n\n#check model output\nsummary(cog_mdl)\n\n\nCall:\nlm(formula = Score ~ Diagnosis * Task, data = cog)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-16.00 -12.25   2.00  11.75  18.00 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                            80.000      5.859  13.653 8.27e-13 ***\nDiagnosisAmnesic                      -20.000      8.287  -2.414  0.02379 *  \nDiagnosisHuntingtons                  -40.000      8.287  -4.827 6.45e-05 ***\nTaskRecognition                        15.000      8.287   1.810  0.08281 .  \nDiagnosisAmnesic:TaskRecognition      -10.000     11.719  -0.853  0.40192    \nDiagnosisHuntingtons:TaskRecognition   40.000     11.719   3.413  0.00228 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.1 on 24 degrees of freedom\nMultiple R-squared:  0.7394,    Adjusted R-squared:  0.6851 \nF-statistic: 13.62 on 5 and 24 DF,  p-value: 2.359e-06\n\n\n\n\n\n\n\nQuestion 6\n\n\nRecall your table of descriptive statistics - map each coefficient from the summary() output from “cog_mdl” to the group means.\n\n\n\n\n Solution \n\n\n\ncog_desc\n\n\n\nDescriptive Statistics\n \n Diagnosis \n    Task \n    Mean \n    SD \n    Min \n    Max \n  \n\n\n Control \n    Grammar \n    80 \n    11.68 \n    70 \n    98 \n  \n\n Control \n    Recognition \n    95 \n    12.98 \n    80 \n    107 \n  \n\n Amnesic \n    Grammar \n    60 \n    14.92 \n    44 \n    76 \n  \n\n Amnesic \n    Recognition \n    65 \n    12.17 \n    51 \n    82 \n  \n\n Huntingtons \n    Grammar \n    40 \n    13.25 \n    24 \n    55 \n  \n\n Huntingtons \n    Recognition \n    95 \n    13.38 \n    80 \n    108 \n  \n\n\n\n\n\n\\(\\hat{\\beta}_0\\) = 80\n= Mean(Control, Grammar)\n\\(\\hat{\\beta}_1\\) = -20\n= Mean(Amnesic, Grammar) - Mean(Control, Grammar)\n= 60 - 80\n\\(\\hat{\\beta}_2\\) = -40\n= Mean(Huntingtons, Grammar) - Mean(Control, Grammar)\n= 40 - 80\n\\(\\hat{\\beta}_3\\) = 15\n= Mean(Control, Recognition) - Mean(Control, Grammar)\n= 95 - 80\n\\(\\hat{\\beta}_4\\) = -10\n= [Mean(Amnesic, Recognition) - Mean(Amnesic, Grammar)] -\n[Mean(Control, Recognition) - Mean(Control, Grammar)]\n= [65 - 60] - [95 - 80] = 5 - 15 = -10\n\\(\\hat{\\beta}_5\\) = 40\n= [Mean(Huntingtons, Recognition) - Mean(Huntingtons, Grammar)] -\n[Mean(Control, Recognition) - Mean(Control, Grammar)]\n= [95 - 40] - [95 - 80] = 55 - 15 = 40\n\n\n\n\n\n\nQuestion 7\n\n\nInterpret your coefficients in the context of the study.\n\n\n\n\n Solution \n\n\nRecall that we can obtain our parameter estimates using various functions such as summary(),coef(), coefficients(), etc.\n\ncoefficients(cog_mdl)\n\n                         (Intercept)                     DiagnosisAmnesic \n                                  80                                  -20 \n                DiagnosisHuntingtons                      TaskRecognition \n                                 -40                                   15 \n    DiagnosisAmnesic:TaskRecognition DiagnosisHuntingtons:TaskRecognition \n                                 -10                                   40 \n\n\n\n\n\\(\\beta_0\\) = (Intercept) = 80\n\nThe intercept, or predicted scores for those in the Control diagnosis condition on the Grammar task.\n\n\n\n\\(\\beta_1\\) = DiagnosisAmnesic = -20\n\nThe difference in scores between Amnesic and Control conditions on the Grammar task\nOn the Grammar task, individuals with Amnesia scored 20 points lower than Control participants.\n\n\n\n\\(\\beta_2\\) = DiagnosisHuntingtons = -40\n\nThe difference in score between Huntingtons and Control conditions on the Grammar task\nOn the Grammar task, individuals with Huntingtons scored 40 points lower than Control participants.\n\n\n\n\\(\\beta_3\\) TaskRecognition = 15\n\nThe difference in score between individuals in the Control diagnosis condition completing Recognition and Grammar tasks.\nControl participants scored 15 points higher when completing Recognition tasks in comparison to Grammar tasks.\n\n\n\n\\(\\beta_4\\) DiagnosisAmnesic:TaskRecognition = -10\n\nThe difference between score in Amnesic and Control diagnosis conditions between Recognition and Grammar tasks.\nThe difference between Grammar and Recognition tasks is 10 points lower in the Amnesiac vs Control diagnosis conditions.\n\n\n\n\\(\\beta_5\\) DiagnosisHuntingtons:TaskRecognition = 40\n\nThe difference between score in Huntingtons and Control diagnosis conditions between Recognition and Grammar tasks.\nThe difference between Grammar and Recognition tasks is 40 points higher in the Huntingtons vs Control diagnosis conditions.\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nUsing the cat_plot() function from the interactions package, visualise the interaction effects from your model.\nTry to summarise the interaction effects in a few short and concise sentences.\n\n\n\n\n Solution \n\n\n\nplt_cog_mdl <- cat_plot(model = cog_mdl, \n                  pred = Diagnosis, \n                  modx = Task, \n                  main.title = \"Scores across Diagnois and Task\",\n                  x.label = \"Diagnosis\",\n                  y.label = \"Score\",\n                  legend.main = \"Task\")\nplt_cog_mdl\n\n\n\nFigure 2: Interaction Plot\n\n\n\n\nThe effect of Task on Scores does appear to vary depending on Diagnosis.\nThe difference in score between recognition and grammar tasks for Huntingtons patients is larger than the difference in score between recognition and grammar tasks for the Control patients.\nThe difference in score between recognition and grammar tasks for Amnesic patients however does not appear to be very different (given the overlapping intervals) than the difference in score between recognition and grammar tasks for the Control patients.\n\n\n\n\n\n\nHow do we know there is an interaction?\n\n\n\n\n\nIf you imagine connecting the dots of the same color with a line (you could specify geom = \"line\" in a new line in the code chunk above to do this), you can see that the two virtual lines are not parallel (see below plot), suggesting the presence of an interaction. The difference in score between recognition and grammar tasks for Huntingtons patients (consider the vertical difference) is larger than the difference in score between recognition and grammar tasks for the Control patients. If those vertical differences were the same, there would be no interaction.\n\n\n\n\nFigure 3: Interaction Plot with Connected Lines\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nProvide key model results in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package.\nRemember that you can rename your DV and IV labels by specifying dv.labels and pred.labels.\n\n\n\n\n\n\n\n Solution \n\n\n\n#create table for results\ntab_model(cog_mdl, \n          show.stat = TRUE,\n          dv.labels = \"Scores\",\n          title = \"Regression table for Scores model\")\n\n\n\n\nTable 2:  Regression table for Scores model \n\n \nScores\n\n\nPredictors\nEstimates\nCI\nStatistic\np\n\n\n(Intercept)\n80.00\n67.91 – 92.09\n13.65\n<0.001\n\n\nDiagnosis [Amnesic]\n-20.00\n-37.10 – -2.90\n-2.41\n0.024\n\n\nDiagnosis [Huntingtons]\n-40.00\n-57.10 – -22.90\n-4.83\n<0.001\n\n\nTask [Recognition]\n15.00\n-2.10 – 32.10\n1.81\n0.083\n\n\nDiagnosis [Amnesic] *Task [Recognition]\n-10.00\n-34.19 – 14.19\n-0.85\n0.402\n\n\nDiagnosis [Huntingtons] *Task [Recognition]\n40.00\n15.81 – 64.19\n3.41\n0.002\n\n\nObservations\n30\n\n\nR2 / R2 adjusted\n0.739 / 0.685\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret your results in the context of the research question and report your model in full.\nMake reference to the interaction plot and regression table.\n\n\n\n\n Solution \n\n\nFull regression results including 95% Confidence Intervals are shown in Table 2. The \\(F\\)-test for model utility was significant \\((F(5,24) = 13.62, p <.001)\\), and the model explained approximately 68.5% of the variability in Scores.\nThe difference in scores between the recognition and grammar tasks, respectively measuring explicit and implicit memory, for amnesiac patients in comparison to controls was estimated to be -10 points, though this difference was not significantly different from zero \\((t(24) = -0.85, p = .40)\\).\nThe difference in scores between the recognition and grammar tasks, respectively measuring explicit and implicit memory, for Huntingtons patients in comparison to controls was significant, and indicated a difference of 40 points in explicit vs implicit memory performance \\((t(24) = 3.41, p = .002)\\). This interaction is visually presented in Figure 2.\nTherefore, we have evidence to reject the null hypothesis (the difference in performance between explicit and implicit memory tasks does not significantly differ between patients with Huntingtons in comparison to Controls).\n\n\n\n\n\nFootnotes\n\nSome researchers may point out that a design where each person was assessed on both tasks might have been more efficient. However, the task factor in such design would then be within-subjects, meaning that the scores corresponding to the same person would be correlated. To analyse such design we will need a different method which is discussed next year!↩︎"
  },
  {
    "objectID": "1_10_assump_diag.html",
    "href": "1_10_assump_diag.html",
    "title": "Assumptions and Diagnostics",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to state the assumptions underlying a linear model.\nSpecify the assumptions underlying a linear model with multiple predictors.\nAssess if a fitted model satisfies the assumptions of your model.\nAssess the effect of influential cases on linear model coefficients and overall model evaluations.\n\n\nBe up to date with lectures\nHave completed previous lab exercises from Week 7, Week 8, and Week 9\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\ncar\nsee\nperformance\n\nYou can download the data required for this lab here and here or read both datsets in via these links https://uoepsy.github.io/data/riverview.csv and https://uoepsy.github.io/data/wellbeing.csv."
  },
  {
    "objectID": "1_10_assump_diag.html#assumptions",
    "href": "1_10_assump_diag.html#assumptions",
    "title": "Assumptions and Diagnostics",
    "section": "Assumptions",
    "text": "Assumptions\n\nAssumptions\nYou can remember the four assumptions by memorising the acronym LINE:\n\n\nLinearity: The relationship between \\(y\\) and \\(x\\) is linear.\n\nIndependence of errors: The error terms should be independent from one another.\n\nNormality: The errors \\(\\epsilon\\) are normally distributed in the population.\n\nEqual variances (“Homoscedasticity”): The variability of the errors \\(\\epsilon\\) is constant across \\(x\\).\n\n\n\nQuestion 1\n\n\nLet’s start by using check_model() for both rv_mdl1 and wb_mdl1 models - we can refer to these plots as a guide as we work through the assumptions questions of the lab.\n\n\n\n\n\n\nThese plots cannot be used in your reports - they are to be used as a guide only.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nrv_mdl1 model\nwb_mdl1 model\n\n\n\n\ncheck_model(rv_mdl1)\n\n\n\n\n\n\n\ncheck_model(wb_mdl1)\n\n\n\n\n\n\n\nThe check_model() function is a useful way to check the assumptions of models, as it also returns some useful notes to aid your interpretation. From the above, it doesn’t appear that any of the assumptions for either have been violated, but we need to check each assumption individually with plots that are more suitable for a statistics report.\n\n\n\n\n\nQuestion 2\n\n\nCheck if the fitted model satisfies the linearity assumption for rv_mdl1.\nWrite a sentence summarising whether or not you consider the assumption to have been met. Justify your answer with reference to the plots.\n\n\n\n\n\n\nHint\n\n\n\n\n\nConsider plotting residual vs fitted values, and/or a scatterplot with loess lines\n\n\n\n\n\n\n\n Solution \n\n\nAs usual, there are multiple equivalent ways to check this. For a model with a single predictor, there are a couple of possibilities:\n\n\nResiduals vs fitted values\nScatterplots with loess lines\n\n\n\n\n#here what we need to see is that the red line is approximately horizontal at zero\nplot(rv_mdl1, which = 1) \n\n\n\n\n\n\n\n\n\n\nThe residuals appear to be randomly scattered around zero, without showing any pattern with respect to the fitted values. Thus, there is no sign of violation of the linearity assumption.\n\n\n\n\n\n\nggplot(rvdata, aes(x = education, y = income)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se=F) +\n    geom_smooth(method = \"loess\", se=F, col = \"red\") +\n    labs(x= \"Education (in years)\", y=\"Income (in thousands of US dollars)\", title = \"Scatterplot with linear (blue) and loess (red) lines\")\n\n\n\n\n\n\n\n\n\n\nFrom our scatterplot, the loess line closely follows the data. Hence, there is no sign of violation of the linearity assumption.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nCheck if the fitted model satisfies the linearity assumption for wb_mdl1.\nWrite a sentence summarising whether or not you consider the assumption to have been met. Justify your answer with reference to the plots.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhen we have multiple predictors, we need to use component-residual plots (also known as partial-residual plots) to check the assumption of linearity.\n\n\n\n\n\n\n\n Solution \n\n\n\ncrPlots(wb_mdl1)\n\n\n\n\n\n\n\n\n\n\nThe smoother (the pink line) follows quite closely to a linear relationship (the dashed blue line). This suggests that the linearity assumption is met.\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nCheck if the fitted models rv_mdl1 and wb_mdl1 satisfy the equal variance assumption.\nWrite a sentence summarising whether or not you consider the assumption to have been met for each model. Justify your answer with reference to plots.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse residualPlots() to plot residuals against the predictor.\nFor interpretation - the vertical spread of the residuals should roughly be the same everywhere.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nrv_mdl1 model\nwb_mdl1 model\n\n\n\nWe can visually assess by plotting the Pearson residuals against the fitted values:\n\nresidualPlots(rv_mdl1)\n\n\n\n\n           Test stat Pr(>|Test stat|)\neducation    -0.0744           0.9412\nTukey test   -0.0744           0.9407\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nQuick Tip\nAs the residuals can be positive or negative, we can make it easier to assess equal spread by improving the ‘resolution’ of the points.\nWe can make all residuals positive by discarding the sign (take the absolute value), and then take the square root to make them closer to each other.\nA plot of \\(\\sqrt{|\\text{Standardized residuals}|}\\) against the fitted values is shown below:\n\nplot(rv_mdl1, which = 3)\n\n\n\n\nThe plot above has the points closer to each other, and all above 0. The line seems to be relatively flat (as it should be if the spread was constant). The data met the assumption of equal variance.\n\n\n\n\n\n\n\n\n\nThe spread of the standardized residuals appears to be constant as the fitted values vary.\n\n\n\n\n\n\nresidualPlots(wb_mdl1)\n\n\n\n\n             Test stat Pr(>|Test stat|)\noutdoor_time   -0.3478           0.7306\nsocial_int     -0.1068           0.9157\nTukey test     -0.4189           0.6753\n\n\n\n\n\n\n\n\nPartial residual plots show no clear non-linear trends between residuals and predictors, hence there is little sign of non-constant variance. The data met the assumption of equal variance.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nFor both rv_mdl1 and wb_mdl1, visually assess whether there is autocorrelation in the error terms.\nWrite a sentence summarising whether or not you consider the assumption of independence to have been met for each (you may have to assume certain aspects of the study design).\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo get a single figure made up of 2 by 1 panels, you can use the command par(mfrow = c(1,2)). Then create the plot. Then you need to go back to a single figure made up by a single panel with the command par(mfrow = c(1,1)).\n\n\n\n\n\n\n\n Solution \n\n\n\n\nrv_mdl1 model\nwb_mdl1 model\n\n\n\n\npar(mfrow = c(1,2))\nplot(resid(rv_mdl1))\nplot(fitted(rv_mdl1), resid(rv_mdl1))\n\n\n\npar(mfrow = c(1,1))\n\n\n\n\n\n\n\nSince our data were collected from a between-persons study design, we can assume the errors to be independent. This is supported by the plot of the residuals vs their index, as this shows no clear dependence (if they were collected over time, for example, and there was an increasing trend this would highlight a violation). The residuals vs fitted plot shows that there is no association between the errors and the model predictions.\n\n\n\n\n\n\npar(mfrow = c(1,2))\nplot(resid(wb_mdl1))\nplot(fitted(wb_mdl1), resid(wb_mdl1))\n\n\n\npar(mfrow = c(1,1))\n\n\n\n\n\n\n\nSince our data were collected from a between-persons study design, we can assume the errors to be independent. This is supported by the plot of the residuals vs their index which shows no clear dependence, and the residuals vs fitted plot shows that there is no association between the errors and the model predictions.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nCheck if the fitted models rv_mdl1 and wb_mdl1 satisfy the normality assumption.\nWrite a sentence summarising whether or not you consider the assumption to have been met. Justify your answer with reference to the plots.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can get the QQplot from plot(model, which = ???), and you can plot the frequency distribution of the residuals via hist(model$residuals).\nFor interpretation - remember that departures from a linear trend in QQ plots indicate a lack of normality.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nrv_mdl1 model\nwb_mdl1 model\n\n\n\nHistogram:\n\nhist(rv_mdl1$residuals)\n\n\n\n\nQQ Plot:\n\nplot(rv_mdl1, which = 2)\n\n\n\n\n\n\n\n\n\n\nThe normal quantile plot follows a linear pattern and does not highlight any substantial skew or departure from normality.\n\n\n\n\n\nHistogram:\n\nhist(wb_mdl1$residuals)\n\n\n\n\nQQ Plot:\n\nplot(wb_mdl1, which = 2)\n\n\n\n\n\n\n\n\n\n\nThe QQplot indicated that the residuals follow close to a normal distribution. Although there is some evidence of heavier tails, given the small sample size (\\(n\\)=32) it is not of concern and we can be more conservative in our visual assessment of the plot.\n\n\n\n\n\n\n\n\n\n\n\nFor questions 7-10, we will be working with our wb_mdl1 only. Feel free to apply the below to your rv_mdl1 too as as extra practice during revision."
  },
  {
    "objectID": "1_10_assump_diag.html#multicollinearity",
    "href": "1_10_assump_diag.html#multicollinearity",
    "title": "Assumptions and Diagnostics",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nQuestion 7\n\n\nFor wb_mdl1, calculate the variance inflation factor (VIF) for the predictors in the model.\nWrite a sentence summarising whether or not you consider multicollinearity to be a problem here.\n\n\n\n\n Solution \n\n\n\nvif(wb_mdl1)\n\noutdoor_time   social_int \n     1.13023      1.13023 \n\n\n\n\n\n\n\n\nThe VIF values for all predictors are <5, indicating that multicollinearity is not adversely affecting model estimates."
  },
  {
    "objectID": "1_10_assump_diag.html#diagnostics",
    "href": "1_10_assump_diag.html#diagnostics",
    "title": "Assumptions and Diagnostics",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nQuestion 8\n\n\nCreate a new tibble which contains:\n\nThe original variables from the model (Hint, what does wb_mdl1$model give you?)\nThe fitted values from the model \\(\\hat y\\)\n\nThe residuals \\(\\hat \\epsilon\\)\n\nThe studentised residuals\nThe hat values\nThe Cook’s Distance values\n\n\n\n\n\n Solution \n\n\n\nmdl_diagnost <- \n  tibble(\n  wb_mdl1$model,\n  fitted = fitted(wb_mdl1),\n  resid = residuals(wb_mdl1),\n  studres = rstudent(wb_mdl1),\n  hats = hatvalues(wb_mdl1),\n  cooksd = cooks.distance(wb_mdl1)\n)\n\n\n\n\n\n\nQuestion 9\n\n\nFrom the tibble above, comment on the following:\n\nLooking at the studentised residuals, are there any outliers?\nLooking at the hat values, are there any observations with high leverage?\nLooking at the Cook’s Distance values, are there any highly influential points?\n\n\n\n\n\n Solution \n\n\n\n\nOutliers\nHigh Leverage\nInfluential Points\n\n\n\nIn a standard normal distribution, 95% of the values are roughly between -2 and 2. Because of this, studentised residuals of \\(>2\\) or \\(< -2\\) indicate potential outlyingness.\nWe can ask R whether the absolute values (by specifying abs()) are \\(>2\\):\n\ntable(abs(mdl_diagnost$studres) > 2)\n\n\nFALSE \n   32 \n\n\nAll values are FALSE, which tells us that none of the absolute values are \\(>2\\).\nWe could also filter our newly created tibble to these observations:\n\nmdl_diagnost %>% \n  filter(abs(studres)>2)\n\n# A tibble: 0 × 8\n# … with 8 variables: wellbeing <dbl>, outdoor_time <dbl>, social_int <dbl>,\n#   fitted <dbl>, resid <dbl>, studres <dbl>, hats <dbl>, cooksd <dbl>\n\n\nThere are zero rows in the tibble that have absolute values \\(>2\\).\n\n\n\n\n\n\nThere are no outliers in our data.\n\n\n\n\n\nRecall that hat values of more than \\(2 \\bar{h}\\) (2 times the average hat value) are considered high leverage. The average hat value, \\(\\bar{h}\\) is calculated as \\(\\frac{k + 1}{n}\\), where \\(k\\) is the number of predictors, and \\(n\\) is the sample size.\nFor our model:\n\\[\n\\bar h = \\frac{k+1}{n} = \\frac{2+1}{32} = \\frac{3}{32} = 0.094\n\\]\nWe can ask whether any of observations have hat values which are greater than \\(2 \\bar h\\):\n\nmdl_diagnost %>%\n  filter(hats > (2*0.094))\n\n# A tibble: 0 × 8\n# … with 8 variables: wellbeing <dbl>, outdoor_time <dbl>, social_int <dbl>,\n#   fitted <dbl>, resid <dbl>, studres <dbl>, hats <dbl>, cooksd <dbl>\n\n\nNo values have been returned (the tibble has zero rows), hence we do not have any observations that have hat values which are greater than \\(2 \\bar h\\).\n\n\n\n\n\n\nThere are 0 observations that have high leverage.\n\n\n\n\n\nRecall that we have a Cook’s Distance cut-off of \\(\\frac{4}{n-k-1}\\), where \\(k\\) is the number of predictors, and \\(n\\) is the sample size.\nFor our model:\n\\[\nD_{cutoff} = \\frac{4}{n-k-1} = \\frac{4}{32 - 2 - 1} = \\frac{4}{29} = 0.138\n\\]\nWe can ask whether any of observations have a high influence on our model estimates:\n\nmdl_diagnost %>%\n  filter(cooksd > 0.138)\n\n# A tibble: 0 × 8\n# … with 8 variables: wellbeing <dbl>, outdoor_time <dbl>, social_int <dbl>,\n#   fitted <dbl>, resid <dbl>, studres <dbl>, hats <dbl>, cooksd <dbl>\n\n\nNo values have been returned (the tibble has zero rows), hence we do not have any observations that have a high influence on our model estimates.\nYou can also display the Cook’s Distance values themselves using plot(model, which = 4):\n\nplot(wb_mdl1, which = 4)\n\n\n\n\n\n\n\n\n\n\nNone of our observations have a high influence on our model estimates, as the maximum value of Cook’s distance in our sample is 0.108, which is less than the cutt-off value of 0.138. Thus, we do not appear to have any influential points in our sample.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nUse the function influence.measures() to extract these delete-1 measures of influence.\nTry plotting the distributions of some of these measures.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe function influence.measures() returns an infl-type object. To plot this, we need to find a way to extract the actual numbers from it.\nWhat do you think names(influence.measures(wb_mdl1)) shows you? How can we use influence.measures(wb_mdl1)$<insert name here> to extract the matrix of numbers?\n\n\n\n\n\n\n\n Solution \n\n\n\n\n\n\ninf_mes <- influence.measures(wb_mdl1)\nround(inf_mes$infmat[1:10,],3)\n\n   dfb.1_ dfb.otd_ dfb.scl_  dffit cov.r cook.d   hat\n1   0.437   -0.111   -0.322  0.448 1.157  0.066 0.149\n2  -0.282    0.032    0.230 -0.292 1.225  0.029 0.141\n3   0.296    0.076   -0.290  0.354 1.088  0.042 0.097\n4  -0.264   -0.131    0.293 -0.351 1.117  0.041 0.107\n5  -0.271    0.227    0.105 -0.329 1.279  0.037 0.177\n6   0.125   -0.027   -0.083  0.146 1.141  0.007 0.060\n7  -0.174   -0.034    0.153 -0.223 1.087  0.017 0.060\n8  -0.029   -0.073    0.061 -0.092 1.309  0.003 0.156\n9   0.169    0.088   -0.178  0.248 1.088  0.021 0.067\n10 -0.248    0.331   -0.006 -0.427 1.027  0.059 0.096\n\n\nLet’s plot the distribution of COVRATIO statistics.\nRecall that values which are \\(>1+\\frac{3(k+1)}{n}\\) or \\(<1-\\frac{3(k+1)}{n}\\) are considered as having strong influence.\nFor our model:\n\\[\n1 \\pm \\frac{3(k+1)}{n} \\quad = \\quad 1 \\pm\\frac{3(2+1)}{32} \\quad = \\quad 1\\pm \\frac{9}{32} \\quad = \\quad 1\\pm0.28\n\\]\nThe “infmat” bit of an infl-type object contains the numbers, as we can see from out output above. To use it with ggplot, we will need to turn it into a dataframe (as.data.frame()), or a tibble (as_tibble()):\n\ninfdata <- inf_mes$infmat %>%\n  as_tibble()\n\nNow we can build our plot. It would be useful to add vertical lines at the values \\(\\quad 1\\pm0.28\\). To do so, we can use the geom_vline() function:\n\nggplot(data = infdata, aes(x = cov.r)) + \n  geom_histogram() +\n  geom_vline(aes(xintercept = c(1-0.28), col = \"blue\")) +\n  geom_vline(aes(xintercept = c(1+0.28), col = \"red\")) + \n  theme(legend.position = \"none\")  #remove legend\n\n\n\n\nIt looks like a few observations may be having quite a high influence here. This is perhaps not that surprising as we only have 32 datapoints.\n\n\n\n\n\n \n\n \n\n      © Copyright 2019-2022 The University of Edinburgh. Site licensed under the GNU AGPLv3 license."
  },
  {
    "objectID": "1_11_writeup_recap.html",
    "href": "1_11_writeup_recap.html",
    "title": "Write Up Example & Block 2 Recap",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand how to write-up and provide interpretation of a linear model with multiple predictors.\n\n\nBe up to date with lectures\nHave completed Labs 7-10\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\nsjPlot\nsandwich\ninteractions\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/scs_study.csv.\nNote: This is the same data as Lab 8."
  },
  {
    "objectID": "1_11_writeup_recap.html#study-overview",
    "href": "1_11_writeup_recap.html#study-overview",
    "title": "Write Up Example & Block 2 Recap",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Question\nControlling for other personality traits, does neuroticism moderate effects of social comparison on symptoms of depression, anxiety and stress?\n\nPrevious research has identified an association between an individual’s perception of their social rank and symptoms of depression, anxiety and stress. We are interested in the individual differences in this association.\nTo investigate whether the effect of social comparison on symptoms of depression, anxiety and stress varies depending on level of Neuroticism, we will need to fit a multiple regression model with an interaction term and control for other personality traits.\n\n Social Comparison Study data codebook\n\n\nDescription\nData from 656 participants containing information on scores on each trait of a Big 5 personality measure, their perception of their own social rank, and their scores on a measure of depression.\nThe data in scs_study.csv contain seven attributes collected from a random sample of \\(n=656\\) participants:\n\n\nzo: Openness (Z-scored), measured on the Big-5 Aspects Scale (BFAS)\n\nzc: Conscientiousness (Z-scored), measured on the Big-5 Aspects Scale (BFAS)\n\nze: Extraversion (Z-scored), measured on the Big-5 Aspects Scale (BFAS)\n\nza: Agreeableness (Z-scored), measured on the Big-5 Aspects Scale (BFAS)\n\nzn: Neuroticism (Z-scored), measured on the Big-5 Aspects Scale (BFAS)\n\nscs: Social Comparison Scale - An 11-item scale that measures an individual’s perception of their social rank, attractiveness and belonging relative to others. The scale is scored as a sum of the 11 items (each measured on a 5-point scale), with higher scores indicating more favourable perceptions of social rank.\n\ndass: Depression Anxiety and Stress Scale - The DASS-21 includes 21 items, each measured on a 4-point scale. The score is derived from the sum of all 21 items, with higher scores indicating higher a severity of symptoms.\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\n\n\nzo\n      zc\n      ze\n      za\n      zn\n      scs\n      dass\n    \n\n\n0.76\n1.58\n-0.79\n-0.09\n1.32\n30\n56\n\n\n0.30\n-0.27\n-0.09\n0.09\n-0.40\n30\n48\n\n\n-0.13\n0.66\n-0.80\n-0.95\n0.93\n35\n48\n\n\n1.06\n-1.02\n-0.16\n-0.50\n-0.02\n29\n48\n\n\n1.74\n-0.78\n-1.55\n-2.86\n-1.14\n41\n43\n\n\n0.22\n-0.41\n0.78\n0.90\n-0.25\n37\n60\n\n\n\n\n\n\n\n\n\n\n Provided Analysis Code\n\n\n\nlibrary(tidyverse) # for all things!\nlibrary(psych) # good for descriptive stats\nlibrary(kableExtra) # useful for creating nice tables\nlibrary(car) # for assumption tests\nlibrary(sandwich)\nlibrary(interactions) # for plotting models\n\nscs_study <- read_csv(\"https://uoepsy.github.io/data/scs_study.csv\")\n\n# standardise scs score\nscs_study <- \n  scs_study %>% \n    mutate(\n      zscs = (scs-mean(scs))/sd(scs)\n    )\n#alternatively, you could do zscs = scale(scs, center = TRUE, scale = TRUE)\n\n# the describe() function is from the psych package, and kable() from kableExtra which is used to make a nice table where the values are rounded to 2 decimal places using digits = 2. \ndescribe(scs_study %>% \n        select(dass, scs, zn))[,c(2:4,8:9)] %>% \n        kable(., caption = \"DASS-21, SCS, and Neuroticism Descriptive Statistics\", digits = 2) %>%\n        kable_styling()\n\n\n\nDASS-21, SCS, and Neuroticism Descriptive Statistics\n \n   \n    n \n    mean \n    sd \n    min \n    max \n  \n\n\n dass \n    656 \n    44.72 \n    6.76 \n    23.00 \n    68.00 \n  \n\n scs \n    656 \n    35.77 \n    3.53 \n    27.00 \n    54.00 \n  \n\n zn \n    656 \n    0.00 \n    1.00 \n    -1.45 \n    3.35 \n  \n\n\n\n\n\n# scatterplot matrix of dataset without the zscs variable\npairs.panels(scs_study %>% select(-zscs))\n\n\n\n\n\ndass_mdl <- lm(dass ~ zscs*zn + zo + zc + ze + za, data = scs_study)\npar(mfrow=c(2,2))\nplot(dass_mdl)\n\n\n\n# 35 seems to be a very influential point, lets remove it and re-run the model\n\n\ndass_mdl2 <- lm(dass ~ zscs*zn + zo + zc + ze + za, data = scs_study[-35, ])\n\n# check assumptions for updated model\npar(mfrow=c(2,2))\nplot(dass_mdl2)\n\n\n\npar(mfrow=c(1,1))\n\n# N.B. we cannot use crPlots for interactions\n\n\n# Additional diagnostic plots for independence and homoscedasticity\n\n# checking for independence\nplot(resid(dass_mdl2))\n\n\n\n# alternative check for equal variances (Homoscedasticity) - \nresidualPlots(dass_mdl2)\n\n\n\n\n           Test stat Pr(>|Test stat|)  \nzscs          1.8141          0.07013 .\nzn           -0.5911          0.55467  \nzo            1.7801          0.07553 .\nzc           -0.2403          0.81018  \nze           -0.9951          0.32004  \nza            0.0725          0.94219  \nTukey test   -1.6406          0.10089  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# multicollinearity\nvif(dass_mdl2)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n    zscs       zn       zo       zc       ze       za  zscs:zn \n1.015133 1.015736 1.013310 1.008235 2.332486 2.342220 1.012475 \n\n\n\n# model output\nsummary(dass_mdl2)\n\n\nCall:\nlm(formula = dass ~ zscs * zn + zo + zc + ze + za, data = scs_study[-35, \n    ])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1455  -3.8155  -0.0066   3.6905  18.1483 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 44.97703    0.22635 198.708  < 2e-16 ***\nzscs        -1.93818    0.23042  -8.412 2.58e-16 ***\nzn           1.41639    0.22661   6.250 7.44e-10 ***\nzo          -0.31435    0.22056  -1.425    0.155    \nzc           0.09134    0.22515   0.406    0.685    \nze           0.52695    0.34233   1.539    0.124    \nza           0.33847    0.34281   0.987    0.324    \nzscs:zn     -2.76609    0.24097 -11.479  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.733 on 647 degrees of freedom\nMultiple R-squared:  0.279, Adjusted R-squared:  0.2712 \nF-statistic: 35.76 on 7 and 647 DF,  p-value: < 2.2e-16\n\n\n\n#create table for results\ntab_model(dass_mdl2,\n          dv.labels = c(\"DASS-21 Scores\"),\n          pred.labels = c(\"zscs\"=\"Social Comparison Scale (Z-scored)\", \n                          \"zn\"=\"Neuroticism (Z-scored)\", \n                          \"zo\"=\"Openness (Z-scored)\", \n                          \"zc\"=\"Conscientiousness (Z-scored)\",\n                          \"ze\"=\"Extraversion (Z-scored)\",\n                          \"za\"=\"Agreeableness (Z-scored)\",\n                          \"zscs:zn\"=\"Social Comparison Scale (Z-scored): Neutoricism (Z-scored)\"),\n          title = \"Regression table for DASS-21 model\")\n\n\n\nRegression table for DASS-21 model\n\n \nDASS-21 Scores\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n44.98\n44.53 – 45.42\n<0.001\n\n\nSocial Comparison Scale(Z-scored)\n-1.94\n-2.39 – -1.49\n<0.001\n\n\nNeuroticism (Z-scored)\n1.42\n0.97 – 1.86\n<0.001\n\n\nOpenness (Z-scored)\n-0.31\n-0.75 – 0.12\n0.155\n\n\nConscientiousness(Z-scored)\n0.09\n-0.35 – 0.53\n0.685\n\n\nExtraversion (Z-scored)\n0.53\n-0.15 – 1.20\n0.124\n\n\nAgreeableness (Z-scored)\n0.34\n-0.33 – 1.01\n0.324\n\n\nSocial Comparison Scale(Z-scored): Neutoricism(Z-scored)\n-2.77\n-3.24 – -2.29\n<0.001\n\n\nObservations\n655\n\n\nR2 / R2 adjusted\n0.279 / 0.271\n\n\n\n\n\n#interaction plot and simple slopes:\nplt_scs_mdl <- probe_interaction(model = dass_mdl2, \n                  pred = zscs, \n                  modx = zn, \n                  cond.int = T,\n                  interval = T, \n                  jnplot = T,\n                  main.title = \"Neuroticism moderating the effect of\\nsocial comparison on depression and anxiety\",\n                  x.label = \"Social Comparison Scale (Z-scored)\",\n                  y.label = \"DASS-21 Scores\",\n                  legend.main = \"Neuroticism (Z-scored)\")\n\nplt_scs_mdl$interactplot\n\n\n\nplt_scs_mdl$simslopes\n\nJOHNSON-NEYMAN INTERVAL \n\nWhen zn is OUTSIDE the interval [-0.93, -0.52], the slope of zscs is p <\n.05.\n\nNote: The range of observed values of zn is [-1.45, 3.35]\n\n\n\n\n\nSIMPLE SLOPES ANALYSIS \n\nWhen zn = -1.000344918 (- 1 SD): \n\n                               Est.   S.E.   t val.      p\n--------------------------- ------- ------ -------- ------\nSlope of zscs                  0.83   0.33     2.49   0.01\nConditional intercept         43.53   0.32   136.42   0.00\n\nWhen zn = -0.003414067 (Mean): \n\n                               Est.   S.E.   t val.      p\n--------------------------- ------- ------ -------- ------\nSlope of zscs                 -1.93   0.23    -8.37   0.00\nConditional intercept         44.96   0.23   199.64   0.00\n\nWhen zn =  0.993516784 (+ 1 SD): \n\n                               Est.   S.E.   t val.      p\n--------------------------- ------- ------ -------- ------\nSlope of zscs                 -4.69   0.33   -14.05   0.00\nConditional intercept         46.39   0.32   145.48   0.00\n\n\n\n\n\nSetup\n\nSetup\n\n\n\nCreate a new RMarkdown file\nLoad the required package(s)\nRead the scs dataset into R, assigning it to an object named scs\n\n\n\n\n\n\n Solution \n\n\n\nlibrary(tidyverse)\nlibrary(psych) \nlibrary(kableExtra)\nlibrary(car)\nlibrary(sjPlot)\n\nscs <- read_csv(\"https://uoepsy.github.io/data/scs_study.csv\")\n\n\n\n\nThe 3-Act Structure\nWe need to present our report in three clear sections - think of your sections like the 3 key parts of a play or story - we need to (1) provide some background and scene setting for the reader, (2) present our results in the context of the research question, and (3) present a resolution to our story - relate our findings back to the question we were asked and provide our answer.\nAct I: Analysis Strategy\n\nQuestion 1\n\n\nAttempt to draft a discussion section based on the above research question and analysis provided.\n\n\n\n\n\n\n\n\n\nAnalysis Strategy - What to Include\n\n\n\n\n\nYour analysis strategy will contain a number of different elements detailing plans and changes to your plan. Remember, your analysis strategy should not contain any results. You may wish to include the following sections:\n\nVery brief data and design description:\n\nGive the reader some background on the context of your write-up. For example, you may wish to describe the data source, data collection strategy, study design, number of observational units.\nSpecify the variables of interest in relation to the research question, including their unit of measurement, the allowed range (for Likert scales), how they are scored, and if they are factors make sure to list the order of the levels.\n\n\nData management:\n\nDescribe any data cleaning and/or recoding.\nAre there any observations that have been excluded based on pre-defined criteria? How/why, and how many?\n* Describe any transformations performed to aid your interpretation (i.e., log transformation, mean centering, standardisation, etc.)\n\n\nModel specification:\n\nClearly state your hypotheses and specify your chosen significance level.\nWhat type of statistical analysis do you plan to use to answer the research question? (e.g., t-test, simple linear regression, multiple linear regression, etc.)\nIn some cases, you may wish to include some visualisations and descriptive tables to motivate your model specification.\nSpecify the model(s) to be fitted to answer your given research question and analysis structure. Clearly specify the response and explanatory variables included in your model(s) and remember to describe the coding of categorical variables (i.e., factors) so the reader is aware of any reference levels.\nDetail the steps that you will undertake to ensure that your model(s) do not violate the appropriate assumptions.\nIf applicable, detail any required changes/modifications to the model specification to satisfy assumptions. Consider the following: Was there anything you had to do differently than planned during the analysis? Did the modelling highlight issues in your data? Did you have to do anything (e.g., transform any variables, exclude any observations) in order to meet assumptions?\n\n\n\n\nNote that the * used on occasion in the above indicates that you may/should in some cases repeat these steps if you decide to make any modifications to your data (e.g., removing outliers, etc.).\n\nAs noted and encouraged throughout the course, one of the main benefits of using RMarkdown is the ability to include inline R code in your document. Try to incorporate this in your write up so you can automatically pull the specified values from your code. If you need a reminder on how to do this, see Lesson 4 of the Rmd Bootcamp.\n\n\n\n\n Example Write-Up of Analysis Strategy Section\n\n\nThe dataset contained information on 656 participants, including \\(Z\\)-scores on 5 personality traits assessed by the Big-Five Aspects Scale (BFAS; Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism). Participants were also assessed on the Social Comparison Scale (SCS), which is an 11-item scale measuring self-perception (relative to others) of social rank, attractiveness and belonging, and the Depression Anxiety and Stress Scale (DASS-21) - a 21 item measure with higher scores indicating higher severity of symptoms. For both of these measures, only total scores were available. Items in the SCS were measured on a 5-point scale, giving minimum and maximum possible scores of 11 and 55 respectively. Items in the DASS-21 were measured on a 4-point scale, meaning that scores could range from 21 to 84.\nAll participant data was complete (no missing values), with scores on the SCS and the DASS-21 all within possible ranges.\nTo investigate whether, when controlling for other personality traits, Neuroticism moderated the effect of social comparison on symptoms of depression, anxiety and stress, total scores on the DASS-21 were modeled using multiple linear regression. The \\(Z\\)-scored measures on each of the big-five personality traits were included as predictors, along with scores on the SCS (\\(Z\\)-scored) and its interaction with the measure of Neuroticism. Effects were considered statistically significant at \\(\\alpha = 0.05\\).\nThe following model specification was used:\n\\[\n\\begin{aligned}\n\\text{DASS-21}\n= \\beta_0 + \\beta_1 \\text{O} + \\beta_2 \\text{C} + \\beta_3 \\text{E} + \\beta_4 \\text{A}\n+ \\beta_5 \\text{N} + \\beta_6 \\text{SCS} + \\beta_7 (\\text{SCS} \\cdot \\text{N})\n+ \\epsilon  \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{where } \\\\\n\\\\\n& \\text{O = Openness, z-scored} \\\\\n& \\text{C = Conscientiousness, z-scored} \\\\\n& \\text{E = Extraversion, z-scored} \\\\\n& \\text{A = Agreeableness, z-scored} \\\\\n& \\text{N = Neuroticism, z-scored} \\\\\n& \\text{SCS = Social Comparison Scale, z-scored} \\\\\n\\end{aligned}\n\\]\nTo address the research question of whether Neuroticism moderated the effect of social comparison on depression and anxiety, we tested whether the interaction between SCS and Neuroticism was significant. Formally, this corresponded to testing whether the interaction coefficient was equal to zero:\n\\[\n\\begin{aligned}\nH_0: \\beta_7 = 0  \\\\  \nH_1: \\beta_7 \\neq 0  \\\\\n\\end{aligned}\n\\]\nThe following assumptions were assessed visually using diagnostic plots: linearity (via plot of residuals vs fitted values; red line should be approximately horizontal, with residual values randomly scattered around zero and thus showing no pattern in relation to fitted values), independence (with the previous plot and a plot of residuals vs index; no dependence should be indicated), equal variances (via a scale-location plot; residuals should be evenly spread across the range of fitted values, where the spread should be constant across the range of fitted values), and normality (via a qqplot of the residuals; points should follow along the diagonal line). We also checked if there was any evidence of multicollinearity by checking VIF values, where values > 5 were considered to indicate moderate multicollinearity, and values > 10 severe. Outliers were assessed via Cooks Distance, where values >2 indicated influential points.\n\n\n\nAct II: Results\n\nQuestion 2\n\n\nAttempt to draft a results section based on your detailed analysis strategy and the analysis provided.\n\n\n\n\n\n\n\n\n\nResults - What To Include\n\n\n\n\n\nThe results section should follow from your analysis strategy. This is where you would present the evidence and results that will be used to answer the research questions and can support your conclusions. Make sure that you address all aspects of the approach you outlined in the analysis strategy.\nIn this section, it is useful to include tables and plots to clearly present your findings to your reader. It is important, however, to carefully select what is the key information that should be presented. You don’t want to overload the reader with unnecessary information, and you also want to save space in case there is a page limit. Make use of figures with multiple panels where you can.\nAs a broad guideline, you want to start with the results of an exploratory data analysis, presenting tables of summary statistics and exploratory plots. You may also want to visualise relationships between variables and report covariances or correlations. Then, you should move on to the results from your model. Remember that in the main part of the report you should only interpret and report for models that do not violate the assumptions. You should also interpret all of the results presented, and remember to make reference to and comment on your assumption and diagnostic checks for key models.\n\n\n\n\n Example Write-Up of Results Section\n\n\nOne observation (unit 35) was judged to be too influential on the model (Cook’s Distance = 2.66) and as such was excluded from the final analysis, leaving 655 observations.\nDescriptive statistics are displayed in Table 1. Bivariate correlations showed a moderate negative association between DASS-21 and SCS scores; a moderate positive association between DASS-21 and Neuroticism, and a weak positive correlation between SCS and Neuroticism (see Figure 1).\n\n\n\n\n\nTable 1:  Regression table for DASS-21 model \n \n   \n    n \n    mean \n    sd \n    min \n    max \n  \n\n\n dass \n    656 \n    44.72 \n    6.76 \n    23.00 \n    68.00 \n  \n\n scs \n    656 \n    35.77 \n    3.53 \n    27.00 \n    54.00 \n  \n\n zn \n    656 \n    0.00 \n    1.00 \n    -1.45 \n    3.35 \n  \n\n\n\n\n\n\n\n\n\nFigure 1: Bivariate scatter plots (below diagonal), histograms (diagonal), and Pearson correlation coefficient (above diagonal) for DASS-21 scores, SCS, and Big 5 Personality variables\n\n\n\n\nThe model met assumptions of linearity and independence (see top left panel of Figure 2; residuals were randomly scattered with a mean of zero and there was no clear dependence), homoscedasticity (see bottom left panel of Figure 2; there was a constant spread of residuals), and normality (see top right panel of Figure 2; the QQplot showed very little deviation from the diagonal line). All VIF values were <5, and hence there was no evidence of multicollinearity.\n\n\n\n\nFigure 2: Diagnostic Plots\n\n\n\n\n\n\n\nFull regression results including 95% Confidence Intervals are shown in Table 2. The \\(F\\)-test for model utility was significant (\\(F(7,647) = 35.76, p<.001\\)), and the model explained approximately 27.12% of the variability in DASS-21 Scores.\n\n\n\n\n\n\nTable 2:  Regression table for DASS-21 model \n\n \nDASS-21 Scores\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n44.98\n44.53 – 45.42\n<0.001\n\n\nSocial Comparison Scale(Z-scored)\n-1.94\n-2.39 – -1.49\n<0.001\n\n\nNeuroticism (Z-scored)\n1.42\n0.97 – 1.86\n<0.001\n\n\nOpenness (Z-scored)\n-0.31\n-0.75 – 0.12\n0.155\n\n\nConscientiousness(Z-scored)\n0.09\n-0.35 – 0.53\n0.685\n\n\nExtraversion (Z-scored)\n0.53\n-0.15 – 1.20\n0.124\n\n\nAgreeableness (Z-scored)\n0.34\n-0.33 – 1.01\n0.324\n\n\nSocial Comparison Scale(Z-scored): Neutoricism(Z-scored)\n-2.77\n-3.24 – -2.29\n<0.001\n\n\nObservations\n655\n\n\nR2 / R2 adjusted\n0.279 / 0.271\n\n\n\n\n\n\n\nResults showed a significant conditional association between SCS scores (\\(Z\\)-scored) and DASS-21 Scores (\\(\\beta\\) = -1.94, \\(SE\\) = 0.23, \\(p\\) <.001), which suggested that for those at the mean level of Neuroticism, scores on the DASS-21 decreased by 1.94 for every 1 standard deviation increase in SCS scores. A significant conditional association was also evident between Neuroticism (Z-scored) and DASS-21 Scores (\\(\\beta\\) = 1.42, \\(SE\\) = 0.23, \\(p\\) <.001), which suggested that for those with average scores on the SCS, scores on the DASS-21 increased by 1.42 for every 1 standard deviation increase in Neuroticism.\nCrucially, the association between social comparison and symptoms of depression and anxiety was found to be dependent upon the level of Neuroticism, with a greater negative association between the two for those with high levels of Neuroticism (\\(\\beta\\) = -2.77, \\(SE\\) = 0.24, \\(p\\) <.001). This interaction is visually presented in Figure 3.\n\n\n\n\n\n\n\nFigure 3: Interaction Plot\n\n\n\n\nThe results presented here indicated that the association between social comparison and depression and anxiety did depend upon individuals’ levels of Neuroticism, with perceived social rank perhaps leading to more symptoms of depression and anxiety for highly neurotic individuals. The Johnson-Neyman technique (see Figure 4) indicated that the association between DASS-21 scores and SCS was significant when Neuroticism scores were less than 0.93 standard deviations below the mean or more than -0.52 standard deviations above the mean.\n\n\n\n\nFigure 4: Simple Slopes\n\n\n\n\n\n\n\nAct III: Discussion\n\nQuestion 3\n\n\nAttempt to draft a discussion section based on your results and the analysis provided.\n\n\n\n\n\n\n\n\n\nDiscussion - What To Include\n\n\n\n\n\nIn the discussion section, you should summarise the key findings from the results section and provide the reader with take-home sentences drawing the analysis together and relating it back to the original question.\nThe discussion should be relatively brief, and should not include any statistical analysis - instead think of the discussion as a conclusion, providing an answer to the research question(s).\n\n\n\n\n Example Write-Up of Discussion Section\n\n\nPrevious research had identified an association between an individual’s perception of their social rank and symptoms of depression, anxiety and stress. We investigated if Neuroticism moderated effects of social comparison on symptoms of depression, anxiety, and stress after controlling for other personality traits.\nOur results led us to reject the null hypothesis that the interaction coefficient was equal to zero, as the results indicated that the association between social comparison and depression and anxiety did depend upon individuals’ levels of Neuroticism, with perceived social rank perhaps leading to more symptoms of depression and anxiety for highly neurotic individuals. However, it is important to note that we can make no claims on the directions of these associations from these data based on significance alone - it may be that social comparison leads to more depression and anxiety in neurotic individuals, but also consistent is the view that - for these individuals - higher levels of depression leads to a greater reduction in perceived social rank."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr Umberto Noe\nDr Josiah King\nDr Emma Waterston\nDepartment of Psychology, The University of Edinburgh"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 2 (DAPR2) lab workbook. Using the menu above, you can find lab materials for each week."
  },
  {
    "objectID": "index.html#help-support-feedback",
    "href": "index.html#help-support-feedback",
    "title": "Home",
    "section": "Help, Support & Feedback",
    "text": "Help, Support & Feedback\n\nWithin Lab Workbook\n\nHints, Notes, and Example Write-Up / Interpretation\nHints are shown in a green box, with the title ‘Hint’. If you are unsure what to do, check the collapsible hint provided (note that these are only present for some questions).\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is an example of a hint.\n\n\n\nNotes are displayed in blue boxes, with the title ‘Note’. These are occasionally used to draw your attention to a specific point.\n\n\n\n\n\n\nNote\n\n\n\nThis is an example of a note.\n\n\nExample write-ups and/or interpretations are shown with a red right border. These are helpful to check your interpretation against, and a useful guide to use in future when interpreting and/or writing up results.\n\n\n\n\n\n\nThis is an example write-up / interpretation block.\n\n\n\n\n\nSolutions\nSolutions are made available immediately below each exercise. To view solutions, click the drop down ‘Solution’ button.\nImportant  Before checking the solution you should attempt the question. You should also avoid copying and pasting code from the solutions. If you do check solutions right away and simply copy the answer, you will struggle to learn from the exercises.\nInstead, you should:\n\nTry to figure out the answer yourself or with your peers (and/or ask for help from a staff member if needed)\nType the code out yourself (and annotate your R code chunks so you know what your code is doing & why - future you will thank you for this).\n\n\n\n\nAsking Questions\n\nDuring labs, if you have a question, please ask one of the tutors for support.\nOutside of labs, we encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#tips-on-googling-statistics-and-r",
    "href": "index.html#tips-on-googling-statistics-and-r",
    "title": "Home",
    "section": "Tips on Googling Statistics and R",
    "text": "Tips on Googling Statistics and R\nSearching online for help with statistics and R can be both a help and a hindrance. If you have an error message in R, copy the error message into Google. The results returned can sometimes just cause more confusion, but sometimes something might jump out at you and help you solve the problem. The same applies with searching the internet for help with statistics - search for “what is a p-value”, and you’ll find many many different articles and forum discussions etc. Some of them you will find too technical, but don’t be scared - the vast majority of people work in statistics will find these too technical too. Some of them you might feel are too simple/not helpful. As a general guide, keep clicking around the search responses, and you may end up finding that someone, somewhere, has provided an explanation at the right level. If you find something during your search which you don’t quite understand, feel free to link it in a post on the discussion forum!"
  },
  {
    "objectID": "index.html#feedback-on-labs",
    "href": "index.html#feedback-on-labs",
    "title": "Home",
    "section": "Feedback on Labs",
    "text": "Feedback on Labs\nIf you wish to make suggestions for improvements to these workbooks (or if you spot any typos!), please email ppls.psych.stats@ed.ac.uk making sure to include the course name in the subject."
  }
]