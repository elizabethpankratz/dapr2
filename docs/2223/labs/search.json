[
  {
    "objectID": "1_01_function.html",
    "href": "1_01_function.html",
    "title": "Functions and Models",
    "section": "",
    "text": "At the end of this lab, you will:\n\nHave reviewed the main concepts from introductory statistics.\nUnderstand the concept of a function.\nBe able to discuss what a statistical model is.\nUnderstand the link between models and functions.\n\n\nHave attended and/or watched Week 1 lectures.\nHave installed R and RStudio on your own computer (unless you have a Chromebook where you may continue to use the PPLS RStudio Server).\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nggExtra\nkableExtra\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/handheight.csv."
  },
  {
    "objectID": "1_01_function.html#functions-and-mathematical-models-plots",
    "href": "1_01_function.html#functions-and-mathematical-models-plots",
    "title": "Functions and Models",
    "section": "Functions and Mathematical Models: Plots",
    "text": "Functions and Mathematical Models: Plots\n\nQuestion 4\n\n\nCreate a data set called squares containing the perimeter of four squares having sides of length \\(0, 2, 5, 9\\) metres, and then plot the squares data as points\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember that to combine multiple numbers together we use the function c().\n\n\n\n\n\n\n\n Solution \n\n\n\n#Create 'squares' dataset using tibble()\nsquares <- tibble(\n  side = c(0, 2, 5, 9),\n  perimeter = 4 * side\n)\n\nsquares\n\n# A tibble: 4 × 2\n   side perimeter\n  <dbl>     <dbl>\n1     0         0\n2     2         8\n3     5        20\n4     9        36\n\n\n\n#Create Plot\nggplot(data = squares, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\nFigure 1: ?(caption)\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nGenerate one hundred data points, and use them to visualise the relationship between side and perimeter of squares. To do so, you need to complete four steps:\n\nCreate a sequence of one hundred side lengths (x) going from 0 to 3 metres.\nCompute the corresponding perimeters (y).\nPlot the side and perimeter data as points on a graph.\nVisualise the functional relationship between side and perimeter of squares. To do so, use the function geom_line() to connect the computed points with lines.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember that to create a sequence of numbers, we can use the function seq().\n\n\n\n\n\n\n\n Solution \n\n\nLet’s start by creating the side and perimeter data:\n\n#step 1 & 2\nsquares_grid <- tibble(\n  side = seq(0, 3, length.out = 100),\n  perimeter = 4 * side\n)\n\nPlot the individual points:\n\n# step 3\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4*Side')\n\n\n\n\nVisualise the functional relationship by connecting the individual points with lines:\n\n#step 4\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_line(colour = 'blue') +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4*Side')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function \\(y = 4 \\ x\\) that you plotted above is an example of a function representing a mathematical model.\nWe typically validate a model using experimental data. However, we all know how squares work and that two squares with the same side will have the same perimeter (more on this later).\n\n\n\n\nQuestion 6\n\n\nThe Scottish National Gallery kindly provided us with measurements of side and perimeter (in metres) for a sample of 10 square paintings.\nThe data are provided below:\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nPlot the mathematical model of the relationship between side and perimeter for squares, and superimpose on top the experimental data from the Scottish National Gallery.\n\n\n\n\n Solution \n\n\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nggplot() +\n  geom_line(data = squares_grid, aes(x = side, y = perimeter), colour = 'blue') +\n  geom_point(data = sng, aes(x = side, y = perimeter), colour = 'black', \n             alpha = 0.5, size = 3) +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\nFigure 2: The exact relationship between side and perimeter of squares\n\n\n\n\nThe above plot shows perfect agreement between the observed data and the model.\n\n\n\n\n\nQuestion 7\n\n\nUse the mathematical model to predict the perimeter of a painting with a side of 1.5 metres.\n\n\n\n\n\n\nHint\n\n\n\n\n\nDon’t forget to always include the measurement units when reporting/writing-up results!\n\n\n\n\n\n\n\n Solution \n\n\nWe do not have a painting with a side of 1.5 metres within the random sample of paintings from the Scottish National Gallery. However, we can predict the perimeter of an unobserved squared painting having a 1.5 metre side using the mathematical model.\nYou can obtain this prediction using either a visual approach or an algebraic one.\n\n\nVisual Approach\nAlgebraic Approach\n\n\n\n\n\n\n\n\nSometimes we can directly read a predicted value from the graph of the functional relationship.\nConsider the plot created in the previous question. First, we need to check where x = 1.5. Then, we draw a vertical dashed line until it meets the blue line. The y value corresponding to x = 1.5 can be read off the y-axis.\nHowever, in this case it is not that easy to read it from the drawing…\n\n\nYou can substitute the x value in the formula and calculate the corresponding y value.\n\\[\ny = 4 * x = 4 * 1.5 = 6\n\\]\n\n\n\n\n\n\n\n\n\nThe predicted perimeter of squared paintings having a 1.5m side is 6m."
  },
  {
    "objectID": "1_01_function.html#study-overview",
    "href": "1_01_function.html#study-overview",
    "title": "Functions and Models",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Question\nHow does handspan vary as a function of height?\n\nConsider now the relationship between height (in inches) and handspan (in cm). Utts and Heckard (2015) provided data for a sample of 167 students which reported their height and handspan as part of a class survey.\nUsing the handheight data you already loaded at the start of the lab, your task is to investigate how handspan varies as a function of height for the students in the sample.\n\n Handheight codebook.\n\n\nDescription\nThe data set records the height and handspan reported by a random sample of 167 students as part of a class survey.\nThe variables are:\n\n\nheight, measured in inches\n\nhandspan, measured in centimetres\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\n height \n    handspan \n  \n\n\n 68 \n    21.5 \n  \n\n 71 \n    23.5 \n  \n\n 73 \n    22.5 \n  \n\n 64 \n    18.0 \n  \n\n 68 \n    23.5 \n  \n\n 59 \n    20.0 \n  \n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nUsing a scatterplot (since the variables are numeric and continuous) to visualise the relationship between the two numeric variables, comment on any main differences you notice with the relationship between side and perimeter of squares. Note if you detected outliers or points that do not fit with the pattern in the rest of the data.\n\n\n\n\n Solution \n\n\n\nplt <- ggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\nplt\n\n\n\nFigure 3: Simple Scatterplot\n\n\n\n\nWe can also add marginal boxplots for each variable using the package ggExtra.\n\nggMarginal(plt, type = 'boxplot')\n\n\n\nFigure 4: The statistical relationship between height and handspan\n\n\n\n\nOutliers are extreme observations that are not possible values of a variable or that do not seem to fit with the rest of the data. This could either be:\n\n\nmarginally along one axis: points that have an unusual (too high or too low) x-coordinate or y-coordinate;\n\njointly: observations that do not fit with the rest of the point cloud.\n\nThe boxplots in Figure 4 do not highlight any outliers in the marginal distributions of height and handspan. Furthermore, from the scatterplot we do not notice any extreme observations or points that do not fit with the rest of the point cloud.\nWe notice a moderate, positive linear relationship between height and handspan.\nRecall Figure 2, displaying the relationship between side and perimeters of squares. In the plot we notice two points on top of each other, reflecting the fact that two squares having the same side will always have the same perimeter. In fact, the data from the Scottish National Gallery include two squared paintings with a side of 1.1m, both having a measured perimeter of 4.4m.\nFigure 4, instead, displays the relationship between height and handspan of a sample of students. The first thing that grabs our attention is the fact that students having the same height do not necessarily have the same handspan. Rather, we clearly see a variety of handspan values for students all having a height of, for example, 70in. To be more precise, the seven students who are 70 in. tall all have differing handspans.\n\n\n\n\n\nQuestion 9\n\n\nUsing the following command, superimpose on top of the scatterplot a best-fit line describing how handspan varies as a function of height. For the moment, the argument se = FALSE tells R to not display uncertainty bands.\n\ngeom_smooth(method = lm, se = FALSE)\n\nComment on any differences between the lines representing the linear relationship between (a) the side and perimeter of square and (b) height and handspan.\n\n\n\n\n Solution \n\n\n\nggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_smooth(method = lm, se = FALSE) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\n\n\nFigure 5: The best-fit line\n\n\n\n\nThe line representing the relationship between side and perimeter of squares is able to predict the actual perimeter value from the measurement of the side of a square. This is possible because the relationship between side and perimeter is an exact one.\nThat is, any squares having the same side will have the same perimeter, and there will be no variation in those values.\nThe line that best fits the relationship between height and handspan (see Figure 5), instead, is only able to predict the average handspan for a given value of height.\nThis is because there will be a distribution of handspans at each value of height. The line will fit the trend/pattern in the values, but there will be individual-to-individual variability that we must accept around that average pattern.\n\n\n\n\n\nQuestion 10\n\n\n\n\n\nThe line of best-fit is given by:1\n\\[\n\\widehat{Handspan} = -3 + 0.35 \\ Height\n\\]\nCalculate the predicted handspan of a student who is (a) 73in tall, and (b) 5in tall.\n\n\n\n\n Solution \n\n\n\nThe predicted average handspan for students who are 73in tall is: \\(-3 + 0.35 * 73 = 22.55\\)cm. \n\nThe predicted average handspan for students who are 5in tall is: \\(-3 + 0.35 * 5 = -1.25\\)cm.\n\nBut wait, handspan can not be negative… This does not make any sense! That’s right, we went too far off the range of the available data on heights, which were between 57in and 78in. We extrapolated. This is very dangerous…\n\n\n\n\nSource: Randall Munroe, xkcd.com"
  },
  {
    "objectID": "1_02_slr.html",
    "href": "1_02_slr.html",
    "title": "Intro to Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to specify a simple linear model.\nUnderstand what fitted values and residuals are.\nBe able to interpret the coefficients of a fitted model.\nBe able to test hypotheses and construct confidence intervals for the regression coefficients.\n\n\nBe up to date with lectures from Weeks 1 & 2\nHave completed Week 1 lab exercises\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/riverview.csv."
  },
  {
    "objectID": "1_02_slr.html#data-exploration",
    "href": "1_02_slr.html#data-exploration",
    "title": "Intro to Linear Regression",
    "section": "Data Exploration",
    "text": "Data Exploration\nThe common first port of call for almost any statistical analysis is to explore the data, and we can do this visually and/or numerically.\n\n\n\n\n\n\n\n\nMarginal Distributions\nBivariate Associations\n\n\n\nDescription\nThe distribution of each variable (e.g., employee incomes and education levels) without reference to the values of the other variables\nDescribing the relationship between two numeric variables\n\n\n\nVisually \n\n\nPlot each variable individually.  You could use, for example, geom_density() for a density plot or geom_histogram() for a histogram to comment on and/or examine: \n\nThe shape of the distribution. Look at the shape, centre and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\n\n\nIdentify any unusual observations. Do you notice any extreme observations (i.e., outliers)?\n\n\n\nPlot associations among two variables.  You could use, for example, a scatterplot to comment on and/or examine: \nThe direction of the association indicates whether there is a positive or negative association\n\n\nThe form of association refers to whether the relationship between the variables can be summarized well with a straight line or some more complicated pattern\n\n\nThe strength of association entails how closely the points fall to a recognizable pattern such as a line\n\n\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail\n\n\n\n\n\nNumerically \n\nCompute and report summary statistics e.g., mean, standard deviation, median, min, max, etc.  You could, for example, calculate summary statistics such as the mean (mean()) and standard deviation (sd()), etc. within summarize()\n\nCompute and report the correlation coefficient.  You can use the cor() function to calculate this\n\n\n\nMarginal Distributions\n\nQuestion 1\n\n\nVisualise and describe the marginal distribution of employee incomes.\n\n\n\n\n Solution \n\n\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = income)) +\n  geom_density() +\n  geom_boxplot(width = 1/300) +\n  labs(x = \"Income (in thousands of U.S. dollars)\", \n       y = \"Probability density\")\n\n\n\nFigure 1: Density plot and boxplot of employee incomes\n\n\n\n\nThe plot suggested that the distribution of employee incomes was unimodal, and most of the incomes were between roughly $45,000 and $70,000. The lowest income in the sample was approximately $25,000 and the highest over $80,000. This suggested there was a fair high degree of variation in the data. Furthermore, the boxplot did not highlight any outliers in the data.\nSummary statistics for the employees’ incomes:\n\ndesc_income <- riverview %>% \n  summarize(\n    M = mean(income), \n    SD = sd(income)\n    )\ndesc_income\n\n# A tibble: 1 × 2\n      M    SD\n  <dbl> <dbl>\n1  53.7  14.6\n\n\nFollowing the exploration above, we can describe the income variable as follows:\n\n\n\n\n\n\nThe marginal distribution of income was unimodal with a mean of approximately $53,700. There was variation in employees’ salaries (SD = $14,553).\n\n\n\n\n\n\n\nQuestion 2\n\n\nVisualise and describe the marginal distribution of education level.\n\n\n\n\n Solution \n\n\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = education)) +\n  geom_density() +\n  geom_boxplot(width = 1/100) +\n  labs(x = \"Education (in years)\", \n       y = \"Probability density\")\n\n\n\nFigure 2: Density plot and boxplot of employee education levels\n\n\n\n\nSummary statistics for the employees’ level of education:\n\ndesc_education <- riverview %>%\n  summarize(\n    M = mean(education),\n    SD = sd(education)\n    )\ndesc_education\n\n# A tibble: 1 × 2\n      M    SD\n  <dbl> <dbl>\n1    16  4.36\n\n\n\n\n\n\n\n\n\nThe marginal distribution of education was unimodal with an average of of 16 years. There was variation in employees’ level of education (SD = 4.4 years).\n\n\n\n\n\n\nAssociations among Variables\n\nQuestion 3\n\n\nCreate a scatterplot of income and education level before calculating the correlation between income and education level.\nMaking reference to both the plot and correlation coefficient, describe the association between income and level of education among the employees in the sample.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe are trying to investigate how income varies when varying years of formal education. Hence, income is the dependent variable (on the y-axis), and education is the independent variable (on the x-axis).\n\n\n\n\n\n\n\n Solution \n\n\nLet’s produce a scatterplot:\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Education (in years)\", \n       y = \"Income (in thousands of U.S. dollars)\")\n\n\n\nFigure 3: The association between employees’ education level and income\n\n\n\n\nTo comment on the strength of the linear association we compute the correlation coefficient:\n\ncorr <- riverview %>%\n  select(education, income) %>%\n  cor()\ncorr\n\n          education    income\neducation 1.0000000 0.7947847\nincome    0.7947847 1.0000000\n\n\nthat is,\n\\[\nr_{\\text{education, income}} = 0.79\n\\]\n\n\n\n\n\n\n\nThere was a strong positive linear association between education level and income for the employees in the sample. High incomes tended to be observed, on average, with more years of formal education (\\(r\\) = .79).\nThe scatterplot did not highlight any outliers."
  },
  {
    "objectID": "1_02_slr.html#model-specification-and-fitting",
    "href": "1_02_slr.html#model-specification-and-fitting",
    "title": "Intro to Linear Regression",
    "section": "Model Specification and Fitting",
    "text": "Model Specification and Fitting\nThe scatterplot highlighted a linear relationship, where the data points were scattered around an underlying linear pattern with a roughly-constant spread as x varied.\nHence, we will try to fit a simple (i.e., one x variable only) linear regression model:\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\\\\n\\quad \\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nwhere “\\(\\epsilon \\sim N(0, \\sigma) \\text{ independently}\\)” means that the errors around the line have mean zero and constant spread as x varies.\n\nQuestion 4\n\n\nUsing the lm() function, fit a simple linear model to predict income (DV) by Education (IV), naming the output mdl.\nWrite down the equation of the fitted line.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe syntax of the lm() function is:\n\n[model name] <- lm([response variable i.e., dependent variable] ~ [explanatory variable i.e., independent variable], data = [dataframe])\n\n\n\n\n\n\n\n\n Solution \n\n\nThe fitted model can be written as\n\\[\n\\widehat{Income} = \\hat \\beta_0 + \\hat \\beta_1 \\ Education\n\\]\nor\n\\[\n\\widehat{Income} = \\hat \\beta_0 \\cdot 1 + \\hat \\beta_1 \\cdot Education\n\\]\nWhen we specify the linear model in R, we include after the tilde sign, ~, the variables that appear to the right of the \\(\\hat \\beta\\)s. That’s why the 1 is included.\nAs the variables are in the riverview dataframe, we would write:\n\nmdl <- lm(income ~ 1 + education, data = riverview)\nmdl\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\nNote that by calling the name of the fitted model, mdl, you can see the estimated regression coefficients \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). The fitted line is\n\n\n\n\n\n\n\\[\n\\widehat{Income} = 11.32 + 2.65 \\ Education \\\\\n\\]\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nExplore the following equivalent ways to obtain the estimated regression coefficients — that is, \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) — from the fitted model:\n\nmdl\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\n\n\n Solution \n\n\nThe estimated parameters returned by the below methods are all equivalent. However, summary() returns more information.\n\n\nmdl()\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\nSimply invoke the name of the fitted model:\n\nmdl\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\n\n\n\nmdl$coefficients\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\n\ncoef(mdl)\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\n\ncoefficients(mdl)\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\n\n\n\n\n\n\n\n\n\nThe estimated intercept is \\(\\hat \\beta_0 = 11.32\\) and the estimated slope is \\(\\hat \\beta_1 = 2.65\\).\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nExplore the following equivalent ways to obtain the estimated standard deviation of the errors — that is, \\(\\hat \\sigma\\) — from the fitted model mdl:\n\nsigma(mdl)\nsummary(mdl)\n\n\n Huh? What is \\(\\sigma\\)?\n\n\nThe standard deviation of the errors, denoted by \\(\\sigma\\) is an important quantity to estimate because it measures how much individual data points tend to deviate above and below the regression line.\nA small \\(\\sigma\\) indicates that the points hug the line closely and we should expect fairly accurate predictions, while a large \\(\\sigma\\) suggests that, even if we estimate the line perfectly, we can expect individual values to deviate from it by substantial amounts.\nThe estimated standard deviation of the errors is (surprisingly) denoted \\(\\hat \\sigma\\) and is equal to\n\\[\n\\hat \\sigma = \\sqrt{\\frac{SS_{Residual}}{n - 2}}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\nThe estimated standard deviation of the errors can be equivalently obtained by the below methods. However, summary() returns more information.\n\n\nsigma(mdl)\nsummary(mdl)\n\n\n\n\nsigma(mdl)\n\n[1] 8.978116\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term “Residual standard error” is a misnomer, as the help page for sigma says (check ?sigma). However, it’s hard to get rid of this bad name as it has been used in too many books showing R output.\n\n\n\n\n\n\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma = 8.98\\).\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nInterpret the estimated intercept and slope in the context of the research question.\n\n\n\n\n Solution \n\n\nWe can interpret the estimated intercept as follows:\n\n\n\n\n\n\nThe estimated average income associated to zero years of formal education is $11,321.\n\n\n\nFor the estimated slope we might write:\n\n\n\n\n\n\nThe estimated increase in average income associated to a one year increase in education is $2,651.\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nInterpret the estimated standard deviation of the errors in the context of the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo interpret the estimated standard deviation of the errors we can use the fact that about 95% of values from a normal distribution fall within two standard deviations of the centre.\n\n\n\n\n\n\n\n Solution \n\n\nWe can interpret the estimated standard deviation of the errors as follows:\n\n\n\n\n\n\nFor any particular level of education, employee incomes should be distributed above and below the regression line with standard deviation estimated to be \\(\\hat \\sigma = 8.98\\). Since \\(2 \\hat \\sigma = 2 (8.98) = 17.96\\), we expect most (about 95%) of the employee incomes to be within about $18,000 from the regression line.\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nPlot the data and the fitted regression line. To do so:\n\nExtract the estimated regression coefficients e.g., via betas <- coef(mdl)\n\nExtract the first entry of betas (i.e., the intercept) via betas[1]\n\nExtract the second entry of betas (i.e., the slope) via betas[2]\n\nProvide the intercept and slope to the function\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn your ggplot(), you will need to specify geom_abline(). This might help get you started:\n\ngeom_abline(intercept = <intercept>, slope = <slope>)\n\n\n\n\n\n\n\n\n Solution \n\n\nThe function coef(mdl) returns a vector: that is, a sequence of numbers all of the same type. To get the first element of the sequence you append [1], and [2] for the second.\nWe can plot the model as follows:\n\nbetas <- coef(mdl)\nintercept <- betas[1]\nslope <- betas[2]\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = intercept, slope = slope, \n              color = 'blue', size = 1) + \n  labs(x = \"Education (in years)\", \n       y = \"Income (in thousands of U.S. dollars)\")"
  },
  {
    "objectID": "1_02_slr.html#fitted-and-predicted-values",
    "href": "1_02_slr.html#fitted-and-predicted-values",
    "title": "Intro to Linear Regression",
    "section": "Fitted and Predicted Values",
    "text": "Fitted and Predicted Values\nTo compute the model-predicted values for the data in the sample, we can use various funcitons:\n\npredict(<fitted model>)\nfitted(<fitted model>)\nfitted.values(<fitted model>)\nmdl$fitted.values\n\nFor example, this will give us the estimated income (point on our regression line) for each observed value of education level.\n\npredict(mdl)\n\n       1        2        3        4        5        6        7        8 \n32.53175 32.53175 37.83435 37.83435 37.83435 43.13694 43.13694 43.13694 \n       9       10       11       12       13       14       15       16 \n43.13694 48.43953 48.43953 48.43953 51.09083 53.74212 53.74212 53.74212 \n      17       18       19       20       21       22       23       24 \n53.74212 53.74212 56.39342 59.04472 59.04472 61.69601 61.69601 64.34731 \n      25       26       27       28       29       30       31       32 \n64.34731 64.34731 64.34731 66.99861 66.99861 69.64990 69.64990 74.95250 \n\n\nWe can also compute model-predicted values for other (unobserved) data:\n\npredict(<fitted model>, newdata = <dataframe>)\n\nWe first need to remember that the model predicts income using the independent variable education. Hence, if we want predictions for new data, we first need to create a tibble with a column called education containing the years of education for which we want the prediction.\n\nnewdata <- tibble(education = c(11, 23))\nnewdata\n\n# A tibble: 2 × 1\n  education\n      <dbl>\n1        11\n2        23\n\n\nThen we take newdata and add a new column called income_hat, computed as the prediction from the fitted mdl using the newdata above:\n\nnewdata <- newdata %>%\n  mutate(\n    income_hat = predict(mdl, newdata = newdata)\n  )\nnewdata\n\n# A tibble: 2 × 2\n  education income_hat\n      <dbl>      <dbl>\n1        11       40.5\n2        23       72.3"
  },
  {
    "objectID": "1_02_slr.html#residuals",
    "href": "1_02_slr.html#residuals",
    "title": "Intro to Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nThe residuals represent the deviations between the actual responses and the predicted responses and can be obtained either as\n\n\nmdl$residuals;\n\nresid(mdl);\n\nresiduals(mdl);\ncomputing them as the difference between the response and the predicted response."
  },
  {
    "objectID": "1_03_mlr.html",
    "href": "1_03_mlr.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Setup\n\nSetup\n\n\n\nCreate a new RMarkdown file\nLoad the required package(s)\nRead the wellbeing dataset into R, assigning it to an object named mwdata\n\n\n\n\n\n\n Solution \n\n\n\n#Loading the required package(s)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(sjPlot)\n\n# Reading in data and storing to an object named 'mwdata'\nmwdata <- read_csv(\"https://uoepsy.github.io/data/wellbeing.csv\")\n\n\n\n\nStudy Overview\n\nResearch Question\nIs there an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions?\n\n\n Wellbeing data codebook.\n\n\nDescription\nResearchers interviewed 32 participants, selected at random from the population of residents of Edinburgh & Lothians. They used the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\nThe researchers also asked participants to estimate the average number of hours they spend outdoors each week, the average number of social interactions they have each week (whether on-line or in-person), and whether they believe that they stick to a routine throughout the week (Yes/No).\nThe data in wellbeing.csv contain five attributes collected from a random sample of \\(n=32\\) hypothetical residents over Edinburgh & Lothians, and include:\n\n\nwellbeing: Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n\n\noutdoor_time: Self report estimated number of hours per week spent outdoors\n\n\nsocial_int: Self report estimated number of social interactions per week (both online and in-person)\n\nroutine: Binary Yes/No response to the question “Do you follow a daily routine throughout the week?”\n\nlocation: Location of primary residence (City, Suburb, Rural)\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\n\n\nwellbeing\n      outdoor_time\n      social_int\n      location\n      routine\n    \n\n\n30\n7\n8\nSuburb\nRoutine\n\n\n21\n9\n8\nCity\nNo Routine\n\n\n38\n14\n10\nSuburb\nRoutine\n\n\n27\n16\n10\nCity\nNo Routine\n\n\n20\n1\n10\nRural\nNo Routine\n\n\n37\n11\n12\nSuburb\nNo Routine\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nProduce plots of the marginal distributions (the distributions of each variable in the analysis without reference to the other variables) of the wellbeing, outdoor_time, and social_int variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nYou could use, for example, geom_density() for a density plot or geom_histogram() for a histogram.\nLook at the shape, center and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\n\n\n\n\n\n\n\n\n Solution \n\n\nWe should be familiar now with how to visualise a marginal distribution. You might choose histograms, density curves, or boxplots, or a combination:\n\nwellbeing_plot <- \n  ggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  geom_boxplot(width = 1/250) +\n  labs(x = \"Score on WEMWBS (range 14-70)\", y = \"Probability\\ndensity\")\n\noutdoortime_plot <- \n  ggplot(data = mwdata, aes(x = outdoor_time)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Probability\\ndensity\")\n\nsocial_plot <- \n  ggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  geom_boxplot(width = 1/150) +\n  labs(x = \"Number of social interactions per week\", y = \"Probability\\ndensity\")\n\n# the \"patchwork\" library allows us to arrange multiple plots\nwellbeing_plot / outdoortime_plot / social_plot\n\n\n\nFigure 1: Marginal distribution plots of wellbeing sores, weekly hours spent outdoors, and social interactions\n\n\n\n\nSummary statistics for wellbeing, outdoor time, and social interactions:\n\ndescriptives <- mwdata %>% \n  summarize(\n    M_Wellbeing = mean(wellbeing), \n    SD_Wellbeing = sd(wellbeing),\n    M_OutTime = mean(outdoor_time), \n    SD_OutTime = sd(outdoor_time),\n    M_SocInt = mean(social_int), \n    SD_SocInt = sd(social_int)\n    )\ndescriptives\n\n# A tibble: 1 × 6\n  M_Wellbeing SD_Wellbeing M_OutTime SD_OutTime M_SocInt SD_SocInt\n        <dbl>        <dbl>     <dbl>      <dbl>    <dbl>     <dbl>\n1          43         11.7      14.8       6.95       16      4.36\n\n\n\n\nThe marginal distribution of scores on the WEMWBS is unimodal with a mean of approximately 43. There is variation in WEMWBS scores (SD = 11.7).\n\nThe marginal distribution of weekly hours spent outdoors is unimodal with a mean of approximately 14.8. There is variation in weekly hours spent outdoors (SD = 6.9).\n\nThe marginal distribution of numbers of social interactions per week is unimodal with a mean of approximately 16. There is variation in numbers of social interactions (SD = 4.4).\n\n\n\n\n\n\n\nQuestion 2\n\n\nProduce plots of the associations between the outcome variable (wellbeing) and each of the explanatory variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThink about:\n\n\nDirection of association\n\nForm of association (can it be summarised well with a straight line?)\n\n\nStrength of association (how closely do points fall to a recognizable pattern such as a line?)\n\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail.\n\nPlot tips:\n\nuse \\n to wrap text in your titles and or axis labels\nconsider using geom_smooth() to superimpose the best-fitting line describing the association of interest\n\n\n\n\n\n\n\n\n Solution \n\n\n\nwellbeing_outdoor <- \n  ggplot(data = mwdata, aes(x = outdoor_time, y = wellbeing)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Time spent outdoors \\nper week (hours)\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_social <- \n  ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Number of social interactions \\nper week\", y = \"Wellbeing score (WEMWBS)\")\n\n# place plots adjacent to one another\nwellbeing_outdoor | wellbeing_social\n\n\n\nFigure 2: Scatterplots displaying the relationships between scores on the WEMWBS and a) weekly outdoor time (hours), and b) weekly number of social interactions\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nProduce a correlation matrix of the variables which are to be used in the analysis, and write a short paragraph describing the associations.\n\nCorrelation matrix\nA table showing the correlation coefficients - \\(r_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\\) - between variables. Each cell in the table shows the relationship between two variables. The diagonals show the correlation of a variable with itself (and are therefore always equal to 1).\n\nIn R, we can create a correlation matrix by giving the cor() function a dataframe. However, we only want to give it 3 columns here. Think about how we select specific columns, either using select(), or giving the column numbers inside [].\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nMake sure to round your numbers in-line with APA 7th edition guidelines. The round() function will come in handy here, as might this APA numbers and statistics guide!\n\n\n\n\n\n\n\n Solution \n\n\nWe can either use:\n\n# correlation matrix of the first 3 columns\nround(cor(mwdata[,1:3]), digits = 2)\n\nor:\n\n# select only the columns we want by name, and pass this to cor()\nmwdata %>% \n  select(wellbeing, outdoor_time, social_int) %>%\n  cor() %>%\n    round(digits = 2)\n\n             wellbeing outdoor_time social_int\nwellbeing         1.00         0.58       0.79\noutdoor_time      0.58         1.00       0.34\nsocial_int        0.79         0.34       1.00\n\n\n\n\n\n\n\n\n\nThere was a moderate, positive, linear association between weekly outdoor time and WEMWBS scores for the participants in the sample (\\(r\\) = .58). Higher number of hours spent outdoors each week was associated, on average, with higher wellbeing scores,\n\nThere was a moderate, positive, linear association between the weekly number of social interactions and WEMWBS scores for the participants in the sample (\\(r\\) = .79). More social interactions were associated, on average, with higher wellbeing scores.\nThere was a weak positive correlation between weekly outdoor time and the weekly number of social interactions (\\(r\\) = .34).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there is a weak correlation between our two explanatory variables (outdoor_time and social_int). We will return to how this might affect our model when later on we look at the assumptions of multiple regression.\n\n\n\n\n\n\n\nQuestion 4\n\n\nThe scatterplots we created above show moderate, positive, and linear relationships both between outdoor time and wellbeing, and between numbers of social interactions and wellbeing.\n\nSpecify the form of your model, where \\(y\\) = scores on the WEMWBS, \\(x_1\\) = weekly number of social interactions, and \\(x_2\\) = weekly outdoor time.\nWhat are the parameters of the model. How do we denote parameter estimates?\nFit the linear model in using lm(), assigning the output to an object called mdl1.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAs we did for simple linear regression, we can fit our multiple regression model using the lm() function. We can add as many explanatory variables as we like, separating them with a +.\n\n( <response variable> ~ 1 + <explanatory variable 1> + <explanatory variable 2> + ... , data = <dataframe> )\n\n\n\n\n\n\n\n\n Solution \n\n\nA model for the relationship between \\(x_1\\) = weekly outdoor time, \\(x_2\\) = weekly numbers of social interactions and \\(y\\) = scores on the WEMWBS is given by:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\\\ \\quad \\\\\n\\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nIn the model specified above,\n\n\n\\(\\mu_{y|x_1, x_2} = \\beta_0 + \\beta_1 x + \\beta_2 x_2\\) represents the systematic part of the model giving the mean of \\(y\\) at each combination of values of \\(x_1\\) and \\(x_2\\);\n\n\\(\\epsilon\\) represents the error (deviation) from that mean, and the errors are independent from one another.\n\nThe parameters of our model are:\n\n\n\\(\\beta_0\\) (The intercept);\n\n\\(\\beta_1\\) (The slope across values of \\(x_1\\));\n\n\\(\\beta_2\\) (The slope across values of \\(x_2\\));\n\n\n\\(\\sigma\\) (The standard deviation of the errors).\n\nWhen we estimate these parameters from the available data, we have a fitted model (recall that the h\\(\\hat{\\textrm{a}}\\)ts are used to distinguish our estimates from the true unknown parameters):\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Social Interactions + \\hat \\beta_2 \\cdot Outdoor Time\n\\]\nAnd we have residuals \\(\\hat \\epsilon = y - \\hat y\\) which are the deviations from the observed values and our model-predicted responses.\nFitting the model in R:\n\nmdl1 <- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\n\n\n\n\n\n\nVisual\nNote that for simple linear regression we talked about our model as a line in 2 dimensions: the systematic part \\(\\beta_0 + \\beta_1 x\\) defined a line for \\(\\mu_y\\) across the possible values of \\(x\\), with \\(\\epsilon\\) as the random deviations from that line. But in multiple regression we have more than two variables making up our model.\nIn this particular case of three variables (one outcome + two explanatory), we can think of our model as a regression surface (see Figure 3). The systematic part of our model defines the surface across a range of possible values of both \\(x_1\\) and \\(x_2\\). Deviations from the surface are determined by the random error component, \\(\\hat \\epsilon\\).\n\n\n\n\nFigure 3: Regression surface for wellbeing ~ social_int + outdoor_time, from two different angles\n\n\n\n\nDon’t worry about trying to figure out how to visualise it if we had any more explanatory variables! We can only concieve of 3 spatial dimensions. One could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it’s not worth trying!.\n\n\n\nQuestion 5\n\n\nState the research question in the form of a testable hypothesis.\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou must define both a null (\\(H_0\\)) and alternative hypothesis (\\(H_1\\)).\n\n\n\n\n\n\n\n Solution \n\n\nIn words:\n\\(H_0\\): There is no association between well-being and time spent outdoors after taking into account the relationship between well-being and social interactions\n\\(H_1\\): There is an association between well-being and time spent outdoors after taking into account the relationship between well-being and social interactions\nIn symbols:\n\\(H_0: \\beta_2 = 0\\)\n\\(H_1: \\beta_2 \\neq 0\\)\n\n\n\n\n\nQuestion 6\n\n\nUsing any of:\n\nmdl1\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\nWrite out the estimated parameter values of:\n\n\n\\(\\hat \\beta_0\\), the estimated average wellbeing score associated with zero hours of outdoor time and zero social interactions per week.\n\n\n\\(\\hat \\beta_1\\), the estimated increase in average wellbeing score associated with an additional social interaction per week (an increase of one), holding weekly outdoor time constant.\n\n\n\\(\\hat \\beta_2\\), the estimated increase in average wellbeing score associated with one hour increase in weekly outdoor time, holding the number of social interactions constant\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ: What do we mean by hold constant / controlling for / partialling out / residualizing for?\nA: When the remaining explanatory variables are held at the same value or are fixed.\n\n\n\n\n\n\n Solution \n\n\n\n\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\n\n\n\nmdl1$coefficients\n\n (Intercept)   social_int outdoor_time \n   5.3703775    1.8034489    0.5923673 \n\n\n\n\n\ncoef(mdl1)\n\n (Intercept)   social_int outdoor_time \n   5.3703775    1.8034489    0.5923673 \n\n\n\n\n\ncoefficients(mdl1)\n\n (Intercept)   social_int outdoor_time \n   5.3703775    1.8034489    0.5923673 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\n\n\n\n\n\n\\(\\hat \\beta_0\\) = 5.37\n\n\n\\(\\hat \\beta_1\\) = 1.8\n\n\n\\(\\hat \\beta_2\\) = 0.59\n\n\n\n\n\n\nQuestion 7\n\n\nWithin what distance from the model predicted values (the regression surface) would we expect 95% of wEMWBS wellbeing scores to be?\n\n\n\n\n\n\nHint\n\n\n\n\n\nEither sigma() or part of the output from summary() will help you here.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nsigma(mdl1)\nsummary(mdl1)\n\n\n\n\nsigma(mdl1)\n\n[1] 6.148276\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma\\) = 6.15. We would expect 95% of wellbeing scores to be within about 12.3 (\\(2 \\hat \\sigma\\)) from the model fit.\n\n\n\n\n\nQuestion 8\n\n\nBased on the model, predict the wellbeing scores for the following individuals:\n\nLeah: Social Interactions = 24; Outdoor Time = 3\nSean: Social Interactions = 19; Outdoor Time = 26\nMike: Social Interactions = 15; Outdoor Time = 20\nDonna: Social Interactions = 7; Outdoor Time = 2\n\nWho has the highest predicted wellbeing score, and who has the lowest?\n\n\n\n\n Solution \n\n\nFirst we need to pass the data into R:\n\nwellbeing_query <- tibble(social_int = c(24, 19, 15, 7),\n                          outdoor_time = c(3, 26, 20, 2))\n\nAnd next use predict() to get their estimated wellbeing scores:\n\npredict(mdl1, newdata = wellbeing_query)\n\n       1        2        3        4 \n50.43025 55.03746 44.26946 19.17925 \n\n\nSean has the highest predicted wellbeing score (55.04), and Donna the lowest (19.18).\n\n\n\n\n\nQuestion 9\n\n\nShould we reject or fail to reject \\(H_0\\)? Why?\n\n\n\n\n Solution \n\n\nThe research question asked whether there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. This was equivalent to testing the following null hypothesis:\n\\(H_0: \\beta_2 = 0\\)\nBased on the model output (if we considered effects to be significant at \\(\\alpha\\) = .05), we should reject the null hypothesis since our \\(p\\)-value smaller than this (\\(p\\) = .0015). In short, we reject the null since \\(p\\) < .05.\n\n\n\n\n\nQuestion 10\n\n\nInterpret the outdoor time coefficient in the context of the research question.\n\n\n\n\n Solution \n\n\nA multiple regression model was used to determine if there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. Outdoor time was significantly associated with wellbeing scores (\\(\\beta\\) = 0.59, SE = 0.17, \\(p\\) < .001) after controlling for the number of weekly social interactions. Results suggested that for every additional hour spent outdoors each week, wellbeing scores increased by 0.59 points."
  },
  {
    "objectID": "1_04_model_fit.html",
    "href": "1_04_model_fit.html",
    "title": "Model Fit and Standardization",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand how to interpret significance tests for \\(\\beta\\) coefficients\nUnderstand how to calculate the interpret \\(R^2\\) and adjusted-\\(R^2\\) as a measure of model quality.\nUnderstand the calculation and interpretation of the \\(F\\)-test of model utility.\nUnderstand how to standardize model coefficients and when this is appropriate to do.\n\n\nBe up to date with lectures\nHave completed previous lab exercises from Week 1, Week 2, and Week 3\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\nsjPlot\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing.csv.\nNote: this is the same data as Lab 3."
  },
  {
    "objectID": "1_04_model_fit.html#lab-purpose",
    "href": "1_04_model_fit.html#lab-purpose",
    "title": "Model Fit and Standardization",
    "section": "Lab Purpose",
    "text": "Lab Purpose\nIn this lab (Lab 4), you will focus on the statistics contained within the highlighted sections of the summary() output below. You will be both calculating these by hand and deriving via R code before interpreting these values in the context of the research question following APA guidelines.\n\n\n\n\n\n\n\nQuestion 3\n\n\nTest the hypothesis that the population slope for outdoor time is zero — that is, that there is no linear association between wellbeing and outdoor time (after controlling for the number of social interactions) in the population.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRecall the formula for obtaining a test statistic:\nA test statistic for the null hypothesis \\(H_0: \\beta_j = 0\\) is\n\\[\nt = \\frac{\\hat \\beta_j - 0}{SE(\\hat \\beta_j)}\n\\]\nwhich follows a \\(t\\)-distribution with \\(n-k-1 = n - 2 - 1 = n - 3\\) degrees of freedom.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR function\n\n\n\nWe calculate the test statistic for \\(\\beta_2\\)\n\\[\nt = \\frac{\\hat \\beta_2 - 0}{SE(\\hat \\beta_2)} = \\frac{0.5924 - 0}{0.1689} = 3.5074\n\\]\nand compare it with the 5% critical value from a \\(t\\)-distribution with \\(n-3\\) degrees of freedom, which is:\n\nn <- nrow(mwdata)\ntstar <- qt(0.975, df = n - 3)\ntstar\n\n[1] 2.04523\n\n#tstar = 2.04523\n\nAs \\(|t|\\) (\\(|t|\\) = 3.51) is much larger than \\(t^*\\) (\\(t^*\\) = 2.05), we can reject then null hypothesis as we have strong evidence against it.\nThe \\(p\\)-value, shown below, also confirms this conclusion.\n\n2 * (1 - pt(3.506, n - 3))\n\n[1] 0.001500588\n\n\n\n\nPlease note that the same information was already contained in the row corresponding to the variable “outdoor_time” in the output of summary(mdl), which reported the \\(t\\)-statistic under t value and the \\(p\\)-value under Pr(>|t|):\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\nThe result is exactly the same (up to rounding errors) as calculating manually.\nBefore we interpret the results, note that sometimes \\(p\\)-values will be reported to \\(e^X\\). For example, look in the Pr(>|t|) column for “social_int”. The value \\(2.37e^{-07}\\) simply means \\(2.37 \\times 10^{-7}\\). This is a very small value (i.e., 0.000000237), hence we will report it as <.001 following the APA guidelines.\n\n\n\n\n\n\n\n\n\nWe performed a \\(t\\)-test against the null hypothesis that outdoor time was not associated with wellbeing scores after controlling for social interactions. A significant association was found between outdoor time (in hours per week) and wellbeing (WEMWBS scores) \\(t(29) = 3.51,\\ p = 002\\), two-sided. Thus, we have evidence to reject the null hypothesis.\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nObtain 95% confidence intervals for the regression coefficients, and write a sentence about each one.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRecall the formula for obtaining a confidence interval:\nA confidence interval for the population slope is\n\\[\n\\hat \\beta_j \\pm t^* \\cdot SE(\\hat \\beta_j)\n\\]\nwhere \\(t^*\\) denotes the critical value chosen from t-distribution with \\(n-k-1 = n - 2 - 1 = n - 3\\) degrees of freedom for a desired \\(\\alpha\\) level of confidence.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR function\n\n\n\nFor 95% confidence we have \\(t^* = 2.05\\):\n\nn <- nrow(mwdata)\ntstar <- qt(0.975, df = n - 3)\ntstar\n\n[1] 2.04523\n\n\nThe confidence intervals are:\n\ntibble(\n  b0_LowerCI = round(5.3704 - (qt(0.975, n-3) * 4.3205), 3),\n  b0_UpperCI = round(5.3704 + (qt(0.975, n-3)* 4.3205), 3),\n  b1_LowerCI = round(1.8034 - (qt(0.975, n-3) * 0.2691), 3),\n  b1_UpperCI = round(1.8034 + (qt(0.975, n-3)* 0.2691), 3),\n  b2_LowerCI = round(0.5924 - (qt(0.975, n-3) * 0.1689), 3),\n  b2_UpperCI = round(0.5924 + (qt(0.975, n-3)* 0.1689), 3)\n      )\n\n# A tibble: 1 × 6\n  b0_LowerCI b0_UpperCI b1_LowerCI b1_UpperCI b2_LowerCI b2_UpperCI\n       <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>\n1      -3.47       14.2       1.25       2.35      0.247      0.938\n\n\n\n\nWe can easily obtain the confidence intervals for the regression coefficients using the command confint():\n\nconfint(mdl1, level = 0.95)\n\n                  2.5 %     97.5 %\n(Intercept)  -3.4660660 14.2068209\nsocial_int    1.2530813  2.3538164\noutdoor_time  0.2468371  0.9378975\n\n\nThe result is exactly the same (up to rounding errors) as calculating manually.\n\n\n\n\n\n\n\n\n\n\nThe average wellbeing score for all those with zero hours of outdoor time and zero social interactions per week was between -3.47 and 14.21.\n\nWhen holding weekly outdoor time constant, each increase of one social interaction per week was associated with a difference in wellbeing scores between 1.25 and 2.35, on average.\n\nWhen holding the number of social interactions per week constant, each one hour increase in weekly outdoor time was associated with a difference in wellbeing scores between 0.25 and 0.94, on average.\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nWhat is the proportion of the total variability in wellbeing scores explained by the model?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe question asks to compute the value of \\(R^2\\). Since the model includes 2 predictors, you should report the Adjusted \\(R^2\\).\n\n\n\n\n\n\n\n Solution \n\n\nThe proportion of the total variability explained is given by R-squared.\nThe R-squared coefficient is defined as:\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n\\]\nThe Adjusted R-squared coefficient is defined as:\n\\[\n\\hat R^2 = 1 - \\frac{(1 - R^2)(n-1)}{n-k-1}\n\\quad \\\\\n\\begin{align}\n& \\text{Where:} \\\\\n& n = \\text{sample size} \\\\\n& k = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\]\n\n\nManually\nR function\n\n\n\nIn R we can write:\n\n#R squared & adjusted R squared\n\nwellbeing_fitted <- mwdata %>%\n  mutate(\n    wellbeing_hat = predict(mdl1),\n    resid = wellbeing - wellbeing_hat\n  )\n\n\nwellbeing_fitted %>%\n  summarise(\n    SSModel = sum( (wellbeing_hat - mean(wellbeing))^2 ),\n    SSTotal = sum( (wellbeing - mean(wellbeing))^2 )\n  ) %>%\n  summarise(\n    RSquared = SSModel / SSTotal,\n    AdjRSquared = 1-((1-(RSquared))*(32-1)/(32-2-1))\n  )\n\n# A tibble: 1 × 2\n  RSquared AdjRSquared\n     <dbl>       <dbl>\n1    0.740       0.722\n\n\n\n\n\n#look in second bottom row - Multiple R Squared and Adjusted R Squared both reported here\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\nThe output of summary() displays the Adjusted \\(R\\)-squared value in the following line:\nAdjusted R-squared:  0.7224 \n\n\n\n\nInterpretation\n\n\n\n\n\n\nApproximately 72% of the total variability in wellbeing scores is explained by associations with social interactions and outdoor time.\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nPerform a model utility test at the 5% significance level and report your results.\nIn other words, conduct an \\(F\\)-test against the null hypothesis that the model is ineffective at predicting wellbeing scores using social interactions and outdoor time by computing the \\(F\\)-statistic using its definition.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe F-ratio is used to test the null hypothesis that all regression slopes are zero.\nIt is called the F-ratio because it is the ratio of the how much of the variation is explained by the model (per paramater) versus how much of the variation is left unexplained in the residuals (per remaining degrees of freedom).\n\\[\nF_{df_{model},df_{residual}} = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\\\\n\\quad \\\\\n\\begin{align}\n& \\text{Where:} \\\\\n& df_{model} = k \\\\\n& df_{residual} = n-k-1 \\\\\n& n = \\text{sample size} \\\\\n& k  = \\text{number of explanatory variables} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR function\n\n\n\n\ndf1 <- 2\ndf2 <- nrow(mwdata) - 2 - 1\nf_star <- qf(0.95, df1, df2)\nf_star\n\n[1] 3.327654\n\n\n\nmodel_utility <- wellbeing_fitted %>%\n  summarise(\n    SSModel = sum((wellbeing_hat - mean(wellbeing))^2 ),\n    SSResid = sum( resid^2 ),\n    MSModel = SSModel / df1,\n    MSResid = SSResid / df2,\n    FObs = MSModel / MSResid\n  )\nmodel_utility\n\n# A tibble: 1 × 5\n  SSModel SSResid MSModel MSResid  FObs\n    <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n1   3126.   1096.   1563.    37.8  41.3\n\n\nWe can also compute the p-value:\n\npvalue <- 1 - pf(model_utility$FObs, df1, df2)\npvalue\n\n[1] 3.225548e-09\n\n\nThe value 3.225548e-09 simply means \\(3.2 \\times 10^{-9}\\), so it’s a really small number.\n\n\n\n#look in bottom row\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\nThe relevant row is the following:\n\nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\n\n\n\n\n\n\n\n\nWe performed an \\(F\\)-test of model utility at the 5% significance level, where \\(F(2,29) = 41.34, p <.001\\).\nThe large \\(F\\)-statistic and small \\(p\\)-value (\\(<.001\\)) suggested that we have very strong evidence against the null hypothesis that the model is ineffective.\nIn other words, the data provide strong evidence that the number of social interactions and outdoor time are effective predictors of wellbeing scores.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nProduce a visualisation of the association between wellbeing and outdoor time, after accounting for social interactions.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo visualise just one association, you might need to specify the terms argument in plot_model(). Don’t forget you can look up the documentation by typing ?plot_model in the console.\n\n\n\n\n\n\n\n Solution \n\n\n\nplot_model(mdl1, type = \"eff\",\n           terms = c(\"outdoor_time\"), \n           show.data = TRUE)"
  },
  {
    "objectID": "1_04_model_fit.html#standardization",
    "href": "1_04_model_fit.html#standardization",
    "title": "Model Fit and Standardization",
    "section": "Standardization",
    "text": "Standardization\n\nQuestion 8\n\n\nFit the regression model using the standardized response and explanatory variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can either:\n\nAdd to the “mwdata” dataset three variables called z_wellbeing, z_social_int, and z_outdoor_time representing the standardized welllbeing, social interactions and outdoor time variables, respectively.\n\nRecall the formula for the \\(z\\)-score:\n\\[\nz_x = \\frac{x - \\bar{x}}{s_x}, \\qquad z_y = \\frac{y - \\bar{y}}{s_y}\n\\]\nOR\n\nUse the scale() function when specifying your lm() statement.\n\n\n\n\n\n\n\n\n Solution \n\n\n\n\nZ-Score\nscale function\n\n\n\nz score variables\n\nmwdata <- mwdata %>%\n  mutate(\n    z_wellbeing = (wellbeing - mean(wellbeing)) / sd(wellbeing),\n    z_social_int = (social_int - mean(social_int)) / sd(social_int),\n    z_outdoor_time = (outdoor_time - mean(outdoor_time)) / sd(outdoor_time)\n  )\n\nCheck that they are standardized\n\nmwdata %>%\n  summarise(\n    M_z_wellbeing = round(mean(z_wellbeing),2), SD_z_wellbeing = sd(z_wellbeing), \n    M_z_social_int = round(mean(z_social_int),2), SD_z_social_int = sd(z_social_int),\n    M_z_outdoor_time = round(mean(z_outdoor_time),2), SD_z_outdoor_time = sd(z_outdoor_time)\n  )\n\n# A tibble: 1 × 6\n  M_z_wellbeing SD_z_wellbeing M_z_social_int SD_z_social_int M_z_outd…¹ SD_z_…²\n          <dbl>          <dbl>          <dbl>           <dbl>      <dbl>   <dbl>\n1             0              1              0               1          0       1\n# … with abbreviated variable names ¹​M_z_outdoor_time, ²​SD_z_outdoor_time\n\n#mean of 0, SD of 1 - all good to go\n\nRun model\n\n#with z scoring\nmdl_z <- lm(z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\nsummary(mdl_z)\n\n\nCall:\nlm(formula = z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8347 -0.4212 -0.1075  0.4822  0.9371 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    1.327e-16  9.313e-02   0.000   1.0000    \nz_social_int   6.742e-01  1.006e-01   6.702 2.37e-07 ***\nz_outdoor_time 3.527e-01  1.006e-01   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5268 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\nround(summary(mdl_z)$coefficients,3)\n\n               Estimate Std. Error t value Pr(>|t|)\n(Intercept)       0.000      0.093   0.000    1.000\nz_social_int      0.674      0.101   6.702    0.000\nz_outdoor_time    0.353      0.101   3.506    0.001\n\n\n\n\n\nmdl_s <- lm(scale(wellbeing) ~ scale(social_int) + scale(outdoor_time), data = mwdata)\nsummary(mdl_s)\n\n\nCall:\nlm(formula = scale(wellbeing) ~ scale(social_int) + scale(outdoor_time), \n    data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8347 -0.4212 -0.1075  0.4822  0.9371 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         1.327e-16  9.313e-02   0.000   1.0000    \nscale(social_int)   6.742e-01  1.006e-01   6.702 2.37e-07 ***\nscale(outdoor_time) 3.527e-01  1.006e-01   3.506   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5268 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\nround(summary(mdl_s)$coefficients,3)\n\n                    Estimate Std. Error t value Pr(>|t|)\n(Intercept)            0.000      0.093   0.000    1.000\nscale(social_int)      0.674      0.101   6.702    0.000\nscale(outdoor_time)    0.353      0.101   3.506    0.001\n\n\n\n\n\nFrom comparing either the summary() or rounded output, you should see that the estimates are the same under both approaches.\n\n\n\n\n\nQuestion 9\n\n\nCreate a table to present your results from the standardized model.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package.\nRemember that you can rename your DV and IV labels by specifying dv.labels and pred.labels.\n\n\n\n\n\n\n\n Solution \n\n\n\ntab_model(mdl_z,\n          dv.labels = \"Wellbeing (WEMWBS Scores)\",\n          pred.labels = c(\"z_social_int\" = \"Social Interactions (number per week)\",\n                          \"z_outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression table for Wellbeing model. Outcome variable and predictors are Z-scored\")\n\n\n\nRegression table for Wellbeing model. Outcome variable and predictors are Z-scored\n\n \nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n0.00\n-0.19 – 0.19\n1.000\n\n\nSocial Interactions(number per week)\n0.67\n0.47 – 0.88\n<0.001\n\n\nOutdoor Time (hours perweek)\n0.35\n0.15 – 0.56\n0.001\n\n\nObservations\n32\n\n\nR2 / R2 adjusted\n0.740 / 0.722\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret the standardized variables presented in the above table.\n\n\n\n\n Solution \n\n\n\nFor every standard deviation increase in social interactions, wellbeing scores increased on average by 0.67 standard deviations.\nFor every standard deviation increase in outdoor time, wellbeing scores increased on average by 0.35 standard deviations."
  },
  {
    "objectID": "1_05_cat_recap.html",
    "href": "1_05_cat_recap.html",
    "title": "Categorical Predictors & Block 1 Recap",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand the meaning of (and how to interpret) a multiple regression model with a binary predictor\nUnderstand how to specify a new baseline/reference level for categorical variables\n\n\nBe up to date with lectures\nHave completed Labs 1 - 4\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing.csv.\nNote: this is the same data as Lab 3 & 4."
  },
  {
    "objectID": "1_05_cat_recap.html#study-overview",
    "href": "1_05_cat_recap.html#study-overview",
    "title": "Categorical Predictors & Block 1 Recap",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Question\nIs there an assocation between well-being and time spent outdoors after taking into account the assocation between well-being and having a routine?\n\n\n Wellbeing data codebook.\n\n\nDescription\nResearchers interviewed 32 participants, selected at random from the population of residents of Edinburgh & Lothians. They used the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\nThe researchers also asked participants to estimate the average number of hours they spend outdoors each week, the average number of social interactions they have each week (whether on-line or in-person), and whether they believe that they stick to a routine throughout the week (Yes/No).\nThe data in wellbeing.csv contain five attributes collected from a random sample of \\(n=32\\) hypothetical residents over Edinburgh & Lothians, and include:\n\n\nwellbeing: Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n\n\noutdoor_time: Self report estimated number of hours per week spent outdoors\n\n\nsocial_int: Self report estimated number of social interactions per week (both online and in-person)\n\nroutine: Binary Yes/No response to the question “Do you follow a daily routine throughout the week?”\n\nlocation: Location of primary residence (City, Suburb, Rural)\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\n\n\nwellbeing\n      outdoor_time\n      social_int\n      location\n      routine\n    \n\n\n30\n7\n8\nSuburb\nRoutine\n\n\n21\n9\n8\nCity\nNo Routine\n\n\n38\n14\n10\nSuburb\nRoutine\n\n\n27\n16\n10\nCity\nNo Routine\n\n\n20\n1\n10\nRural\nNo Routine\n\n\n37\n11\n12\nSuburb\nNo Routine"
  },
  {
    "objectID": "1_07_int1_nc.html",
    "href": "1_07_int1_nc.html",
    "title": "Interactions I: Num x Cat",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand the concept of an interaction.\nBe able to interpret the meaning of a numeric \\(\\times\\) categorical interaction.\nUnderstand the principle of marginality and why this impacts modelling choices with interactions.\nVisualize and probe interactions.\n\n\nBe up to date with lectures\nHave completed all labs from Block 1 Semester 1 (Weeks 1 - 5)\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npsych\nsjPlot\npatchwork\nkableExtra\nplotly\npander\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv."
  },
  {
    "objectID": "1_07_int1_nc.html#exercises",
    "href": "1_07_int1_nc.html#exercises",
    "title": "Interactions I: Num x Cat",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion 1\n\n\nFormally state:\n\na linear model to investigate if the association between wellbeing and social interactions differs among rural and non-rural residents\nyour chosen significance level\nthe null and alternative hypotheses\n\n\n\n\n\n“Except in special circumstances, a model including a product term for interaction between two explanatory variables should also include terms with each of the explanatory variables individually, even though their coefficients may not be significantly different from zero. Following this rule avoids the logical inconsistency of saying that the effect of \\(X_1\\) depends on the level of \\(X_2\\) but that there is no effect of \\(X_1\\).”\n— Ramsey and Schafer (2012)\n\n\n Solution \n\n\nTo address the research question, we are going to fit the following model, where \\(y\\) = wellbeing; \\(x_1\\) = social interactions; and \\(x_2\\) = whether or not the respondent lives in a rural location.\n\\[\ny = \\beta_0 + \\beta_1  x_1 + \\beta_2  x_2 + \\beta_3 (x_1 \\cdot x_2) + \\epsilon \\\\\n\\quad \\\\ \\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\quad \\text{independently}\n\\]\nor\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Social Interactions + \\hat \\beta_2 \\cdot Location  + \\hat \\beta_3 \\cdot (Social Interactions \\cdot Location)  \n\\]\nEffects will be considered statistically significant at \\(\\alpha=.05\\)\nOur hypotheses are:\n\\(H_0: \\beta_3 = 0\\)\nThe association between wellbeing and social interactions is not moderated by whether or not a person lives in a rural area.\n\\(H_1: \\beta_3 \\neq 0\\)\nThe association between wellbeing and social interactions is moderated by whether or not a person lives in a rural area.\n\n\n\n\n\nQuestion 2\n\n\nCheck coding of variables (e.g., that categorical variables are coded as factors).\nNote that the “location” variable currently has three levels (Rural/Suburb/City), but we only want two (Rural/Not Rural) in order to address our research question, so you’ll need to fix this. One way to do this would be to use ifelse() to define a variable which takes one value (“Rural”) if the observation meets from some condition, or another value (“Not Rural”) if it does not.\n\n\n\n\n\n\nHint\n\n\n\n\n\nType ?ifelse in the console if you want to see the help function. You can use it to add a new variable either inside mutate(), or using data$new_variable_name <- ifelse(test, x, y) syntax.\n\n\n\n\n\n\n\n Solution \n\n\nCreate a new variable for Rural/Not Rural:\n\nwrdata <- wrdata %>% \n  mutate(\n    isRural = ifelse(location == \"rural\", \"rural\", \"not rural\")\n  )\n\nCheck coding of variables within wrdata and ensure isRural is a factor with two levels, ‘rural’ and ‘not rural’:\n\nstr(wrdata) #overall 'structure of data. Or could run is.factor()\n\ntibble [200 × 8] (S3: tbl_df/tbl/data.frame)\n $ age         : num [1:200] 28 56 25 60 19 34 41 41 35 53 ...\n $ outdoor_time: num [1:200] 12 5 19 25 9 18 17 11 12 13 ...\n $ social_int  : num [1:200] 13 15 11 15 18 13 19 12 13 15 ...\n $ routine     : num [1:200] 1 1 1 0 1 1 1 1 0 1 ...\n $ wellbeing   : num [1:200] 36 41 35 35 32 34 39 43 35 37 ...\n $ location    : chr [1:200] \"rural\" \"rural\" \"rural\" \"rural\" ...\n $ steps_k     : num [1:200] 21.6 12.3 49.8 NA 48.1 67.3 1.9 50.9 NA 35.5 ...\n $ isRural     : chr [1:200] \"rural\" \"rural\" \"rural\" \"rural\" ...\n\nwrdata$isRural <- as_factor(wrdata$isRural)\n\n\n\n\n\n\nQuestion 3\n\n\nVisualise your data.\nIn particular:\n\nExplore the associations among the variables included in your analysis\nProduce a visualisation of the association between weekly number of social interactions and well-being, with separate facets for rural vs non-rural respondents\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nThe pairs.panels() function from the psych package will plot all variables in a dataset against one another. This will save you the time you would have spent creating individual plots.\nWithin your ggplot argument, you will need to specify + facet_wrap()\n\n\n\n\n\n\n\n\n Solution \n\n\nLet’s first plot the variables included within our model (note that we could use this for the whole dataset, but we don’t want to include irrelevant variables):\n\nwrdata %>% \n  select(wellbeing, social_int, isRural) %>%\n  pairs.panels()\n\n\n\n\nNext, lets produce our plots with a facet for rural vs non-rural residents:\n\nggplot(data = wrdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  facet_wrap(~isRural) \n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nFit your model using lm(), and assign it as an object with the name “rural_mod”.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhen fitting a regression model in R with two explanatory variables A and B, and their interaction, these two are equivalent:\n\ny ~ A + B + A:B\ny ~ A*B\n\n\n\n\n\n\n\n\n Solution \n\n\n\n#fit model including interaction between social_int and isRural\nrural_mod <- lm(wellbeing ~  social_int * isRural, data = wrdata)\n\n#check model output\nsummary(rural_mod)\n\n\nCall:\nlm(formula = wellbeing ~ social_int * isRural, data = wrdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.4845  -2.7975   0.0155   2.4539  15.6743 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                  32.3851     1.4719  22.003  < 2e-16 ***\nsocial_int                    0.1312     0.1123   1.168  0.24416    \nisRuralnot rural             -1.3866     2.0510  -0.676  0.49981    \nsocial_int:isRuralnot rural   0.5176     0.1615   3.206  0.00157 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.558 on 196 degrees of freedom\nMultiple R-squared:  0.2962,    Adjusted R-squared:  0.2854 \nF-statistic: 27.49 on 3 and 196 DF,  p-value: 6.97e-15\n\n\n\n\n\n\nInterpreting coefficients for A and B in the presence of an interaction A:B\nWhen you include an interaction between \\(x_1\\) and \\(x_2\\) in a regression model, you are estimating the extent to which the effect of \\(x_1\\) on \\(y\\) is different across the values of \\(x_2\\).\nWhat this means is that the effect of \\(x_1\\) on \\(y\\) depends on/is conditional upon the value of \\(x_2\\).\n(and vice versa, the effect of \\(x_2\\) on \\(y\\) is different across the values of \\(x_1\\)).\nThis means that we can no longer talk about the “effect of \\(x_1\\) holding \\(x_2\\) constant”. Instead we can talk about a marginal effect of \\(x_1\\) on \\(y\\) at a specific value of \\(x_2\\).\n\nWhen we fit the model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 \\cdot x_2) + \\epsilon\\) using lm():\n\nthe parameter estimate \\(\\hat \\beta_1\\) is the marginal effect of \\(x_1\\) on \\(y\\) where \\(x_2 = 0\\)\n\nthe parameter estimate \\(\\hat \\beta_2\\) is the marginal effect of \\(x_2\\) on \\(y\\) where \\(x_1 = 0\\)\n\n\n\n\n N.B. Regardless of whether or not there is an interaction term in our model, all parameter estimates in multiple regression are “conditional” in the sense that they are dependent upon the inclusion of other variables in the model. For instance, in \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\) the coefficient \\(\\hat \\beta_1\\) is conditional upon holding \\(x_2\\) constant. \n\n\n\nInterpreting the interaction term A:B\nThe coefficient for an interaction term can be thought of as providing an adjustment to the slope.\nIn the model below, we have a numeric*categorical interaction:\n\\[\n\\begin{align}\n\\text{wellbeing} \\ = \\ &\\beta_0 + \\beta_1 \\text{social_interactions} + \\beta_2 \\text{isRural} + \\\\\n&\\beta_3 (\\text{social_interactions} \\cdot \\text{isRural}) + \\epsilon\n\\end{align}\n\\]\nThe estimate \\(\\hat \\beta_3\\) is the adjustment to the slope \\(\\hat \\beta_1\\) to be made for the individuals in the \\(\\text{isRural}=1\\) group.\n\n\nQuestion 5\n\n\nLook at the parameter estimates from your model, and write a description of what each one corresponds to on the plot shown in Figure 1 (it may help to sketch out the plot yourself and annotate it).\n\n“The best method of communicating findings about the presence of significant interaction may be to present a table of graph of the estimated means at various combinations of the interacting variables.”\n— Ramsey and Schafer (2012)\n\n\n\n\n\nFigure 1: Multiple regression model: Wellbeing ~ Social Interactions * is Rural\n\n\n\n\n\n Hints.\n\n\nHere are some options to choose from:\n\nThe point at which the blue line cuts the y-axis (where social_int = 0)\nThe point at which the red line cuts the y-axis (where social_int = 0)\nThe average vertical distance between the red and blue lines.\nThe vertical distance from the blue to the red line at the y-axis (where social_int = 0)\nThe vertical distance from the red to the blue line at the y-axis (where social_int = 0)\nThe vertical distance from the blue to the red line at the center of the plot\n\nThe vertical distance from the red to the blue line at the center of the plot\n\nThe slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line\nThe slope of the red line\nHow the slope of the line changes when you move from the blue to the red line\nHow the slope of the line changes when you move from the red to the blue line\n\n\n\n\n\n\n\n\n Solution \n\n\nWe can obtain our parameter estimates using various functions such as summary(rural_mod),coef(rural_mod), coefficients(rural_mod) etc.\n\ncoefficients(rural_mod)\n\n                (Intercept)                  social_int \n                 32.3851376                   0.1312089 \n           isRuralnot rural social_int:isRuralnot rural \n                 -1.3865688                   0.5175856 \n\n\n\n\n\\(\\hat \\beta_0\\) = (Intercept) = 32.39: The point at which the blue line cuts the y-axis (where social_int = 0).\n\n\n\\(\\hat \\beta_1\\) = social_int = 0.13: The slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line.\n\n\\(\\hat \\beta_2\\) = isRuralrural = -1.39: The vertical distance from the blue to the red line at the y-axis (where social_int = 0).\n\n\n\\(\\hat \\beta_3\\) = social_int:isRuralrural = 0.52: How the slope of the line changes when you move from the blue to the red line.\n\n\n\n\n\n\nQuestion 6\n\n\nUsing plot_model(), visualize the interaction between social interactions and rural.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTry to create a plot like Figure 1, which shows the two lines.\nThink about what you need to specify in the type = argument - what types of models are these questions all about?\n\n\n\n\n\n\n\n Solution \n\n\n\nplot_model(rural_mod, \n           type=\"int\", \n           axis.title = c(\"Social Interactions (number per week)\", \"Wellbeing (WEMWBS Scores)\"))\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nInterpret your results in the context of the research question and report your model in full.\nProvide key model results in a formatted table.\n\n\n\n\n Solution \n\n\n\n#create table for results\ntab_model(rural_mod,\n          dv.labels = \"Wellbeing (WEMWBS Scores)\",\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\",\n                          \"isRural\" = \"Location\"),\n          title = \"Regression Table for Wellbeing Model\")\n\n\n\n\nTable 1:  Regression Table for Wellbeing Model \n\n \nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n32.39\n29.48 – 35.29\n<0.001\n\n\nSocial Interactions(number per week)\n0.13\n-0.09 – 0.35\n0.244\n\n\nisRuralnot rural\n-1.39\n-5.43 – 2.66\n0.500\n\n\nsocial_int:isRuralnot rural\n0.52\n0.20 – 0.84\n0.002\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.296 / 0.285\n\n\n\n\n\nAnd now lets write our results up:\n\n\n\n\n\n\nFull regression results including 95% Confidence Intervals are shown in Table 1. The \\(F\\)-test for model utility was significant \\((F(3, 196) = 27.49, p<.001)\\), and the model explained approximately 28.54% of the variability in wellbeing scores.\n\n\n\n\n\n\n\n\nMean Centering\nMean centering a variable involves subtracting the mean of that variable from every individual value. When working with models that contain interaction terms (like our rural_mod), it is generally a good idea to center your continuous predictor variables. This is because:\n\ncentering helps to address issues of multicollinearity (you’ll learn more about this later in the course)\ncentering makes the model coefficients easier to interpret\n\n\n\nQuestion 8\n\n\nMean center the continuous IV, and re-run your model.\n\n\n\n\n Solution \n\n\nCreate mean centred variable for ‘social_int’:\n\n#wrdata <-\n # wrdata %>%\n  #mutate(\n   # mc_social_int = scs - mean(social_int)\n    #)\n\nRe-run model:\n\n#fit model including interaction between social_int and isRural\n#rural_mod1 <- lm(wellbeing ~  mc_social_int * isRural, data = wrdata)\n\n#check model output\n#summary(rural_mod1)"
  },
  {
    "objectID": "1_08_int1_nn.html",
    "href": "1_08_int1_nn.html",
    "title": "Interactions I: Num x Cat & Num x Num",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand the concept of an interaction.\nBe able to interpret the meaning of a numeric \\(\\times\\) categorical interaction.\nBe able to interpret the meaning of a numeric \\(\\times\\) numeric interaction.\nUnderstand the principle of marginality and why this impacts modelling choices with interactions.\nVisualize and probe interactions.\n\nYou will need to have completed [Lab X].\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npsych\nsjPlot\npatchwork\nkableExtra\nplotly\npander\n\nYou can download the data required for Part A of this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv.\nYou can download the data required for Part B of this lab here or read it in via this link https://uoepsy.github.io/data/scs_study.csv."
  },
  {
    "objectID": "1_08_int1_nn.html#exercises",
    "href": "1_08_int1_nn.html#exercises",
    "title": "Interactions I: Num x Cat & Num x Num",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion A1\n\n\n\nSpecify a multiple regression model to address the research question.\nVisually explore the associations among the variables included in your analysis.\n\n\n“Except in special circumstances, a model including a product term for interaction between two explanatory variables should also include terms with each of the explanatory variables individually, even though their coefficients may not be significantly different from zero. Following this rule avoids the logical inconsistency of saying that the effect of \\(X_1\\) depends on the level of \\(X_2\\) but that there is no effect of \\(X_1\\).”\n— Ramsey and Schafer (2012)\n\n\n\nHint 1: Check the “location” variable. It currently has three levels (Rural/Suburb/City), but we only want two (Rural/Not Rural). You’ll need to fix this. One way to do this would be to use ifelse() to define a variable which takes one value (“Rural”) if the observation meets from some condition, or another value (“Not Rural”) if it does not. Type ?ifelse in the console if you want to see the help function. You can use it to add a new variable either inside mutate(), or using data$new_variable_name <- ifelse(test, x, y) syntax.\n\nHint 2: The pairs.panels() function from the psych package will plot all variables in a dataset against one another. This will save you the time you would have spent creating individual plots.\n\n\n\n\n\n Solution \n\n\nTo address the research question, we are going to fit the following model, where \\(y\\) = wellbeing; \\(x_1\\) = weekly outdoor time; and \\(x_2\\) = whether or not the respondent lives in a rural location or not.\n\\[\ny = \\beta_0 + \\beta_1  x_1 + \\beta_2  x_2 + \\beta_3 (x_1 \\cdot x_2) + \\epsilon \\\\\n\\quad \\\\ \\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\quad \\text{independently}\n\\]\nFirst let’s create a new variable for Rural/Not Rural\n\nwrdata <- wrdata %>% \n  mutate(\n    isRural = ifelse(location == \"rural\", \"rural\", \"not rural\")\n  )\n\nNow let’s use the pairs.panels() function from the psych package. We could use it on the whole dataset, but for now we’ll just do it on the variables we’re interested in:\n\nwrdata %>% \n  select(wellbeing, social_int, isRural) %>%\n  pairs.panels()\n\n\n\n\n\n\n\n\nQuestion A2\n\n\nProduce a visualisation of the association between weekly number of social interactions and well-being, with separate facets for rural vs non-rural respondents.\n\n\n\n\n Solution \n\n\n\nggplot(data = wrdata, aes(x = social_int, y = wellbeing))\n\n\n\n  geom_point() \n\ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\n  facet_wrap(~isRural) \n\n<ggproto object: Class FacetWrap, Facet, gg>\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  <ggproto object: Class FacetWrap, Facet, gg>\n\n\n\n\n\n\nQuestion A3\n\n\nFit your model using lm(), and assign it as an object with the name “rural_mod”.\nHint: When fitting a regression model in R with two explanatory variables A and B, and their interaction, these two are equivalent:\n\ny ~ A + B + A:B\ny ~ A*B\n\n\n\n\n\n Solution \n\n\n\n#fit model including interaction between social_int and isRural\nrural_mod <- lm(wellbeing ~ 1 + social_int * isRural, data = wrdata)\n\n#check model output\nsummary(rural_mod)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int * isRural, data = wrdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.4845  -2.7975   0.0155   2.4539  15.6743 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              30.9986     1.4284  21.702  < 2e-16 ***\nsocial_int                0.6488     0.1160   5.593 7.42e-08 ***\nisRuralrural              1.3866     2.0510   0.676  0.49981    \nsocial_int:isRuralrural  -0.5176     0.1615  -3.206  0.00157 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.558 on 196 degrees of freedom\nMultiple R-squared:  0.2962,    Adjusted R-squared:  0.2854 \nF-statistic: 27.49 on 3 and 196 DF,  p-value: 6.97e-15\n\n\n\n\n\n\nInterpreting coefficients for A and B in the presence of an interaction A:B\nWhen you include an interaction between \\(x_1\\) and \\(x_2\\) in a regression model, you are estimating the extent to which the effect of \\(x_1\\) on \\(y\\) is different across the values of \\(x_2\\).\nWhat this means is that the effect of \\(x_1\\) on \\(y\\) depends on/is conditional upon the value of \\(x_2\\).\n(and vice versa, the effect of \\(x_2\\) on \\(y\\) is different across the values of \\(x_1\\)).\nThis means that we can no longer talk about the “effect of \\(x_1\\) holding \\(x_2\\) constant”. Instead we can talk about a marginal effect of \\(x_1\\) on \\(y\\) at a specific value of \\(x_2\\).\n\nWhen we fit the model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 \\cdot x_2) + \\epsilon\\) using lm():\n\nthe parameter estimate \\(\\hat \\beta_1\\) is the marginal effect of \\(x_1\\) on \\(y\\) where \\(x_2 = 0\\)\n\nthe parameter estimate \\(\\hat \\beta_2\\) is the marginal effect of \\(x_2\\) on \\(y\\) where \\(x_1 = 0\\)\n\n\n\n\n N.B. Regardless of whether or not there is an interaction term in our model, all parameter estimates in multiple regression are “conditional” in the sense that they are dependent upon the inclusion of other variables in the model. For instance, in \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\) the coefficient \\(\\hat \\beta_1\\) is conditional upon holding \\(x_2\\) constant. \n\n\n\nInterpreting the interaction term A:B\nThe coefficient for an interaction term can be thought of as providing an adjustment to the slope.\nIn the model below, we have a numeric*categorical interaction:\n\\[\n\\begin{align}\n\\text{wellbeing} \\ = \\ &\\beta_0 + \\beta_1 \\text{social_interactions} + \\beta_2 \\text{isRural} + \\\\\n&\\beta_3 (\\text{social_interactions} \\cdot \\text{isRural}) + \\epsilon\n\\end{align}\n\\]\nThe estimate \\(\\hat \\beta_3\\) is the adjustment to the slope \\(\\hat \\beta_1\\) to be made for the individuals in the \\(\\text{isRural}=1\\) group.\n\n\nQuestion A4\n\n\nLook at the parameter estimates from your model, and write a description of what each one corresponds to on the plot shown in Figure @ref(fig:plot-annotate-int) (it may help to sketch out the plot yourself and annotate it).\n\n“The best method of communicating findings about the presence of significant interaction may be to present a table of graph of the estimated means at various combinations of the interacting variables.”\n— Ramsey and Schafer (2012)\n\n\n\nScale for 'colour' is already present. Adding another scale for 'colour',\nwhich will replace the existing scale.\n\n\nScale for 'x' is already present. Adding another scale for 'x', which will\nreplace the existing scale.\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\nMultiple regression model: Wellbeing ~ Social Interactions * is RuralNote that the dashed lines represent predicted values below the minimum observed number of social interactions, to ensure that zero on the x-axis is visible\n\n\n\n\n\n Hints.\n\n\nHere are some options to choose from:\n\nThe point at which the blue line cuts the y-axis (where social_int = 0)\nThe point at which the red line cuts the y-axis (where social_int = 0)\nThe average vertical distance between the red and blue lines.\nThe vertical distance from the blue to the red line at the y-axis (where social_int = 0)\nThe vertical distance from the red to the blue line at the y-axis (where social_int = 0)\nThe vertical distance from the blue to the red line at the center of the plot\n\nThe vertical distance from the red to the blue line at the center of the plot\n\nThe slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line\nThe slope of the red line\nHow the slope of the line changes when you move from the blue to the red line\nHow the slope of the line changes when you move from the red to the blue line\n\n\n\n\n\n\n\n\n Solution \n\n\nWe can obtain our parameter estimates using various functions such as summary(rural_mod),coef(rural_mod), coefficients(rural_mod) etc.\n\ncoefficients(rural_mod)\n\n            (Intercept)              social_int            isRuralrural \n             30.9985688               0.6487945               1.3865688 \nsocial_int:isRuralrural \n             -0.5175856 \n\n\n\n\n\\(\\hat \\beta_0\\) = (Intercept) = 31: The point at which the blue line cuts the y-axis (where social_int = 0).\n\n\n\\(\\hat \\beta_1\\) = social_int = 0.65: The slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line.\n\n\\(\\hat \\beta_2\\) = isRuralrural = 1.39: The vertical distance from the blue to the red line at the y-axis (where social_int = 0).\n\n\n\\(\\hat \\beta_3\\) = social_int:isRuralrural = -0.52: How the slope of the line changes when you move from the blue to the red line.\n\n\n\n\n\nQuestion A5\n\n\nLoad the sjPlot package and try using the function plot_model().\nThe default behaviour of plot_model() is to plot the parameter estimates and their confidence intervals. This is where type = \"est\". Try to create a plot like Figure @ref(fig:plot-annotate-int), which shows the two lines (Hint: what are this weeks’ exercises all about? type = ???.)\n\n\n\n\n Solution \n\n\n\nplot_model(rural_mod, type=\"int\")"
  },
  {
    "objectID": "1_08_int1_nn.html#exercises-1",
    "href": "1_08_int1_nn.html#exercises-1",
    "title": "Interactions I: Num x Cat & Num x Num",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion B1\n\n\nProduce plots of the relevant distributions and associations involved in the research question.\n\n\n\n\n Solution \n\n\n\nggplot(data = scs_study, aes(x=dass)) + \n  geom_density() + \n  geom_boxplot(width = 1/50) +\n  labs(title=\"Marginal distribution of DASS-21 Scores\", \n       x = \"Depression Anxiety and Stress Scale\", y = \"Probability density\")\n\n\n\n\n\nThe marginal distribution of scores on the Depression, Anxiety and Stress Scale (DASS-21) is unimodal with a mean of approximately 45 and a standard deviation of 7.\n\n\nggplot(data = scs_study, aes(x=scs)) + \n  geom_density() + \n  geom_boxplot(width = 1/50) +\n  labs(title=\"Marginal distribution of Social Comparison Scale (SCS) scores\", \n       x = \"Social Comparison Scale Score\", y = \"Probability density\")\n\n\n\n\n\nThe marginal distribution of score on the Social Comparison Scale (SCS) is unimodal with a mean of approximately 36 and a standard deviation of 4. There look to be a number of outliers at the upper end of the scale.\n\n\nggplot(data = scs_study, aes(x=zn)) + \n  geom_density() + \n  geom_boxplot(width = 1/50) +\n  labs(title=\"Marginal distribution of Neuroticism (Z-Scored)\", \n       x = \"Neuroticism (Z-Scored)\", y = \"Probability density\")\n\n\n\n\n\nThe marginal distribution of Neuroticism (Z-scored) is positively skewed, with the 25% of scores falling below -0.8, 75% of scores falling below 0.59.\n\n\np1 <- ggplot(data = scs_study, aes(x=scs, y=dass)) + \n  geom_point()+\n  labs(x = \"SCS\", y = \"DASS-21\")\n\np2 <- ggplot(data = scs_study, aes(x=zn, y=dass)) + \n  geom_point()+\n  labs(x = \"Neuroticism\", y = \"DASS-21\")\n\np1 | p2\n\n\n\n# the kable() function from the kableExtra package can make table outputs print nicely into html.\nscs_study %>%\n  select(dass, scs, zn) %>%\n  cor() %>% \n  kable(digits = 2) %>%\n  kable_styling(full_width = FALSE)\n\n\n\n\n   \n    dass \n    scs \n    zn \n  \n\n\n dass \n    1.00 \n    -0.23 \n    0.20 \n  \n\n scs \n    -0.23 \n    1.00 \n    0.11 \n  \n\n zn \n    0.20 \n    0.11 \n    1.00 \n  \n\n\n\n\n\nThere is a weak, negative, linear relationship between scores on the Social Comparison Scale and scores on the Depression Anxiety and Stress Scale for the participants in the sample. Severity of symptoms measured on the DASS-21 tend to decrease, on average, the more favourably participants view their social rank.\nThere is a weak, positive, linear relationship between the levels of Neuroticism and scores on the DASS-21. Participants who are more neurotic tend to, on average, display a higher severity of symptoms of depression, anxiety and stress.\n\n\n\n\n\nQuestion B2\n\n\nRun the code below. It takes the dataset, and uses the cut() function to add a new variable called “zn_group”, which is the “zn” variable split into 4 groups.\nRemember: we have to re-assign this output as the name of the dataset (the scs_study <- bit at the beginning) to make these changes occur in our environment (the top-right window of Rstudio). If we didn’t have the first line, then it would simply print the output.\n\nscs_study <-\n  scs_study %>%\n  mutate(\n    zn_group = cut(zn, 4)\n  )\n\nWe can see how it has split the “zn” variable by plotting the two against one another:\n(Note that the levels of the new variable are named according to the cut-points).\n\nggplot(data = scs_study, aes(x = zn_group, y = zn)) + \n  geom_point()\n\n\n\n\nPlot the association between scores on the SCS and scores on the DASS-21, for each group of the variable we just created.\nHow does the pattern differ? Does it suggest an interaction?\nTip: Rather than creating four separate plots, you might want to map some feature of the plot to the variable we created in the data, or make use of facet_wrap()/facet_grid().\n\n\n\n\n Solution \n\n\n\nggplot(data = scs_study, aes(x = scs, y = dass, col = zn_group)) + \n  geom_point() + \n  facet_grid(~zn_group) +\n  theme(legend.position = \"none\") # remove the legend\n\n\n\n\nThe associaiton between SCS scores and DASS-21 scores appears to be different between these groups. For those with a relatively high neuroticism score, the association seems stronger, while for those with a low neuroticism score there is almost no discernable association. This suggests an interaction - the association of DASS-21 ~ SCS differs across the values of neuroticism!\n\n\n\n\nCutting one of the explanatory variables up into groups essentially turns a numeric variable into a categorical one. We did this just to make it easier to visualise how a an association differs across the values of another variable, because we can imagine a separate line for the association between SCS and DASS-21 scores for each of the groups of neuroticism. However, in grouping a numeric variable like this we lose information. Neuroticism is measured on a continuous scale, and we want to capture how the association between SCS and DASS-21 differs across that continuum (rather than cutting it into chunks).\nWe could imagine cutting it into more and more chunks (see Figure @ref(fig:reglinescut)), until what we end up with is a an infinite number of lines - i.e., a three-dimensional plane/surface (recall that in for a multiple regression model with 2 explanatory variables, we can think of the model as having three-dimensions). The inclusion of the interaction term simply results in this surface no longer being necessarily flat. You can see this in Figure @ref(fig:3dint).\n\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\nSeparate regression lines DASS ~ SCS for neuroticism when cut into 4 (left) or 6 (center) or 12 (right) groups\n\n\n\n\n\n\n\n3D plot of regression surface with interaction. You can explore the plot in the figure below from different angles by moving it around with your mouse.\n\n\n\n\nQuestion B3\n\n\nSpecify the model required in order to answer the research question (e.g., \\(\\text{??} = \\beta_0 + \\beta_1 \\cdot \\text{??} + .... + \\epsilon\\)), and then fit your model using lm().\n\n\n\n\n Solution \n\n\n\n\n\nModel to answer research question:\n\\[\n\\text{DASS-21 Score} = \\beta_0 + \\beta_1 \\cdot \\text{SCS Score} + \\beta_2 \\cdot \\text{Neuroticism} + \\beta_3 \\cdot (\\text{SCS score} \\cdot \\text{Neuroticism}) + \\epsilon\n\\]\n\n#fit interaction model\ndass_mdl <- lm(dass ~ 1 + scs*zn, data = scs_study)\n\nsummary(dass_mdl)\n\n\nCall:\nlm(formula = dass ~ 1 + scs * zn, data = scs_study)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.301  -3.825  -0.173   3.733  45.777 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 60.80887    2.45399  24.780  < 2e-16 ***\nscs         -0.44391    0.06834  -6.495 1.64e-10 ***\nzn          20.12813    2.35951   8.531  < 2e-16 ***\nscs:zn      -0.51861    0.06552  -7.915 1.06e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.123 on 652 degrees of freedom\nMultiple R-squared:  0.1825,    Adjusted R-squared:  0.1787 \nF-statistic:  48.5 on 3 and 652 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n\n\n \n\n \n\n      © Copyright 2019-2022 The University of Edinburgh. Site licensed under the GNU AGPLv3 license."
  },
  {
    "objectID": "1_09_int2_cc.html",
    "href": "1_09_int2_cc.html",
    "title": "Interactions II: Cat x Cat",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to interpret a categorical \\(\\times\\) categorical interaction.\nVisualize and probe interactions.\nBe able to read interaction plots.\n\nYou will need to have completed [Lab X].\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\nemmeans\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/cognitive_experiment_3_by_2.csv"
  },
  {
    "objectID": "1_09_int2_cc.html#visualising-the-interactions",
    "href": "1_09_int2_cc.html#visualising-the-interactions",
    "title": "Interactions II: Cat x Cat",
    "section": "Visualising the interactions",
    "text": "Visualising the interactions\n\n\n\n\n\nIn the interaction plot above you can see three highlighted differences, where the differences are denoted with the Greek letter \\(\\Delta\\) (“delta”) with a hat on top, \\(\\hat \\Delta\\), to denote that those are estimates for the unknown population differences based on the available sample data. The corresponding population differences are unknown as we don’t have the data for the entire population, and they are denoted with a \\(\\Delta\\) without a hat on top.\nYou can see highlighted:\n\nThe difference in the mean score between Recognition and Grammar for Control patients, \\(\\hat \\Delta_{\\text{Control}}\\)\n\nThe difference in the mean score between Recognition and Grammar for Amnesic patients, \\(\\hat \\Delta_{\\text{Amnesic}}\\)\n\nThe difference in the mean score between Recognition and Grammar for Huntingtons patients, \\(\\hat \\Delta_{\\text{Huntingtons}}\\)\n\n\nAn interaction is present if the effect of Task (i.e. the difference in mean score between Recognition and Grammar tasks) substantially varies across the possible values for Diagnosis. That is, if the difference for Amnesic is not the same as that for Control, or if the difference for Huntingtons is not the same as that for Control, or both.\nThe model summary returns two rows for the interactions. Let’s focus on this row:\n## DiagnosisAmnesic:TaskRecognition      -10.000     11.719  -0.853  0.40192    \nIt returns an estimate for the difference\n\\[\n\\hat \\beta_4 = \\hat \\Delta_{\\text{Amnesic}} - \\hat \\Delta_{\\text{Control}}\n\\]\nand also performs a test for the hypothesis that the differences are equal in the population:\n\\[\nH_0: \\Delta_{\\text{Amnesic}} = \\Delta_{\\text{Control}} \\\\\nH_1: \\Delta_{\\text{Amnesic}} \\neq \\Delta_{\\text{Control}}\n\\]\nor, equivalently, that:\n\\[\nH_0: \\Delta_{\\text{Amnesic}} - \\Delta_{\\text{Control}} = 0 \\\\\nH_1: \\Delta_{\\text{Amnesic}} - \\Delta_{\\text{Control}} \\neq 0\n\\]\nLet’s now focus on the last row:\n## DiagnosisHuntingtons:TaskRecognition   40.000     11.719   3.413  0.00228 ** \nIt returns an estimate for the difference\n\\[\n\\hat \\beta_5 = \\hat \\Delta_{\\text{Huntingtons}} - \\hat \\Delta_{\\text{Control}}\n\\]\nand also performs a test for the hypothesis that the differences are equal in the population:\n\\[\nH_0: \\Delta_{\\text{Huntingtons}} = \\Delta_{\\text{Control}} \\\\\nH_1: \\Delta_{\\text{Huntingtons}} \\neq \\Delta_{\\text{Control}}\n\\]\nor, equivalently, that:\n\\[\nH_0: \\Delta_{\\text{Huntingtons}} - \\Delta_{\\text{Control}} = 0 \\\\\nH_1: \\Delta_{\\text{Huntingtons}} - \\Delta_{\\text{Control}} \\neq 0\n\\]\n\nQuestion 8\n\n\nInterpret the model output in the context of the research hypothesis.\n\n\n\n\n Solution \n\n\nLet’s recall the researchers’ hypothesis:\n\nA group of researchers wants to test an hypothesised theory according to which the difference in performance between explicit and implicit memory tasks will be greatest for Huntington patients in comparison to controls.\nOn the other hand, the difference in performance between explicit and implicit memory tasks will not significantly differ between patients with amnesia in comparison to controls.\n\nWe can get a nice printout of the model summary as follows:\n\n\ntab_model(mdl_int, show.stat = TRUE)\n\n\n\n\n \nScore\n\n\nPredictors\nEstimates\nCI\nStatistic\np\n\n\n(Intercept)\n80.00\n67.91 – 92.09\n13.65\n<0.001\n\n\nDiagnosis [Amnesic]\n-20.00\n-37.10 – -2.90\n-2.41\n0.024\n\n\nDiagnosis [Huntingtons]\n-40.00\n-57.10 – -22.90\n-4.83\n<0.001\n\n\nTask [Recognition]\n15.00\n-2.10 – 32.10\n1.81\n0.083\n\n\nDiagnosis [Amnesic] *Task [Recognition]\n-10.00\n-34.19 – 14.19\n-0.85\n0.402\n\n\nDiagnosis [Huntingtons] *Task [Recognition]\n40.00\n15.81 – 64.19\n3.41\n0.002\n\n\nObservations\n30\n\n\nR2 / R2 adjusted\n0.739 / 0.685\n\n\n\n\n\n\nWe could interpret it as follows:\n\nThe difference in scores between the recognition and grammar tasks, respectively measuring explicit and implicit memory, for Huntingtons patients in comparison to controls was significant and indicated a difference of \\(\\hat \\beta_5 = 40\\) points in explicit vs implicit memory performance: \\(t(24) = 3.41, p = 0.002\\).\nThe difference in scores between the recognition and grammar tasks, respectively measuring explicit and implicit memory, for amnesiac patients in comparison to controls was estimated to be \\(\\hat \\beta_4 = -10\\) points but it is not found to be significantly different from 0: \\(t(24) = -0.85, p = 0.40\\).\nThis indicates that the researchers’ hypothesis that the difference in performance between explicit and implicit memory tasks does not differ significantly between amnesic and control patients, while it does differ significantly between Huntington and control patients.\n\nWe can also provide an interpretation of the interaction plot:\n\nplot_model(mdl_int, type = \"int\")\n\n\n\n\nCompared to controls, amnesiac patients will have a significant deficit in explicit memory (as measured by the recognition task), but not on implicit memory (as measured by the grammar task).\nCompared to controls, Huntingtons patients will have a significant deficit in implicit memory (as measured by the grammar task) but not in explicit memory (as measured by the recognition task)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr Umberto Noe\nDr Josiah King\nDr Emma Waterston\nDepartment of Psychology, The University of Edinburgh"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 2 (DAPR2) lab workbook. Using the menu above, you can find lab materials for each week."
  },
  {
    "objectID": "index.html#help-support-feedback",
    "href": "index.html#help-support-feedback",
    "title": "Home",
    "section": "Help, Support & Feedback",
    "text": "Help, Support & Feedback\n\nWithin Lab Workbook\n\nHints, Notes, and Example Write-Up / Interpretation\nHints are shown in a green box, with the title ‘Hint’. If you are unsure what to do, check the collapsible hint provided (note that these are only present for some questions).\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is an example of a hint.\n\n\n\nNotes are displayed in blue boxes, with the title ‘Note’. These are occasionally used to draw your attention to a specific point.\n\n\n\n\n\n\nNote\n\n\n\nThis is an example of a note.\n\n\nExample write-ups and/or interpretations are shown with a red right border. These are helpful to check your interpretation against, and a useful guide to use in future when interpreting and/or writing up results.\n\n\n\n\n\n\nThis is an example write-up / interpretation block.\n\n\n\n\n\nSolutions\nSolutions are made available immediately below each exercise. To view solutions, click the drop down ‘Solution’ button.\nImportant  Before checking the solution you should attempt the question. You should also avoid copying and pasting code from the solutions. If you do check solutions right away and simply copy the answer, you will struggle to learn from the exercises.\nInstead, you should:\n\nTry to figure out the answer yourself or with your peers (and/or ask for help from a staff member if needed)\nType the code out yourself (and annotate your R code chunks so you know what your code is doing & why - future you will thank you for this).\n\n\n\n\nAsking Questions\n\nDuring labs, if you have a question, please ask one of the tutors for support.\nOutside of labs, we encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#tips-on-googling-statistics-and-r",
    "href": "index.html#tips-on-googling-statistics-and-r",
    "title": "Home",
    "section": "Tips on Googling Statistics and R",
    "text": "Tips on Googling Statistics and R\nSearching online for help with statistics and R can be both a help and a hindrance. If you have an error message in R, copy the error message into Google. The results returned can sometimes just cause more confusion, but sometimes something might jump out at you and help you solve the problem. The same applies with searching the internet for help with statistics - search for “what is a p-value”, and you’ll find many many different articles and forum discussions etc. Some of them you will find too technical, but don’t be scared - the vast majority of people work in statistics will find these too technical too. Some of them you might feel are too simple/not helpful. As a general guide, keep clicking around the search responses, and you may end up finding that someone, somewhere, has provided an explanation at the right level. If you find something during your search which you don’t quite understand, feel free to link it in a post on the discussion forum!"
  },
  {
    "objectID": "index.html#feedback-on-labs",
    "href": "index.html#feedback-on-labs",
    "title": "Home",
    "section": "Feedback on Labs",
    "text": "Feedback on Labs\nIf you wish to make suggestions for improvements to these workbooks (or if you spot any typos!), please email ppls.psych.stats@ed.ac.uk making sure to include the course name in the subject."
  }
]