[
  {
    "objectID": "1_01_slr.html",
    "href": "1_01_slr.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to specify a simple linear model\n\nUnderstand what fitted values and residuals are\nBe able to interpret the coefficients of a fitted model\n\n\nBe up to date with lectures\n\nHave watched course intro video in Week 0 folder, and completed associated tasks\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\nkableExtra\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\nThe example write-up sections included as part of the solutions are not perfect - they instead should give you a good example of what information you should include and how to structure this. Note that you must not copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more here.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv"
  },
  {
    "objectID": "1_01_slr.html#marginal-distributions",
    "href": "1_01_slr.html#marginal-distributions",
    "title": "Simple Linear Regression",
    "section": "Marginal Distributions",
    "text": "Marginal Distributions\n\nQuestion 1\n\n\nVisualise and describe the marginal distributions of wellbeing scores and social interactions.\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the many ways to numerically and visually explore your data by reading over the data exploration flashcards.\nFor examples of mariginal distrubtion visualisation examples, see the data visualisation - marginal examples flashcards.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nWellbeing (WEMWBS) Scores\nSocial Interactions\n\n\n\nVisualisation of distribution:\n\nggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  labs(x = \"Wellbeing (WEMWBS) Scores\", \n       y = \"Probability density\")\n\n\n\nFigure 1: Distribution of Wellbeing (WEMWBS) Scores\n\n\n\nInitial observations from plot:\n\nThe distribution of wellbeing scores was unimodal\nMost of the wellbeing scores were between roughly 30 and 45\nThe lowest wellbeing in the sample was approximately 22 and the highest approximately 59. This suggested there was a fair high degree of variation in the data\nScores were within the range of possible values\n\nDescriptive (or summary) statistics for wellbeing scores:\n\nmwdata %&gt;% \n  summarize(\n    M = mean(wellbeing), \n    SD = sd(wellbeing)\n    ) %&gt;%\n    kable(caption = \"Wellbeing Descriptive Statistics\", align = \"c\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 1: Wellbeing Descriptive Statistics\n\nM\nSD\n\n\n36.3\n5.39\n\n\n\n\n\n\n\nFollowing the exploration above, we can describe the wellbeing variable as follows:\n\n\n\n\n\n\nThe marginal distribution of scores on the WEMWBS was unimodal with a mean of approximately 36.3. There was variation in WEMWBS scores (SD = 5.39)\n\n\n\n\n\nVisualisation of distribution:\n\nggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  labs(x = \"Social Interactions (Number per Week)\", \n       y = \"Probability density\")\n\n\n\nFigure 2: Distribution of Number of Social Interactions\n\n\n\nInitial observations from plot:\n\nThe distribution of social interactions was unimodal\nMost of the participants had between 8 and 15 social interactions per week\nThe fewest social interactions per week was approximately 3, and the highest approximately 24. This suggested there was a fair high degree of variation in the data\n\nDescriptive (or summary) statistics for the number of weekly social interactions per week:\n\nmwdata %&gt;% \n  summarize(\n    M = mean(social_int), \n    SD = sd(social_int)\n    ) %&gt;%\n    kable(caption = \"Social Interactions Descriptive Statistics\", align = \"c\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 2: Social Interactions Descriptive Statistics\n\nM\nSD\n\n\n12.06\n4.02\n\n\n\n\n\n\n\nFollowing the exploration above, we can describe the social interactions variable as follows:\n\n\n\n\n\n\nThe marginal distribution of numbers of social interactions per week was unimodal with a mean of approximately 12.06. There was variation in numbers of social interactions (SD = 4.02)"
  },
  {
    "objectID": "1_01_slr.html#associations-among-variables",
    "href": "1_01_slr.html#associations-among-variables",
    "title": "Simple Linear Regression",
    "section": "Associations among Variables",
    "text": "Associations among Variables\n\nQuestion 2\n\n\nCreate a scatterplot of wellbeing score and social interactions before calculating the correlation between them.\nMaking reference to both the plot and correlation coefficient, describe the association between wellbeing and social interactions among participants in the Edinburgh & Lothians sample.\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the many ways to numerically and visually explore your data by reading over the data exploration flashcards.\n\n\n\n\n\n\n\n Solution \n\n\nLet’s produce a scatterplot:\n\nggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  labs(x = \"Social Interactions (Number per Week)\", \n       y = \"Wellbeing (WEMWBS) Scores\")\n\n\n\nFigure 3: Association between Wellbeing and Social Interactions\n\n\n\nTo comment on the strength of the linear association we compute the correlation coefficient in either of the following ways:\n\n\nIndex dataframe ([])\nVariable selection (select())\n\n\n\n\n# correlation matrix of the two columns of interest - (check with columns we need, in this case 3 & 5)\nround(cor(mwdata[,c(3,5)]), digits = 2)\n\n           social_int wellbeing\nsocial_int       1.00      0.24\nwellbeing        0.24      1.00\n\n\n\n\n\n# select only the columns we want by name, and pass this to cor()\nmwdata %&gt;% \n  select(social_int, wellbeing) %&gt;%\n  cor() %&gt;%\n    round(digits = 2)\n\n           social_int wellbeing\nsocial_int       1.00      0.24\nwellbeing        0.24      1.00\n\n\n\n\n\nAnd we can see that via either method, the correlation is \\[\nr_{\\text({Social~Interactions,~~ Wellbeing})} = .24\n\\] \n\n\n\n\n\n\nThere was a weak, positive, linear association between the weekly number of social interactions and WEMWBS scores for the participants in the sample (\\(r\\) = .24). More social interactions were associated, on average, with higher wellbeing scores."
  },
  {
    "objectID": "1_01_slr.html#model-specification-and-fitting",
    "href": "1_01_slr.html#model-specification-and-fitting",
    "title": "Simple Linear Regression",
    "section": "Model Specification and Fitting",
    "text": "Model Specification and Fitting\nThe scatterplot highlighted a linear association, where the data points were scattered around an underlying linear pattern with a roughly-constant spread as x varied.\nHence, we will try to fit a simple (i.e., one x variable only) linear regression model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\\\\n\\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\n\\]\n\nQuestion 3\n\n\nFirst, write the equation of the fitted line.\nNext, using the lm() function, fit a simple linear model to predict wellbeing (DV) by social interactions (IV), naming the output mdl.\nLastly, update your equation of the fitted line to include the estimated coefficients.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee the statistical models flashcards for a reminder on how to specify models.\nFor how to format and write your model in RMarkdown, see the LaTeX symbols and equations flashcard.\n\n\n\n\n\n\n\n Solution \n\n\nFirst, lets specify the fitted model, which can be written either as:\n\n\nOption A\nOption B\n\n\n\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Social~Interactions\n\\]\n\n\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 \\cdot 1 + \\hat \\beta_1 \\cdot Social~Interactions\n\\]\n\n\n\nTo fit the model in R, as the variables are in the mwdata dataframe, we would write:\n\n\nOption A\nOption B\n\n\n\n\nmdl &lt;- lm(wellbeing ~ social_int, data = mwdata)\nmdl\n\n\nCall:\nlm(formula = wellbeing ~ social_int, data = mwdata)\n\nCoefficients:\n(Intercept)   social_int  \n    32.4077       0.3222  \n\n\n\n\n\nmdl &lt;- lm(wellbeing ~ 1 + social_int, data = mwdata)\nmdl\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nCoefficients:\n(Intercept)   social_int  \n    32.4077       0.3222  \n\n\n\n\n\nNote that by calling the name of the fitted model, mdl, you can see the estimated regression coefficients \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). We can add these values to the fitted line:\n\\[\n\\widehat{Wellbeing} = 32.41 + 0.32 \\cdot Social~Interactions \\\\\n\\]\n\n\n\n\n\nQuestion 4\n\n\nExplore the following equivalent ways to obtain the estimated regression coefficients — that is, \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) — from the fitted model:\n\nmdl\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\n\n\n Solution \n\n\nThe estimated parameters returned by the below methods are all equivalent. However, summary() returns more information.\n\n\nmdl\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\nSimply invoke the name of the fitted model:\n\nmdl\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nCoefficients:\n(Intercept)   social_int  \n    32.4077       0.3222  \n\n\n\n\n\nmdl$coefficients\n\n(Intercept)  social_int \n 32.4077070   0.3221959 \n\n\n\n\n\ncoef(mdl)\n\n(Intercept)  social_int \n 32.4077070   0.3221959 \n\n\n\n\n\ncoefficients(mdl)\n\n(Intercept)  social_int \n 32.4077070   0.3221959 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5628  -3.2741  -0.7908   3.3703  20.4706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 32.40771    1.17532  27.573  &lt; 2e-16 ***\nsocial_int   0.32220    0.09243   3.486 0.000605 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.247 on 198 degrees of freedom\nMultiple R-squared:  0.05781,   Adjusted R-squared:  0.05306 \nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n\n\n\n\n\n\n\n\n\n\n\nThe estimated intercept is \\(\\hat \\beta_0 = 32.41\\) and the estimated slope is \\(\\hat \\beta_1 = 0.32\\).\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nExplore the following equivalent ways to obtain the estimated standard deviation of the errors — that is, \\(\\hat \\sigma\\) — from the fitted model mdl:\n\nsigma(mdl)\nsummary(mdl)\n\n\n\n\n\n Solution \n\n\nThe estimated standard deviation of the errors can be equivalently obtained by the below methods. However, summary() returns more information.\n\n\nsigma(mdl)\nsummary(mdl)\n\n\n\n\nsigma(mdl)\n\n[1] 5.246982\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5628  -3.2741  -0.7908   3.3703  20.4706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 32.40771    1.17532  27.573  &lt; 2e-16 ***\nsocial_int   0.32220    0.09243   3.486 0.000605 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.247 on 198 degrees of freedom\nMultiple R-squared:  0.05781,   Adjusted R-squared:  0.05306 \nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term “Residual standard error” is a misnomer, as the help page for sigma says (check ?sigma). However, it’s hard to get rid of this bad name as it has been used in too many books showing R output. For more information, check the simple & multiple regression models - extracting information flashcard flashcard.\n\n\n\n\n\n\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma = 5.25\\).\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nInterpret the estimated intercept, slope, and standard deviation of the errors in the context of the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo interpret the estimated standard deviation of the errors we can use the fact that about 95% of values from a normal distribution fall within two standard deviations of the center.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nIntercept\nSlope\nStandard deviation of the errors\n\n\n\nThe estimated wellbeing score associated with zero weekly social interactions is 32.41.\n\n\nThe estimated increase in wellbeing associated with one additional weekly social interaction is 0.32.\n\n\nFor any particular number of weekly social interactions, participants’ wellbeing scores should be distributed above and below the regression line with standard deviation estimated to be \\(\\hat \\sigma = 5.25\\). Since \\(2 \\hat \\sigma = 10.49\\), we expect most (about 95%) of the participants’ wellbeing scores to be within about 11 points from the regression line.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nPlot the data and the fitted regression line. To do so:\n\nExtract the estimated regression coefficients e.g., via betas &lt;- coef(mdl)\n\nExtract the first entry of betas (i.e., the intercept) via betas[1]\n\nExtract the second entry of betas (i.e., the slope) via betas[2]\n\nProvide the intercept and slope to the function\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nExtracting values\nThe function coef(mdl) returns a vector (a sequence of numbers all of the same type). To get the first element of the sequence you append [1], and [2] for the second.\nPlotting\nIn your ggplot(), you will need to specify geom_abline(). This might help get you started:\n\n geom_abline(intercept = intercept, slope = slope) \n\n\nFor further ggplot() guidance, see the how to visualise data flashcard.\n\n\n\n\n\n\n\n Solution \n\n\nFirst extract the values required:\n\nbetas &lt;- coef(mdl)\nintercept &lt;- betas[1]\nslope &lt;- betas[2]\n\nWe can plot the model as follows:\n\nggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_abline(intercept = intercept, slope = slope, color = 'blue') + \n  labs(x = \"Social Interactions (Number per Week)\", y = \"Wellbeing (WEMWBS) Scores\")"
  },
  {
    "objectID": "1_01_slr.html#predicted-values-residuals",
    "href": "1_01_slr.html#predicted-values-residuals",
    "title": "Simple Linear Regression",
    "section": "Predicted Values & Residuals",
    "text": "Predicted Values & Residuals\n\nQuestion 8\n\n\nUse predict(mdl) to compute the fitted values and residuals. Mutate the mwdata dataframe to include the fitted values and residuals as extra columns.\nAssign to the following symbols the corresponding numerical values:\n\n\n\\(y_{3}\\) (response variable for unit \\(i = 3\\) in the sample data)\n\n\\(\\hat y_{3}\\) (fitted value for the third unit)\n\n\\(\\hat \\epsilon_{5}\\) (the residual corresponding to the 5th unit, i.e., \\(y_{5} - \\hat y_{5}\\))\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nFor a more detailed description and worked example, check the model predicted values & residuals flashcards.\n\n\n\n\n\n\n\n Solution \n\n\n\nmwdata_fitted &lt;- mwdata %&gt;%\n  mutate(\n    wellbeing_hat = predict(mdl),\n    resid = wellbeing - wellbeing_hat\n  )\n\nhead(mwdata_fitted)\n\n# A tibble: 6 × 9\n    age outdoor_time social_int routine wellbeing location steps_k wellbeing_hat\n  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1    28           12         13       1        36 rural       21.6          36.6\n2    56            5         15       1        41 rural       12.3          37.2\n3    25           19         11       1        35 rural       49.8          36.0\n4    60           25         15       0        35 rural       NA            37.2\n5    19            9         18       1        32 rural       48.1          38.2\n6    34           18         13       1        34 rural       67.3          36.6\n# ℹ 1 more variable: resid &lt;dbl&gt;\n\n\n\n\n\\(y_{3}\\) = 35 (see row 3, column 5)\n\n\\(\\hat y_{3}\\) = 35.95 (see row 3, column 8)\n\n\\(\\hat \\epsilon_{5} = y_{5} - \\hat y_{5}\\) = 32 - 38.21 = -6.21 (see row 5, columns 5 and 8)"
  },
  {
    "objectID": "1_01_slr.html#writing-up-presenting-results",
    "href": "1_01_slr.html#writing-up-presenting-results",
    "title": "Simple Linear Regression",
    "section": "Writing Up & Presenting Results",
    "text": "Writing Up & Presenting Results\n\nQuestion 9\n\n\nProvide key model results in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package. For a quick guide, review the tables flashcard.\n\n\n\n\n\n\n\n Solution \n\n\n\ntab_model(mdl,\n          dv.labels = \"Wellbeing (WEMWBS) Scores\",\n          pred.labels = c(\"social_int\" = \"Social Interactions (Number per Week)\"),\n          title = \"Regression Table for Wellbeing Model\")\n\n\n\nTable 3: Regression Table for Wellbeing Model\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS) Scores\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n32.41\n30.09 – 34.73\n&lt;0.001\n\n\nSocial Interactions\n(Number per Week)\n0.32\n0.14 – 0.50\n0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.058 / 0.053\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nDescribe the design of the study (see Study Overview codebook), and the analyses that you undertook. Interpret your results in the context of the research question and report your model results in full.\nMake reference to your descriptive plots and/or statistics and regression table.\n\n\n\n\n Solution \n\n\n\n\n\n\n\n\nThe mwdata dataset contained information on 200 hypothetical participants who lived in Edinburgh & Lothians area. Using a between-subjects design, the researchers collected information on participants’ wellbeing (measured via WEMWBS), outdoor time (hours per week), social interactions (number per week), routine (whether or not one was followed), location of residence (City, Suburb, or Rural), average weekly steps (in thousands), and age (in years).\nTo visualise the marginal distributions of wellbeing and social interactions, density plots were used. To understand the strength of association between the two variables, the correlation coefficient was estimated. To investigate whether the number of weekly social interactions influences wellbeing (WEMWBS) scores, the following simple linear regression model was used:\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social Interactions}\n\\] From Figure 1 and Figure 2, we can see that both wellbeing \\((M = 36.3, SD = 5.39)\\) and social interactions \\((M = 12.06, SD = 4.02)\\) followed unimodal distributions. There was a weak, positive, linear association between WEMWBS scores and the weekly number of social interactions for the participants in the sample \\((r = .24)\\).\nFull regression results are displayed in Table 3. There was a significant association between wellbeing scores and social interactions \\((\\beta = 0.32, SE = 0.09, p &lt; .001)\\). The estimated wellbeing score with no social interactions per week was 32.41. Each additional social interaction was associated with a 0.32 point increase in wellbeing scores."
  },
  {
    "objectID": "1_01_slrBANANA.html",
    "href": "1_01_slrBANANA.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to specify a simple linear model\n\nUnderstand what fitted values and residuals are\nBe able to interpret the coefficients of a fitted model\n\n\nBe up to date with lectures\n\nHave watched course intro video in Week 0 folder, and completed associated tasks\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\nkableExtra\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\nThe example write-up sections included as part of the solutions are not perfect - they instead should give you a good example of what information you should include and how to structure this. Note that you must not copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more here.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv"
  },
  {
    "objectID": "1_01_slrBANANA.html#marginal-distributions",
    "href": "1_01_slrBANANA.html#marginal-distributions",
    "title": "Simple Linear Regression",
    "section": "Marginal Distributions",
    "text": "Marginal Distributions\n\nQuestion 1\n\n\nVisualise and describe the marginal distributions of wellbeing scores and social interactions.\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the many ways to numerically and visually explore your data by reading over the data exploration flashcards.\nFor examples of mariginal distrubtion visualisation examples, see the data visualisation - marginal examples flashcards.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nWellbeing (WEMWBS) Scores\nSocial Interactions\n\n\n\nVisualisation of distribution:\n\nggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  labs(x = \"Wellbeing (WEMWBS) Scores\", \n       y = \"Probability density\")\n\n\n\nFigure 1: Distribution of Wellbeing (WEMWBS) Scores\n\n\n\nInitial observations from plot:\n\nThe distribution of wellbeing scores was unimodal\nMost of the wellbeing scores were between roughly 30 and 45\nThe lowest wellbeing in the sample was approximately 22 and the highest approximately 59. This suggested there was a fair high degree of variation in the data\nScores were within the range of possible values\n\nDescriptive (or summary) statistics for wellbeing scores:\n\nmwdata %&gt;% \n  summarize(\n    M = mean(wellbeing), \n    SD = sd(wellbeing)\n    ) %&gt;%\n    kable(caption = \"Wellbeing Descriptive Statistics\", align = \"c\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 1: Wellbeing Descriptive Statistics\n\nM\nSD\n\n\n36.3\n5.39\n\n\n\n\n\n\n\nFollowing the exploration above, we can describe the wellbeing variable as follows:\n\n\n\n\n\n\nThe marginal distribution of scores on the WEMWBS was unimodal with a mean of approximately 36.3. There was variation in WEMWBS scores (SD = 5.39)\n\n\n\n\n\nVisualisation of distribution:\n\nggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  labs(x = \"Social Interactions (Number per Week)\", \n       y = \"Probability density\")\n\n\n\nFigure 2: Distribution of Number of Social Interactions\n\n\n\nInitial observations from plot:\n\nThe distribution of social interactions was unimodal\nMost of the participants had between 8 and 15 social interactions per week\nThe fewest social interactions per week was approximately 3, and the highest approximately 24. This suggested there was a fair high degree of variation in the data\n\nDescriptive (or summary) statistics for the number of weekly social interactions per week:\n\nmwdata %&gt;% \n  summarize(\n    M = mean(social_int), \n    SD = sd(social_int)\n    ) %&gt;%\n    kable(caption = \"Social Interactions Descriptive Statistics\", align = \"c\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 2: Social Interactions Descriptive Statistics\n\nM\nSD\n\n\n12.06\n4.02\n\n\n\n\n\n\n\nFollowing the exploration above, we can describe the social interactions variable as follows:\n\n\n\n\n\n\nThe marginal distribution of numbers of social interactions per week was unimodal with a mean of approximately 12.06. There was variation in numbers of social interactions (SD = 4.02)"
  },
  {
    "objectID": "1_01_slrBANANA.html#associations-among-variables",
    "href": "1_01_slrBANANA.html#associations-among-variables",
    "title": "Simple Linear Regression",
    "section": "Associations among Variables",
    "text": "Associations among Variables\n\nQuestion 2\n\n\nCreate a scatterplot of wellbeing score and social interactions before calculating the correlation between them.\nMaking reference to both the plot and correlation coefficient, describe the association between wellbeing and social interactions among participants in the Edinburgh & Lothians sample.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot\nWe are trying to investigate how wellbeing varies by varying numbers of weekly social interactions. Hence, wellbeing is the dependent variable (on the y-axis), and social interactions is the independent variable (on the x-axis). Checking the bivariate examples flashcard might help.\nCorrelation\nHave a look at this set of flashcards for all things correlation. Remember to round your numbers in-line with APA 7th edition guidelines. The round() function will come in handy here.\n\n\n\n\n\n\n\n Solution \n\n\nLet’s produce a scatterplot:\n\nggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  labs(x = \"Social Interactions (Number per Week)\", \n       y = \"Wellbeing (WEMWBS) Scores\")\n\n\n\nFigure 3: Association between Wellbeing and Social Interactions\n\n\n\nTo comment on the strength of the linear association we compute the correlation coefficient in either of the following ways:\n\n\nIndex dataframe ([])\nVariable selection (select())\n\n\n\n\n# correlation matrix of the two columns of interest - (check with columns we need, in this case 3 & 5)\nround(cor(mwdata[,c(3,5)]), digits = 2)\n\n           social_int wellbeing\nsocial_int       1.00      0.24\nwellbeing        0.24      1.00\n\n\n\n\n\n# select only the columns we want by name, and pass this to cor()\nmwdata %&gt;% \n  select(social_int, wellbeing) %&gt;%\n  cor() %&gt;%\n    round(digits = 2)\n\n           social_int wellbeing\nsocial_int       1.00      0.24\nwellbeing        0.24      1.00\n\n\n\n\n\nAnd we can see that via either method, the correlation is \\[\nr_{\\text({Social~Interactions,~~ Wellbeing})} = .24\n\\] \n\n\n\n\n\n\nThere was a weak, positive, linear association between the weekly number of social interactions and WEMWBS scores for the participants in the sample (\\(r\\) = .24). More social interactions were associated, on average, with higher wellbeing scores."
  },
  {
    "objectID": "1_01_slrBANANA.html#model-specification-and-fitting",
    "href": "1_01_slrBANANA.html#model-specification-and-fitting",
    "title": "Simple Linear Regression",
    "section": "Model Specification and Fitting",
    "text": "Model Specification and Fitting\nThe scatterplot highlighted a linear association, where the data points were scattered around an underlying linear pattern with a roughly-constant spread as x varied.\nHence, we will try to fit a simple (i.e., one x variable only) linear regression model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\\\\n\\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\n\\]\n\nQuestion 3\n\n\nFirst, write the equation of the fitted line.\nNext, using the lm() function, fit a simple linear model to predict wellbeing (DV) by social interactions (IV), naming the output mdl.\nLastly, update your equation of the fitted line to include the estimated coefficients.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee the statistical models flashcards for a reminder on how to specify models.\nFor how to format and write your model in RMarkdown, see the LaTeX symbols and equations flashcard.\n\n\n\n\n\n\n\n Solution \n\n\nFirst, lets specify the fitted model, which can be written either as:\n\n\nOption A\nOption B\n\n\n\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Social~Interactions\n\\]\n\n\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 \\cdot 1 + \\hat \\beta_1 \\cdot Social~Interactions\n\\]\n\n\n\nTo fit the model in R, as the variables are in the mwdata dataframe, we would write:\n\n\nOption A\nOption B\n\n\n\n\nmdl &lt;- lm(wellbeing ~ social_int, data = mwdata)\nmdl\n\n\nCall:\nlm(formula = wellbeing ~ social_int, data = mwdata)\n\nCoefficients:\n(Intercept)   social_int  \n    32.4077       0.3222  \n\n\n\n\n\nmdl &lt;- lm(wellbeing ~ 1 + social_int, data = mwdata)\nmdl\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nCoefficients:\n(Intercept)   social_int  \n    32.4077       0.3222  \n\n\n\n\n\nNote that by calling the name of the fitted model, mdl, you can see the estimated regression coefficients \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). We can add these values to the fitted line:\n\\[\n\\widehat{Wellbeing} = 32.41 + 0.32 \\cdot Social~Interactions \\\\\n\\]\n\n\n\n\n\nQuestion 4\n\n\nExplore the following equivalent ways to obtain the estimated regression coefficients — that is, \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) — from the fitted model:\n\nmdl\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\n\n\n Solution \n\n\nThe estimated parameters returned by the below methods are all equivalent. However, summary() returns more information.\n\n\nmdl\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\nSimply invoke the name of the fitted model:\n\nmdl\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nCoefficients:\n(Intercept)   social_int  \n    32.4077       0.3222  \n\n\n\n\n\nmdl$coefficients\n\n(Intercept)  social_int \n 32.4077070   0.3221959 \n\n\n\n\n\ncoef(mdl)\n\n(Intercept)  social_int \n 32.4077070   0.3221959 \n\n\n\n\n\ncoefficients(mdl)\n\n(Intercept)  social_int \n 32.4077070   0.3221959 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5628  -3.2741  -0.7908   3.3703  20.4706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 32.40771    1.17532  27.573  &lt; 2e-16 ***\nsocial_int   0.32220    0.09243   3.486 0.000605 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.247 on 198 degrees of freedom\nMultiple R-squared:  0.05781,   Adjusted R-squared:  0.05306 \nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n\n\n\n\n\n\n\n\n\n\n\nThe estimated intercept is \\(\\hat \\beta_0 = 32.41\\) and the estimated slope is \\(\\hat \\beta_1 = 0.32\\).\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nExplore the following equivalent ways to obtain the estimated standard deviation of the errors — that is, \\(\\hat \\sigma\\) — from the fitted model mdl:\n\nsigma(mdl)\nsummary(mdl)\n\n\n\n\n\n Solution \n\n\nThe estimated standard deviation of the errors can be equivalently obtained by the below methods. However, summary() returns more information.\n\n\nsigma(mdl)\nsummary(mdl)\n\n\n\n\nsigma(mdl)\n\n[1] 5.246982\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5628  -3.2741  -0.7908   3.3703  20.4706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 32.40771    1.17532  27.573  &lt; 2e-16 ***\nsocial_int   0.32220    0.09243   3.486 0.000605 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.247 on 198 degrees of freedom\nMultiple R-squared:  0.05781,   Adjusted R-squared:  0.05306 \nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term “Residual standard error” is a misnomer, as the help page for sigma says (check ?sigma). However, it’s hard to get rid of this bad name as it has been used in too many books showing R output. For more information, check the simple & multiple regression models - extracting information flashcard flashcard.\n\n\n\n\n\n\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma = 5.25\\).\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nInterpret the estimated intercept, slope, and standard deviation of the errors in the context of the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo interpret the estimated standard deviation of the errors we can use the fact that about 95% of values from a normal distribution fall within two standard deviations of the center.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nIntercept\nSlope\nStandard deviation of the errors\n\n\n\nThe estimated wellbeing score associated with zero weekly social interactions is 32.41.\n\n\nThe estimated increase in wellbeing associated with one additional weekly social interaction is 0.32.\n\n\nFor any particular number of weekly social interactions, participants’ wellbeing scores should be distributed above and below the regression line with standard deviation estimated to be \\(\\hat \\sigma = 5.25\\). Since \\(2 \\hat \\sigma = 10.49\\), we expect most (about 95%) of the participants’ wellbeing scores to be within about 11 points from the regression line.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nPlot the data and the fitted regression line. To do so:\n\nExtract the estimated regression coefficients e.g., via betas &lt;- coef(mdl)\n\nExtract the first entry of betas (i.e., the intercept) via betas[1]\n\nExtract the second entry of betas (i.e., the slope) via betas[2]\n\nProvide the intercept and slope to the function\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nExtracting values\nThe function coef(mdl) returns a vector (a sequence of numbers all of the same type). To get the first element of the sequence you append [1], and [2] for the second.\nPlotting\nIn your ggplot(), you will need to specify geom_abline(). This might help get you started:\n\n geom_abline(intercept = intercept, slope = slope) \n\n\nFor further ggplot() guidance, see the how to visualise data flashcard.\n\n\n\n\n\n\n\n Solution \n\n\nFirst extract the values required:\n\nbetas &lt;- coef(mdl)\nintercept &lt;- betas[1]\nslope &lt;- betas[2]\n\nWe can plot the model as follows:\n\nggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_abline(intercept = intercept, slope = slope, color = 'blue') + \n  labs(x = \"Social Interactions (Number per Week)\", y = \"Wellbeing (WEMWBS) Scores\")"
  },
  {
    "objectID": "1_01_slrBANANA.html#predicted-values-residuals",
    "href": "1_01_slrBANANA.html#predicted-values-residuals",
    "title": "Simple Linear Regression",
    "section": "Predicted Values & Residuals",
    "text": "Predicted Values & Residuals\n\nQuestion 8\n\n\nUse predict(mdl) to compute the fitted values and residuals. Mutate the mwdata dataframe to include the fitted values and residuals as extra columns.\nAssign to the following symbols the corresponding numerical values:\n\n\n\\(y_{3}\\) (response variable for unit \\(i = 3\\) in the sample data)\n\n\\(\\hat y_{3}\\) (fitted value for the third unit)\n\n\\(\\hat \\epsilon_{5}\\) (the residual corresponding to the 5th unit, i.e., \\(y_{5} - \\hat y_{5}\\))\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nFor a more detailed description and worked example, check the model predicted values & residuals flashcards.\n\n\n\n\n\n\n\n Solution \n\n\n\nmwdata_fitted &lt;- mwdata %&gt;%\n  mutate(\n    wellbeing_hat = predict(mdl),\n    resid = wellbeing - wellbeing_hat\n  )\n\nhead(mwdata_fitted)\n\n# A tibble: 6 × 9\n    age outdoor_time social_int routine wellbeing location steps_k wellbeing_hat\n  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1    28           12         13       1        36 rural       21.6          36.6\n2    56            5         15       1        41 rural       12.3          37.2\n3    25           19         11       1        35 rural       49.8          36.0\n4    60           25         15       0        35 rural       NA            37.2\n5    19            9         18       1        32 rural       48.1          38.2\n6    34           18         13       1        34 rural       67.3          36.6\n# ℹ 1 more variable: resid &lt;dbl&gt;\n\n\n\n\n\\(y_{3}\\) = 35 (see row 3, column 5)\n\n\\(\\hat y_{3}\\) = 35.95 (see row 3, column 8)\n\n\\(\\hat \\epsilon_{5} = y_{5} - \\hat y_{5}\\) = 32 - 38.21 = -6.21 (see row 5, columns 5 and 8)"
  },
  {
    "objectID": "1_01_slrBANANA.html#writing-up-presenting-results",
    "href": "1_01_slrBANANA.html#writing-up-presenting-results",
    "title": "Simple Linear Regression",
    "section": "Writing Up & Presenting Results",
    "text": "Writing Up & Presenting Results\n\nQuestion 9\n\n\nProvide key model results in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package. For a quick guide, review the tables flashcard.\n\n\n\n\n\n\n\n Solution \n\n\n\ntab_model(mdl,\n          dv.labels = \"Wellbeing (WEMWBS) Scores\",\n          pred.labels = c(\"social_int\" = \"Social Interactions (Number per Week)\"),\n          title = \"Regression Table for Wellbeing Model\")\n\n\n\nTable 3: Regression Table for Wellbeing Model\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS) Scores\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n32.41\n30.09 – 34.73\n&lt;0.001\n\n\nSocial Interactions\n(Number per Week)\n0.32\n0.14 – 0.50\n0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.058 / 0.053\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nDescribe the design of the study (see Study Overview codebook), and the analyses that you undertook. Interpret your results in the context of the research question and report your model results in full.\nMake reference to your descriptive plots and/or statistics and regression table.\n\n\n\n\n Solution \n\n\n\n\n\n\n\n\nThe mwdata dataset contained information on 200 hypothetical participants who lived in Edinburgh & Lothians area. Using a between-subjects design, the researchers collected information on participants’ wellbeing (measured via WEMWBS), outdoor time (hours per week), social interactions (number per week), routine (whether or not one was followed), location of residence (City, Suburb, or Rural), average weekly steps (in thousands), and age (in years).\nTo visualise the marginal distributions of wellbeing and social interactions, density plots were used. To understand the strength of association between the two variables, the correlation coefficient was estimated. To investigate whether the number of weekly social interactions influences wellbeing (WEMWBS) scores, the following simple linear regression model was used:\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social Interactions}\n\\] From Figure 1 and Figure 2, we can see that both wellbeing \\((M = 36.3, SD = 5.39)\\) and social interactions \\((M = 12.06, SD = 4.02)\\) followed unimodal distributions. There was a weak, positive, linear association between WEMWBS scores and the weekly number of social interactions for the participants in the sample \\((r = .24)\\).\nFull regression results are displayed in Table 3. There was a significant association between wellbeing scores and social interactions \\((\\beta = 0.32, SE = 0.09, p &lt; .001)\\). The estimated wellbeing score with no social interactions per week was 32.41. Each additional social interaction was associated with a 0.32 point increase in wellbeing scores."
  },
  {
    "objectID": "1_02_mlr.html",
    "href": "1_02_mlr.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nExtend the ideas of single linear regression to consider regression models with two or more predictors\nUnderstand and interpret the coefficients in multiple linear regression models\n\n\nBe up to date with lectures\nHave completed Week 1 lab exercises\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npsych\npatchwork\nsjPlot\nkableExtra\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\nThe example write-up sections included as part of the solutions are not perfect - they instead should give you a good example of what information you should include and how to structure this. Note that you must not copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more here.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv"
  },
  {
    "objectID": "1_02_mlr.html#study-analysis-plan-overview",
    "href": "1_02_mlr.html#study-analysis-plan-overview",
    "title": "Multiple Linear Regression",
    "section": "Study & Analysis Plan Overview",
    "text": "Study & Analysis Plan Overview\n\nQuestion 1\n\n\nProvide a brief overview of the study design and data, before detailing your analysis plan to address the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nGive the reader some background on the context of the study (you might be able to re-use some of the content you wrote for Lab 1 Q10 here)\nState what type of analysis you will conduct in order to address the research question\nSpecify the model to be fitted to address the research question\nSpecify your chosen significance (\\(\\alpha\\)) level\nState your hypotheses\n\nMuch of the information required can be found in the Study Overview codebook. The statistical models flashcards may also be useful to refer to.\n\n\n\n\n\n\n\n Solution \n\n\nThe mwdata dataset contained information on 200 hypothetical participants who lived in Edinburgh & Lothians area. Using a between-subjects design, the researchers collected information on participants’ wellbeing (measured via WEMWBS), outdoor time (hours per week), social interactions (number per week), routine (whether or not one was followed), location of residence (City, Suburb, or Rural), average weekly steps (in thousands), and age (in years).\nDensity plots and boxplots will be used to visualise the marginal distributions of wellbeing, social interactions, and outdoor time. To understand the strength of association among the variables, we will estimate the the correlation coefficients. To address the research question of whether there is an association between wellbeing and time spent outdoors after taking into account the association between wellbeing and social interactions, we are going to fit the following multiple linear regression model:\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social Interactions} + \\beta_2 \\cdot \\text{Outdoor Time} + \\epsilon\n\\]\nEffects will be considered statistically significant at \\(\\alpha=.05\\).\nOur hypotheses are:\n\\(H_0: \\beta_2 = 0\\): There is no association between wellbeing and time spent outdoors after taking into account the association between wellbeing and social interactions\n\\(H_1: \\beta_2 \\neq 0\\): There is an association between wellbeing and time spent outdoors after taking into account the association between wellbeing and social interactions"
  },
  {
    "objectID": "1_02_mlr.html#descriptive-statistics-visualisations",
    "href": "1_02_mlr.html#descriptive-statistics-visualisations",
    "title": "Multiple Linear Regression",
    "section": "Descriptive Statistics & Visualisations",
    "text": "Descriptive Statistics & Visualisations\n\nQuestion 2\n\n\nAlongside descriptive statistics, visualize the marginal distributions of the wellbeing, outdoor_time, and social_int variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the many ways to numerically and visually explore your data by reading over the data exploration flashcards.\nFor examples, see flashcards on descriptives statistics tables and data visualisation - marginal.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nNumeric\nVisual\n\n\n\nWe can present our summary statistics for wellbeing, outdoor time, and social interactions as a well formatted table using kable():\n\nmwdata %&gt;% \n  select(wellbeing, outdoor_time, social_int) %&gt;%\n    describe() %&gt;%\n    kable(caption = \"Wellbeing, Social Interactions, and Outdoor Time Descriptive Statistics\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 1: Wellbeing, Social Interactions, and Outdoor Time Descriptive Statistics\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\nwellbeing\n1\n200\n36.30\n5.39\n35\n36.07\n4.45\n22\n59\n37\n0.58\n0.92\n0.38\n\n\noutdoor_time\n2\n200\n18.25\n7.10\n18\n18.14\n7.41\n1\n35\n34\n0.06\n-0.62\n0.50\n\n\nsocial_int\n3\n200\n12.06\n4.02\n12\n11.96\n4.45\n3\n24\n21\n0.21\n-0.40\n0.28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe marginal distribution of scores on the WEMWBS was unimodal with a mean of approximately 36.3. There was variation in WEMWBS scores (SD = 5.39)\n\nThe marginal distribution of weekly hours spent outdoors was unimodal with a mean of approximately 18.25. There was variation in weekly hours spent outdoors (SD = 7.1)\nThe marginal distribution of numbers of social interactions per week was unimodal with a mean of approximately 12.06. There was variation in numbers of social interactions (SD = 4.02)\n\n\n\n\n\n\nYou should be familiar now with how to visualise a marginal distribution:\n\nwellbeing_plot &lt;- \n  ggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  labs(x = \"Score on WEMWBS (range 14-70)\", y = \"Density\")\n\noutdoortime_plot &lt;- \n  ggplot(data = mwdata, aes(x = outdoor_time)) +\n  geom_density() +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Density\")\n\nsocial_plot &lt;- \n  ggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  labs(x = \"Number of social interactions per week\", y = \"Density\")\n\n# arrange plots vertically \nwellbeing_plot / outdoortime_plot / social_plot\n\n\n\nFigure 1: Marginal distribution plots of wellbeing sores, weekly hours spent outdoors, and social interactions\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nProduce plots of the associations between the outcome variable (wellbeing) and each of the explanatory variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview how to visually explore bivariate associations via the data visualisation flashcards.\nFor specifically visualising associations between variables, see the bivariate examples flashcards.\n\n\n\n\n\n\n\n Solution \n\n\n\nwellbeing_outdoor &lt;- \n  ggplot(data = mwdata, aes(x = outdoor_time, y = wellbeing)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Time spent outdoors \\nper week (hours)\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_social &lt;- \n  ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Number of social interactions \\nper week\", y = \"Wellbeing score (WEMWBS)\")\n\n# place plots adjacent to one another\nwellbeing_outdoor | wellbeing_social\n\n\n\nFigure 2: Scatterplots displaying the relationships between scores on the WEMWBS and a) weekly outdoor time (hours), and b) weekly number of social interactions\n\n\n\nBoth scatterplots indicated weak, positive, and linear associations both between wellbeing and outdoor time, and between wellbeing and the number of weekly social interactions.\n\n\n\n\n\nQuestion 4\n\n\nProduce a correlation matrix of the variables which are to be used in the analysis, and write a short paragraph describing the associations.\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the correlation flashcards, and remember to interpret in the context of the research question.\n\n\n\n\n\n\n\n Solution \n\n\nWe can either index the dataframe or select the variables of interest:\n\n\nIndex dataframe ([])\nVariable selection (select())\n\n\n\n\n# correlation matrix of the three columns of interest (check which columns we need - in this case, 2,3, and 5)\nround(cor(mwdata[,c(5,3,2)]), digits = 2)\n\n             wellbeing social_int outdoor_time\nwellbeing         1.00       0.24         0.25\nsocial_int        0.24       1.00        -0.04\noutdoor_time      0.25      -0.04         1.00\n\n\n\n\n\n# select only the columns we want by variable name, and pass this to cor()\nmwdata %&gt;% \n  select(wellbeing, social_int, outdoor_time) %&gt;%\n  cor() %&gt;%\n  round(digits = 2)\n\n             wellbeing social_int outdoor_time\nwellbeing         1.00       0.24         0.25\nsocial_int        0.24       1.00        -0.04\noutdoor_time      0.25      -0.04         1.00\n\n\n\n\n\n\n\n\n\n\n\n\nThere was a weak, positive, linear association between WEMWBS scores and weekly outdoor time for the participants in the sample (\\(r\\) = .25). Higher number of hours spent outdoors each week was associated, on average, with higher wellbeing scores\n\nThere was a weak, positive, linear association between WEMWBS scores and the weekly number of social interactions for the participants in the sample (\\(r\\) = .24). More social interactions were associated, on average, with higher wellbeing scores\nThere was a negligible negative correlation between weekly outdoor time and the weekly number of social interactions (\\(r\\) = -.04)"
  },
  {
    "objectID": "1_02_mlr.html#model-fitting-interpretation",
    "href": "1_02_mlr.html#model-fitting-interpretation",
    "title": "Multiple Linear Regression",
    "section": "Model Fitting & Interpretation",
    "text": "Model Fitting & Interpretation\n\nQuestion 5\n\n\nRecall the model specified in Q1, and:\n\nState the parameters of the model. How do we denote parameter estimates?\n\nFit the linear model in using lm(), assigning the output to an object called mdl1.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAs we did for simple linear regression, we can fit our multiple regression model using the lm() function. For a recap, see the statistical models flashcards.\n\n\n\n\n\n\n\n Solution \n\n\nA model for the association between \\(x_1\\) = weekly numbers of social interactions, \\(x_2\\) = weekly outdoor time, and \\(y\\) = scores on the WEMWBS can be given by: \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\\\ \\quad \\\\\n\\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nIn the model specified above,\n\n\n\\(\\mu_{y|x_1, x_2} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) represents the systematic part of the model giving the mean of \\(y\\) at each combination of values of \\(x_1\\) and \\(x_2\\);\n\n\\(\\epsilon\\) represents the error (deviation) from that mean, and the errors are independent from one another.\n\nThe parameters of our model are:\n\n\n\\(\\beta_0\\) (The intercept);\n\n\\(\\beta_1\\) (The slope across values of \\(x_1\\));\n\n\\(\\beta_2\\) (The slope across values of \\(x_2\\));\n\n\n\\(\\sigma\\) (The standard deviation of the errors).\n\nWhen we estimate these parameters from the available data, we have a fitted model (recall that the h\\(\\hat{\\textrm{a}}\\)ts are used to distinguish our estimates from the true unknown parameters):\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Social~Interactions + \\hat \\beta_2 \\cdot Outdoor~Time\n\\] And we have residuals \\(\\hat \\epsilon = y - \\hat y\\) which are the deviations from the observed values and our model-predicted responses.\nFitting the model in R:\n\nmdl1 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\n\n\n\n\n\n\nAdditional Visual Explanation\nNote that for simple linear regression we talked about our model as a line in 2 dimensions: the systematic part \\(\\beta_0 + \\beta_1 x\\) defined a line for \\(\\mu_y\\) across the possible values of \\(x\\), with \\(\\epsilon\\) as the random deviations from that line. But in multiple regression we have more than two variables making up our model.\nIn this particular case of three variables (one outcome + two explanatory), we can think of our model as a regression surface (see Figure 3). The systematic part of our model defines the surface across a range of possible values of both \\(x_1\\) and \\(x_2\\). Deviations from the surface are determined by the random error component, \\(\\hat \\epsilon\\).\n\n\n\n\nFigure 3: Regression surface for wellbeing ~ social_int + outdoor_time, from two different angles\n\n\n\nDon’t worry about trying to figure out how to visualise it if we had any more explanatory variables! We can only concieve of 3 spatial dimensions. One could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it’s not worth trying!\n\n\n\nQuestion 6\n\n\nUsing any of:\n\nmdl1\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\nWrite out the estimated parameter values of:\n\n\n\\(\\hat \\beta_0\\), the estimated average wellbeing score associated with zero hours of outdoor time and zero social interactions per week.\n\n\n\\(\\hat \\beta_1\\), the estimated increase in average wellbeing score associated with an additional social interaction per week (an increase of one), holding weekly outdoor time constant.\n\n\n\\(\\hat \\beta_2\\), the estimated increase in average wellbeing score associated with one hour increase in weekly outdoor time, holding the number of social interactions constant\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo interpret and extract information, see the simple & multiple regression models - extracing information flashcards\n\n\n\n\n\n\n\n Solution \n\n\n\n\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\n\n\n\nmdl1$coefficients\n\n (Intercept)   social_int outdoor_time \n  28.6201808    0.3348821    0.1990943 \n\n\n\n\n\ncoef(mdl1)\n\n (Intercept)   social_int outdoor_time \n  28.6201808    0.3348821    0.1990943 \n\n\n\n\n\ncoefficients(mdl1)\n\n (Intercept)   social_int outdoor_time \n  28.6201808    0.3348821    0.1990943 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\n\n\n\\(\\hat \\beta_0\\) = 28.62\n\n\n\\(\\hat \\beta_1\\) = 0.33\n\n\n\\(\\hat \\beta_2\\) = 0.2\n\n\n\n\n\n\nQuestion 7\n\n\nWithin what distance from the model predicted values (the regression line) would we expect 95% of WEMWBS wellbeing scores to be?\n\n\n\n\n\n\nHint\n\n\n\n\n\nEither sigma() or part of the output from summary() will help you here. See Lab 1 Q5 for an example, or the simple & multiple regression models - extracting information flashcard.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nsigma(mdl1)\nsummary(mdl1)\n\n\n\n\nsigma(mdl1)\n\n[1] 5.065003\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma\\) = 5.07. We would expect 95% of wellbeing scores to be within about 10.13 (\\(2 \\hat \\sigma\\)) from the model fit.\n\n\n\n\n\nQuestion 8\n\n\nBased on the model, predict the wellbeing scores for the following individuals who were not included in the original sample:\n\nLeah: Social Interactions = 25; Outdoor Time = 3\nSean: Social Interactions = 19; Outdoor Time = 36\nMike: Social Interactions = 15; Outdoor Time = 20\nDonna: Social Interactions = 7; Outdoor Time = 1\n\nWho has the highest predicted wellbeing score, and who has the lowest?\n\n\n\n\n\n\nHint\n\n\n\n\n\nIt might be helpful to review the predicted values &gt; model predicted values for other (unobserved) data.\n\n\n\n\n\n\n\n Solution \n\n\nFirst we need to pass the data into R:\n\nwellbeing_query &lt;- tibble(social_int = c(25, 19, 15, 7),\n                          outdoor_time = c(3, 36, 20, 1))\n\nAnd next use predict() to get their estimated wellbeing scores:\n\npredict(mdl1, newdata = wellbeing_query)\n\n       1        2        3        4 \n37.58952 42.15034 37.62530 31.16345 \n\n\nSean has the highest predicted wellbeing score (42.15), and Donna the lowest (31.16)."
  },
  {
    "objectID": "1_02_mlr.html#writing-up-presenting-results",
    "href": "1_02_mlr.html#writing-up-presenting-results",
    "title": "Multiple Linear Regression",
    "section": "Writing Up & Presenting Results",
    "text": "Writing Up & Presenting Results\n\nQuestion 9\n\n\nProvide key model results in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package. For a quick guide, review the tables flashcard.\n\n\n\n\n\n\n\n Solution \n\n\n\ntab_model(mdl1,\n          dv.labels = \"Wellbeing (WEMWBS Scores)\",\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\",\n                          \"outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression Table for Wellbeing Model\")\n\n\n\nTable 2: Regression Table for Wellbeing Model\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n28.62\n25.69 – 31.55\n&lt;0.001\n\n\nSocial Interactions\n(number per week)\n0.33\n0.16 – 0.51\n&lt;0.001\n\n\nOutdoor Time (hours per\nweek)\n0.20\n0.10 – 0.30\n&lt;0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.126 / 0.118\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret your results in the context of the research question.\nMake reference to the regression table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nMake sure to include a decision in relation to your null hypothesis - based on the evidence, should you reject or fail to reject the null?\n\n\n\n\n\n\n\n Solution \n\n\nFrom Figure 1, we can see that wellbeing \\((M = 36.3, SD = 5.39)\\), social interactions \\((M = 12.06, SD = 4.02)\\), and outdoor time \\((M = 18.25, SD = 7.1)\\) followed unimodal distributions. There were weak, positive, linear associations between WEMWBS scores and the weekly number of social interactions \\((r = .24)\\), and between WEMWBS scores and outdoor time \\((r = .25)\\) in the sample.\nA multiple regression model was used to determine if there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. As presented in Table 2, outdoor time was significantly associated with wellbeing scores \\((\\beta = 0.20, SE = 0.05, p &lt; .001)\\) after controlling for the number of weekly social interactions. Results suggested that, holding constant social interactions, for every additional hour spent outdoors each week, wellbeing scores increased by 0.20 points. Therefore, we should reject the null hypothesis since \\(p &lt; .05\\)."
  },
  {
    "objectID": "1_02_mlrORANGE.html",
    "href": "1_02_mlrORANGE.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nExtend the ideas of single linear regression to consider regression models with two or more predictors\nUnderstand and interpret the coefficients in multiple linear regression models\n\n\nBe up to date with lectures\nHave completed Week 1 lab exercises\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npsych\npatchwork\nsjPlot\nkableExtra\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\nThe example write-up sections included as part of the solutions are not perfect - they instead should give you a good example of what information you should include and how to structure this. Note that you must not copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more here.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv"
  },
  {
    "objectID": "1_02_mlrORANGE.html#study-analysis-plan-overview",
    "href": "1_02_mlrORANGE.html#study-analysis-plan-overview",
    "title": "Multiple Linear Regression",
    "section": "Study & Analysis Plan Overview",
    "text": "Study & Analysis Plan Overview\n\nQuestion 1\n\n\nProvide a brief overview of the study design and data, before detailing your analysis plan to address the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nGive the reader some background on the context of the study (you might be able to re-use some of the content you wrote for Lab 1 Q10 here)\nState what type of analysis you will conduct in order to address the research question\nSpecify the model to be fitted to address the research question\nSpecify your chosen significance (\\(\\alpha\\)) level\nState your hypotheses\n\nMuch of the information required can be found in the Study Overview codebook. The statistical models flashcards may also be useful to refer to.\n\n\n\n\n\n\n\n Solution \n\n\nThe mwdata dataset contained information on 200 hypothetical participants who lived in Edinburgh & Lothians area. Using a between-subjects design, the researchers collected information on participants’ wellbeing (measured via WEMWBS), outdoor time (hours per week), social interactions (number per week), routine (whether or not one was followed), location of residence (City, Suburb, or Rural), average weekly steps (in thousands), and age (in years).\nDensity plots and boxplots will be used to visualise the marginal distributions of wellbeing, social interactions, and outdoor time. To understand the strength of association among the variables, we will estimate the the correlation coefficients. To address the research question of whether there is an association between wellbeing and time spent outdoors after taking into account the association between wellbeing and social interactions, we are going to fit the following multiple linear regression model:\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social Interactions} + \\beta_2 \\cdot \\text{Outdoor Time} + \\epsilon\n\\]\nEffects will be considered statistically significant at \\(\\alpha=.05\\).\nOur hypotheses are:\n\\(H_0: \\beta_2 = 0\\): There is no association between wellbeing and time spent outdoors after taking into account the association between wellbeing and social interactions\n\\(H_1: \\beta_2 \\neq 0\\): There is an association between wellbeing and time spent outdoors after taking into account the association between wellbeing and social interactions"
  },
  {
    "objectID": "1_02_mlrORANGE.html#descriptive-statistics-visualisations",
    "href": "1_02_mlrORANGE.html#descriptive-statistics-visualisations",
    "title": "Multiple Linear Regression",
    "section": "Descriptive Statistics & Visualisations",
    "text": "Descriptive Statistics & Visualisations\n\nQuestion 2\n\n\nAlongside descriptive statistics, visualize the marginal distributions of the wellbeing, outdoor_time, and social_int variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the many ways to numerically and visually explore your data by reading over the data exploration flashcards.\nFor examples, see flashcards on descriptives statistics tables and data visualisation - marginal.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nNumeric\nVisual\n\n\n\nWe can present our summary statistics for wellbeing, outdoor time, and social interactions as a well formatted table using kable():\n\nmwdata %&gt;% \n  select(wellbeing, outdoor_time, social_int) %&gt;%\n    describe() %&gt;%\n    kable(caption = \"Wellbeing, Social Interactions, and Outdoor Time Descriptive Statistics\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 1: Wellbeing, Social Interactions, and Outdoor Time Descriptive Statistics\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\nwellbeing\n1\n200\n36.30\n5.39\n35\n36.07\n4.45\n22\n59\n37\n0.58\n0.92\n0.38\n\n\noutdoor_time\n2\n200\n18.25\n7.10\n18\n18.14\n7.41\n1\n35\n34\n0.06\n-0.62\n0.50\n\n\nsocial_int\n3\n200\n12.06\n4.02\n12\n11.96\n4.45\n3\n24\n21\n0.21\n-0.40\n0.28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe marginal distribution of scores on the WEMWBS was unimodal with a mean of approximately 36.3. There was variation in WEMWBS scores (SD = 5.39)\n\nThe marginal distribution of weekly hours spent outdoors was unimodal with a mean of approximately 18.25. There was variation in weekly hours spent outdoors (SD = 7.1)\nThe marginal distribution of numbers of social interactions per week was unimodal with a mean of approximately 12.06. There was variation in numbers of social interactions (SD = 4.02)\n\n\n\n\n\n\nYou should be familiar now with how to visualise a marginal distribution:\n\nwellbeing_plot &lt;- \n  ggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  labs(x = \"Score on WEMWBS (range 14-70)\", y = \"Density\")\n\noutdoortime_plot &lt;- \n  ggplot(data = mwdata, aes(x = outdoor_time)) +\n  geom_density() +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Density\")\n\nsocial_plot &lt;- \n  ggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  labs(x = \"Number of social interactions per week\", y = \"Density\")\n\n# arrange plots vertically \nwellbeing_plot / outdoortime_plot / social_plot\n\n\n\nFigure 1: Marginal distribution plots of wellbeing sores, weekly hours spent outdoors, and social interactions\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nProduce plots of the associations between the outcome variable (wellbeing) and each of the explanatory variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview how to visually explore bivariate associations via the data visualisation flashcards.\nFor specifically visualising associations between variables, see the bivariate examples flashcards.\n\n\n\n\n\n\n\n Solution \n\n\n\nwellbeing_outdoor &lt;- \n  ggplot(data = mwdata, aes(x = outdoor_time, y = wellbeing)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Time spent outdoors \\nper week (hours)\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_social &lt;- \n  ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(x = \"Number of social interactions \\nper week\", y = \"Wellbeing score (WEMWBS)\")\n\n# place plots adjacent to one another\nwellbeing_outdoor | wellbeing_social\n\n\n\nFigure 2: Scatterplots displaying the relationships between scores on the WEMWBS and a) weekly outdoor time (hours), and b) weekly number of social interactions\n\n\n\nBoth scatterplots indicated weak, positive, and linear associations both between wellbeing and outdoor time, and between wellbeing and the number of weekly social interactions.\n\n\n\n\n\nQuestion 4\n\n\nProduce a correlation matrix of the variables which are to be used in the analysis, and write a short paragraph describing the associations.\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the correlation flashcards, and remember to interpret in the context of the research question.\n\n\n\n\n\n\n\n Solution \n\n\nWe can either index the dataframe or select the variables of interest:\n\n\nIndex dataframe ([])\nVariable selection (select())\n\n\n\n\n# correlation matrix of the three columns of interest (check which columns we need - in this case, 2,3, and 5)\nround(cor(mwdata[,c(5,3,2)]), digits = 2)\n\n             wellbeing social_int outdoor_time\nwellbeing         1.00       0.24         0.25\nsocial_int        0.24       1.00        -0.04\noutdoor_time      0.25      -0.04         1.00\n\n\n\n\n\n# select only the columns we want by variable name, and pass this to cor()\nmwdata %&gt;% \n  select(wellbeing, social_int, outdoor_time) %&gt;%\n  cor() %&gt;%\n  round(digits = 2)\n\n             wellbeing social_int outdoor_time\nwellbeing         1.00       0.24         0.25\nsocial_int        0.24       1.00        -0.04\noutdoor_time      0.25      -0.04         1.00\n\n\n\n\n\n\n\n\n\n\n\n\nThere was a weak, positive, linear association between WEMWBS scores and weekly outdoor time for the participants in the sample (\\(r\\) = .25). Higher number of hours spent outdoors each week was associated, on average, with higher wellbeing scores\n\nThere was a weak, positive, linear association between WEMWBS scores and the weekly number of social interactions for the participants in the sample (\\(r\\) = .24). More social interactions were associated, on average, with higher wellbeing scores\nThere was a negligible negative correlation between weekly outdoor time and the weekly number of social interactions (\\(r\\) = -.04)"
  },
  {
    "objectID": "1_02_mlrORANGE.html#model-fitting-interpretation",
    "href": "1_02_mlrORANGE.html#model-fitting-interpretation",
    "title": "Multiple Linear Regression",
    "section": "Model Fitting & Interpretation",
    "text": "Model Fitting & Interpretation\n\nQuestion 5\n\n\nRecall the model specified in Q1, and:\n\nState the parameters of the model. How do we denote parameter estimates?\n\nFit the linear model in using lm(), assigning the output to an object called mdl1.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAs we did for simple linear regression, we can fit our multiple regression model using the lm() function. For a recap, see the statistical models flashcards.\n\n\n\n\n\n\n\n Solution \n\n\nA model for the association between \\(x_1\\) = weekly numbers of social interactions, \\(x_2\\) = weekly outdoor time, and \\(y\\) = scores on the WEMWBS can be given by: \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\\\ \\quad \\\\\n\\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nIn the model specified above,\n\n\n\\(\\mu_{y|x_1, x_2} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) represents the systematic part of the model giving the mean of \\(y\\) at each combination of values of \\(x_1\\) and \\(x_2\\);\n\n\\(\\epsilon\\) represents the error (deviation) from that mean, and the errors are independent from one another.\n\nThe parameters of our model are:\n\n\n\\(\\beta_0\\) (The intercept);\n\n\\(\\beta_1\\) (The slope across values of \\(x_1\\));\n\n\\(\\beta_2\\) (The slope across values of \\(x_2\\));\n\n\n\\(\\sigma\\) (The standard deviation of the errors).\n\nWhen we estimate these parameters from the available data, we have a fitted model (recall that the h\\(\\hat{\\textrm{a}}\\)ts are used to distinguish our estimates from the true unknown parameters):\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Social~Interactions + \\hat \\beta_2 \\cdot Outdoor~Time\n\\] And we have residuals \\(\\hat \\epsilon = y - \\hat y\\) which are the deviations from the observed values and our model-predicted responses.\nFitting the model in R:\n\nmdl1 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\n\n\n\n\n\n\nAdditional Visual Explanation\nNote that for simple linear regression we talked about our model as a line in 2 dimensions: the systematic part \\(\\beta_0 + \\beta_1 x\\) defined a line for \\(\\mu_y\\) across the possible values of \\(x\\), with \\(\\epsilon\\) as the random deviations from that line. But in multiple regression we have more than two variables making up our model.\nIn this particular case of three variables (one outcome + two explanatory), we can think of our model as a regression surface (see Figure 3). The systematic part of our model defines the surface across a range of possible values of both \\(x_1\\) and \\(x_2\\). Deviations from the surface are determined by the random error component, \\(\\hat \\epsilon\\).\n\n\n\n\nFigure 3: Regression surface for wellbeing ~ social_int + outdoor_time, from two different angles\n\n\n\nDon’t worry about trying to figure out how to visualise it if we had any more explanatory variables! We can only concieve of 3 spatial dimensions. One could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it’s not worth trying!\n\n\n\nQuestion 6\n\n\nUsing any of:\n\nmdl1\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\nWrite out the estimated parameter values of:\n\n\n\\(\\hat \\beta_0\\), the estimated average wellbeing score associated with zero hours of outdoor time and zero social interactions per week.\n\n\n\\(\\hat \\beta_1\\), the estimated increase in average wellbeing score associated with an additional social interaction per week (an increase of one), holding weekly outdoor time constant.\n\n\n\\(\\hat \\beta_2\\), the estimated increase in average wellbeing score associated with one hour increase in weekly outdoor time, holding the number of social interactions constant\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo interpret and extract information, see the simple & multiple regression models - extracing information flashcards\n\n\n\n\n\n\n\n Solution \n\n\n\n\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\n\n\n\nmdl1$coefficients\n\n (Intercept)   social_int outdoor_time \n  28.6201808    0.3348821    0.1990943 \n\n\n\n\n\ncoef(mdl1)\n\n (Intercept)   social_int outdoor_time \n  28.6201808    0.3348821    0.1990943 \n\n\n\n\n\ncoefficients(mdl1)\n\n (Intercept)   social_int outdoor_time \n  28.6201808    0.3348821    0.1990943 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\n\n\n\\(\\hat \\beta_0\\) = 28.62\n\n\n\\(\\hat \\beta_1\\) = 0.33\n\n\n\\(\\hat \\beta_2\\) = 0.2\n\n\n\n\n\n\nQuestion 7\n\n\nWithin what distance from the model predicted values (the regression line) would we expect 95% of WEMWBS wellbeing scores to be?\n\n\n\n\n\n\nHint\n\n\n\n\n\nEither sigma() or part of the output from summary() will help you here. See Lab 1 Q5 for an example, or the simple & multiple regression models - extracting information flashcard.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nsigma(mdl1)\nsummary(mdl1)\n\n\n\n\nsigma(mdl1)\n\n[1] 5.065003\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma\\) = 5.07. We would expect 95% of wellbeing scores to be within about 10.13 (\\(2 \\hat \\sigma\\)) from the model fit.\n\n\n\n\n\nQuestion 8\n\n\nBased on the model, predict the wellbeing scores for the following individuals who were not included in the original sample:\n\nLeah: Social Interactions = 25; Outdoor Time = 3\nSean: Social Interactions = 19; Outdoor Time = 36\nMike: Social Interactions = 15; Outdoor Time = 20\nDonna: Social Interactions = 7; Outdoor Time = 1\n\nWho has the highest predicted wellbeing score, and who has the lowest?\n\n\n\n\n\n\nHint\n\n\n\n\n\nIt might be helpful to review the predicted values &gt; model predicted values for other (unobserved) data.\n\n\n\n\n\n\n\n Solution \n\n\nFirst we need to pass the data into R:\n\nwellbeing_query &lt;- tibble(social_int = c(25, 19, 15, 7),\n                          outdoor_time = c(3, 36, 20, 1))\n\nAnd next use predict() to get their estimated wellbeing scores:\n\npredict(mdl1, newdata = wellbeing_query)\n\n       1        2        3        4 \n37.58952 42.15034 37.62530 31.16345 \n\n\nSean has the highest predicted wellbeing score (42.15), and Donna the lowest (31.16)."
  },
  {
    "objectID": "1_02_mlrORANGE.html#writing-up-presenting-results",
    "href": "1_02_mlrORANGE.html#writing-up-presenting-results",
    "title": "Multiple Linear Regression",
    "section": "Writing Up & Presenting Results",
    "text": "Writing Up & Presenting Results\n\nQuestion 9\n\n\nProvide key model results in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package. For a quick guide, review the tables flashcard.\n\n\n\n\n\n\n\n Solution \n\n\n\ntab_model(mdl1,\n          dv.labels = \"Wellbeing (WEMWBS Scores)\",\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\",\n                          \"outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression Table for Wellbeing Model\")\n\n\n\nTable 2: Regression Table for Wellbeing Model\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n28.62\n25.69 – 31.55\n&lt;0.001\n\n\nSocial Interactions\n(number per week)\n0.33\n0.16 – 0.51\n&lt;0.001\n\n\nOutdoor Time (hours per\nweek)\n0.20\n0.10 – 0.30\n&lt;0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.126 / 0.118\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret your results in the context of the research question.\nMake reference to the regression table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nMake sure to include a decision in relation to your null hypothesis - based on the evidence, should you reject or fail to reject the null?\n\n\n\n\n\n\n\n Solution \n\n\nFrom Figure 1, we can see that wellbeing \\((M = 36.3, SD = 5.39)\\), social interactions \\((M = 12.06, SD = 4.02)\\), and outdoor time \\((M = 18.25, SD = 7.1)\\) followed unimodal distributions. There were weak, positive, linear associations between WEMWBS scores and the weekly number of social interactions \\((r = .24)\\), and between WEMWBS scores and outdoor time \\((r = .25)\\) in the sample.\nA multiple regression model was used to determine if there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. As presented in Table 2, outdoor time was significantly associated with wellbeing scores \\((\\beta = 0.20, SE = 0.05, p &lt; .001)\\) after controlling for the number of weekly social interactions. Results suggested that, holding constant social interactions, for every additional hour spent outdoors each week, wellbeing scores increased by 0.20 points. Therefore, we should reject the null hypothesis since \\(p &lt; .05\\)."
  },
  {
    "objectID": "1_03_mlr_stz.html",
    "href": "1_03_mlr_stz.html",
    "title": "Multiple Linear Regression & Standardization",
    "section": "",
    "text": "At the end of this lab, you will:\n\nExtend the ideas of single linear regression to consider regression models with two or more predictors\nUnderstand how to interpret significance tests for \\(\\beta\\) coefficients\nUnderstand how to standardize model coefficients and when this is appropriate to do\nUnderstand how to interpret standardized model coefficients in multiple linear regression models\n\n\nBe up to date with lectures\nHave completed Week 1 and Week 2 lab exercises\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\nsjPlot\nppcor\nkableExtra\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\nThe example write-up sections included as part of the solutions are not perfect - they instead should give you a good example of what information you should include and how to structure this. Note that you must not copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more here.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv"
  },
  {
    "objectID": "1_03_mlr_stz.html#lab-2-recap",
    "href": "1_03_mlr_stz.html#lab-2-recap",
    "title": "Multiple Linear Regression & Standardization",
    "section": "Lab 2 Recap",
    "text": "Lab 2 Recap\n\nQuestion 1\n\n\nFit the following multiple linear regression model, and assign the output to an object called mdl, and examine the summary output.\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time + \\epsilon\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can fit our multiple regression model using the lm() function. For a recap, see the statistical models flashcards, specifically the multiple linear regression models - description & specification card.\n\n\n\n\n\n\n\n Solution \n\n\n\nmdl &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06"
  },
  {
    "objectID": "1_03_mlr_stz.html#significance-tests-for-beta-coefficients",
    "href": "1_03_mlr_stz.html#significance-tests-for-beta-coefficients",
    "title": "Multiple Linear Regression & Standardization",
    "section": "Significance Tests for \\(\\beta\\) Coefficients",
    "text": "Significance Tests for \\(\\beta\\) Coefficients\n\nQuestion 2\n\n\nTest the hypothesis that the population slope for outdoor time is zero — that is, that there is no linear association between wellbeing and outdoor time (after controlling for the number of social interactions) in the population.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee the t value flashcard (within simple & multiple regression models - extracting Information &gt; model coefficients &gt; t value).\nReview this weeks lecture slides for an example of how to do this by-hand and in R.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR Function\n\n\n\nWe calculate the test statistic for \\(\\beta_2\\) as:\n\\[\nt = \\frac{\\hat \\beta_2 - 0}{SE(\\hat \\beta_2)} = \\frac{0.19909 - 0}{0.05060} = 3.934585\n\\]\nand compare it with the 5% critical value from a \\(t\\)-distribution with \\(n-3\\) degrees of freedom (since \\(k = 2\\), we have \\(n-2-1\\)), which is:\n\nn &lt;- nrow(mwdata)\nk &lt;- 2\ntstar &lt;- qt(0.975, df = n - k - 1)\ntstar\n\n[1] 1.972079\n\n#tstar = 1.972079\n\nAs \\(|t|\\) (\\(|t|\\) = 3.93) is much larger than \\(t^*\\) (\\(t^*\\) = 1.97), we can reject the null hypothesis as we have strong evidence against it.\nThe \\(p\\)-value, shown below, also confirms this conclusion.\n\n2 * (1 - pt(3.934585, n - 3))\n\n[1] 0.0001154709\n\n\n\n\nPlease note that the same information was already contained in the row corresponding to the variable “outdoor_time” in the output of summary(mdl), which reported the \\(t\\)-statistic under t value and the \\(p\\)-value under Pr(&gt;|t|):\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe result is exactly the same (up to rounding errors) as calculating manually.\nBefore we interpret the results, note that sometimes \\(p\\)-values will be reported to \\(e^X\\). For example, look in the Pr(&gt;|t|) column for “(Intercept)”. The value \\(2e^{-16}\\) simply means \\(2 \\times 10^{-16}\\). This is a very small value (i.e., 0.0000000000000002), hence we would simply report it as &lt;.001 following the APA guidelines.\n\n\n\n\n\n\n\n\n\nWe performed a \\(t\\)-test against the null hypothesis that outdoor time was not associated with wellbeing scores after controlling for social interactions. A significant association was found between outdoor time (hours per week) and wellbeing (WEMWBS scores) \\(t(197) = 3.94,\\ p &lt; .001\\), two-sided. Thus, we have evidence to reject the null hypothesis.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nObtain 95% confidence intervals for the regression coefficients, and write a sentence about each one.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRecall the formula for obtaining a confidence interval:\nA confidence interval for the population slope is \\[\n\\hat \\beta_j \\pm t^* \\cdot SE(\\hat \\beta_j)\n\\] where \\(t^*\\) denotes the critical value chosen from t-distribution with \\(n-k-1\\) degrees of freedom (where \\(k\\) = number of predictors and \\(n\\) = sample size) for a desired \\(\\alpha\\) level of confidence.\nReview this weeks lecture slides for an example of how to do this by-hand and in R.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR Function\n\n\n\nFor 95% confidence we have \\(t^* = 1.97\\):\n\nn &lt;- nrow(mwdata)\nk &lt;- 2\ntstar &lt;- qt(0.975, df = n - k - 1)\ntstar\n\n[1] 1.972079\n\n\nThe confidence intervals are:\n\ntibble(\n  b0_LowerCI = round(28.62018 - (qt(0.975, n-3) * 1.48786), 3),\n  b0_UpperCI = round(28.62018 + (qt(0.975, n-3)* 1.48786), 3),\n  b1_LowerCI = round(0.33488 - (qt(0.975, n-3) * 0.08929), 3),\n  b1_UpperCI = round(0.33488 + (qt(0.975, n-3)* 0.08929), 3),\n  b2_LowerCI = round(0.19909 - (qt(0.975, n-3) * 0.05060), 3),\n  b2_UpperCI = round(0.19909 + (qt(0.975, n-3)* 0.05060), 3)\n      )\n\n# A tibble: 1 × 6\n  b0_LowerCI b0_UpperCI b1_LowerCI b1_UpperCI b2_LowerCI b2_UpperCI\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1       25.7       31.6      0.159      0.511      0.099      0.299\n\n\n\n\nWe can much more easily obtain the confidence intervals for the regression coefficients using the command confint():\n\nconfint(mdl, level = 0.95)\n\n                   2.5 %     97.5 %\n(Intercept)  25.68600170 31.5543598\nsocial_int    0.15880045  0.5109638\noutdoor_time  0.09931273  0.2988759\n\n\nThe result is exactly the same (up to rounding errors) as calculating manually.\n\n\n\n\n\n\n\n\n\n\nThe average wellbeing score for all those with zero hours of outdoor time and zero social interactions per week was between 25.69 and 31.55.\n\nWhen holding weekly outdoor time constant, each increase of one social interaction per week was associated with a difference in wellbeing scores between 0.16 and 0.51, on average.\n\nWhen holding the number of social interactions per week constant, each one hour increase in weekly outdoor time was associated with a difference in wellbeing scores between 0.1 and 0.3, on average."
  },
  {
    "objectID": "1_03_mlr_stz.html#standardization",
    "href": "1_03_mlr_stz.html#standardization",
    "title": "Multiple Linear Regression & Standardization",
    "section": "Standardization",
    "text": "Standardization\n\nQuestion 4\n\n\nFit two regression models using the standardized response and explanatory variables. For demonstration purposes, fit one model using z-scored variables, and the other using the scale() function.\n\n\n\n\n\n\nHint\n\n\n\n\n\nBoth of these methods - z-scoring and scale() - will give us a standardized model.\nSee the scaling and standardisation flashcards.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nz-score\nscale() function\n\n\n\nz-score variables:\n\nmwdata &lt;- mwdata %&gt;%\n  mutate(\n    z_wellbeing = (wellbeing - mean(wellbeing)) / sd(wellbeing),\n    z_social_int = (social_int - mean(social_int)) / sd(social_int),\n    z_outdoor_time = (outdoor_time - mean(outdoor_time)) / sd(outdoor_time)\n  )\n\nCheck that they are standardized:\n\nmwdata %&gt;%\n  summarise(\n    M_z_wellbeing = round(mean(z_wellbeing),2), SD_z_wellbeing = sd(z_wellbeing), \n    M_z_social_int = round(mean(z_social_int),2), SD_z_social_int = sd(z_social_int),\n    M_z_outdoor_time = round(mean(z_outdoor_time),2), SD_z_outdoor_time = sd(z_outdoor_time)\n  )\n\n# A tibble: 1 × 6\n  M_z_wellbeing SD_z_wellbeing M_z_social_int SD_z_social_int M_z_outdoor_time\n          &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1             0              1              0               1                0\n# ℹ 1 more variable: SD_z_outdoor_time &lt;dbl&gt;\n\n#mean of 0, SD of 1 - all good to go\n\nRun model:\n\nmdl_z &lt;- lm(z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\n\n\n\n\nmdl_s &lt;- lm(scale(wellbeing) ~ scale(social_int) + scale(outdoor_time), data = mwdata)\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nExamine the estimates from both standardized models - what do you notice?\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the simple & multiple regression models - extracting information &gt; model coefficients flashcards.\nConsider whether the values the same, or different? What would you expect them to be and why?\n\n\n\n\n\n\n\n Solution \n\n\n\n\nZ-Score\nscale() function\n\n\n\n\nsummary(mdl_z)\n\n\nCall:\nlm(formula = z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9231 -0.5806 -0.0781  0.6144  3.4942 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -4.168e-16  6.642e-02   0.000 1.000000    \nz_social_int    2.499e-01  6.663e-02   3.751 0.000232 ***\nz_outdoor_time  2.622e-01  6.663e-02   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9394 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\nround(summary(mdl_z)$coefficients, 2)\n\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)        0.00       0.07    0.00        1\nz_social_int       0.25       0.07    3.75        0\nz_outdoor_time     0.26       0.07    3.93        0\n\n\n\n\n\nsummary(mdl_s)\n\n\nCall:\nlm(formula = scale(wellbeing) ~ scale(social_int) + scale(outdoor_time), \n    data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9231 -0.5806 -0.0781  0.6144  3.4942 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -4.106e-16  6.642e-02   0.000 1.000000    \nscale(social_int)    2.499e-01  6.663e-02   3.751 0.000232 ***\nscale(outdoor_time)  2.622e-01  6.663e-02   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9394 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\nround(summary(mdl_s)$coefficients, 2)\n\n                    Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)             0.00       0.07    0.00        1\nscale(social_int)       0.25       0.07    3.75        0\nscale(outdoor_time)     0.26       0.07    3.93        0\n\n\n\n\n\nFrom comparing either the summary() or rounded output, we can see that the estimates are the same under both approaches. That means you can use either approach to standardize the variables in your model.\n\n\n\n\n\nQuestion 6\n\n\nExamine the ‘Coefficients’ section of the summary() output from the standardized and unstandardized models - what do you notice? In other words, what is the same / different?\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the simple & multiple regression models - extracting information &gt; model coefficients flashcards.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nUnstandardized\nStandardized\n\n\n\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\nround(summary(mdl)$coefficients, 2)\n\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     28.62       1.49   19.24        0\nsocial_int       0.33       0.09    3.75        0\noutdoor_time     0.20       0.05    3.93        0\n\n\n\n\n\nsummary(mdl_z)\n\n\nCall:\nlm(formula = z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9231 -0.5806 -0.0781  0.6144  3.4942 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -4.168e-16  6.642e-02   0.000 1.000000    \nz_social_int    2.499e-01  6.663e-02   3.751 0.000232 ***\nz_outdoor_time  2.622e-01  6.663e-02   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9394 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\nround(summary(mdl_z)$coefficients, 2)\n\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)        0.00       0.07    0.00        1\nz_social_int       0.25       0.07    3.75        0\nz_outdoor_time     0.26       0.07    3.93        0\n\n\n\n\n\nSimilarities\n\nThe \\(t\\) and \\(p\\)-values for the two predictor variables in both models are the same. This is because the significance of these values remains the same for the standardized coefficients as for unstandardized coefficients\n\nDifferences\n\nThe estimates and standard errors for the intercept and both predictor variables are different under the unstandardized and standardized models\n\nThe \\(t\\) and \\(p\\)-values are different in each model for the intercept. This is because:\n\nIn the unstandardized model, the intercept is significantly different from 0 (it is 28.62), and hence has a very small \\(p\\)-value (&lt; .001)\n\nIn the standardized model, the intercept is not significantly different from 0 (it is 0!), and hence has a \\(p\\)-value of 1.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nHow do these standardized estimates relate to the semi-partial correlation coefficients?\nProduce a visualisation of the association between wellbeing and outdoor time, after accounting for social interactions.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSemi-partial (part) correlation coefficient\nFirstly, think about what semi-partial correlation coefficients and standardized \\(\\beta\\) coefficients represent:\n\nSemi-partial correlation coefficients (which you may also see referred to as part correlations) estimate the unique contribution of each predictor variable to the explained variance in the dependent variable, while controlling for the influence of all other predictors in the model.\n\nStandardized \\(\\beta\\) estimates represent the change in the dependent variable in standard deviation units for a one-standard-deviation change in the predictor variable, whilst holding all other predictors constant.\n\nTo calculate semi-partial (part) correlation coefficients, you will need to use the spcor.test() from the ppcor package.\nRecall that you can look at the estimates from either ‘mdl_s’ or ‘mdl_z’ - they contain the same standardized model estimates.\nNote this is quite a difficult question (really it could be optional), and the exercise is designed to get you to think about how semi-partial correlation coefficients and standardized \\(\\beta\\) coefficients are related.\nPlotting\nTo visualise just one association, you need to specify the terms argument in plot_model(). Don’t forget you can look up the documentation by typing ?plot_model in the console.\nSince using plot_model(), We need to use ‘mdl_z’ here not ‘mdl_s’ - it won’t work with a model that’s used the scale() function.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nSemi-partial (part) correlation coefficient\nVisualisation\n\n\n\nFirst, lets recall the estimates from our standardized model (rounding to 2 decimal places):\n\nround(mdl_z$coefficients, 2)\n\n   (Intercept)   z_social_int z_outdoor_time \n          0.00           0.25           0.26 \n\n\nNext, lets calculate the semi-partial correlation coefficients:\n\n#semi-partial (part) correlation between wellbeing & social interactions\nwb_soc &lt;- spcor.test(mwdata$wellbeing, mwdata$social_int, mwdata$outdoor_time,  method=\"pearson\")\n#round correlation coefficient estimate to 2 decimal places\nround(wb_soc$estimate, 2)\n\n[1] 0.25\n\n#semi-partial (part) correlation between wellbeing & outdoor time\nwb_out &lt;- spcor.test(mwdata$wellbeing, mwdata$outdoor_time, mwdata$social_int, method=\"pearson\")\n#round correlation coefficient estimate to 2 decimal places\nround(wb_out$estimate, 2)\n\n[1] 0.26\n\n\nWe can see that the slope estimates from the standardized model are equivalent to the semi-partial (part) correlation coefficients. This makes theoretical sense given that:\nIn our example, we had a multiple regression model with two predictors, so in our case this means that the \\(\\beta^*\\) coefficients quantify the change in the dependent variable when one predictor (i.e., outdoor time) changes by one standard deviation while the other predictor remains constant (i.e., number of weekly social interactions); whilst the semi-partial correlation for a given predictor (i.e., outdoor time) represents the correlation between the dependent variable and that predictor (i.e., wellbeing and outdoor time) while controlling for the other predictor (i.e., number of weekly social interactions). Thus, the standardized estimate (i.e., \\(\\beta^*\\) coefficient) for one predictor in a multiple regression model with two predictors is equivalent to the semi-partial correlation coefficient for that predictor because, in this context, “holding all other predictors constant” refers to the one remaining predictor.\nNote\nIf this seems a bit confusing, try not to worry - it was more a demonstration of the relationship between \\(r\\) and \\(\\beta^*\\) for when you have 2 predictors (since you saw how this worked with 1 predictor in lecture, we thought it would be useful to extend to 2 predictors). Also, this can become pretty messy very quickly when you have a model with 3+ predictors as the associations among variables becomes more complex.\n\n\n\nplot_model(mdl_z, type = \"eff\",\n           terms = c(\"z_outdoor_time\"), \n           show.data = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nPlot the data and the fitted regression line from both the unstandardized and standardized models. To do so, for each model:\n\nExtract the estimated regression coefficients e.g., via betas &lt;- coef(mdl)\n\nExtract the first entry of betas (i.e., the intercept) via betas[1]\n\nExtract the second entry of betas (i.e., the slope) via betas[2]\n\nProvide the intercept and slope to the function\n\nNote down what you observe from the plots - what is the same / different?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is very similar to Lab 1 Q7.\nExtracting values\nThe function coef() returns a vector (a sequence of numbers all of the same type). To get the first element of the sequence you append [1], and [2] for the second.\nPlotting\nIn your ggplot(), you will need to specify geom_abline(). This might help get you started:\n\n geom_abline(intercept = intercept, slope = slope) \n\n\nYou may also want to plot these side by side to more easily compare, so consider using | from patchwork. For further ggplot() guidance, see the how to visualise data flashcard.\n\n\n\n\n\n\n\n Solution \n\n\nFirst extract the values required for both non-standardized and standardized models:\n\n#non-standardized (from 'mdl')\nbetas &lt;- coef(mdl)\nintercept &lt;- betas[1]\nslope &lt;- betas[2]\n\n#standardized (from 'mdl_z')\nbetas_z &lt;- coef(mdl_z)\nintercept_z &lt;- betas_z[1]\nslope_z &lt;- betas_z[2]\n\nWe can plot the models as follows:\n\np1 &lt;- ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_abline(intercept = intercept, slope = slope, color = 'blue') + \n  labs(x = \"Social Interactions \\n(Number per Week)\", y = \"Wellbeing (WEMWBS) Scores\")\n\np2 &lt;- ggplot(data = mwdata, aes(x = z_social_int, y = z_wellbeing)) +\n  geom_point() +\n  geom_abline(intercept = intercept_z, slope = slope_z, color = 'red') + \n  labs(x = \"Social Interactions \\n(Number per Week; Z-Scored)\", y = \"Wellbeing (WEMWBS) Scores; Z-Scored\")\n\np1 | p2\n\n\n\n\n\n\n\nSimilarities\n\nThe data points are distributed in the same pattern\n\nThe slope of the line follows the same gradient\n\nDifferences\n\nThe x- and y-axis scales are different for each plot. This is because:\n\nThe unstandardized is in the original units where we interpret the slope as the change in \\(y\\) units for a unit change in \\(x\\)\n\nThe standardized is in SD units where we interpret the slope as the SD change in \\(y\\) for 1 SD change in \\(x\\)"
  },
  {
    "objectID": "1_03_mlr_stz.html#writing-up-presenting-results",
    "href": "1_03_mlr_stz.html#writing-up-presenting-results",
    "title": "Multiple Linear Regression & Standardization",
    "section": "Writing Up & Presenting Results",
    "text": "Writing Up & Presenting Results\n\nQuestion 9\n\n\nProvide key model results from the standardized model in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package. For a quick guide, review the tables flashcard.\nSince using tab_model(), We need to use ‘mdl_z’ here not ‘mdl_s’ - it won’t work with a model that’s used the scale() function.\n\n\n\n\n\n\n\n Solution \n\n\n\ntab_model(mdl_z,\n          dv.labels = \"Wellbeing (WEMWBS Scores)\",\n          pred.labels = c(\"z_social_int\" = \"Social Interactions (number per week)\",\n                          \"z_outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression Results for Wellbeing Model (both DV and IVs z-scored)\")\n\n\n\nTable 1: Regression Results for Wellbeing Model (both DV and IVs z-scored)\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-0.00\n-0.13 – 0.13\n1.000\n\n\nSocial Interactions\n(number per week)\n0.25\n0.12 – 0.38\n&lt;0.001\n\n\nOutdoor Time (hours per\nweek)\n0.26\n0.13 – 0.39\n&lt;0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.126 / 0.118\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret the results from the standardized model the context of the research question.\nMake reference to the your regression table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember to inform the reader of the scale of your variables.\n\n\n\n\n\n\n\n Solution \n\n\nA multiple regression model was used to determine if there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. All variables (wellbeing, social interactions, and outdoor time) were \\(z\\)-scored. As presented in Table 1, outdoor time was significantly associated with wellbeing scores \\((\\beta = 0.26, SE = 0.07, p &lt; .001)\\) after controlling for the number of weekly social interactions. Results suggested that, holding constant social interactions, for every standard deviation increase in outdoor time, wellbeing scores increased on average by 0.26 standard deviations. Therefore, we should reject the null hypothesis since \\(p &lt; .05\\)."
  },
  {
    "objectID": "1_03_mlr_stzAPPLE.html",
    "href": "1_03_mlr_stzAPPLE.html",
    "title": "Multiple Linear Regression & Standardization",
    "section": "",
    "text": "At the end of this lab, you will:\n\nExtend the ideas of single linear regression to consider regression models with two or more predictors\nUnderstand how to interpret significance tests for \\(\\beta\\) coefficients\nUnderstand how to standardize model coefficients and when this is appropriate to do\nUnderstand how to interpret standardized model coefficients in multiple linear regression models\n\n\nBe up to date with lectures\nHave completed Week 1 and Week 2 lab exercises\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\nsjPlot\nppcor\nkableExtra\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\nThe example write-up sections included as part of the solutions are not perfect - they instead should give you a good example of what information you should include and how to structure this. Note that you must not copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more here.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv"
  },
  {
    "objectID": "1_03_mlr_stzAPPLE.html#lab-2-recap",
    "href": "1_03_mlr_stzAPPLE.html#lab-2-recap",
    "title": "Multiple Linear Regression & Standardization",
    "section": "Lab 2 Recap",
    "text": "Lab 2 Recap\n\nQuestion 1\n\n\nFit the following multiple linear regression model, and assign the output to an object called mdl, and examine the summary output.\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time + \\epsilon\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can fit our multiple regression model using the lm() function. For a recap, see the statistical models flashcards, specifically the multiple linear regression models - description & specification card.\n\n\n\n\n\n\n\n Solution \n\n\n\nmdl &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06"
  },
  {
    "objectID": "1_03_mlr_stzAPPLE.html#significance-tests-for-beta-coefficients",
    "href": "1_03_mlr_stzAPPLE.html#significance-tests-for-beta-coefficients",
    "title": "Multiple Linear Regression & Standardization",
    "section": "Significance Tests for \\(\\beta\\) Coefficients",
    "text": "Significance Tests for \\(\\beta\\) Coefficients\n\nQuestion 2\n\n\nTest the hypothesis that the population slope for outdoor time is zero — that is, that there is no linear association between wellbeing and outdoor time (after controlling for the number of social interactions) in the population.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee the t value flashcard (within simple & multiple regression models - extracting Information &gt; model coefficients &gt; t value).\nReview this weeks lecture slides for an example of how to do this by-hand and in R.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR Function\n\n\n\nWe calculate the test statistic for \\(\\beta_2\\) as:\n\\[\nt = \\frac{\\hat \\beta_2 - 0}{SE(\\hat \\beta_2)} = \\frac{0.19909 - 0}{0.05060} = 3.934585\n\\]\nand compare it with the 5% critical value from a \\(t\\)-distribution with \\(n-3\\) degrees of freedom (since \\(k = 2\\), we have \\(n-2-1\\)), which is:\n\nn &lt;- nrow(mwdata)\nk &lt;- 2\ntstar &lt;- qt(0.975, df = n - k - 1)\ntstar\n\n[1] 1.972079\n\n#tstar = 1.972079\n\nAs \\(|t|\\) (\\(|t|\\) = 3.93) is much larger than \\(t^*\\) (\\(t^*\\) = 1.97), we can reject the null hypothesis as we have strong evidence against it.\nThe \\(p\\)-value, shown below, also confirms this conclusion.\n\n2 * (1 - pt(3.934585, n - 3))\n\n[1] 0.0001154709\n\n\n\n\nPlease note that the same information was already contained in the row corresponding to the variable “outdoor_time” in the output of summary(mdl), which reported the \\(t\\)-statistic under t value and the \\(p\\)-value under Pr(&gt;|t|):\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe result is exactly the same (up to rounding errors) as calculating manually.\nBefore we interpret the results, note that sometimes \\(p\\)-values will be reported to \\(e^X\\). For example, look in the Pr(&gt;|t|) column for “(Intercept)”. The value \\(2e^{-16}\\) simply means \\(2 \\times 10^{-16}\\). This is a very small value (i.e., 0.0000000000000002), hence we would simply report it as &lt;.001 following the APA guidelines.\n\n\n\n\n\n\n\n\n\nWe performed a \\(t\\)-test against the null hypothesis that outdoor time was not associated with wellbeing scores after controlling for social interactions. A significant association was found between outdoor time (hours per week) and wellbeing (WEMWBS scores) \\(t(197) = 3.94,\\ p &lt; .001\\), two-sided. Thus, we have evidence to reject the null hypothesis.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nObtain 95% confidence intervals for the regression coefficients, and write a sentence about each one.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRecall the formula for obtaining a confidence interval:\nA confidence interval for the population slope is \\[\n\\hat \\beta_j \\pm t^* \\cdot SE(\\hat \\beta_j)\n\\] where \\(t^*\\) denotes the critical value chosen from t-distribution with \\(n-k-1\\) degrees of freedom (where \\(k\\) = number of predictors and \\(n\\) = sample size) for a desired \\(\\alpha\\) level of confidence.\nReview this weeks lecture slides for an example of how to do this by-hand and in R.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR Function\n\n\n\nFor 95% confidence we have \\(t^* = 1.97\\):\n\nn &lt;- nrow(mwdata)\nk &lt;- 2\ntstar &lt;- qt(0.975, df = n - k - 1)\ntstar\n\n[1] 1.972079\n\n\nThe confidence intervals are:\n\ntibble(\n  b0_LowerCI = round(28.62018 - (qt(0.975, n-3) * 1.48786), 3),\n  b0_UpperCI = round(28.62018 + (qt(0.975, n-3)* 1.48786), 3),\n  b1_LowerCI = round(0.33488 - (qt(0.975, n-3) * 0.08929), 3),\n  b1_UpperCI = round(0.33488 + (qt(0.975, n-3)* 0.08929), 3),\n  b2_LowerCI = round(0.19909 - (qt(0.975, n-3) * 0.05060), 3),\n  b2_UpperCI = round(0.19909 + (qt(0.975, n-3)* 0.05060), 3)\n      )\n\n# A tibble: 1 × 6\n  b0_LowerCI b0_UpperCI b1_LowerCI b1_UpperCI b2_LowerCI b2_UpperCI\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1       25.7       31.6      0.159      0.511      0.099      0.299\n\n\n\n\nWe can much more easily obtain the confidence intervals for the regression coefficients using the command confint():\n\nconfint(mdl, level = 0.95)\n\n                   2.5 %     97.5 %\n(Intercept)  25.68600170 31.5543598\nsocial_int    0.15880045  0.5109638\noutdoor_time  0.09931273  0.2988759\n\n\nThe result is exactly the same (up to rounding errors) as calculating manually.\n\n\n\n\n\n\n\n\n\n\nThe average wellbeing score for all those with zero hours of outdoor time and zero social interactions per week was between 25.69 and 31.55.\n\nWhen holding weekly outdoor time constant, each increase of one social interaction per week was associated with a difference in wellbeing scores between 0.16 and 0.51, on average.\n\nWhen holding the number of social interactions per week constant, each one hour increase in weekly outdoor time was associated with a difference in wellbeing scores between 0.1 and 0.3, on average."
  },
  {
    "objectID": "1_03_mlr_stzAPPLE.html#standardization",
    "href": "1_03_mlr_stzAPPLE.html#standardization",
    "title": "Multiple Linear Regression & Standardization",
    "section": "Standardization",
    "text": "Standardization\n\nQuestion 4\n\n\nFit two regression models using the standardized response and explanatory variables. For demonstration purposes, fit one model using z-scored variables, and the other using the scale() function.\n\n\n\n\n\n\nHint\n\n\n\n\n\nBoth of these methods - z-scoring and scale() - will give us a standardized model.\nSee the scaling and standardisation flashcards.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nz-score\nscale() function\n\n\n\nz-score variables:\n\nmwdata &lt;- mwdata %&gt;%\n  mutate(\n    z_wellbeing = (wellbeing - mean(wellbeing)) / sd(wellbeing),\n    z_social_int = (social_int - mean(social_int)) / sd(social_int),\n    z_outdoor_time = (outdoor_time - mean(outdoor_time)) / sd(outdoor_time)\n  )\n\nCheck that they are standardized:\n\nmwdata %&gt;%\n  summarise(\n    M_z_wellbeing = round(mean(z_wellbeing),2), SD_z_wellbeing = sd(z_wellbeing), \n    M_z_social_int = round(mean(z_social_int),2), SD_z_social_int = sd(z_social_int),\n    M_z_outdoor_time = round(mean(z_outdoor_time),2), SD_z_outdoor_time = sd(z_outdoor_time)\n  )\n\n# A tibble: 1 × 6\n  M_z_wellbeing SD_z_wellbeing M_z_social_int SD_z_social_int M_z_outdoor_time\n          &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1             0              1              0               1                0\n# ℹ 1 more variable: SD_z_outdoor_time &lt;dbl&gt;\n\n#mean of 0, SD of 1 - all good to go\n\nRun model:\n\nmdl_z &lt;- lm(z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\n\n\n\n\nmdl_s &lt;- lm(scale(wellbeing) ~ scale(social_int) + scale(outdoor_time), data = mwdata)\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nExamine the estimates from both standardized models - what do you notice?\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the simple & multiple regression models - extracting information &gt; model coefficients flashcards.\nConsider whether the values the same, or different? What would you expect them to be and why?\n\n\n\n\n\n\n\n Solution \n\n\n\n\nZ-Score\nscale() function\n\n\n\n\nsummary(mdl_z)\n\n\nCall:\nlm(formula = z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9231 -0.5806 -0.0781  0.6144  3.4942 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -4.168e-16  6.642e-02   0.000 1.000000    \nz_social_int    2.499e-01  6.663e-02   3.751 0.000232 ***\nz_outdoor_time  2.622e-01  6.663e-02   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9394 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\nround(summary(mdl_z)$coefficients, 2)\n\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)        0.00       0.07    0.00        1\nz_social_int       0.25       0.07    3.75        0\nz_outdoor_time     0.26       0.07    3.93        0\n\n\n\n\n\nsummary(mdl_s)\n\n\nCall:\nlm(formula = scale(wellbeing) ~ scale(social_int) + scale(outdoor_time), \n    data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9231 -0.5806 -0.0781  0.6144  3.4942 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -4.106e-16  6.642e-02   0.000 1.000000    \nscale(social_int)    2.499e-01  6.663e-02   3.751 0.000232 ***\nscale(outdoor_time)  2.622e-01  6.663e-02   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9394 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\nround(summary(mdl_s)$coefficients, 2)\n\n                    Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)             0.00       0.07    0.00        1\nscale(social_int)       0.25       0.07    3.75        0\nscale(outdoor_time)     0.26       0.07    3.93        0\n\n\n\n\n\nFrom comparing either the summary() or rounded output, we can see that the estimates are the same under both approaches. That means you can use either approach to standardize the variables in your model.\n\n\n\n\n\nQuestion 6\n\n\nExamine the ‘Coefficients’ section of the summary() output from the standardized and unstandardized models - what do you notice? In other words, what is the same / different?\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the simple & multiple regression models - extracting information &gt; model coefficients flashcards.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nUnstandardized\nStandardized\n\n\n\n\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\nround(summary(mdl)$coefficients, 2)\n\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     28.62       1.49   19.24        0\nsocial_int       0.33       0.09    3.75        0\noutdoor_time     0.20       0.05    3.93        0\n\n\n\n\n\nsummary(mdl_z)\n\n\nCall:\nlm(formula = z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9231 -0.5806 -0.0781  0.6144  3.4942 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -4.168e-16  6.642e-02   0.000 1.000000    \nz_social_int    2.499e-01  6.663e-02   3.751 0.000232 ***\nz_outdoor_time  2.622e-01  6.663e-02   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9394 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\nround(summary(mdl_z)$coefficients, 2)\n\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)        0.00       0.07    0.00        1\nz_social_int       0.25       0.07    3.75        0\nz_outdoor_time     0.26       0.07    3.93        0\n\n\n\n\n\nSimilarities\n\nThe \\(t\\) and \\(p\\)-values for the two predictor variables in both models are the same. This is because the significance of these values remains the same for the standardized coefficients as for unstandardized coefficients\n\nDifferences\n\nThe estimates and standard errors for the intercept and both predictor variables are different under the unstandardized and standardized models\n\nThe \\(t\\) and \\(p\\)-values are different in each model for the intercept. This is because:\n\nIn the unstandardized model, the intercept is significantly different from 0 (it is 28.62), and hence has a very small \\(p\\)-value (&lt; .001)\n\nIn the standardized model, the intercept is not significantly different from 0 (it is 0!), and hence has a \\(p\\)-value of 1.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nHow do these standardized estimates relate to the semi-partial correlation coefficients?\nProduce a visualisation of the association between wellbeing and outdoor time, after accounting for social interactions.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSemi-partial (part) correlation coefficient\nFirstly, think about what semi-partial correlation coefficients and standardized \\(\\beta\\) coefficients represent:\n\nSemi-partial correlation coefficients (which you may also see referred to as part correlations) estimate the unique contribution of each predictor variable to the explained variance in the dependent variable, while controlling for the influence of all other predictors in the model.\n\nStandardized \\(\\beta\\) estimates represent the change in the dependent variable in standard deviation units for a one-standard-deviation change in the predictor variable, whilst holding all other predictors constant.\n\nTo calculate semi-partial (part) correlation coefficients, you will need to use the spcor.test() from the ppcor package.\nRecall that you can look at the estimates from either ‘mdl_s’ or ‘mdl_z’ - they contain the same standardized model estimates.\nNote this is quite a difficult question (really it could be optional), and the exercise is designed to get you to think about how semi-partial correlation coefficients and standardized \\(\\beta\\) coefficients are related.\nPlotting\nTo visualise just one association, you need to specify the terms argument in plot_model(). Don’t forget you can look up the documentation by typing ?plot_model in the console.\nSince using plot_model(), We need to use ‘mdl_z’ here not ‘mdl_s’ - it won’t work with a model that’s used the scale() function.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nSemi-partial (part) correlation coefficient\nVisualisation\n\n\n\nFirst, lets recall the estimates from our standardized model (rounding to 2 decimal places):\n\nround(mdl_z$coefficients, 2)\n\n   (Intercept)   z_social_int z_outdoor_time \n          0.00           0.25           0.26 \n\n\nNext, lets calculate the semi-partial correlation coefficients:\n\n#semi-partial (part) correlation between wellbeing & social interactions\nwb_soc &lt;- spcor.test(mwdata$wellbeing, mwdata$social_int, mwdata$outdoor_time,  method=\"pearson\")\n#round correlation coefficient estimate to 2 decimal places\nround(wb_soc$estimate, 2)\n\n[1] 0.25\n\n#semi-partial (part) correlation between wellbeing & outdoor time\nwb_out &lt;- spcor.test(mwdata$wellbeing, mwdata$outdoor_time, mwdata$social_int, method=\"pearson\")\n#round correlation coefficient estimate to 2 decimal places\nround(wb_out$estimate, 2)\n\n[1] 0.26\n\n\nWe can see that the slope estimates from the standardized model are equivalent to the semi-partial (part) correlation coefficients. This makes theoretical sense given that:\nIn our example, we had a multiple regression model with two predictors, so in our case this means that the \\(\\beta^*\\) coefficients quantify the change in the dependent variable when one predictor (i.e., outdoor time) changes by one standard deviation while the other predictor remains constant (i.e., number of weekly social interactions); whilst the semi-partial correlation for a given predictor (i.e., outdoor time) represents the correlation between the dependent variable and that predictor (i.e., wellbeing and outdoor time) while controlling for the other predictor (i.e., number of weekly social interactions). Thus, the standardized estimate (i.e., \\(\\beta^*\\) coefficient) for one predictor in a multiple regression model with two predictors is equivalent to the semi-partial correlation coefficient for that predictor because, in this context, “holding all other predictors constant” refers to the one remaining predictor.\nNote\nIf this seems a bit confusing, try not to worry - it was more a demonstration of the relationship between \\(r\\) and \\(\\beta^*\\) for when you have 2 predictors (since you saw how this worked with 1 predictor in lecture, we thought it would be useful to extend to 2 predictors). Also, this can become pretty messy very quickly when you have a model with 3+ predictors as the associations among variables becomes more complex.\n\n\n\nplot_model(mdl_z, type = \"eff\",\n           terms = c(\"z_outdoor_time\"), \n           show.data = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nPlot the data and the fitted regression line from both the unstandardized and standardized models. To do so, for each model:\n\nExtract the estimated regression coefficients e.g., via betas &lt;- coef(mdl)\n\nExtract the first entry of betas (i.e., the intercept) via betas[1]\n\nExtract the second entry of betas (i.e., the slope) via betas[2]\n\nProvide the intercept and slope to the function\n\nNote down what you observe from the plots - what is the same / different?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is very similar to Lab 1 Q7.\nExtracting values\nThe function coef() returns a vector (a sequence of numbers all of the same type). To get the first element of the sequence you append [1], and [2] for the second.\nPlotting\nIn your ggplot(), you will need to specify geom_abline(). This might help get you started:\n\n geom_abline(intercept = intercept, slope = slope) \n\n\nYou may also want to plot these side by side to more easily compare, so consider using | from patchwork. For further ggplot() guidance, see the how to visualise data flashcard.\n\n\n\n\n\n\n\n Solution \n\n\nFirst extract the values required for both non-standardized and standardized models:\n\n#non-standardized (from 'mdl')\nbetas &lt;- coef(mdl)\nintercept &lt;- betas[1]\nslope &lt;- betas[2]\n\n#standardized (from 'mdl_z')\nbetas_z &lt;- coef(mdl_z)\nintercept_z &lt;- betas_z[1]\nslope_z &lt;- betas_z[2]\n\nWe can plot the models as follows:\n\np1 &lt;- ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point() +\n  geom_abline(intercept = intercept, slope = slope, color = 'blue') + \n  labs(x = \"Social Interactions \\n(Number per Week)\", y = \"Wellbeing (WEMWBS) Scores\")\n\np2 &lt;- ggplot(data = mwdata, aes(x = z_social_int, y = z_wellbeing)) +\n  geom_point() +\n  geom_abline(intercept = intercept_z, slope = slope_z, color = 'red') + \n  labs(x = \"Social Interactions \\n(Number per Week; Z-Scored)\", y = \"Wellbeing (WEMWBS) Scores; Z-Scored\")\n\np1 | p2\n\n\n\n\n\n\n\nSimilarities\n\nThe data points are distributed in the same pattern\n\nThe slope of the line follows the same gradient\n\nDifferences\n\nThe x- and y-axis scales are different for each plot. This is because:\n\nThe unstandardized is in the original units where we interpret the slope as the change in \\(y\\) units for a unit change in \\(x\\)\n\nThe standardized is in SD units where we interpret the slope as the SD change in \\(y\\) for 1 SD change in \\(x\\)"
  },
  {
    "objectID": "1_03_mlr_stzAPPLE.html#writing-up-presenting-results",
    "href": "1_03_mlr_stzAPPLE.html#writing-up-presenting-results",
    "title": "Multiple Linear Regression & Standardization",
    "section": "Writing Up & Presenting Results",
    "text": "Writing Up & Presenting Results\n\nQuestion 9\n\n\nProvide key model results from the standardized model in a formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse tab_model() from the sjPlot package. For a quick guide, review the tables flashcard.\nSince using tab_model(), We need to use ‘mdl_z’ here not ‘mdl_s’ - it won’t work with a model that’s used the scale() function.\n\n\n\n\n\n\n\n Solution \n\n\n\ntab_model(mdl_z,\n          dv.labels = \"Wellbeing (WEMWBS Scores)\",\n          pred.labels = c(\"z_social_int\" = \"Social Interactions (number per week)\",\n                          \"z_outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression Results for Wellbeing Model (both DV and IVs z-scored)\")\n\n\n\nTable 1: Regression Results for Wellbeing Model (both DV and IVs z-scored)\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-0.00\n-0.13 – 0.13\n1.000\n\n\nSocial Interactions\n(number per week)\n0.25\n0.12 – 0.38\n&lt;0.001\n\n\nOutdoor Time (hours per\nweek)\n0.26\n0.13 – 0.39\n&lt;0.001\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.126 / 0.118\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nInterpret the results from the standardized model the context of the research question.\nMake reference to the your regression table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember to inform the reader of the scale of your variables.\n\n\n\n\n\n\n\n Solution \n\n\nA multiple regression model was used to determine if there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. All variables (wellbeing, social interactions, and outdoor time) were \\(z\\)-scored. As presented in Table 1, outdoor time was significantly associated with wellbeing scores \\((\\beta = 0.26, SE = 0.07, p &lt; .001)\\) after controlling for the number of weekly social interactions. Results suggested that, holding constant social interactions, for every standard deviation increase in outdoor time, wellbeing scores increased on average by 0.26 standard deviations. Therefore, we should reject the null hypothesis since \\(p &lt; .05\\)."
  },
  {
    "objectID": "1_04_model_fit_comp.html",
    "href": "1_04_model_fit_comp.html",
    "title": "Model Fit and Comparisons",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand how to calculate the interpret \\(R^2\\) and adjusted-\\(R^2\\) as a measure of model quality.\nUnderstand the calculation and interpretation of the \\(F\\)-test of model utility.\nUnderstand measures of model fit using \\(F\\).\n\nUnderstand the principles of model selection and how to compare models via \\(F\\) tests, \\(AIC\\), and \\(BIC\\).\n\n\nBe up to date with lectures\nHave completed Week 1 and Week 2, and Week 3 lab exercises\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\nkableExtra\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\nThe example write-up sections included as part of the solutions are not perfect - they instead should give you a good example of what information you should include and how to structure this. Note that you must not copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more here.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv"
  },
  {
    "objectID": "1_04_model_fit_comp.html#section-i-model-fit",
    "href": "1_04_model_fit_comp.html#section-i-model-fit",
    "title": "Model Fit and Comparisons",
    "section": "Section I: Model Fit",
    "text": "Section I: Model Fit\nIn the first section of this lab, you will focus on the statistics contained within the highlighted sections of the summary() output below. You will be both calculating these by hand and deriving via R before interpreting these values in the context of the research question.\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nFit the following multiple linear regression model, and assign the output to an object called mdl.\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time + \\epsilon\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is the same model that you have fitted in the previous couple of weeks.\nWe can fit our multiple regression model using the lm() function.\nFor a recap, see the statistical models flashcards, specifically the multiple linear regression models - description & specification card.\n\n\n\n\n\n\n\n Solution \n\n\n\nmdl &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\n\n\nQuestion 2\n\n\nWhat is the proportion of the total variability in wellbeing scores explained by the model?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe proportion of the total variability explained is given by \\(R^2\\). Since the model includes 2 predictors, you should report the Adjusted-\\(R^2\\).\nFor a more detailed overview, see the R-squared and Adjusted R-squared flashcard.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR Function\n\n\n\nIn R we can write:\n\n#Define n & k\nn &lt;- nrow(mwdata)\nk &lt;- 2\n\n#Predicted scores\nwellbeing_fitted &lt;- mwdata %&gt;%\n  mutate(\n    wellbeing_pred = predict(mdl),\n    wellbeing_resid = wellbeing - wellbeing_pred)\n\n# Sums of Squares, and R / Adjusted R Squared\nwellbeing_fitted %&gt;%\n  summarise(\n    SSModel = sum((wellbeing_pred - mean(wellbeing))^2),\n    SSTotal = sum((wellbeing - mean(wellbeing))^2),\n    SSResid = sum(wellbeing_resid^2)\n  ) %&gt;% \n  summarise(\n    RSquared = SSModel / SSTotal,\n    AdjRSquared = 1-((1-(RSquared))*(n-1)/(n-k-1))\n  )\n\n# A tibble: 1 × 2\n  RSquared AdjRSquared\n     &lt;dbl&gt;       &lt;dbl&gt;\n1    0.126       0.118\n\n\nThe output displays the Adjusted \\(R^2\\) value in the following column:\nAdjRSquared\n &lt;dbl&gt;\n 0.118\n\n\n\n#look in second bottom row - Multiple R Squared and Adjusted R Squared both reported here\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe output of summary() displays the Adjusted \\(R^2\\) value in the following line:\nAdjusted R-squared:  0.1176 \n\n\n\n\n\n\n\n\n\nApproximately 12% of the total variability in wellbeing scores is accounted for by social interactions and outdoor time.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nWhat do you notice about the unadjusted and adjusted \\(R^2\\) values?\n\n\n\n\n\n\nHint\n\n\n\n\n\nAre they similar or quite different? Why might this be?\nThink about when you would report \\(R^2\\) and Adjusted-\\(R^2\\) - the R-squared and Adjusted R-squared flashcard has some detail on this, but it will also be useful to think about how each is calculated.\n\n\n\n\n\n\n\n Solution \n\n\nThe values of the unadjusted (0.1265) and adjusted \\(R^2\\) (0.1176) values are quite similar. This is because the sample size is quite large \\((n = 200)\\), and the number of predictors \\((k = 2)\\) is small.\n\n\n\n\n\nQuestion 4\n\n\nPerform a model utility test at the 5% significance level and report your results.\nIn other words, conduct an \\(F\\)-test against the null hypothesis that the model is ineffective at predicting wellbeing scores using social interactions and outdoor time by computing the \\(F\\)-statistic using its definition.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe \\(F\\)-ratio is used to test the null hypothesis that all regression slopes are zero.\nSee the F-ratio flashcard for a more detailed overview.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR Function\n\n\n\n\n#df(model) = k \ndf1 &lt;- 2\n\n#df(residual) = n - k - 1\ndf2 &lt;- nrow(mwdata) - 2 - 1\n\nf_star &lt;- qf(0.95, df1, df2)\n\n#check value\nf_star\n\n[1] 3.041753\n\n\n\nmodel_utility &lt;- wellbeing_fitted %&gt;%\n  summarise(\n    SSModel = sum((wellbeing_pred - mean(wellbeing))^2),\n    SSResid = sum(wellbeing_resid^2),\n    MSModel = SSModel / df1,\n    MSResid = SSResid / df2,\n    FObs = MSModel / MSResid\n  )\nmodel_utility\n\n# A tibble: 1 × 5\n  SSModel SSResid MSModel MSResid  FObs\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    732.   5054.    366.    25.7  14.3\n\n\nWe can also compute the p-value:\n\npvalue &lt;- 1 - pf(model_utility$FObs, df1, df2)\npvalue\n\n[1] 1.643779e-06\n\n\nThe value 1.643779e-06 simply means \\(1.6 \\times 10^{-6}\\), so it’s a really small number (i.e., 0.000001643779).\n\n\n\n#look in bottom row\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe relevant row is the following:\n\nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\n\n\n\n\n\nWe performed an \\(F\\)-test of model utility at the 5% significance level, where \\(F(2,197) = 14.26, p &lt;.001\\).\nThe large \\(F\\)-statistic and small \\(p\\)-value \\((p &lt;.001)\\) suggested that we have very strong evidence against the null hypothesis.\nIn other words, the data provide strong evidence that the number of social interactions and outdoor time are predictors of wellbeing scores."
  },
  {
    "objectID": "1_04_model_fit_comp.html#section-ii-model-comparisons",
    "href": "1_04_model_fit_comp.html#section-ii-model-comparisons",
    "title": "Model Fit and Comparisons",
    "section": "Section II: Model Comparisons",
    "text": "Section II: Model Comparisons\nIn the second section of this lab, you will focus on model comparison where you will formally test a number of research questions:\n\n\nRQ1: Is the number of weekly social interactions a useful predictor of wellbeing scores?\nRQ2: Does weekly outdoor time explain a significant amount of variance in wellbeing scores over and above the number of weekly social interactions?\n\n\n\n\nQuestion 5\n\n\nFit the below 3 models required to address the 2 research questions stated above. Note down which model(s) will be used to address each research question, and examine the results of each model.\nName the models as follows: “wb_mdl0”, “wb_mdl1”, “wb_mdl2”\n\\[\n\\text{Wellbeing} = \\beta_0  + \\epsilon\n\\]\n\n\\[\n\\text{Wellbeing} = \\beta_0  + \\beta_1 \\cdot Social~Interactions + \\epsilon\n\\]\n\n\\[\n\\text{Wellbeing} = \\beta_0  + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time + \\epsilon\n\\] \n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can fit our multiple regression models individually using the lm() function. For a recap, see the statistical models flashcards, specifically the multiple linear regression models - description & specification card.\nThe summary() function will be useful to examine the model output.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nwb_mdl0\nwb_mdl1\nwb_mdl2\n\n\n\n\n#null/intercept only model\nwb_mdl0 &lt;- lm(wellbeing ~ 1, data = mwdata)\nsummary(wb_mdl0)\n\n\nCall:\nlm(formula = wellbeing ~ 1, data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.295  -3.295  -1.295   3.705  22.705 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.2950     0.3813   95.19   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 199 degrees of freedom\n\n\n\n\n\n#model with social interactions\nwb_mdl1 &lt;- lm(wellbeing ~ social_int, data = mwdata)\nsummary(wb_mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5628  -3.2741  -0.7908   3.3703  20.4706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 32.40771    1.17532  27.573  &lt; 2e-16 ***\nsocial_int   0.32220    0.09243   3.486 0.000605 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.247 on 198 degrees of freedom\nMultiple R-squared:  0.05781,   Adjusted R-squared:  0.05306 \nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n\n\n\n\n\n#model with social interactions and outdoor time\nwb_mdl2 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(wb_mdl2)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\nThe models required to address each research question (RQ) are:\n\nRQ1: Models wb_mdl0 and wb_mdl1\nRQ2: Models wb_mdl1 and wb_mdl2\n\n\n\n\n\n\nQuestion 6\n\n\nRQ1: Is the number of weekly social interactions a useful predictor of wellbeing scores?\nCheck that the \\(F\\)-statistic and the \\(p\\)-value are the same from the model comparison as that which are given at the bottom of summary(wb_mdl1).\nProvide the key model results from the two models in a single formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember that the null model tests the null hypothesis that all beta coefficients are zero. By comparing wb_mdl0 to wb_mdl1, we can test whether we should include the IV of ‘social_int’.\nWhen considering what method(s) you can use to compare the models, remember to determine whether the models are nested or non-nested.\nYou can use KableExtra to present your model comparison results in a well formatted table. For a quick guide, review the tables flashcard.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nModel Comparison\nComparing summary() and anova() Outputs\nTable of Model Results\n\n\n\nRun model comparison via anova(), and present results in well formatted table:\n\nanova(wb_mdl0, wb_mdl1) %&gt;%\n    kable(caption = \"Model Comparison - wb_mdl0 vs wb_mdl1\", align = \"c\", digits = c(2,2,2,2,2,4)) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 1: Model Comparison - wb_mdl0 vs wb_mdl1\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n199\n5785.6\nNA\nNA\nNA\nNA\n\n\n198\n5451.1\n1\n334.49\n12.15\n6e-04\n\n\n\n\n\n\n\n\n\n\nThe output of anova(wb_mdl0, wb_mdl1) displays the \\(F\\)-statistic and the \\(p\\)-value in the following line:\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)  \n2    198 5451.1  1    334.49 12.15 0.0006045 ***\nWe can check that the \\(F\\)-statistic and the \\(p\\)-value are the the same as that which is given at the bottom of summary(wb_mdl1):\nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\nThe \\(F\\)-statistic and the \\(p\\)-value from anova(wb_mdl0, wb_mdl1) and summary(wb_mdl1) both match! This is because the \\(F\\)-test from a model with a single predictor (i.e, ‘wb_mdl1’) is really just a comparison against the null model (i.e, ‘wb_mdl0’).\n\n\n\ntab_model(wb_mdl0, wb_mdl1,\n          dv.labels = c(\"Wellbeing (WEMWBS Scores)\", \"Wellbeing (WEMWBS Scores)\"),\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\"),\n          title = \"Regression Table for Wellbeing Models wb0 and wb1\")\n\n\n\nTable 2: Regression Table for Wellbeing Models wb0 and wb1\n\n\n\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n36.29\n35.54 – 37.05\n&lt;0.001\n32.41\n30.09 – 34.73\n&lt;0.001\n\n\nSocial Interactions\n(number per week)\n\n\n\n0.32\n0.14 – 0.50\n0.001\n\n\nObservations\n200\n200\n\n\nR2 / R2 adjusted\n0.000 / 0.000\n0.058 / 0.053\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of social interactions was found to explain a significant amount of variance in wellbeing scores \\((F(1 ,198) = 12.15, p&lt;.001)\\). The model with social interactions was significantly better fitting than the intercept-only model, and thus social interactions is a useful predictor of wellbeing scores. Full regression results are presented in Table 2.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nLook at the amount of variation in wellbeing scores explained by models “wb_mdl1” and “wb_mdl2”.\nFrom this, can we answer the second research question of whether weekly outdoor time explains a significant amount of variance in wellbeing scores over and above social interactions?\nProvide justification/rationale for your answer.\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou will need to review the \\(R^2\\) and Adjusted \\(R^2\\) values.\nConsider whether comparing these numeric values would constitute a statistical comparison.\n\n\n\n\n\n\n\n Solution \n\n\nLet’s look at the amount of variance explained by each model:\n\nsummary(wb_mdl1)$r.squared\n\n[1] 0.0578147\n\nsummary(wb_mdl2)$adj.r.squared\n\n[1] 0.1176021\n\n\nThe model with weekly outdoor time as a predictor explains 12% of the variance, and the model without explains 6%. But, from only looking at the proportion of variance accounted for in each model, we cannot determine which model is statistically a better fit.\nTo answer the question ‘Does including weekly outdoor time as a predictor provide a significantly better fit of the data?’ we need to statistically compare wb_mdl1 to wb_mdl2.\n\n\n\n\n\nQuestion 8\n\n\nDoes weekly outdoor time explain a significant amount of variance in wellbeing scores over and above social interactions?\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo address RQ2, you need to statistically compare “wb_mdl1” and “wb_mdl2”.\nWhen considering what method(s) you can use to compare the models, remember to determine whether the models are nested or non-nested.\nYou can use KableExtra to present your model comparison results in a well formatted table. For a quick guide, review the tables flashcard.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nModel Comparison\nTable of Model Results\n\n\n\nTo statistically compare models, we could use an incremental \\(F\\)-test to compare the models since the models are nested and from the same dataset:\n\nanova(wb_mdl1, wb_mdl2) %&gt;%\n    kable(caption = \"Model Comparison - wb_mdl1 vs wb_mdl2\", align = \"c\", digits = c(2,2,2,2,2,4)) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 3: Model Comparison - wb_mdl1 vs wb_mdl2\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n198\n5451.10\nNA\nNA\nNA\nNA\n\n\n197\n5053.89\n1\n397.21\n15.48\n1e-04\n\n\n\n\n\n\n\n\n\n\nPresent results from both models:\n\ntab_model(wb_mdl1, wb_mdl2,\n          dv.labels = c(\"Wellbeing (WEMWBS Scores)\", \"Wellbeing (WEMWBS Scores)\"),\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\",\n                          \"outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression Table for Wellbeing Models wb1 and wb2\")\n\n\n\nTable 4: Regression Table for Wellbeing Models wb1 and wb2\n\n\n\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n32.41\n30.09 – 34.73\n&lt;0.001\n28.62\n25.69 – 31.55\n&lt;0.001\n\n\nSocial Interactions\n(number per week)\n0.32\n0.14 – 0.50\n0.001\n0.33\n0.16 – 0.51\n&lt;0.001\n\n\nOutdoor Time (hours per\nweek)\n\n\n\n0.20\n0.10 – 0.30\n&lt;0.001\n\n\nObservations\n200\n200\n\n\nR2 / R2 adjusted\n0.058 / 0.053\n0.126 / 0.118\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs presented in Table 3, weekly outdoor time was found to explain a significant amount of variance in wellbeing scores over and above weekly social interactions \\((F(1 ,197) = 15.48, p&lt;.001)\\).\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nA fellow researcher has suggested to examine the role of age in wellbeing scores. Based on their recommendation, compare the two following models, each looking at the associations of Wellbeing scores and different predictor variables.\n\\(\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social~Interactions} + \\beta_2 \\cdot \\text{Age} + \\epsilon\\)\n\\(\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Outdoor~Time} + \\epsilon\\)\nReport which model you think best fits the data, and justify your answer.\n\n\n\n\n\n\nHint\n\n\n\n\n\nAre the models are nested or non-nested? This will impact what method(s) you can use to compare the models.\n\n\n\n\n\n\n\n Solution \n\n\n\n#fit models\nwb_socint_age &lt;- lm(wellbeing ~ social_int + age, data = mwdata)\nwb_outdoor &lt;- lm(wellbeing ~ outdoor_time, data = mwdata)\n\n\n#AIC values\nAIC(wb_socint_age, wb_outdoor)\n\n              df      AIC\nwb_socint_age  4 1236.575\nwb_outdoor     3 1233.289\n\n#BIC values\nBIC(wb_socint_age, wb_outdoor)\n\n              df      BIC\nwb_socint_age  4 1249.769\nwb_outdoor     3 1243.184\n\n\n\n\n\n\n\n\nWe used \\(AIC\\) and \\(BIC\\) model selection to distinguish between two possible models describing the association between several personal factors and wellbeing scores. The model with outdoor time included as a single predictor was better fitting \\((AIC = 1233.29)\\) than the alternative model with weekly number of social interactions and age \\((AIC = 1236.58)\\) included. Based on the BIC value of the former model \\((BIC = 1243.18)\\), we concluded that it was better fitting than the alternative, latter model \\((BIC = 1249.77)\\).\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nThe code below fits 6 different models based on our mwdata:\n\nmodel1 &lt;- lm(wellbeing ~ social_int, data = mwdata)\nmodel2 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nmodel3 &lt;- lm(wellbeing ~ social_int + age, data = mwdata)\nmodel4 &lt;- lm(wellbeing ~ social_int + outdoor_time + age, data = mwdata)\nmodel5 &lt;- lm(wellbeing ~ social_int + outdoor_time + age + steps_k, data = mwdata)\nmodel6 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = wb_data)\n\nFor each of the below pairs of models, what methods are/are not available for us to use for comparison and why?\n\n\nmodel1 vs model2\n\n\nmodel2 vs model3\n\n\nmodel1 vs model4\n\n\nmodel3 vs model5\n\n\nmodel2 vs model6\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis flowchart might help you to reach your decision.\nYou may need to examine the dataset. It is especially important to check for completeness (e.g., are there any missing values?).\nRemember that not all models can be compared!\n\n\n\n\n\n\n\n Solution \n\n\n\n\nmodel1 vs model2\nmodel2 vs model3\nmodel1 vs model4\nmodel3 vs model5\nmodel2 vs model6\n\n\n\n\nThese models are nested - model2 contains all the variables of model1 and they are fitted on the same dataset.\n\nWe can therefore use an \\(F\\)-test or AIC and BIC.\n\n\n\n\nThese models are not nested, but they are fitted on the same dataset.\n\nWe can therefore use AIC or BIC, but we cannot use an \\(F\\)-test.\n\n\n\n\nThese models are nested - model4 contains all the variables of model1 and they are fitted on the same dataset.\n\nWe can therefore use an \\(F\\)-test or AIC and BIC.\n\n\n\n\nThese models are not nested, and they are not fitted on the same dataset. The “steps_k” variable contains missing values (over 30% of the data is missing for this variable), and so these whole rows are excluded from model5 (but they are included in model3).\nWe cannot compare these models.\n\n\n\n\nThese models are nested, but they are not fitted on the same dataset: model2 uses the ‘mwdata’ dataset, whilst model6 uses the ‘wb_data’ dataset.\nWe cannot compare these models."
  },
  {
    "objectID": "1_04_model_fit_compKIWI.html",
    "href": "1_04_model_fit_compKIWI.html",
    "title": "Model Fit and Comparisons",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand how to calculate the interpret \\(R^2\\) and adjusted-\\(R^2\\) as a measure of model quality.\nUnderstand the calculation and interpretation of the \\(F\\)-test of model utility.\nUnderstand measures of model fit using \\(F\\).\n\nUnderstand the principles of model selection and how to compare models via \\(F\\) tests, \\(AIC\\), and \\(BIC\\).\n\n\nBe up to date with lectures\nHave completed Week 1 and Week 2, and Week 3 lab exercises\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\nkableExtra\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\nThe example write-up sections included as part of the solutions are not perfect - they instead should give you a good example of what information you should include and how to structure this. Note that you must not copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more here.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv"
  },
  {
    "objectID": "1_04_model_fit_compKIWI.html#section-i-model-fit",
    "href": "1_04_model_fit_compKIWI.html#section-i-model-fit",
    "title": "Model Fit and Comparisons",
    "section": "Section I: Model Fit",
    "text": "Section I: Model Fit\nIn the first section of this lab, you will focus on the statistics contained within the highlighted sections of the summary() output below. You will be both calculating these by hand and deriving via R before interpreting these values in the context of the research question.\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nFit the following multiple linear regression model, and assign the output to an object called mdl.\n\\[\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time + \\epsilon\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is the same model that you have fitted in the previous couple of weeks.\nWe can fit our multiple regression model using the lm() function.\nFor a recap, see the statistical models flashcards, specifically the multiple linear regression models - description & specification card.\n\n\n\n\n\n\n\n Solution \n\n\n\nmdl &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\n\n\nQuestion 2\n\n\nWhat is the proportion of the total variability in wellbeing scores explained by the model?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe proportion of the total variability explained is given by \\(R^2\\). Since the model includes 2 predictors, you should report the Adjusted-\\(R^2\\).\nFor a more detailed overview, see the R-squared and Adjusted R-squared flashcard.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR Function\n\n\n\nIn R we can write:\n\n#Define n & k\nn &lt;- nrow(mwdata)\nk &lt;- 2\n\n#Predicted scores\nwellbeing_fitted &lt;- mwdata %&gt;%\n  mutate(\n    wellbeing_pred = predict(mdl),\n    wellbeing_resid = wellbeing - wellbeing_pred)\n\n# Sums of Squares, and R / Adjusted R Squared\nwellbeing_fitted %&gt;%\n  summarise(\n    SSModel = sum((wellbeing_pred - mean(wellbeing))^2),\n    SSTotal = sum((wellbeing - mean(wellbeing))^2),\n    SSResid = sum(wellbeing_resid^2)\n  ) %&gt;% \n  summarise(\n    RSquared = SSModel / SSTotal,\n    AdjRSquared = 1-((1-(RSquared))*(n-1)/(n-k-1))\n  )\n\n# A tibble: 1 × 2\n  RSquared AdjRSquared\n     &lt;dbl&gt;       &lt;dbl&gt;\n1    0.126       0.118\n\n\nThe output displays the Adjusted \\(R^2\\) value in the following column:\nAdjRSquared\n &lt;dbl&gt;\n 0.118\n\n\n\n#look in second bottom row - Multiple R Squared and Adjusted R Squared both reported here\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe output of summary() displays the Adjusted \\(R^2\\) value in the following line:\nAdjusted R-squared:  0.1176 \n\n\n\n\n\n\n\n\n\nApproximately 12% of the total variability in wellbeing scores is accounted for by social interactions and outdoor time.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nWhat do you notice about the unadjusted and adjusted \\(R^2\\) values?\n\n\n\n\n\n\nHint\n\n\n\n\n\nAre they similar or quite different? Why might this be?\nThink about when you would report \\(R^2\\) and Adjusted-\\(R^2\\) - the R-squared and Adjusted R-squared flashcard has some detail on this, but it will also be useful to think about how each is calculated.\n\n\n\n\n\n\n\n Solution \n\n\nThe values of the unadjusted (0.1265) and adjusted \\(R^2\\) (0.1176) values are quite similar. This is because the sample size is quite large \\((n = 200)\\), and the number of predictors \\((k = 2)\\) is small.\n\n\n\n\n\nQuestion 4\n\n\nPerform a model utility test at the 5% significance level and report your results.\nIn other words, conduct an \\(F\\)-test against the null hypothesis that the model is ineffective at predicting wellbeing scores using social interactions and outdoor time by computing the \\(F\\)-statistic using its definition.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe \\(F\\)-ratio is used to test the null hypothesis that all regression slopes are zero.\nSee the F-ratio flashcard for a more detailed overview.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nManually\nR Function\n\n\n\n\n#df(model) = k \ndf1 &lt;- 2\n\n#df(residual) = n - k - 1\ndf2 &lt;- nrow(mwdata) - 2 - 1\n\nf_star &lt;- qf(0.95, df1, df2)\n\n#check value\nf_star\n\n[1] 3.041753\n\n\n\nmodel_utility &lt;- wellbeing_fitted %&gt;%\n  summarise(\n    SSModel = sum((wellbeing_pred - mean(wellbeing))^2),\n    SSResid = sum(wellbeing_resid^2),\n    MSModel = SSModel / df1,\n    MSResid = SSResid / df2,\n    FObs = MSModel / MSResid\n  )\nmodel_utility\n\n# A tibble: 1 × 5\n  SSModel SSResid MSModel MSResid  FObs\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    732.   5054.    366.    25.7  14.3\n\n\nWe can also compute the p-value:\n\npvalue &lt;- 1 - pf(model_utility$FObs, df1, df2)\npvalue\n\n[1] 1.643779e-06\n\n\nThe value 1.643779e-06 simply means \\(1.6 \\times 10^{-6}\\), so it’s a really small number (i.e., 0.000001643779).\n\n\n\n#look in bottom row\nsummary(mdl)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\nThe relevant row is the following:\n\nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\n\n\n\n\n\nWe performed an \\(F\\)-test of model utility at the 5% significance level, where \\(F(2,197) = 14.26, p &lt;.001\\).\nThe large \\(F\\)-statistic and small \\(p\\)-value \\((p &lt;.001)\\) suggested that we have very strong evidence against the null hypothesis.\nIn other words, the data provide strong evidence that the number of social interactions and outdoor time are predictors of wellbeing scores."
  },
  {
    "objectID": "1_04_model_fit_compKIWI.html#section-ii-model-comparisons",
    "href": "1_04_model_fit_compKIWI.html#section-ii-model-comparisons",
    "title": "Model Fit and Comparisons",
    "section": "Section II: Model Comparisons",
    "text": "Section II: Model Comparisons\nIn the second section of this lab, you will focus on model comparison where you will formally test a number of research questions:\n\n\nRQ1: Is the number of weekly social interactions a useful predictor of wellbeing scores?\nRQ2: Does weekly outdoor time explain a significant amount of variance in wellbeing scores over and above the number of weekly social interactions?\n\n\n\n\nQuestion 5\n\n\nFit the below 3 models required to address the 2 research questions stated above. Note down which model(s) will be used to address each research question, and examine the results of each model.\nName the models as follows: “wb_mdl0”, “wb_mdl1”, “wb_mdl2”\n\\[\n\\text{Wellbeing} = \\beta_0  + \\epsilon\n\\]\n\n\\[\n\\text{Wellbeing} = \\beta_0  + \\beta_1 \\cdot Social~Interactions + \\epsilon\n\\]\n\n\\[\n\\text{Wellbeing} = \\beta_0  + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time + \\epsilon\n\\] \n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can fit our multiple regression models individually using the lm() function. For a recap, see the statistical models flashcards, specifically the multiple linear regression models - description & specification card.\nThe summary() function will be useful to examine the model output.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nwb_mdl0\nwb_mdl1\nwb_mdl2\n\n\n\n\n#null/intercept only model\nwb_mdl0 &lt;- lm(wellbeing ~ 1, data = mwdata)\nsummary(wb_mdl0)\n\n\nCall:\nlm(formula = wellbeing ~ 1, data = mwdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.295  -3.295  -1.295   3.705  22.705 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.2950     0.3813   95.19   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.392 on 199 degrees of freedom\n\n\n\n\n\n#model with social interactions\nwb_mdl1 &lt;- lm(wellbeing ~ social_int, data = mwdata)\nsummary(wb_mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ social_int, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5628  -3.2741  -0.7908   3.3703  20.4706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 32.40771    1.17532  27.573  &lt; 2e-16 ***\nsocial_int   0.32220    0.09243   3.486 0.000605 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.247 on 198 degrees of freedom\nMultiple R-squared:  0.05781,   Adjusted R-squared:  0.05306 \nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n\n\n\n\n\n#model with social interactions and outdoor time\nwb_mdl2 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(wb_mdl2)\n\n\nCall:\nlm(formula = wellbeing ~ social_int + outdoor_time, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7611  -3.1308  -0.4213   3.3126  18.8406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.62018    1.48786  19.236  &lt; 2e-16 ***\nsocial_int    0.33488    0.08929   3.751 0.000232 ***\noutdoor_time  0.19909    0.05060   3.935 0.000115 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.065 on 197 degrees of freedom\nMultiple R-squared:  0.1265,    Adjusted R-squared:  0.1176 \nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n\n\n\n\nThe models required to address each research question (RQ) are:\n\nRQ1: Models wb_mdl0 and wb_mdl1\nRQ2: Models wb_mdl1 and wb_mdl2\n\n\n\n\n\n\nQuestion 6\n\n\nRQ1: Is the number of weekly social interactions a useful predictor of wellbeing scores?\nCheck that the \\(F\\)-statistic and the \\(p\\)-value are the same from the model comparison as that which are given at the bottom of summary(wb_mdl1).\nProvide the key model results from the two models in a single formatted table.\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember that the null model tests the null hypothesis that all beta coefficients are zero. By comparing wb_mdl0 to wb_mdl1, we can test whether we should include the IV of ‘social_int’.\nWhen considering what method(s) you can use to compare the models, remember to determine whether the models are nested or non-nested.\nYou can use KableExtra to present your model comparison results in a well formatted table. For a quick guide, review the tables flashcard.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nModel Comparison\nComparing summary() and anova() Outputs\nTable of Model Results\n\n\n\nRun model comparison via anova(), and present results in well formatted table:\n\nanova(wb_mdl0, wb_mdl1) %&gt;%\n    kable(caption = \"Model Comparison - wb_mdl0 vs wb_mdl1\", align = \"c\", digits = c(2,2,2,2,2,4)) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 1: Model Comparison - wb_mdl0 vs wb_mdl1\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n199\n5785.6\nNA\nNA\nNA\nNA\n\n\n198\n5451.1\n1\n334.49\n12.15\n6e-04\n\n\n\n\n\n\n\n\n\n\nThe output of anova(wb_mdl0, wb_mdl1) displays the \\(F\\)-statistic and the \\(p\\)-value in the following line:\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)  \n2    198 5451.1  1    334.49 12.15 0.0006045 ***\nWe can check that the \\(F\\)-statistic and the \\(p\\)-value are the the same as that which is given at the bottom of summary(wb_mdl1):\nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\nThe \\(F\\)-statistic and the \\(p\\)-value from anova(wb_mdl0, wb_mdl1) and summary(wb_mdl1) both match! This is because the \\(F\\)-test from a model with a single predictor (i.e, ‘wb_mdl1’) is really just a comparison against the null model (i.e, ‘wb_mdl0’).\n\n\n\ntab_model(wb_mdl0, wb_mdl1,\n          dv.labels = c(\"Wellbeing (WEMWBS Scores)\", \"Wellbeing (WEMWBS Scores)\"),\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\"),\n          title = \"Regression Table for Wellbeing Models wb0 and wb1\")\n\n\n\nTable 2: Regression Table for Wellbeing Models wb0 and wb1\n\n\n\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n36.29\n35.54 – 37.05\n&lt;0.001\n32.41\n30.09 – 34.73\n&lt;0.001\n\n\nSocial Interactions\n(number per week)\n\n\n\n0.32\n0.14 – 0.50\n0.001\n\n\nObservations\n200\n200\n\n\nR2 / R2 adjusted\n0.000 / 0.000\n0.058 / 0.053\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of social interactions was found to explain a significant amount of variance in wellbeing scores \\((F(1 ,198) = 12.15, p&lt;.001)\\). The model with social interactions was significantly better fitting than the intercept-only model, and thus social interactions is a useful predictor of wellbeing scores. Full regression results are presented in Table 2.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nLook at the amount of variation in wellbeing scores explained by models “wb_mdl1” and “wb_mdl2”.\nFrom this, can we answer the second research question of whether weekly outdoor time explains a significant amount of variance in wellbeing scores over and above social interactions?\nProvide justification/rationale for your answer.\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou will need to review the \\(R^2\\) and Adjusted \\(R^2\\) values.\nConsider whether comparing these numeric values would constitute a statistical comparison.\n\n\n\n\n\n\n\n Solution \n\n\nLet’s look at the amount of variance explained by each model:\n\nsummary(wb_mdl1)$r.squared\n\n[1] 0.0578147\n\nsummary(wb_mdl2)$adj.r.squared\n\n[1] 0.1176021\n\n\nThe model with weekly outdoor time as a predictor explains 12% of the variance, and the model without explains 6%. But, from only looking at the proportion of variance accounted for in each model, we cannot determine which model is statistically a better fit.\nTo answer the question ‘Does including weekly outdoor time as a predictor provide a significantly better fit of the data?’ we need to statistically compare wb_mdl1 to wb_mdl2.\n\n\n\n\n\nQuestion 8\n\n\nDoes weekly outdoor time explain a significant amount of variance in wellbeing scores over and above social interactions?\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo address RQ2, you need to statistically compare “wb_mdl1” and “wb_mdl2”.\nWhen considering what method(s) you can use to compare the models, remember to determine whether the models are nested or non-nested.\nYou can use KableExtra to present your model comparison results in a well formatted table. For a quick guide, review the tables flashcard.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nModel Comparison\nTable of Model Results\n\n\n\nTo statistically compare models, we could use an incremental \\(F\\)-test to compare the models since the models are nested and from the same dataset:\n\nanova(wb_mdl1, wb_mdl2) %&gt;%\n    kable(caption = \"Model Comparison - wb_mdl1 vs wb_mdl2\", align = \"c\", digits = c(2,2,2,2,2,4)) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\n\nTable 3: Model Comparison - wb_mdl1 vs wb_mdl2\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n198\n5451.10\nNA\nNA\nNA\nNA\n\n\n197\n5053.89\n1\n397.21\n15.48\n1e-04\n\n\n\n\n\n\n\n\n\n\nPresent results from both models:\n\ntab_model(wb_mdl1, wb_mdl2,\n          dv.labels = c(\"Wellbeing (WEMWBS Scores)\", \"Wellbeing (WEMWBS Scores)\"),\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\",\n                          \"outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression Table for Wellbeing Models wb1 and wb2\")\n\n\n\nTable 4: Regression Table for Wellbeing Models wb1 and wb2\n\n\n\n\n\n\n\n\n\n\n\n \nWellbeing (WEMWBS Scores)\nWellbeing (WEMWBS Scores)\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n32.41\n30.09 – 34.73\n&lt;0.001\n28.62\n25.69 – 31.55\n&lt;0.001\n\n\nSocial Interactions\n(number per week)\n0.32\n0.14 – 0.50\n0.001\n0.33\n0.16 – 0.51\n&lt;0.001\n\n\nOutdoor Time (hours per\nweek)\n\n\n\n0.20\n0.10 – 0.30\n&lt;0.001\n\n\nObservations\n200\n200\n\n\nR2 / R2 adjusted\n0.058 / 0.053\n0.126 / 0.118\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs presented in Table 3, weekly outdoor time was found to explain a significant amount of variance in wellbeing scores over and above weekly social interactions \\((F(1 ,197) = 15.48, p&lt;.001)\\).\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nA fellow researcher has suggested to examine the role of age in wellbeing scores. Based on their recommendation, compare the two following models, each looking at the associations of Wellbeing scores and different predictor variables.\n\\(\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social~Interactions} + \\beta_2 \\cdot \\text{Age} + \\epsilon\\)\n\\(\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Outdoor~Time} + \\epsilon\\)\nReport which model you think best fits the data, and justify your answer.\n\n\n\n\n\n\nHint\n\n\n\n\n\nAre the models are nested or non-nested? This will impact what method(s) you can use to compare the models.\n\n\n\n\n\n\n\n Solution \n\n\n\n#fit models\nwb_socint_age &lt;- lm(wellbeing ~ social_int + age, data = mwdata)\nwb_outdoor &lt;- lm(wellbeing ~ outdoor_time, data = mwdata)\n\n\n#AIC values\nAIC(wb_socint_age, wb_outdoor)\n\n              df      AIC\nwb_socint_age  4 1236.575\nwb_outdoor     3 1233.289\n\n#BIC values\nBIC(wb_socint_age, wb_outdoor)\n\n              df      BIC\nwb_socint_age  4 1249.769\nwb_outdoor     3 1243.184\n\n\n\n\n\n\n\n\nWe used \\(AIC\\) and \\(BIC\\) model selection to distinguish between two possible models describing the association between several personal factors and wellbeing scores. The model with outdoor time included as a single predictor was better fitting \\((AIC = 1233.29)\\) than the alternative model with weekly number of social interactions and age \\((AIC = 1236.58)\\) included. Based on the BIC value of the former model \\((BIC = 1243.18)\\), we concluded that it was better fitting than the alternative, latter model \\((BIC = 1249.77)\\).\n\n\n\n\n\n\n\n\nQuestion 10\n\n\nThe code below fits 6 different models based on our mwdata:\n\nmodel1 &lt;- lm(wellbeing ~ social_int, data = mwdata)\nmodel2 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nmodel3 &lt;- lm(wellbeing ~ social_int + age, data = mwdata)\nmodel4 &lt;- lm(wellbeing ~ social_int + outdoor_time + age, data = mwdata)\nmodel5 &lt;- lm(wellbeing ~ social_int + outdoor_time + age + steps_k, data = mwdata)\nmodel6 &lt;- lm(wellbeing ~ social_int + outdoor_time, data = wb_data)\n\nFor each of the below pairs of models, what methods are/are not available for us to use for comparison and why?\n\n\nmodel1 vs model2\n\n\nmodel2 vs model3\n\n\nmodel1 vs model4\n\n\nmodel3 vs model5\n\n\nmodel2 vs model6\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis flowchart might help you to reach your decision.\nYou may need to examine the dataset. It is especially important to check for completeness (e.g., are there any missing values?).\nRemember that not all models can be compared!\n\n\n\n\n\n\n\n Solution \n\n\n\n\nmodel1 vs model2\nmodel2 vs model3\nmodel1 vs model4\nmodel3 vs model5\nmodel2 vs model6\n\n\n\n\nThese models are nested - model2 contains all the variables of model1 and they are fitted on the same dataset.\n\nWe can therefore use an \\(F\\)-test or AIC and BIC.\n\n\n\n\nThese models are not nested, but they are fitted on the same dataset.\n\nWe can therefore use AIC or BIC, but we cannot use an \\(F\\)-test.\n\n\n\n\nThese models are nested - model4 contains all the variables of model1 and they are fitted on the same dataset.\n\nWe can therefore use an \\(F\\)-test or AIC and BIC.\n\n\n\n\nThese models are not nested, and they are not fitted on the same dataset. The “steps_k” variable contains missing values (over 30% of the data is missing for this variable), and so these whole rows are excluded from model5 (but they are included in model3).\nWe cannot compare these models.\n\n\n\n\nThese models are nested, but they are not fitted on the same dataset: model2 uses the ‘mwdata’ dataset, whilst model6 uses the ‘wb_data’ dataset.\nWe cannot compare these models."
  },
  {
    "objectID": "1_05_writeup_recap.html",
    "href": "1_05_writeup_recap.html",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand how to write-up and provide interpretation of linear models with single and multiple predictors.\n\n\nBe up to date with lectures\nHave completed Labs 1 - 4\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npsych\npatchwork\nsjPlot\nkableExtra\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/FOMOdataset.csv"
  },
  {
    "objectID": "1_05_writeup_recap.html#study-overview",
    "href": "1_05_writeup_recap.html#study-overview",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Aim\nExplore the associations among Fear of Missing Out (FoMO), age, social media networks, and the Big Five personality traits.\nResearch Questions\n\nRQ1: Does age predict FoMO?\nRQ2: Does the number of Instagram followers explain a significant amount of variance in FoMO over and above age?\nRQ3: Does personality predict FoMO?\n\n\n\n FoMO data codebook.\n\n\nDescription\nThe data used for this write-up exercise are simulated, drawing on recent work on Fear of Missing Out (FoMO), socio-demographic factors, and the Big Five personality traits. The simulated data are based on the findings of this work, and acted to expand upon the methods and results reported in the following paper:\nRozgonjuk, D., Sindermann, C., Elhai, J. D., & Montag, C. (2021). Individual differences in Fear of Missing Out (FoMO): Age, gender, and the Big Five personality trait domains, facets, and items. Personality and Individual Differences, 171, 110546. https://doi.org/10.1016/j.paid.2020.110546\nIn the current study, participants were invited to an online study investigating the associations among FoMO, socio-demographic factors, and personality. The final sample comprised 3370 people. Participants completed a FOMO scale and a personality inventory. The 10-item FOMO scale measured the extent of experiencing apprehension regarding missing out on interesting events of others on a 5-point scale (1 = “not at all true of me” to 5 = “extremely true of me”), producing a possible range of scores between 10 and 50. The Big Five Inventory (BFI) is a 45-item personality assessment questionnaire (note that only 43 items were used to match the study above) that uses a five-point response scale (1 = “very inapplicable” to 5 = “very applicable”). The BFI consists of five domains: Neuroticism (8 items; possible range of scores 8-40), Extraversion (8 items; possible range of scores 8-40), Openness to Experience (10 items; possible range of scores 10-50), Agreeableness (8 items; possible range of scores 8-40), and Conscientiousness (9 items; possible range of scores 9-45). We extended the aforementioned study to include an extra socio-demographic variable - a measure of popularity on social media based on the number of followers. Unlike the original study, we do not have measures of gender, education level, or specific country of residence.\nData Dictionary\nThe data in FOMOdataset.csv contain eight attributes collected from a simulated sample of \\(n=3370\\) hypothetical individuals across the UK, and include:\n\n\n\n\n\n\n\nVariable\n      Description\n    \n\n\nFOMO\nFoMO Score (as measured by the 10-item FoMO scale)\n\n\nAge\nAge (in years)\n\n\nN\nScore on personality items assessing Neuroticism from the Big Five Inventory (BFI)\n\n\nE\nScore on personality items assessing Extraversion from the Big Five Inventory (BFI)\n\n\nO\nScore on personality items assessing Openness from the Big Five Inventory (BFI)\n\n\nA\nScore on personality items assessing Agreeableness from the Big Five Inventory (BFI)\n\n\nC\nScore on personality items assessing Conscientiousness from the Big Five Inventory (BFI)\n\n\nTotalFollowers\nTotal Number of Instagram Followers\n\n\n\n\n\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\n\n\nFOMO\n      Age\n      N\n      E\n      O\n      A\n      C\n      TotalFollowers\n    \n\n\n28\n38\n26\n30\n32\n27\n38\n98\n\n\n26\n30\n23\n27\n33\n32\n30\n192\n\n\n23\n33\n14\n30\n36\n27\n24\n177\n\n\n18\n44\n26\n21\n37\n28\n34\n119\n\n\n19\n43\n32\n21\n41\n35\n40\n278\n\n\n24\n43\n22\n25\n31\n30\n24\n184\n\n\n\n\n\n\n\n\n\n\n\n\nSetup\n\nSetup\n\n\n\nCreate a new RMarkdown file\nLoad the required package(s)\nRead the FOMO dataset into R, assigning it to an object named fomo\n\n\n\n\n\n\n Solution \n\n\n\n#Loading the required package(s)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(sjPlot)\nlibrary(kableExtra)\nlibrary(psych)\n\n#Reading in fomo data and storing in object named 'fomo'\nfomo &lt;- read_csv(\"https://uoepsy.github.io/data/FOMOdataset.csv\")\n\n#check first six rows\nhead(fomo)\n\n# A tibble: 6 × 8\n   FOMO   Age     N     E     O     A     C TotalFollowers\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1    28    38    26    30    32    27    38             98\n2    26    30    23    27    33    32    30            192\n3    23    33    14    30    36    27    24            177\n4    18    44    26    21    37    28    34            119\n5    19    43    32    21    41    35    40            278\n6    24    43    22    25    31    30    24            184\n\n\n\n\n\n\nProvided Analysis Code\nBelow you will find the code required to conduct the analysis to address the research questions. This should look similar (in most areas) to what you worked through in lecture.\n\n Provided Analysis Code"
  },
  {
    "objectID": "1_05_writeup_recap.html#data-management",
    "href": "1_05_writeup_recap.html#data-management",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "Data Management",
    "text": "Data Management\n\nlibrary(tidyverse) # for all things!\nlibrary(psych) # good for descriptive stats\nlibrary(patchwork) # grouping plots together\nlibrary(kableExtra) # useful for creating nice tables\nlibrary(sjPlot) #regression tables & plots\n\nfomo &lt;- read_csv(\"https://uoepsy.github.io/data/FOMOdataset.csv\")\n\n# standardise FoMO & personality scores for RQ3\nfomo &lt;- \n  fomo %&gt;% \n    mutate(\n      FOMOz = (FOMO-mean(FOMO))/sd(FOMO),\n      Oz = (O-mean(O))/sd(O),\n      Cz = (C-mean(C))/sd(C),\n      Ez = (E-mean(E))/sd(E),\n      Az = (A-mean(A))/sd(A),      \n      Nz = (N-mean(N))/sd(N))\n#alternatively, you could do FOMOz = scale(FOMO, center = TRUE, scale = TRUE)"
  },
  {
    "objectID": "1_05_writeup_recap.html#overall",
    "href": "1_05_writeup_recap.html#overall",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "Overall",
    "text": "Overall\n\n#######\n#Descriptive Stats\n#######\n\n\n# the describe() function is from the psych package, and kable() from kableExtra which is used to make a nice table where the values are rounded to 2 decimal places using digits = 2. \n# We are first renaming our variables to give more appropriate / informative names.\n#Next we are selecting columns 2, 3, 4, 8, and 9 from the describe output (n, mean, sd, min, max)\n\nfomo %&gt;% \n    select(FOMO, Age, TotalFollowers, N, E, O, C, A) %&gt;%\n    rename(\"Fear of Missing Out\" = FOMO, \"Age (in years)\" = Age, \"Number of Instagram Followers\" = TotalFollowers, \"Neuroticism\" = N, \"Extraversion\" = E, \"Openness\" = O, \"Conscientiousness\" = C, \"Agreeableness\" = A) %&gt;%\n    describe() %&gt;%\n    select(2:4, 8:9) %&gt;%\n    rename(\"N\" = n, \"Mean\" = mean, \"SD\" = sd, \"Minimum\" = min, \"Maximum\" = max) %&gt;%\n        kable(., caption = \"FoMO, Socio-Demographic Factors, and Personality Traits Descriptive Statistics\", digits = 2) %&gt;%\n        kable_styling()   \n\n\nFoMO, Socio-Demographic Factors, and Personality Traits Descriptive Statistics\n\n\nN\nMean\nSD\nMinimum\nMaximum\n\n\n\nFear of Missing Out\n3370\n24.63\n6.42\n10\n46\n\n\nAge (in years)\n3370\n33.61\n10.42\n12\n75\n\n\nNumber of Instagram Followers\n3370\n203.25\n93.42\n1\n594\n\n\nNeuroticism\n3370\n22.92\n5.77\n8\n40\n\n\nExtraversion\n3370\n25.88\n5.94\n8\n40\n\n\nOpenness\n3370\n37.61\n5.80\n14\n50\n\n\nConscientiousness\n3370\n31.04\n5.49\n13\n45\n\n\nAgreeableness\n3370\n30.89\n4.93\n13\n43\n\n\n\n\n#from above, no missing values and scores within range (look at min and max values)\n\n\n# scatterplot matrix, hist, and corr of FoMO, Socio-Demographic Factors, and Personality Traits Descriptive Statistics\npairs.panels(fomo %&gt;%\n    select(-FOMOz, -Oz, -Cz, -Ez, -Az, -Nz))"
  },
  {
    "objectID": "1_05_writeup_recap.html#rq1",
    "href": "1_05_writeup_recap.html#rq1",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "RQ1",
    "text": "RQ1\n\n#######\n#Descriptive Stats\n#######\n\nfomo %&gt;% \n    select(FOMO, Age) %&gt;%\n    rename(\"Fear of Missing Out\" = FOMO, \"Age (in years)\" = Age) %&gt;%\n    describe() %&gt;%\n    select(2:4, 8:9) %&gt;%\n    rename(\"N\" = n, \"Mean\" = mean, \"SD\" = sd, \"Minimum\" = min, \"Maximum\" = max) %&gt;%    \n        kable(., caption = \"FoMO and Age Descriptive Statistics\", digits = 2) %&gt;%\n        kable_styling()    \n\n\nFoMO and Age Descriptive Statistics\n\n\nN\nMean\nSD\nMinimum\nMaximum\n\n\n\nFear of Missing Out\n3370\n24.63\n6.42\n10\n46\n\n\nAge (in years)\n3370\n33.61\n10.42\n12\n75\n\n\n\n\n# scatterplot\np1 &lt;- ggplot(data = fomo, aes(x = Age, y = FOMO)) + \n    geom_point() + \n  geom_smooth(method = 'lm', se = FALSE, colour = 'red', linewidth=2) +\n  labs(x = \"(a) Age (in years)\", y = \"Fear of Missing Out\")\np1\n\n\n\n\n\n\n\n\n#######\n#Model Building\n#######\n\nfomo_mdl1 &lt;- lm(FOMO ~ Age, data = fomo)\nsummary(fomo_mdl1)\n\n\nCall:\nlm(formula = FOMO ~ Age, data = fomo)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1028  -4.2129  -0.1602   4.0551  22.2321 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 31.22239    0.35408   88.18   &lt;2e-16 ***\nAge         -0.19617    0.01006  -19.50   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.084 on 3368 degrees of freedom\nMultiple R-squared:  0.1014,    Adjusted R-squared:  0.1011 \nF-statistic: 380.1 on 1 and 3368 DF,  p-value: &lt; 2.2e-16\n\nconfint(fomo_mdl1)\n\n                 2.5 %     97.5 %\n(Intercept) 30.5281496 31.9166293\nAge         -0.2158992 -0.1764425\n\n\n\n#######\n#Table for Results\n#######\n\ntab_model(fomo_mdl1,\n          dv.labels = \"FoMO\",\n          pred.labels = c(\"Age\" = \"Age (in years)\"), \n          title = \"RQ1: Regression Table for FoMO Model\")\n\n\nRQ1: Regression Table for FoMO Model\n\n\n \nFoMO\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n31.22\n30.53 – 31.92\n&lt;0.001\n\n\nAge (in years)\n-0.20\n-0.22 – -0.18\n&lt;0.001\n\n\nObservations\n3370\n\n\nR2 / R2 adjusted\n0.101 / 0.101"
  },
  {
    "objectID": "1_05_writeup_recap.html#rq2",
    "href": "1_05_writeup_recap.html#rq2",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "RQ2",
    "text": "RQ2\n\n#######\n#Descriptive Stats\n#######\n\nfomo %&gt;% \n    select(FOMO, Age, TotalFollowers) %&gt;%\n    rename(\"Fear of Missing Out\" = FOMO, \"Age (in years)\" = Age, \"Number of Instagram Followers\" = TotalFollowers) %&gt;%\n    describe() %&gt;%\n    select(2:4, 8:9) %&gt;%\n    rename(\"N\" = n, \"Mean\" = mean, \"SD\" = sd, \"Minimum\" = min, \"Maximum\" = max) %&gt;%    \n        kable(., caption = \"FoMO and Socio-Demographic Factors Descriptive Statistics\", digits = 2) %&gt;%\n        kable_styling()    \n\n\nFoMO and Socio-Demographic Factors Descriptive Statistics\n\n\nN\nMean\nSD\nMinimum\nMaximum\n\n\n\nFear of Missing Out\n3370\n24.63\n6.42\n10\n46\n\n\nAge (in years)\n3370\n33.61\n10.42\n12\n75\n\n\nNumber of Instagram Followers\n3370\n203.25\n93.42\n1\n594\n\n\n\n\n# scatterplots\np2 &lt;- ggplot(data = fomo, aes(x = TotalFollowers, y = FOMO)) + \n    geom_point() + \n  geom_smooth(method = 'lm', se = FALSE, colour = 'purple', linewidth=2) +\n  labs(x = \"(b) Total Number of \\nInstagram Followers\", y = \"Fear of Missing Out\")\n\np2\n\n\n\n\n\n\n\n\n#######\n#Model Building\n#######\n\nfomo_mdl2 &lt;- lm(FOMO ~ Age + TotalFollowers, data = fomo)\nsummary(fomo_mdl2)\n\n\nCall:\nlm(formula = FOMO ~ Age + TotalFollowers, data = fomo)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.273  -4.066  -0.071   3.841  21.705 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    26.521966   0.446021   59.46   &lt;2e-16 ***\nAge            -0.165111   0.009871  -16.73   &lt;2e-16 ***\nTotalFollowers  0.017989   0.001101   16.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.857 on 3367 degrees of freedom\nMultiple R-squared:  0.1674,    Adjusted R-squared:  0.1669 \nF-statistic: 338.6 on 2 and 3367 DF,  p-value: &lt; 2.2e-16\n\nconfint(fomo_mdl2)\n\n                     2.5 %      97.5 %\n(Intercept)    25.64746694 27.39646570\nAge            -0.18446526 -0.14575589\nTotalFollowers  0.01583102  0.02014788\n\n\n\n#######\n#Model Comparison\n#######\n\nanova(fomo_mdl1 ,fomo_mdl2) %&gt;%\n    kable(caption = \"Model Comparison - fomo_mdl1 vs fomo_mdl2\", align = \"c\", digits = c(2,2,2,2,2,60)) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\nModel Comparison - fomo_mdl1 vs fomo_mdl2\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n3368\n124676.4\nNA\nNA\nNA\nNA\n\n\n3367\n115515.0\n1\n9161.44\n267.04\n7.98e-58\n\n\n\n\n\n\n#######\n#Table for Results\n#######\n\ntab_model(fomo_mdl1 ,fomo_mdl2,\n          dv.labels = c(\"FoMO\",\"FoMO\"),\n          pred.labels = c(\"Age\" = \"Age (in years)\",\n                          \"TotalFollowers\" = \"Number of Instagram Followers\"), \n          title = \"RQ2 - Regression Table for FoMO Model\")\n\n\nRQ2 - Regression Table for FoMO Model\n\n\n\n\n\n\n\n\n\n\n\n \nFoMO\nFoMO\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n31.22\n30.53 – 31.92\n&lt;0.001\n26.52\n25.65 – 27.40\n&lt;0.001\n\n\nAge (in years)\n-0.20\n-0.22 – -0.18\n&lt;0.001\n-0.17\n-0.18 – -0.15\n&lt;0.001\n\n\nNumber of Instagram\nFollowers\n\n\n\n0.02\n0.02 – 0.02\n&lt;0.001\n\n\nObservations\n3370\n3370\n\n\nR2 / R2 adjusted\n0.101 / 0.101\n0.167 / 0.167"
  },
  {
    "objectID": "1_05_writeup_recap.html#rq3",
    "href": "1_05_writeup_recap.html#rq3",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "RQ3",
    "text": "RQ3\n\n#######\n#Descriptive Stats\n#######\n\nfomo %&gt;% \n    select(FOMO, N, E, O, C, A) %&gt;%\n    rename(\"Fear of Missing Out\" = FOMO, \"Neuroticism\" = N, \"Extraversion\" = E, \"Openness\" = O, \"Conscientiousness\" = C, \"Agreeableness\" = A) %&gt;%\n    describe() %&gt;%\n    select(2:4, 8:9) %&gt;%\n    rename(\"N\" = n, \"Mean\" = mean, \"SD\" = sd, \"Minimum\" = min, \"Maximum\" = max) %&gt;%    \n        kable(., caption = \"FoMO and Personality Descriptive Statistics\", digits = 2) %&gt;%\n        kable_styling()  \n\n\nFoMO and Personality Descriptive Statistics\n\n\nN\nMean\nSD\nMinimum\nMaximum\n\n\n\nFear of Missing Out\n3370\n24.63\n6.42\n10\n46\n\n\nNeuroticism\n3370\n22.92\n5.77\n8\n40\n\n\nExtraversion\n3370\n25.88\n5.94\n8\n40\n\n\nOpenness\n3370\n37.61\n5.80\n14\n50\n\n\nConscientiousness\n3370\n31.04\n5.49\n13\n45\n\n\nAgreeableness\n3370\n30.89\n4.93\n13\n43\n\n\n\n\n\n\n#######\n#Model Building\n#######\n\nfomo_mdl3 &lt;- lm(FOMOz ~ Nz + Ez + Oz + Cz + Az, data = fomo)\nsummary(fomo_mdl3)\n\n\nCall:\nlm(formula = FOMOz ~ Nz + Ez + Oz + Cz + Az, data = fomo)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.74061 -0.60648 -0.01036  0.59506  3.13402 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.453e-17  1.527e-02   0.000    1.000    \nNz           4.266e-01  1.643e-02  25.971  &lt; 2e-16 ***\nEz           1.744e-02  1.570e-02   1.111    0.267    \nOz           1.117e-02  1.550e-02   0.721    0.471    \nCz          -3.079e-01  1.596e-02 -19.289  &lt; 2e-16 ***\nAz          -8.508e-02  1.544e-02  -5.511 3.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8865 on 3364 degrees of freedom\nMultiple R-squared:  0.2152,    Adjusted R-squared:  0.2141 \nF-statistic: 184.5 on 5 and 3364 DF,  p-value: &lt; 2.2e-16\n\nconfint(fomo_mdl3)\n\n                  2.5 %      97.5 %\n(Intercept) -0.02994194  0.02994194\nNz           0.39441959  0.45883445\nEz          -0.01334388  0.04823358\nOz          -0.01921470  0.04155523\nCz          -0.33916065 -0.27657348\nAz          -0.11535145 -0.05481487\n\n\n\n#plot model examining significant personality predictors\nN_plot &lt;- plot_model(fomo_mdl3, type = \"eff\",\n           terms = c(\"Nz\"),\n           show.data = TRUE,\n           axis.title = c(\"Neuroticsm \\n(z-scored)\",\"FoMO Score (z-scored)\"),\n           title = \"FoMO & N\")\n\nC_plot &lt;- plot_model(fomo_mdl3, type = \"eff\",\n           terms = c(\"Cz\"),\n           show.data = TRUE,\n           axis.title = c(\"Conscientiousness \\n(z-scored)\",\"FoMO Score (z-scored)\"),\n           title = \"FoMO & C\")\n\nA_plot &lt;- plot_model(fomo_mdl3, type = \"eff\",\n           terms = c(\"Az\"),\n           show.data = TRUE,\n           axis.title = c(\"Agreeableness \\n(z-scored)\",\"FoMO Score (z-scored)\"),\n           title = \"FoMO & A\")\n\nN_plot | C_plot | A_plot\n\n\n\n\n\n\n\n\n#create table for results - RQ3\ntab_model(fomo_mdl3,\n          dv.labels = \"FoMO (Z-Scored)\",\n          pred.labels = c(\"Nz\" = \"Neuroticism (Z-Scored)\",\n                          \"Ez\" = \"Extraversion (Z-Scored)\",\n                          \"Oz\" = \"Openness (Z-Scored)\",\n                          \"Az\" = \"Agreeableness (Z-Scored)\",\n                          \"Cz\" = \"Conscientiousness (Z-Scored)\"),\n          title = \"RQ3 - Regression Table for FoMO Model\")\n\n\nRQ3 - Regression Table for FoMO Model\n\n\n\n\n\n\n\n\n \nFoMO (Z-Scored)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-0.00\n-0.03 – 0.03\n1.000\n\n\nNeuroticism (Z-Scored)\n0.43\n0.39 – 0.46\n&lt;0.001\n\n\nExtraversion (Z-Scored)\n0.02\n-0.01 – 0.05\n0.267\n\n\nOpenness (Z-Scored)\n0.01\n-0.02 – 0.04\n0.471\n\n\nConscientiousness\n(Z-Scored)\n-0.31\n-0.34 – -0.28\n&lt;0.001\n\n\nAgreeableness (Z-Scored)\n-0.09\n-0.12 – -0.05\n&lt;0.001\n\n\nObservations\n3370\n\n\nR2 / R2 adjusted\n0.215 / 0.214"
  },
  {
    "objectID": "1_05_writeup_recapLEMON.html",
    "href": "1_05_writeup_recapLEMON.html",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand how to write-up and provide interpretation of linear models with single and multiple predictors.\n\n\nBe up to date with lectures\nHave completed Labs 1 - 4\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npsych\npatchwork\nsjPlot\nkableExtra\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/FOMOdataset.csv"
  },
  {
    "objectID": "1_05_writeup_recapLEMON.html#study-overview",
    "href": "1_05_writeup_recapLEMON.html#study-overview",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Aim\nExplore the associations among Fear of Missing Out (FoMO), age, social media networks, and the Big Five personality traits.\nResearch Questions\n\nRQ1: Does age predict FoMO?\nRQ2: Does the number of Instagram followers explain a significant amount of variance in FoMO over and above age?\nRQ3: Does personality predict FoMO?\n\n\n\n FoMO data codebook.\n\n\nDescription\nThe data used for this write-up exercise are simulated, drawing on recent work on Fear of Missing Out (FoMO), socio-demographic factors, and the Big Five personality traits. The simulated data are based on the findings of this work, and acted to expand upon the methods and results reported in the following paper:\nRozgonjuk, D., Sindermann, C., Elhai, J. D., & Montag, C. (2021). Individual differences in Fear of Missing Out (FoMO): Age, gender, and the Big Five personality trait domains, facets, and items. Personality and Individual Differences, 171, 110546. https://doi.org/10.1016/j.paid.2020.110546\nIn the current study, participants were invited to an online study investigating the associations among FoMO, socio-demographic factors, and personality. The final sample comprised 3370 people. Participants completed a FOMO scale and a personality inventory. The 10-item FOMO scale measured the extent of experiencing apprehension regarding missing out on interesting events of others on a 5-point scale (1 = “not at all true of me” to 5 = “extremely true of me”), producing a possible range of scores between 10 and 50. The Big Five Inventory (BFI) is a 45-item personality assessment questionnaire (note that only 43 items were used to match the study above) that uses a five-point response scale (1 = “very inapplicable” to 5 = “very applicable”). The BFI consists of five domains: Neuroticism (8 items; possible range of scores 8-40), Extraversion (8 items; possible range of scores 8-40), Openness to Experience (10 items; possible range of scores 10-50), Agreeableness (8 items; possible range of scores 8-40), and Conscientiousness (9 items; possible range of scores 9-45). We extended the aforementioned study to include an extra socio-demographic variable - a measure of popularity on social media based on the number of followers. Unlike the original study, we do not have measures of gender, education level, or specific country of residence.\nData Dictionary\nThe data in FOMOdataset.csv contain eight attributes collected from a simulated sample of \\(n=3370\\) hypothetical individuals across the UK, and include:\n\n\n\n\n\n\n\nVariable\n      Description\n    \n\n\nFOMO\nFoMO Score (as measured by the 10-item FoMO scale)\n\n\nAge\nAge (in years)\n\n\nN\nScore on personality items assessing Neuroticism from the Big Five Inventory (BFI)\n\n\nE\nScore on personality items assessing Extraversion from the Big Five Inventory (BFI)\n\n\nO\nScore on personality items assessing Openness from the Big Five Inventory (BFI)\n\n\nA\nScore on personality items assessing Agreeableness from the Big Five Inventory (BFI)\n\n\nC\nScore on personality items assessing Conscientiousness from the Big Five Inventory (BFI)\n\n\nTotalFollowers\nTotal Number of Instagram Followers\n\n\n\n\n\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\n\n\nFOMO\n      Age\n      N\n      E\n      O\n      A\n      C\n      TotalFollowers\n    \n\n\n28\n38\n26\n30\n32\n27\n38\n98\n\n\n26\n30\n23\n27\n33\n32\n30\n192\n\n\n23\n33\n14\n30\n36\n27\n24\n177\n\n\n18\n44\n26\n21\n37\n28\n34\n119\n\n\n19\n43\n32\n21\n41\n35\n40\n278\n\n\n24\n43\n22\n25\n31\n30\n24\n184\n\n\n\n\n\n\n\n\n\n\n\n\nSetup\n\nSetup\n\n\n\nCreate a new RMarkdown file\nLoad the required package(s)\nRead the FOMO dataset into R, assigning it to an object named fomo\n\n\n\n\n\n\n Solution \n\n\n\n#Loading the required package(s)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(sjPlot)\nlibrary(kableExtra)\nlibrary(psych)\n\n#Reading in fomo data and storing in object named 'fomo'\nfomo &lt;- read_csv(\"https://uoepsy.github.io/data/FOMOdataset.csv\")\n\n#check first six rows\nhead(fomo)\n\n# A tibble: 6 × 8\n   FOMO   Age     N     E     O     A     C TotalFollowers\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1    28    38    26    30    32    27    38             98\n2    26    30    23    27    33    32    30            192\n3    23    33    14    30    36    27    24            177\n4    18    44    26    21    37    28    34            119\n5    19    43    32    21    41    35    40            278\n6    24    43    22    25    31    30    24            184\n\n\n\n\n\n\nProvided Analysis Code\nBelow you will find the code required to conduct the analysis to address the research questions. This should look similar (in most areas) to what you worked through in lecture.\n\n Provided Analysis Code"
  },
  {
    "objectID": "1_05_writeup_recapLEMON.html#data-management",
    "href": "1_05_writeup_recapLEMON.html#data-management",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "Data Management",
    "text": "Data Management\n\nlibrary(tidyverse) # for all things!\nlibrary(psych) # good for descriptive stats\nlibrary(patchwork) # grouping plots together\nlibrary(kableExtra) # useful for creating nice tables\nlibrary(sjPlot) #regression tables & plots\n\nfomo &lt;- read_csv(\"https://uoepsy.github.io/data/FOMOdataset.csv\")\n\n# standardise FoMO & personality scores for RQ3\nfomo &lt;- \n  fomo %&gt;% \n    mutate(\n      FOMOz = (FOMO-mean(FOMO))/sd(FOMO),\n      Oz = (O-mean(O))/sd(O),\n      Cz = (C-mean(C))/sd(C),\n      Ez = (E-mean(E))/sd(E),\n      Az = (A-mean(A))/sd(A),      \n      Nz = (N-mean(N))/sd(N))\n#alternatively, you could do FOMOz = scale(FOMO, center = TRUE, scale = TRUE)"
  },
  {
    "objectID": "1_05_writeup_recapLEMON.html#overall",
    "href": "1_05_writeup_recapLEMON.html#overall",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "Overall",
    "text": "Overall\n\n#######\n#Descriptive Stats\n#######\n\n\n# the describe() function is from the psych package, and kable() from kableExtra which is used to make a nice table where the values are rounded to 2 decimal places using digits = 2. \n# We are first renaming our variables to give more appropriate / informative names.\n#Next we are selecting columns 2, 3, 4, 8, and 9 from the describe output (n, mean, sd, min, max)\n\nfomo %&gt;% \n    select(FOMO, Age, TotalFollowers, N, E, O, C, A) %&gt;%\n    rename(\"Fear of Missing Out\" = FOMO, \"Age (in years)\" = Age, \"Number of Instagram Followers\" = TotalFollowers, \"Neuroticism\" = N, \"Extraversion\" = E, \"Openness\" = O, \"Conscientiousness\" = C, \"Agreeableness\" = A) %&gt;%\n    describe() %&gt;%\n    select(2:4, 8:9) %&gt;%\n    rename(\"N\" = n, \"Mean\" = mean, \"SD\" = sd, \"Minimum\" = min, \"Maximum\" = max) %&gt;%\n        kable(., caption = \"FoMO, Socio-Demographic Factors, and Personality Traits Descriptive Statistics\", digits = 2) %&gt;%\n        kable_styling()   \n\n\nFoMO, Socio-Demographic Factors, and Personality Traits Descriptive Statistics\n\n\nN\nMean\nSD\nMinimum\nMaximum\n\n\n\nFear of Missing Out\n3370\n24.63\n6.42\n10\n46\n\n\nAge (in years)\n3370\n33.61\n10.42\n12\n75\n\n\nNumber of Instagram Followers\n3370\n203.25\n93.42\n1\n594\n\n\nNeuroticism\n3370\n22.92\n5.77\n8\n40\n\n\nExtraversion\n3370\n25.88\n5.94\n8\n40\n\n\nOpenness\n3370\n37.61\n5.80\n14\n50\n\n\nConscientiousness\n3370\n31.04\n5.49\n13\n45\n\n\nAgreeableness\n3370\n30.89\n4.93\n13\n43\n\n\n\n\n#from above, no missing values and scores within range (look at min and max values)\n\n\n# scatterplot matrix, hist, and corr of FoMO, Socio-Demographic Factors, and Personality Traits Descriptive Statistics\npairs.panels(fomo %&gt;%\n    select(-FOMOz, -Oz, -Cz, -Ez, -Az, -Nz))"
  },
  {
    "objectID": "1_05_writeup_recapLEMON.html#rq1",
    "href": "1_05_writeup_recapLEMON.html#rq1",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "RQ1",
    "text": "RQ1\n\n#######\n#Descriptive Stats\n#######\n\nfomo %&gt;% \n    select(FOMO, Age) %&gt;%\n    rename(\"Fear of Missing Out\" = FOMO, \"Age (in years)\" = Age) %&gt;%\n    describe() %&gt;%\n    select(2:4, 8:9) %&gt;%\n    rename(\"N\" = n, \"Mean\" = mean, \"SD\" = sd, \"Minimum\" = min, \"Maximum\" = max) %&gt;%    \n        kable(., caption = \"FoMO and Age Descriptive Statistics\", digits = 2) %&gt;%\n        kable_styling()    \n\n\nFoMO and Age Descriptive Statistics\n\n\nN\nMean\nSD\nMinimum\nMaximum\n\n\n\nFear of Missing Out\n3370\n24.63\n6.42\n10\n46\n\n\nAge (in years)\n3370\n33.61\n10.42\n12\n75\n\n\n\n\n# scatterplot\np1 &lt;- ggplot(data = fomo, aes(x = Age, y = FOMO)) + \n    geom_point() + \n  geom_smooth(method = 'lm', se = FALSE, colour = 'red', linewidth=2) +\n  labs(x = \"(a) Age (in years)\", y = \"Fear of Missing Out\")\np1\n\n\n\n\n\n\n\n\n#######\n#Model Building\n#######\n\nfomo_mdl1 &lt;- lm(FOMO ~ Age, data = fomo)\nsummary(fomo_mdl1)\n\n\nCall:\nlm(formula = FOMO ~ Age, data = fomo)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1028  -4.2129  -0.1602   4.0551  22.2321 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 31.22239    0.35408   88.18   &lt;2e-16 ***\nAge         -0.19617    0.01006  -19.50   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.084 on 3368 degrees of freedom\nMultiple R-squared:  0.1014,    Adjusted R-squared:  0.1011 \nF-statistic: 380.1 on 1 and 3368 DF,  p-value: &lt; 2.2e-16\n\nconfint(fomo_mdl1)\n\n                 2.5 %     97.5 %\n(Intercept) 30.5281496 31.9166293\nAge         -0.2158992 -0.1764425\n\n\n\n#######\n#Table for Results\n#######\n\ntab_model(fomo_mdl1,\n          dv.labels = \"FoMO\",\n          pred.labels = c(\"Age\" = \"Age (in years)\"), \n          title = \"RQ1: Regression Table for FoMO Model\")\n\n\nRQ1: Regression Table for FoMO Model\n\n\n \nFoMO\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n31.22\n30.53 – 31.92\n&lt;0.001\n\n\nAge (in years)\n-0.20\n-0.22 – -0.18\n&lt;0.001\n\n\nObservations\n3370\n\n\nR2 / R2 adjusted\n0.101 / 0.101"
  },
  {
    "objectID": "1_05_writeup_recapLEMON.html#rq2",
    "href": "1_05_writeup_recapLEMON.html#rq2",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "RQ2",
    "text": "RQ2\n\n#######\n#Descriptive Stats\n#######\n\nfomo %&gt;% \n    select(FOMO, Age, TotalFollowers) %&gt;%\n    rename(\"Fear of Missing Out\" = FOMO, \"Age (in years)\" = Age, \"Number of Instagram Followers\" = TotalFollowers) %&gt;%\n    describe() %&gt;%\n    select(2:4, 8:9) %&gt;%\n    rename(\"N\" = n, \"Mean\" = mean, \"SD\" = sd, \"Minimum\" = min, \"Maximum\" = max) %&gt;%    \n        kable(., caption = \"FoMO and Socio-Demographic Factors Descriptive Statistics\", digits = 2) %&gt;%\n        kable_styling()    \n\n\nFoMO and Socio-Demographic Factors Descriptive Statistics\n\n\nN\nMean\nSD\nMinimum\nMaximum\n\n\n\nFear of Missing Out\n3370\n24.63\n6.42\n10\n46\n\n\nAge (in years)\n3370\n33.61\n10.42\n12\n75\n\n\nNumber of Instagram Followers\n3370\n203.25\n93.42\n1\n594\n\n\n\n\n# scatterplots\np2 &lt;- ggplot(data = fomo, aes(x = TotalFollowers, y = FOMO)) + \n    geom_point() + \n  geom_smooth(method = 'lm', se = FALSE, colour = 'purple', linewidth=2) +\n  labs(x = \"(b) Total Number of \\nInstagram Followers\", y = \"Fear of Missing Out\")\n\np2\n\n\n\n\n\n\n\n\n#######\n#Model Building\n#######\n\nfomo_mdl2 &lt;- lm(FOMO ~ Age + TotalFollowers, data = fomo)\nsummary(fomo_mdl2)\n\n\nCall:\nlm(formula = FOMO ~ Age + TotalFollowers, data = fomo)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.273  -4.066  -0.071   3.841  21.705 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    26.521966   0.446021   59.46   &lt;2e-16 ***\nAge            -0.165111   0.009871  -16.73   &lt;2e-16 ***\nTotalFollowers  0.017989   0.001101   16.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.857 on 3367 degrees of freedom\nMultiple R-squared:  0.1674,    Adjusted R-squared:  0.1669 \nF-statistic: 338.6 on 2 and 3367 DF,  p-value: &lt; 2.2e-16\n\nconfint(fomo_mdl2)\n\n                     2.5 %      97.5 %\n(Intercept)    25.64746694 27.39646570\nAge            -0.18446526 -0.14575589\nTotalFollowers  0.01583102  0.02014788\n\n\n\n#######\n#Model Comparison\n#######\n\nanova(fomo_mdl1 ,fomo_mdl2) %&gt;%\n    kable(caption = \"Model Comparison - fomo_mdl1 vs fomo_mdl2\", align = \"c\", digits = c(2,2,2,2,2,60)) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\nModel Comparison - fomo_mdl1 vs fomo_mdl2\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n3368\n124676.4\nNA\nNA\nNA\nNA\n\n\n3367\n115515.0\n1\n9161.44\n267.04\n7.98e-58\n\n\n\n\n\n\n#######\n#Table for Results\n#######\n\ntab_model(fomo_mdl1 ,fomo_mdl2,\n          dv.labels = c(\"FoMO\",\"FoMO\"),\n          pred.labels = c(\"Age\" = \"Age (in years)\",\n                          \"TotalFollowers\" = \"Number of Instagram Followers\"), \n          title = \"RQ2 - Regression Table for FoMO Model\")\n\n\nRQ2 - Regression Table for FoMO Model\n\n\n\n\n\n\n\n\n\n\n\n \nFoMO\nFoMO\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n31.22\n30.53 – 31.92\n&lt;0.001\n26.52\n25.65 – 27.40\n&lt;0.001\n\n\nAge (in years)\n-0.20\n-0.22 – -0.18\n&lt;0.001\n-0.17\n-0.18 – -0.15\n&lt;0.001\n\n\nNumber of Instagram\nFollowers\n\n\n\n0.02\n0.02 – 0.02\n&lt;0.001\n\n\nObservations\n3370\n3370\n\n\nR2 / R2 adjusted\n0.101 / 0.101\n0.167 / 0.167"
  },
  {
    "objectID": "1_05_writeup_recapLEMON.html#rq3",
    "href": "1_05_writeup_recapLEMON.html#rq3",
    "title": "Block 1 Analysis & Write-Up Example",
    "section": "RQ3",
    "text": "RQ3\n\n#######\n#Descriptive Stats\n#######\n\nfomo %&gt;% \n    select(FOMO, N, E, O, C, A) %&gt;%\n    rename(\"Fear of Missing Out\" = FOMO, \"Neuroticism\" = N, \"Extraversion\" = E, \"Openness\" = O, \"Conscientiousness\" = C, \"Agreeableness\" = A) %&gt;%\n    describe() %&gt;%\n    select(2:4, 8:9) %&gt;%\n    rename(\"N\" = n, \"Mean\" = mean, \"SD\" = sd, \"Minimum\" = min, \"Maximum\" = max) %&gt;%    \n        kable(., caption = \"FoMO and Personality Descriptive Statistics\", digits = 2) %&gt;%\n        kable_styling()  \n\n\nFoMO and Personality Descriptive Statistics\n\n\nN\nMean\nSD\nMinimum\nMaximum\n\n\n\nFear of Missing Out\n3370\n24.63\n6.42\n10\n46\n\n\nNeuroticism\n3370\n22.92\n5.77\n8\n40\n\n\nExtraversion\n3370\n25.88\n5.94\n8\n40\n\n\nOpenness\n3370\n37.61\n5.80\n14\n50\n\n\nConscientiousness\n3370\n31.04\n5.49\n13\n45\n\n\nAgreeableness\n3370\n30.89\n4.93\n13\n43\n\n\n\n\n\n\n#######\n#Model Building\n#######\n\nfomo_mdl3 &lt;- lm(FOMOz ~ Nz + Ez + Oz + Cz + Az, data = fomo)\nsummary(fomo_mdl3)\n\n\nCall:\nlm(formula = FOMOz ~ Nz + Ez + Oz + Cz + Az, data = fomo)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.74061 -0.60648 -0.01036  0.59506  3.13402 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.453e-17  1.527e-02   0.000    1.000    \nNz           4.266e-01  1.643e-02  25.971  &lt; 2e-16 ***\nEz           1.744e-02  1.570e-02   1.111    0.267    \nOz           1.117e-02  1.550e-02   0.721    0.471    \nCz          -3.079e-01  1.596e-02 -19.289  &lt; 2e-16 ***\nAz          -8.508e-02  1.544e-02  -5.511 3.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8865 on 3364 degrees of freedom\nMultiple R-squared:  0.2152,    Adjusted R-squared:  0.2141 \nF-statistic: 184.5 on 5 and 3364 DF,  p-value: &lt; 2.2e-16\n\nconfint(fomo_mdl3)\n\n                  2.5 %      97.5 %\n(Intercept) -0.02994194  0.02994194\nNz           0.39441959  0.45883445\nEz          -0.01334388  0.04823358\nOz          -0.01921470  0.04155523\nCz          -0.33916065 -0.27657348\nAz          -0.11535145 -0.05481487\n\n\n\n#plot model examining significant personality predictors\nN_plot &lt;- plot_model(fomo_mdl3, type = \"eff\",\n           terms = c(\"Nz\"),\n           show.data = TRUE,\n           axis.title = c(\"Neuroticsm \\n(z-scored)\",\"FoMO Score (z-scored)\"),\n           title = \"FoMO & N\")\n\nC_plot &lt;- plot_model(fomo_mdl3, type = \"eff\",\n           terms = c(\"Cz\"),\n           show.data = TRUE,\n           axis.title = c(\"Conscientiousness \\n(z-scored)\",\"FoMO Score (z-scored)\"),\n           title = \"FoMO & C\")\n\nA_plot &lt;- plot_model(fomo_mdl3, type = \"eff\",\n           terms = c(\"Az\"),\n           show.data = TRUE,\n           axis.title = c(\"Agreeableness \\n(z-scored)\",\"FoMO Score (z-scored)\"),\n           title = \"FoMO & A\")\n\nN_plot | C_plot | A_plot\n\n\n\n\n\n\n\n\n#create table for results - RQ3\ntab_model(fomo_mdl3,\n          dv.labels = \"FoMO (Z-Scored)\",\n          pred.labels = c(\"Nz\" = \"Neuroticism (Z-Scored)\",\n                          \"Ez\" = \"Extraversion (Z-Scored)\",\n                          \"Oz\" = \"Openness (Z-Scored)\",\n                          \"Az\" = \"Agreeableness (Z-Scored)\",\n                          \"Cz\" = \"Conscientiousness (Z-Scored)\"),\n          title = \"RQ3 - Regression Table for FoMO Model\")\n\n\nRQ3 - Regression Table for FoMO Model\n\n\n\n\n\n\n\n\n \nFoMO (Z-Scored)\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-0.00\n-0.03 – 0.03\n1.000\n\n\nNeuroticism (Z-Scored)\n0.43\n0.39 – 0.46\n&lt;0.001\n\n\nExtraversion (Z-Scored)\n0.02\n-0.01 – 0.05\n0.267\n\n\nOpenness (Z-Scored)\n0.01\n-0.02 – 0.04\n0.471\n\n\nConscientiousness\n(Z-Scored)\n-0.31\n-0.34 – -0.28\n&lt;0.001\n\n\nAgreeableness (Z-Scored)\n-0.09\n-0.12 – -0.05\n&lt;0.001\n\n\nObservations\n3370\n\n\nR2 / R2 adjusted\n0.215 / 0.214"
  },
  {
    "objectID": "1_06_dummy.html",
    "href": "1_06_dummy.html",
    "title": "Dummy Coding",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand how to specify a baseline/reference level for categorical variables\nUnderstand how to specify dummy coding\nInterpret the output from a model using dummy coding\nUnderstand how to specify contracts to test specific effects\n\n\nBe up to date with lectures\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npsych\nkableExtra\nemmeans\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\nThe example write-up sections included as part of the solutions are not perfect - they instead should give you a good example of what information you should include and how to structure this. Note that you must not copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more here.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/caffeinedrink.csv"
  },
  {
    "objectID": "1_06_dummy.html#study-analysis-plan-overview",
    "href": "1_06_dummy.html#study-analysis-plan-overview",
    "title": "Dummy Coding",
    "section": "Study & Analysis Plan Overview",
    "text": "Study & Analysis Plan Overview\n\nQuestion 1\n\n\nExamine the dataset, and perform any necessary and appropriate data management steps.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nThe str() function will return the overall structure of the dataset, this can be quite handy to look at\n\nConvert categorical variables to factors, and if needed, provide better variable names\n\nLabel appropriately factors to aid with your model interpretations if required\n\nCheck that the dataset is complete (i.e., are there any NA values?). We can check this using is.na()\n\nAre scores within possible ranges (e.g., if we recorded people’s age, it would be impossible to have someone aged -31!)\n\n\n\n\n\n\n\n\nLet’s have a look at the data to see what we’re working with:\n\n#first look at dataset structure\nstr(caffeine)\n\nspc_tbl_ [40 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ treatment: chr [1:40] \"control\" \"control\" \"control\" \"control\" ...\n $ wpm      : num [1:40] 109 114 113 110 116 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   treatment = col_character(),\n  ..   wpm = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n#now lets look at top 6 rows (or the head) of the dataset\nhead(caffeine)\n\n# A tibble: 6 × 2\n  treatment   wpm\n  &lt;chr&gt;     &lt;dbl&gt;\n1 control    109.\n2 control    114.\n3 control    113.\n4 control    110.\n5 control    116.\n6 control    113.\n\n#check for NAs - there are none - all FALSE\ntable(is.na(caffeine))\n\n\nFALSE \n   80 \n\n\nLet’s start with the ‘treatment’ condition variable. This should be coded as factor (&lt;fctr&gt;), but can see from above it is currently coded as a character (&lt;chr&gt;). Let’s correct this.\n\n#Code treatment as a factor\ncaffeine &lt;- caffeine %&gt;%\n  mutate(treatment = as_factor(treatment))\n\nNext, let’s look at the ‘wpm’ variable. Here we want to check for impossible values. Although we don’t know if there is a maximum possible WPM, we do know that we cannot have a negative WPM.\n\n# all looks ok - min and max both positive values\ndescribe(caffeine$wpm)\n\n   vars  n   mean   sd median trimmed  mad    min    max range skew kurtosis\nX1    1 40 113.59 2.92 113.06  113.51 3.62 107.94 120.24  12.3 0.22    -0.84\n     se\nX1 0.46\n\n\n\n\n\n\n\n\nAll participant data was complete (no missing values), with WPM scores within possible ranges. Treatment was coded as a factor with four levels - control (water), coffee, red bull, and mint tea.\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nChoose an appropriate reference level for the Treatment condition.\n\n\n\n\nThe Treatment factor has a group coded ‘Control (Water)’ which lends itself naturally to be the reference category.\n\n#set 'Control' caffeine treatment condition as our reference group \ncaffeine$treatment &lt;- relevel(caffeine$treatment, \"control\")\n\n#check levels - control should be first in the list\nlevels(caffeine$treatment)\n\n[1] \"control\"  \"coffee\"   \"red_bull\" \"mint_tea\"\n\n\n\n\n\n\n\nQuestion 3\n\n\nProvide a brief overview of the study design and data, before detailing your analysis plan to address the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nGive the reader some background on the context of the study\nState what type of analysis you will conduct in order to address the research question\nSpecify the model to be fitted to address the research question (note that you will need to specify the reference level of your categorical variable)\nSpecify your chosen significance (\\(\\alpha\\)) level\nState your hypotheses\n\nMuch of the information required can be found in the Study Overview codebook. The statistical models flashcards may also be useful to refer to.\n\n\n\n\n\n\n\nThe caffeine dataset contained information on 40 hypothetical participants who took part in an experiment examining whether the number of words typed per minute (WPM) differed among caffeine treatment conditions. Using a between-subjects design, the researchers collected information on participants’ WPM (average number of words typed per minute), and which one of four treatment conditions they were randomly assigned to (Control (Water), Coffee, Mint Tea, or Red Bull).\nBoxplots will be used to visualise the associations among WPM and caffeine treatment conditions. To address the research question of whether WPM differs by caffeine treatment condition, we first need to define the dummy variables for Treatment:\n\\[\n\\text{Treatment}_\\text{Coffee} = \\begin{cases}  \n1 & \\text{if Treatment is Coffee} \\\\  \n0 & \\text{otherwise}  \n\\end{cases}  \n\\quad    \n\\]\n\\[\n\\text{Treatment}_\\text{Red Bull} = \\begin{cases}  \n1 & \\text{if Treatment is Red Bull} \\\\  \n0 & \\text{otherwise}  \n\\\\  \n\\end{cases}  \n\\quad  \n\\]\n\\[\n\\text{Treatment}_\\text{Mint Tea} = \\begin{cases}  \n1 & \\text{if Treatment is Mint Tea} \\\\  \n0 & \\text{otherwise}  \n\\end{cases}  \n\\quad  \n\\]\n\\[\n(\\text{Control (Water) is base level})  \n\\]\nBased on the above dummy coding, we are going to fit the following regression model:\n\\[\n\\begin{align}\n\\text{WPM} = \\beta_0 + \\beta_1 \\cdot \\text{Treatment}_\\text{Coffee} \\\\    \n+ \\beta_2 \\cdot \\text{Treatment}_\\text{Red Bull} + \\beta_3 \\cdot \\text{Treatment}_\\text{Mint Tea} + \\epsilon\n\\end{align}\n\\]\nEffects will be considered statistically significant at \\(\\alpha=.05\\)\nOur hypotheses are:\n\\(H_0:\\) All \\(\\beta_j = 0\\) (for \\(j = 1, 2, 3\\))\nThere are no differences in WPM based on caffeine treatment condition.\n\\(H_1:\\) At least one \\(\\beta_j \\neq 0\\) (for \\(j = 1, 2, 3\\))\nThere are differences in WPM based on caffeine treatment condition."
  },
  {
    "objectID": "1_06_dummy.html#descriptive-statistics-visualisations",
    "href": "1_06_dummy.html#descriptive-statistics-visualisations",
    "title": "Dummy Coding",
    "section": "Descriptive Statistics & Visualisations",
    "text": "Descriptive Statistics & Visualisations\n\nQuestion 4\n\n\nProvide a table of descriptive statistics and visualise your data.\nRemember to interpret your plot in the context of the study (i.e., comment on any observed differences among treatment groups).\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the many ways to numerically and visually explore your data by reading over the data exploration flashcards.\nFor examples, see flashcards on descriptives statistics tables - categorical and numeric values examples and data visualisation - bivariate examples, paying particular attention to the type of data that you’re working with.\nMake sure to comment on any observed differences among the sample means of the four treatment conditions.\n\n\n\n\n\n\n\n\n\nNumeric\nVisual\n\n\n\nDescriptive statistics presented in a well formatted table, grouped by treatment condition:\n\ncaffeine %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(n = n(), \n            Mean = mean(wpm), \n            SD = sd(wpm),\n            Minimum = min(wpm),\n            Maximum = max(wpm)) %&gt;%\n    kable(caption = \"Descriptive Statistics\", digits = 2) %&gt;%\n    kable_styling()\n\n\n\nTable 1: Descriptive Statistics\n\ntreatment\nn\nMean\nSD\nMinimum\nMaximum\n\n\n\ncontrol\n10\n112.15\n1.98\n109.43\n116.23\n\n\ncoffee\n10\n114.48\n1.82\n112.07\n117.16\n\n\nred_bull\n10\n116.65\n2.15\n113.00\n120.24\n\n\nmint_tea\n10\n111.09\n2.13\n107.94\n115.82\n\n\n\n\n\n\n\n\n\n\nSince we have a continuous outcome and a categorical predictor - a boxplot would be most appropriate for visualisations:\n\nggplot(data = caffeine, aes(x = treatment, y = wpm, fill = treatment)) +\n  geom_boxplot() +\n  labs(x = 'Treatment Condition', y = 'WPM')\n\n\n\nFigure 1: Association between Treatment Conditions and WPM\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the boxplots, it seems that those in the Red Bull condition, on average, typed the most WPM, whilst those in the Mint Tea condition the fewest.\nOverall, the average WPM appears to be lower for those in the non-caffeine conditions (i.e., control - water / mint tea) in comparison to those in the caffeine drinks condition (red bull / coffee)."
  },
  {
    "objectID": "1_06_dummy.html#model-fitting-interpretation",
    "href": "1_06_dummy.html#model-fitting-interpretation",
    "title": "Dummy Coding",
    "section": "Model Fitting & Interpretation",
    "text": "Model Fitting & Interpretation\n\nQuestion 5\n\n\nFit the specified model, and assign it the name “caf_mdl1”.\nInterpret your coefficients in the context of the study.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can fit our multiple regression model using the lm() function. For a recap, see the statistical models flashcards.\nRecall that R computes the dummy variables for us. Thus, each row in the summary() output of the model will correspond to one of the estimated \\(\\beta\\)’s in the equation above. For a more in-depth recap, see the numeric outcomes & categorical predictors flashcards\n\n\n\n\n\n\n\n\n#fit model\ncaf_mdl1 &lt;- lm(wpm ~ treatment, data=caffeine)\n\n\n#check model output\nsummary(caf_mdl1)\n\n\nCall:\nlm(formula = wpm ~ treatment, data = caffeine)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.652 -1.362 -0.151  1.125  4.729 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       112.1460     0.6402 175.179  &lt; 2e-16 ***\ntreatmentcoffee     2.3350     0.9053   2.579   0.0141 *  \ntreatmentred_bull   4.5060     0.9053   4.977 1.61e-05 ***\ntreatmentmint_tea  -1.0550     0.9053  -1.165   0.2516    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.024 on 36 degrees of freedom\nMultiple R-squared:  0.5563,    Adjusted R-squared:  0.5194 \nF-statistic: 15.05 on 3 and 36 DF,  p-value: 1.651e-06\n\n\nLet’s first map our coefficients and estimates:\n\n\n\n\n\n\n\nCoefficient\nEstimate\nCorresponds to\n\n\n\n(Intercept)\n112.1460\n\\(\\beta_0 = \\hat \\mu_1\\)\n\n\ntreatmentcoffee\n2.3350\n\\(\\beta_0 + \\beta_1 = \\hat \\mu_2\\)\n\n\ntreatmentred_bull\n4.5060\n\\(\\beta_0 + \\beta_2 = \\hat \\mu_3\\)\n\n\ntreatmentmint_tea\n-1.0550\n\\(\\beta_0 + \\beta_3 = \\hat \\mu_4\\)\n\n\n\n\n\n\n\n\n\n\nThe estimate corresponding to (Intercept) contains \\(\\hat \\beta_0 = \\hat \\mu_1 = 112.15\\). The estimated average WPM for those in the control condition (water) was approximately 112.15.\nThe next estimate corresponds to treatmentcoffee and was \\(\\hat \\beta_1 = 2.34\\). The difference in mean WPM between Control and Coffee was estimated to be \\(2.34\\). Thus, \\(\\hat \\mu_2 = 112.15 + 2.34 = 114.49\\). In other words, people who have had coffee typed approximately 114.49 words, which was 2.34 words per minute more than those who have had water. This difference was statistically significant \\((p = .01)\\).\nThe estimate corresponding to treatmentred_bull was \\(\\hat \\beta_2 = 4.51\\). This was the estimated difference in mean WPM between Control and Red Bull, estimated to be \\(4.51\\). Thus, \\(\\hat \\mu_3 = 112.15 + 4.51 = 116.66\\). In other words, people who had red bull typed approximately 116.66 words, 4.51 words per minute more than those who had water. This difference was statistically significant \\((p &lt; .001)\\).\nThe estimate corresponding to treatmentmint_tea was \\(\\hat \\beta_3 = -1.06\\). This was the estimated difference in mean WPM between Control and Mint Tea, estimated to be \\(-1.06\\). Thus, \\(\\hat \\mu_4 = 112.15 + (-1.06) = 111.09\\). In other words, people who had mint tea typed approximately 111.09 words, 1.06 words per minute less than those who had water. This difference was not statistically significant \\((p = .25)\\)."
  },
  {
    "objectID": "1_06_dummy.html#planned-comparisons-contrasts",
    "href": "1_06_dummy.html#planned-comparisons-contrasts",
    "title": "Dummy Coding",
    "section": "Planned Comparisons / Contrasts",
    "text": "Planned Comparisons / Contrasts\n\nQuestion 6\n\n\nFormally state the two planned comparisons that the researchers were interested in as testable hypotheses.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee the manual contrasts flashcards.\n\n\n\n\n\n\n\nRecall that the researchers were also interested in addressing these two questions:\n\nWhether having some kind of caffeine (i.e., red bull or coffee), rather than no caffeine (i.e., control - water or mint tea), resulted in a difference in average WPM\nWhether there was a difference in average WPM between those with hot drinks (i.e., mint tea / coffee) in comparison to those with cold drinks (control - water / red bull)\n\nWe can specify the two hypotheses as follows:\n\n\nCaffeine vs No Caffeine\nHot vs Cold\n\n\n\n\\[\n\\begin{aligned}\n1. \\quad \\text{H}_0 &: \\mu_\\text{No Caffeine} = \\mu_\\text{Caffeine} \\\\\n    \\quad \\text{H}_0 &: \\frac{1}{2} (\\mu_\\text{Control} + \\mu_\\text{Mint Tea}) = \\frac{1}{2} (\\mu_\\text{Coffee} + \\mu_\\text{Red Bull}) \\\\\n\\\\\n    \\quad \\text{H}_1 &: \\mu_\\text{No Caffeine} \\neq \\mu_\\text{Caffeine} \\\\\n    \\quad \\text{H}_1 &: \\frac{1}{2} (\\mu_\\text{Control} + \\mu_\\text{Mint Tea}) \\neq \\frac{1}{2} (\\mu_\\text{Coffee} + \\mu_\\text{Red Bull}) \\\\\n\\\\\n\\end{aligned}\n\\]\n\n\n\\[\n\\begin{aligned}\n2. \\quad H_0 &: \\mu_\\text{Hot Drink} = \\mu_\\text{Cold Drink} \\\\\n    \\quad H_0 &: \\frac{1}{2} (\\mu_\\text{Coffee} + \\mu_\\text{Mint Tea}) = \\frac{1}{2} (\\mu_\\text{Control} + \\mu_\\text{Red Bull})\n\\\\\n\\\\\n    \\quad H_1 &: \\mu_\\text{Hot Drink} \\neq \\mu_\\text{Cold Drink} \\\\\n    \\quad H_1 &: \\frac{1}{2} (\\mu_\\text{Coffee} + \\mu_\\text{Mint Tea}) \\neq \\frac{1}{2} (\\mu_\\text{Control} + \\mu_\\text{Red Bull})\n    \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nAfter checking the levels of the factor treatment, use emmeans() to obtain the estimated treatment means and uncertainties for your factor.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee the manual contrasts flashcards.\n\n\n\n\n\n\n\n\nlevels(caffeine$treatment)\n\n[1] \"control\"  \"coffee\"   \"red_bull\" \"mint_tea\"\n\n\nUse the emmeans() to get the estimated means of our groups:\n\ntreatment_mean &lt;- emmeans(caf_mdl1, ~ treatment)\ntreatment_mean\n\n treatment emmean   SE df lower.CL upper.CL\n control      112 0.64 36      111      113\n coffee       114 0.64 36      113      116\n red_bull     117 0.64 36      115      118\n mint_tea     111 0.64 36      110      112\n\nConfidence level used: 0.95 \n\nplot(treatment_mean)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nSpecify the coefficients of the comparisons and run the contrast analysis, obtaining 95% confidence intervals.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee the manual contrasts flashcards.\nRemember that ordering matters here - look again at the output of levels(caffeine$treatment) as this will help you when assigning your weights.\n\n\n\n\n\n\n\nAs shown above via levels(), the ordering of the treatment factor is:\n\nControl (no caffeine / cold drink)\nCoffee (caffeine / hot drink)\nRed Bull (caffeine / cold drink)\nMint Tea (no caffeine / hot drink)\n\nFrom this ordering, we can specify our weights - based on the hypothesis, lets assign positive values to the no caffeine and hot drink conditions:\n\ntreatment_comp &lt;- list(\"No Caffeine - Caffeine\" = c(1/2, -1/2, -1/2, 1/2),\n             \"Hot Drink - Cold Drink\" = c(-1/2, 1/2, -1/2, 1/2)\n             )\n\nNow lets run our contrast analysis and get confidence intervals - to do so we use the contrast() function from emmeans():\n\n#run contrast analysis\ntreatment_comp_test &lt;- contrast(treatment_mean, method = treatment_comp)\n\n#examine output\ntreatment_comp_test\n\n contrast               estimate   SE df t.ratio p.value\n No Caffeine - Caffeine    -3.95 0.64 36  -6.167  &lt;.0001\n Hot Drink - Cold Drink    -1.61 0.64 36  -2.520  0.0163\n\n#obtain confidence intervals\nconfint(treatment_comp_test)\n\n contrast               estimate   SE df lower.CL upper.CL\n No Caffeine - Caffeine    -3.95 0.64 36    -5.25   -2.650\n Hot Drink - Cold Drink    -1.61 0.64 36    -2.91   -0.315\n\nConfidence level used: 0.95 \n\n\n\n\n\n\n\nQuestion 9\n\n\nInterpret the results of the contrast analysis in the context of the researchers hypotheses.\nSee the manual contrasts flashcards.\n\n\n\n\n\n\nHypothesis 1: Caffeine vs No Caffeine\nHypothesis 2: Hot vs Cold\n\n\n\nWe performed a test against \\(H_0: \\frac{1}{2}(\\mu_1 + \\mu_4) - \\frac{1}{2}(\\mu_2 + \\mu_3) = 0\\). At the 5% significance level, there was evidence that the mean WPM for those who were in the no caffeine condition was significantly different from those in a caffeine condition \\((t(36) = -6.17, p &lt; .001, \\text{two-sided})\\), and this difference was estimated to be -3.95. We are 95% confident that those who consumed no caffeine typed, on average, between 2.7 and 5.3 words less per minute than those who consumed some form of caffeine \\(CI_{95}[-5.25, -2.65]\\).\n\n\nWe performed a test against \\(H_0: \\frac{1}{2}(\\mu_2 + \\mu_4) - \\frac{1}{2}(\\mu_1 + \\mu_3) = 0\\). At the 5% significance level, there was evidence that the average WPM for those in the hot drink condition significantly differed from those in the cold drink condition \\((t(36) = -2.52, p = .02, \\text{two-sided})\\), and this difference was estimated to be -1.61. We are 95% confident that those who consumed a hot drink typed, on average, between 0.3 and 2.9 words less per minute than those who consumed a cold drink \\(CI_{95}[-2.91, -0.32]\\)."
  },
  {
    "objectID": "1_06_dummy.html#study-design",
    "href": "1_06_dummy.html#study-design",
    "title": "Dummy Coding",
    "section": "Study Design",
    "text": "Study Design\n\nQuestion 10\n\n\nFor each of the below experiment descriptions, note (1) the design, (2) number of variables of interest, (3) levels of categorical variables, (4) what you think the reference group should be and why.\n\n\nExperiment 1\nExperiment 2\nExperiment 3\n\n\n\nA group of researchers were interested in whether sleep deprivation influenced reaction time. They hypothesised that sleep deprived individuals would have slower reaction times than non-sleep deprived individuals.\nTo test this, they recruited 60 participants who were matched on a number of demographic variables including age and sex. One member of each pair (e.g., female, aged 18) was placed into a different sleep condition - ‘Sleep Deprived’ (4 hours per night) or ‘Non-Sleep Deprived’ (8 hours per night).\n\n\nA group of researchers were interested in replicating an experiment testing the Stroop Effect.\nThey recruited 50 participants who took part in Task A (word colour and meaning are congruent) and Task B (word colour and meaning are incongruent) where they were asked to name the color of the ink instead of reading the word. The order of presentation was counterbalanced across participants. The researchers hypothesised that participants would take significantly more time (‘response time’ measured in seconds) to complete Task B than Task A.\nYou can test yourself here for fun: Stroop Task\n\n\nA group of researchers wanted to test a hypothesised theory according to which patients with amnesia will have a deficit in explicit memory but not implicit memory. Huntingtons patients, on the other hand, will display the opposite: they will have no deficit in explicit memory, but will have a deficit in implicit memory.\nTo test this, researchers designed a study that included two variables: ‘Diagnosis’ (Amnesic, Huntingtons, Control) and ‘Task’ (Grammar, Classification, Recognition) where participants were randomly assigned to a Task condition. The first two tasks (Grammar and Classification) are known to reflect implicit memory processes, whereas the Recognition task is known to reflect explicit memory processes.\n\n\n\n\n\n\n\n\n\nExperiment 1\nExperiment 2\nExperiment 3\n\n\n\n\nDesign = Between-person: Matched pairs\nNo of variables of interest = 2 - Sleep Condition and Reaction Time\nLevels of variables = Sleep Condition has 2 levels - Sleep Deprived and Non-Sleep Deprived; Reaction Time is a continuous measure, so has no associated levels\nReference Group = Sleep Condition - Non-Sleep Deprived because the research question stated that the researchers were interested in how the sleep deprived group differed from the non-sleep deprived group.\n\n\n\n\nDesign = Within-person: Repeated measures (this is a study design that you will learn more about in DAPR3!)\nNo of variables of interest = 2 - Task and Response Time\nLevels of variables = Task has 2 levels - A and B; Response Time is a continuous measure, so has no associated levels\nReference Group = Task - A because the research question stated that the researchers were interested in how response time in Task B differed from the response time in Task A.\n\n\n\n\nDesign = Between-person: 3×3 factorial design\nNo of variables of interest = 2 - Diagnosis and Task\nLevels of variables = Diagnosis has 3 levels - Amnesic, Huntingtons, and Control; Task has 3 levels - Grammar, Classification, and Recognition\nReference Groups = Diagnosis - Control; Task - Recognition. We have chosen Control since the other two groups have some form of cognitive impairment; and the Recognition task since it measures explicit memory whilst the other two task types implicit."
  },
  {
    "objectID": "1_06_dummyMELON.html",
    "href": "1_06_dummyMELON.html",
    "title": "Dummy Coding",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand how to specify a baseline/reference level for categorical variables\nUnderstand how to specify dummy coding\nInterpret the output from a model using dummy coding\nUnderstand how to specify contracts to test specific effects\n\n\nBe up to date with lectures\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npsych\nkableExtra\nemmeans\n\nAll results should be presented following APA guidelines.If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the rmd bootcamp.\nThe example write-up sections included as part of the solutions are not perfect - they instead should give you a good example of what information you should include and how to structure this. Note that you must not copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more here.\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/caffeinedrink.csv"
  },
  {
    "objectID": "1_06_dummyMELON.html#study-analysis-plan-overview",
    "href": "1_06_dummyMELON.html#study-analysis-plan-overview",
    "title": "Dummy Coding",
    "section": "Study & Analysis Plan Overview",
    "text": "Study & Analysis Plan Overview\n\nQuestion 1\n\n\nExamine the dataset, and perform any necessary and appropriate data management steps.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nThe str() function will return the overall structure of the dataset, this can be quite handy to look at\n\nConvert categorical variables to factors, and if needed, provide better variable names\n\nLabel appropriately factors to aid with your model interpretations if required\n\nCheck that the dataset is complete (i.e., are there any NA values?). We can check this using is.na()\n\nAre scores within possible ranges (e.g., if we recorded people’s age, it would be impossible to have someone aged -31!)\n\n\n\n\n\n\n\n\n Solution \n\n\nLet’s have a look at the data to see what we’re working with:\n\n#first look at dataset structure\nstr(caffeine)\n\nspc_tbl_ [40 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ treatment: chr [1:40] \"control\" \"control\" \"control\" \"control\" ...\n $ wpm      : num [1:40] 109 114 113 110 116 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   treatment = col_character(),\n  ..   wpm = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n#now lets look at top 6 rows (or the head) of the dataset\nhead(caffeine)\n\n# A tibble: 6 × 2\n  treatment   wpm\n  &lt;chr&gt;     &lt;dbl&gt;\n1 control    109.\n2 control    114.\n3 control    113.\n4 control    110.\n5 control    116.\n6 control    113.\n\n#check for NAs - there are none - all FALSE\ntable(is.na(caffeine))\n\n\nFALSE \n   80 \n\n\nLet’s start with the ‘treatment’ condition variable. This should be coded as factor (&lt;fctr&gt;), but can see from above it is currently coded as a character (&lt;chr&gt;). Let’s correct this.\n\n#Code treatment as a factor\ncaffeine &lt;- caffeine %&gt;%\n  mutate(treatment = as_factor(treatment))\n\nNext, let’s look at the ‘wpm’ variable. Here we want to check for impossible values. Although we don’t know if there is a maximum possible WPM, we do know that we cannot have a negative WPM.\n\n# all looks ok - min and max both positive values\ndescribe(caffeine$wpm)\n\n   vars  n   mean   sd median trimmed  mad    min    max range skew kurtosis\nX1    1 40 113.59 2.92 113.06  113.51 3.62 107.94 120.24  12.3 0.22    -0.84\n     se\nX1 0.46\n\n\n\n\n\n\n\n\nAll participant data was complete (no missing values), with WPM scores within possible ranges. Treatment was coded as a factor with four levels - control (water), coffee, red bull, and mint tea.\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nChoose an appropriate reference level for the Treatment condition.\n\n\n\n\n Solution \n\n\nThe Treatment factor has a group coded ‘Control (Water)’ which lends itself naturally to be the reference category.\n\n#set 'Control' caffeine treatment condition as our reference group \ncaffeine$treatment &lt;- relevel(caffeine$treatment, \"control\")\n\n#check levels - control should be first in the list\nlevels(caffeine$treatment)\n\n[1] \"control\"  \"coffee\"   \"red_bull\" \"mint_tea\"\n\n\n\n\n\n\n\nQuestion 3\n\n\nProvide a brief overview of the study design and data, before detailing your analysis plan to address the research question.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nGive the reader some background on the context of the study\nState what type of analysis you will conduct in order to address the research question\nSpecify the model to be fitted to address the research question (note that you will need to specify the reference level of your categorical variable)\nSpecify your chosen significance (\\(\\alpha\\)) level\nState your hypotheses\n\nMuch of the information required can be found in the Study Overview codebook. The statistical models flashcards may also be useful to refer to.\n\n\n\n\n\n\n\n Solution \n\n\nThe caffeine dataset contained information on 40 hypothetical participants who took part in an experiment examining whether the number of words typed per minute (WPM) differed among caffeine treatment conditions. Using a between-subjects design, the researchers collected information on participants’ WPM (average number of words typed per minute), and which one of four treatment conditions they were randomly assigned to (Control (Water), Coffee, Mint Tea, or Red Bull).\nBoxplots will be used to visualise the associations among WPM and caffeine treatment conditions. To address the research question of whether WPM differs by caffeine treatment condition, we first need to define the dummy variables for Treatment:\n\\[\n\\text{Treatment}_\\text{Coffee} = \\begin{cases}  \n1 & \\text{if Treatment is Coffee} \\\\  \n0 & \\text{otherwise}  \n\\end{cases}  \n\\quad    \n\\]\n\\[\n\\text{Treatment}_\\text{Red Bull} = \\begin{cases}  \n1 & \\text{if Treatment is Red Bull} \\\\  \n0 & \\text{otherwise}  \n\\\\  \n\\end{cases}  \n\\quad  \n\\]\n\\[\n\\text{Treatment}_\\text{Mint Tea} = \\begin{cases}  \n1 & \\text{if Treatment is Mint Tea} \\\\  \n0 & \\text{otherwise}  \n\\end{cases}  \n\\quad  \n\\]\n\\[\n(\\text{Control (Water) is base level})  \n\\]\nBased on the above dummy coding, we are going to fit the following regression model:\n\\[\n\\begin{align}\n\\text{WPM} = \\beta_0 + \\beta_1 \\cdot \\text{Treatment}_\\text{Coffee} \\\\    \n+ \\beta_2 \\cdot \\text{Treatment}_\\text{Red Bull} + \\beta_3 \\cdot \\text{Treatment}_\\text{Mint Tea} + \\epsilon\n\\end{align}\n\\]\nEffects will be considered statistically significant at \\(\\alpha=.05\\)\nOur hypotheses are:\n\\(H_0:\\) All \\(\\beta_j = 0\\) (for \\(j = 1, 2, 3\\))\nThere are no differences in WPM based on caffeine treatment condition.\n\\(H_1:\\) At least one \\(\\beta_j \\neq 0\\) (for \\(j = 1, 2, 3\\))\nThere are differences in WPM based on caffeine treatment condition."
  },
  {
    "objectID": "1_06_dummyMELON.html#descriptive-statistics-visualisations",
    "href": "1_06_dummyMELON.html#descriptive-statistics-visualisations",
    "title": "Dummy Coding",
    "section": "Descriptive Statistics & Visualisations",
    "text": "Descriptive Statistics & Visualisations\n\nQuestion 4\n\n\nProvide a table of descriptive statistics and visualise your data.\nRemember to interpret your plot in the context of the study (i.e., comment on any observed differences among treatment groups).\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the many ways to numerically and visually explore your data by reading over the data exploration flashcards.\nFor examples, see flashcards on descriptives statistics tables - categorical and numeric values examples and data visualisation - bivariate examples, paying particular attention to the type of data that you’re working with.\nMake sure to comment on any observed differences among the sample means of the four treatment conditions.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nNumeric\nVisual\n\n\n\nDescriptive statistics presented in a well formatted table, grouped by treatment condition:\n\ncaffeine %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(n = n(), \n            Mean = mean(wpm), \n            SD = sd(wpm),\n            Minimum = min(wpm),\n            Maximum = max(wpm)) %&gt;%\n    kable(caption = \"Descriptive Statistics\", digits = 2) %&gt;%\n    kable_styling()\n\n\n\nTable 1: Descriptive Statistics\n\ntreatment\nn\nMean\nSD\nMinimum\nMaximum\n\n\n\ncontrol\n10\n112.15\n1.98\n109.43\n116.23\n\n\ncoffee\n10\n114.48\n1.82\n112.07\n117.16\n\n\nred_bull\n10\n116.65\n2.15\n113.00\n120.24\n\n\nmint_tea\n10\n111.09\n2.13\n107.94\n115.82\n\n\n\n\n\n\n\n\n\n\nSince we have a continuous outcome and a categorical predictor - a boxplot would be most appropriate for visualisations:\n\nggplot(data = caffeine, aes(x = treatment, y = wpm, fill = treatment)) +\n  geom_boxplot() +\n  labs(x = 'Treatment Condition', y = 'WPM')\n\n\n\nFigure 1: Association between Treatment Conditions and WPM\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the boxplots, it seems that those in the Red Bull condition, on average, typed the most WPM, whilst those in the Mint Tea condition the fewest.\nOverall, the average WPM appears to be lower for those in the non-caffeine conditions (i.e., control - water / mint tea) in comparison to those in the caffeine drinks condition (red bull / coffee)."
  },
  {
    "objectID": "1_06_dummyMELON.html#model-fitting-interpretation",
    "href": "1_06_dummyMELON.html#model-fitting-interpretation",
    "title": "Dummy Coding",
    "section": "Model Fitting & Interpretation",
    "text": "Model Fitting & Interpretation\n\nQuestion 5\n\n\nFit the specified model, and assign it the name “caf_mdl1”.\nInterpret your coefficients in the context of the study.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can fit our multiple regression model using the lm() function. For a recap, see the statistical models flashcards.\nRecall that R computes the dummy variables for us. Thus, each row in the summary() output of the model will correspond to one of the estimated \\(\\beta\\)’s in the equation above. For a more in-depth recap, see the numeric outcomes & categorical predictors flashcards\n\n\n\n\n\n\n\n Solution \n\n\n\n#fit model\ncaf_mdl1 &lt;- lm(wpm ~ treatment, data=caffeine)\n\n\n#check model output\nsummary(caf_mdl1)\n\n\nCall:\nlm(formula = wpm ~ treatment, data = caffeine)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.652 -1.362 -0.151  1.125  4.729 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       112.1460     0.6402 175.179  &lt; 2e-16 ***\ntreatmentcoffee     2.3350     0.9053   2.579   0.0141 *  \ntreatmentred_bull   4.5060     0.9053   4.977 1.61e-05 ***\ntreatmentmint_tea  -1.0550     0.9053  -1.165   0.2516    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.024 on 36 degrees of freedom\nMultiple R-squared:  0.5563,    Adjusted R-squared:  0.5194 \nF-statistic: 15.05 on 3 and 36 DF,  p-value: 1.651e-06\n\n\nLet’s first map our coefficients and estimates:\n\n\n\n\n\n\n\nCoefficient\nEstimate\nCorresponds to\n\n\n\n(Intercept)\n112.1460\n\\(\\beta_0 = \\hat \\mu_1\\)\n\n\ntreatmentcoffee\n2.3350\n\\(\\beta_0 + \\beta_1 = \\hat \\mu_2\\)\n\n\ntreatmentred_bull\n4.5060\n\\(\\beta_0 + \\beta_2 = \\hat \\mu_3\\)\n\n\ntreatmentmint_tea\n-1.0550\n\\(\\beta_0 + \\beta_3 = \\hat \\mu_4\\)\n\n\n\n\n\n\n\n\n\n\nThe estimate corresponding to (Intercept) contains \\(\\hat \\beta_0 = \\hat \\mu_1 = 112.15\\). The estimated average WPM for those in the control condition (water) was approximately 112.15.\nThe next estimate corresponds to treatmentcoffee and was \\(\\hat \\beta_1 = 2.34\\). The difference in mean WPM between Control and Coffee was estimated to be \\(2.34\\). Thus, \\(\\hat \\mu_2 = 112.15 + 2.34 = 114.49\\). In other words, people who have had coffee typed approximately 114.49 words, which was 2.34 words per minute more than those who have had water. This difference was statistically significant \\((p = .01)\\).\nThe estimate corresponding to treatmentred_bull was \\(\\hat \\beta_2 = 4.51\\). This was the estimated difference in mean WPM between Control and Red Bull, estimated to be \\(4.51\\). Thus, \\(\\hat \\mu_3 = 112.15 + 4.51 = 116.66\\). In other words, people who had red bull typed approximately 116.66 words, 4.51 words per minute more than those who had water. This difference was statistically significant \\((p &lt; .001)\\).\nThe estimate corresponding to treatmentmint_tea was \\(\\hat \\beta_3 = -1.06\\). This was the estimated difference in mean WPM between Control and Mint Tea, estimated to be \\(-1.06\\). Thus, \\(\\hat \\mu_4 = 112.15 + (-1.06) = 111.09\\). In other words, people who had mint tea typed approximately 111.09 words, 1.06 words per minute less than those who had water. This difference was not statistically significant \\((p = .25)\\)."
  },
  {
    "objectID": "1_06_dummyMELON.html#planned-comparisons-contrasts",
    "href": "1_06_dummyMELON.html#planned-comparisons-contrasts",
    "title": "Dummy Coding",
    "section": "Planned Comparisons / Contrasts",
    "text": "Planned Comparisons / Contrasts\n\nQuestion 6\n\n\nFormally state the two planned comparisons that the researchers were interested in as testable hypotheses.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee the manual contrasts flashcards.\n\n\n\n\n\n\n\n Solution \n\n\nRecall that the researchers were also interested in addressing these two questions:\n\nWhether having some kind of caffeine (i.e., red bull or coffee), rather than no caffeine (i.e., control - water or mint tea), resulted in a difference in average WPM\nWhether there was a difference in average WPM between those with hot drinks (i.e., mint tea / coffee) in comparison to those with cold drinks (control - water / red bull)\n\nWe can specify the two hypotheses as follows:\n\n\nCaffeine vs No Caffeine\nHot vs Cold\n\n\n\n\\[\n\\begin{aligned}\n1. \\quad \\text{H}_0 &: \\mu_\\text{No Caffeine} = \\mu_\\text{Caffeine} \\\\\n    \\quad \\text{H}_0 &: \\frac{1}{2} (\\mu_\\text{Control} + \\mu_\\text{Mint Tea}) = \\frac{1}{2} (\\mu_\\text{Coffee} + \\mu_\\text{Red Bull}) \\\\\n\\\\\n    \\quad \\text{H}_1 &: \\mu_\\text{No Caffeine} \\neq \\mu_\\text{Caffeine} \\\\\n    \\quad \\text{H}_1 &: \\frac{1}{2} (\\mu_\\text{Control} + \\mu_\\text{Mint Tea}) \\neq \\frac{1}{2} (\\mu_\\text{Coffee} + \\mu_\\text{Red Bull}) \\\\\n\\\\\n\\end{aligned}\n\\]\n\n\n\\[\n\\begin{aligned}\n2. \\quad H_0 &: \\mu_\\text{Hot Drink} = \\mu_\\text{Cold Drink} \\\\\n    \\quad H_0 &: \\frac{1}{2} (\\mu_\\text{Coffee} + \\mu_\\text{Mint Tea}) = \\frac{1}{2} (\\mu_\\text{Control} + \\mu_\\text{Red Bull})\n\\\\\n\\\\\n    \\quad H_1 &: \\mu_\\text{Hot Drink} \\neq \\mu_\\text{Cold Drink} \\\\\n    \\quad H_1 &: \\frac{1}{2} (\\mu_\\text{Coffee} + \\mu_\\text{Mint Tea}) \\neq \\frac{1}{2} (\\mu_\\text{Control} + \\mu_\\text{Red Bull})\n    \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nAfter checking the levels of the factor treatment, use emmeans() to obtain the estimated treatment means and uncertainties for your factor.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee the manual contrasts flashcards.\n\n\n\n\n\n\n\n Solution \n\n\n\nlevels(caffeine$treatment)\n\n[1] \"control\"  \"coffee\"   \"red_bull\" \"mint_tea\"\n\n\nUse the emmeans() to get the estimated means of our groups:\n\ntreatment_mean &lt;- emmeans(caf_mdl1, ~ treatment)\ntreatment_mean\n\n treatment emmean   SE df lower.CL upper.CL\n control      112 0.64 36      111      113\n coffee       114 0.64 36      113      116\n red_bull     117 0.64 36      115      118\n mint_tea     111 0.64 36      110      112\n\nConfidence level used: 0.95 \n\nplot(treatment_mean)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nSpecify the coefficients of the comparisons and run the contrast analysis, obtaining 95% confidence intervals.\n\n\n\n\n\n\nHint\n\n\n\n\n\nSee the manual contrasts flashcards.\nRemember that ordering matters here - look again at the output of levels(caffeine$treatment) as this will help you when assigning your weights.\n\n\n\n\n\n\n\n Solution \n\n\nAs shown above via levels(), the ordering of the treatment factor is:\n\nControl (no caffeine / cold drink)\nCoffee (caffeine / hot drink)\nRed Bull (caffeine / cold drink)\nMint Tea (no caffeine / hot drink)\n\nFrom this ordering, we can specify our weights - based on the hypothesis, lets assign positive values to the no caffeine and hot drink conditions:\n\ntreatment_comp &lt;- list(\"No Caffeine - Caffeine\" = c(1/2, -1/2, -1/2, 1/2),\n             \"Hot Drink - Cold Drink\" = c(-1/2, 1/2, -1/2, 1/2)\n             )\n\nNow lets run our contrast analysis and get confidence intervals - to do so we use the contrast() function from emmeans():\n\n#run contrast analysis\ntreatment_comp_test &lt;- contrast(treatment_mean, method = treatment_comp)\n\n#examine output\ntreatment_comp_test\n\n contrast               estimate   SE df t.ratio p.value\n No Caffeine - Caffeine    -3.95 0.64 36  -6.167  &lt;.0001\n Hot Drink - Cold Drink    -1.61 0.64 36  -2.520  0.0163\n\n#obtain confidence intervals\nconfint(treatment_comp_test)\n\n contrast               estimate   SE df lower.CL upper.CL\n No Caffeine - Caffeine    -3.95 0.64 36    -5.25   -2.650\n Hot Drink - Cold Drink    -1.61 0.64 36    -2.91   -0.315\n\nConfidence level used: 0.95 \n\n\n\n\n\n\n\nQuestion 9\n\n\nInterpret the results of the contrast analysis in the context of the researchers hypotheses.\nSee the manual contrasts flashcards.\n\n\n\n\n Solution \n\n\n\n\nHypothesis 1: Caffeine vs No Caffeine\nHypothesis 2: Hot vs Cold\n\n\n\nWe performed a test against \\(H_0: \\frac{1}{2}(\\mu_1 + \\mu_4) - \\frac{1}{2}(\\mu_2 + \\mu_3) = 0\\). At the 5% significance level, there was evidence that the mean WPM for those who were in the no caffeine condition was significantly different from those in a caffeine condition \\((t(36) = -6.17, p &lt; .001, \\text{two-sided})\\), and this difference was estimated to be -3.95. We are 95% confident that those who consumed no caffeine typed, on average, between 2.7 and 5.3 words less per minute than those who consumed some form of caffeine \\(CI_{95}[-5.25, -2.65]\\).\n\n\nWe performed a test against \\(H_0: \\frac{1}{2}(\\mu_2 + \\mu_4) - \\frac{1}{2}(\\mu_1 + \\mu_3) = 0\\). At the 5% significance level, there was evidence that the average WPM for those in the hot drink condition significantly differed from those in the cold drink condition \\((t(36) = -2.52, p = .02, \\text{two-sided})\\), and this difference was estimated to be -1.61. We are 95% confident that those who consumed a hot drink typed, on average, between 0.3 and 2.9 words less per minute than those who consumed a cold drink \\(CI_{95}[-2.91, -0.32]\\)."
  },
  {
    "objectID": "1_06_dummyMELON.html#study-design",
    "href": "1_06_dummyMELON.html#study-design",
    "title": "Dummy Coding",
    "section": "Study Design",
    "text": "Study Design\n\nQuestion 10\n\n\nFor each of the below experiment descriptions, note (1) the design, (2) number of variables of interest, (3) levels of categorical variables, (4) what you think the reference group should be and why.\n\n\nExperiment 1\nExperiment 2\nExperiment 3\n\n\n\nA group of researchers were interested in whether sleep deprivation influenced reaction time. They hypothesised that sleep deprived individuals would have slower reaction times than non-sleep deprived individuals.\nTo test this, they recruited 60 participants who were matched on a number of demographic variables including age and sex. One member of each pair (e.g., female, aged 18) was placed into a different sleep condition - ‘Sleep Deprived’ (4 hours per night) or ‘Non-Sleep Deprived’ (8 hours per night).\n\n\nA group of researchers were interested in replicating an experiment testing the Stroop Effect.\nThey recruited 50 participants who took part in Task A (word colour and meaning are congruent) and Task B (word colour and meaning are incongruent) where they were asked to name the color of the ink instead of reading the word. The order of presentation was counterbalanced across participants. The researchers hypothesised that participants would take significantly more time (‘response time’ measured in seconds) to complete Task B than Task A.\nYou can test yourself here for fun: Stroop Task\n\n\nA group of researchers wanted to test a hypothesised theory according to which patients with amnesia will have a deficit in explicit memory but not implicit memory. Huntingtons patients, on the other hand, will display the opposite: they will have no deficit in explicit memory, but will have a deficit in implicit memory.\nTo test this, researchers designed a study that included two variables: ‘Diagnosis’ (Amnesic, Huntingtons, Control) and ‘Task’ (Grammar, Classification, Recognition) where participants were randomly assigned to a Task condition. The first two tasks (Grammar and Classification) are known to reflect implicit memory processes, whereas the Recognition task is known to reflect explicit memory processes.\n\n\n\n\n\n\n\n Solution \n\n\n\n\nExperiment 1\nExperiment 2\nExperiment 3\n\n\n\n\nDesign = Between-person: Matched pairs\nNo of variables of interest = 2 - Sleep Condition and Reaction Time\nLevels of variables = Sleep Condition has 2 levels - Sleep Deprived and Non-Sleep Deprived; Reaction Time is a continuous measure, so has no associated levels\nReference Group = Sleep Condition - Non-Sleep Deprived because the research question stated that the researchers were interested in how the sleep deprived group differed from the non-sleep deprived group.\n\n\n\n\nDesign = Within-person: Repeated measures (this is a study design that you will learn more about in DAPR3!)\nNo of variables of interest = 2 - Task and Response Time\nLevels of variables = Task has 2 levels - A and B; Response Time is a continuous measure, so has no associated levels\nReference Group = Task - A because the research question stated that the researchers were interested in how response time in Task B differed from the response time in Task A.\n\n\n\n\nDesign = Between-person: 3×3 factorial design\nNo of variables of interest = 2 - Diagnosis and Task\nLevels of variables = Diagnosis has 3 levels - Amnesic, Huntingtons, and Control; Task has 3 levels - Grammar, Classification, and Recognition\nReference Groups = Diagnosis - Control; Task - Recognition. We have chosen Control since the other two groups have some form of cognitive impairment; and the Recognition task since it measures explicit memory whilst the other two task types implicit."
  },
  {
    "objectID": "1_b1_reading.html",
    "href": "1_b1_reading.html",
    "title": "Block 1 Flash Cards",
    "section": "",
    "text": "The purpose of these flashcards is to complement your Semester 1 Weeks 1 - 5 core learning materials i.e., your lecture and lab materials, by offering additional guidance and examples on key concepts/topics. It’s designed to deepen your understanding, clarify complex concepts, and help you make connections between different areas of study. Think of it as an extra resource that supports what you’re learning in the classroom.\nYou may want to consider using the below as a supporting document whilst your work through lab exercises, and/or refer to in order to aid revision.\n\nWithin this reading, the following packages are used:\n\ntidyverse\nsjPlot\nkableExtra\npsych\n\nNote that you must not copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more here."
  },
  {
    "objectID": "1_b1_reading.html#numeric-exploration",
    "href": "1_b1_reading.html#numeric-exploration",
    "title": "Block 1 Flash Cards",
    "section": "Numeric Exploration",
    "text": "Numeric Exploration\nNumeric exploration of data involves examining key statistics like mean, median, and standard deviation via descriptives tables; and assessing the associations among variables through correlation coefficients. Exploring our data numerically helps us to identify patterns and associations in the data.\nDescriptives\n\n Descriptives Tables\n\n\nThere are numerous packages available that allow us to pull out descriptive statistics from our dataset such as tidyverse and psych.\nWhen we pull out descriptive statistics, it is useful to present these in a well formatted table for your reader. There are lots of different ways of doing this, but one of the most common (and straightforward!) is to use the kable() function from the package kableExtra.\nThis allows us to give our table a clear caption (via caption = \"insert caption here\", align values within columns e.g., center aligned via align = \"??\"), and we can also round to however many decimal places we desire (standard for APA is 2 dp; via digits = ??).\nWe can also add in the function kable_styling(). This is really helpful for customsing your table e.g., the font size, position, and whether or not you want the table full width (as well as lots of other things - check out the helper function!).\nFor an overview of how to make tables in RMarkdown, see Lesson 4 of the RMD bootcamp\n\n\n\n\n Descriptives Tables - Examples\n\n\n\n\nThe tidyverse way\nThe psych way\n\n\n\nWe can use the summarise() function to numerically summarise/describe our data. Some key values we may want to consider extracting are (though not limited to): the mean (via mean(), standard deviation (via sd()), minimum value (via min()), maximum value (via max()), standard error (via se()), and skewness (via skew()).\nFor example:\n\nlibrary(tidyverse)\nlibrary(kableExtra)\n\n# using the pre-loaded iris dataset\n# taking the mean and standard deviation of sepal length via the summarize function\n# returning a table with a caption, where numbers are rounded to 2 dp\n# asking for a table that is not the full width of the window display\niris %&gt;%\n    summarize(\n        M_Length = mean(Sepal.Length),\n        SD_Length = sd(Sepal.Length)\n    ) %&gt;%\n    kable(caption = \"Sepal Length Descriptives (in cm)\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\nSepal Length Descriptives (in cm)\n\nM_Length\nSD_Length\n\n\n5.84\n0.83\n\n\n\n\n\n\nThe describe() function will produce a table of descriptive statistics. If you would like only a subset of this output (e.g., mean, sd), you can use select() after calling describe() e.g., describe() %&gt;% select(mean, sd).\nFor example:\n\nlibrary(psych)\nlibrary(kableExtra)\n\n# using the pre-loaded iris dataset\n# we want to get descriptive statistics of the iris dataset, specifically the sepal length column\n# we specifically want to select the mean and standard deviation from the descriptive statistics available (try this without including this argument to see what values you all get out)\n# returning a table with a caption, where numbers are rounded to 2 dp\n# asking for a table that is not the full width of the window display\ndescribe(iris$Sepal.Length) %&gt;%\n    select(mean, sd) %&gt;%\n    kable(caption = \"Sepal Length Descriptives (in cm)\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\nSepal Length Descriptives (in cm)\n\n\nmean\nsd\n\n\nX1\n5.84\n0.83\n\n\n\n\n\n\n\n\n\n\nCorrelation\n\n Correlation Coefficient\n\n\nThe correlation coefficient is a standardised number which quantifies the strength and direction of the linear association between two variables. In a population it is denoted by \\(\\rho\\), and in a sample it is denoted by \\(r\\).\nValues of \\(r\\) fall between \\(-1\\) and \\(1\\). How to interpret:\n\nSize: More extreme values (i.e., the The closer \\(r\\) is to \\(+/- 1\\)) the stronger the linear association, and the closer to \\(0\\) a weak/no association. Commonly used cut-offs are:\n\nWeak = \\(.1 &lt; |r| &lt; .3\\)\n\nModerate = \\(.3 &lt; |r| &lt; .5\\)\n\nStrong = \\(|r| &gt; .5\\)\n\n\n\nDirection: The sign of \\(r\\) says nothing about the strength of the association, but its nature and direction:\n\nPositive association means that values of one variable tend to be higher when values of the other variable are higher\nNegative association means that values of one variable tend to be lower when values of the other variable are higher\n\n\n\n\n\n\n\n Correlation Matrix\n\n\nA correlation matrix is a table showing the correlation coefficients - \\(r_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\\) - between variables. Each cell in the table shows the association between two variables. The diagonals show the correlation of a variable with itself (and are therefore always equal to 1).\n\nIn R\nWe can create a correlation matrix by giving the cor() function a dataframe. It is important to remember that all variables must be numeric.\n\nLet’s check the structure of the iris dataset to ensure that all variables are numeric:\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nWe can see that the variable Species in column 5 is a factor - this means that we cannot include this in our correlation matrix. Therefore, we need to subset, or, in other words, select specific columns. We can do this either giving the column numbers inside [], or using select(). In our case, we want the variables in columns 1 - 4, just not 5.\nIf you had NA values within your dataset, you could choose to remove these NAs using na.rm = TRUE inside the cor() function.\n\n\nIndex dataframe ([])\nVariable selection (select())\n\n\n\n\nround(cor(iris[,c(1:4)]), digits = 2)\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n\n\n\n\n\n# select only the columns we want by variable name, and pass this to cor()\niris %&gt;% \n  select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) %&gt;%\n  cor() %&gt;%\n  round(digits = 2)\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n\n\n\n\n\n\n\n\n\n Correlation - Hypothesis Testing\n\n\nThe hypotheses of the correlation test are, as always, statements about the population parameter (in this case the correlation between the two variables in the population - i.e., \\(\\rho\\)).\nIf we are conducting a two tailed test, then…\n\n\n\\(H_0: \\rho = 0\\). There is no linear association between \\(x\\) and \\(y\\) in the population.\n\n\n\\(H_1: \\rho \\neq 0\\) There is a linear association between \\(x\\) and \\(y\\).\n\nIf we instead conduct a one-tailed test, then we are testing either…\n\n\n\\(H_0: \\rho \\leq 0\\) There is a negative or no linear association between \\(x\\) and \\(y\\)\n\n\n\\(H_1: \\rho &gt; 0\\) There is a positive linear association between \\(x\\) and \\(y\\).\n\nOR\n\n\n\\(H_0: \\rho \\geq 0\\) There is a positive or no linear association between \\(x\\) and \\(y\\)\n\n\n\\(H_1: \\rho &lt; 0\\) There is a negative linear association between \\(x\\) and \\(y\\).\n\nTest Statistic\nThe test statistic for this test is the \\(t\\) statistic, the formula for which depends on both the observed correlation (\\(r\\)) and the sample size (\\(n\\)):\n\\[t = r \\sqrt{\\frac{n-2}{1-r^2}}\\]\np-value\nWe calculate the \\(p\\)-value for our \\(t\\)-statistic as the long-run probability of a \\(t\\)-statistic with \\(n-2\\) degrees of freedom being less than, greater than, or more extreme in either direction (depending on the direction of our alternative hypothesis) than our observed \\(t\\)-statistic.\nAssumptions\nFor a test of Pearson’s correlation coefficient \\(r\\), we need to make sure a few conditions are met:\n\nBoth variables are quantitative (i.e., numeric)\nBoth variables are drawn from normally distributed populations\nThe association between the two variables is linear\n\n\n\n\n\n Correlation - Hypothesis Testing in R\n\n\n\nIn R\nWe can test the significance of the correlation coefficient really easily with the function cor.test():\n\ncor.test(iris$Sepal.Length, iris$Petal.Length)\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Petal.Length\nt = 21.646, df = 148, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8270363 0.9055080\nsample estimates:\n      cor \n0.8717538 \n\n\nNote, by default, cor.test() will include only observations that have no missing data on either variable.\nWe can specify whether we want to conduct a one- or two-tailed test by adding the argument alternative = and specifying alternative = \"less\", alternative = \"greater\", or alternative = \"two.sided\" (the latter being the default).\n\n\n\n\n\n\n\nExample Interpretation\nThere was a strong positive association between sepal length and petal length \\((r = .87, t(148) = 21.65, p &lt; .001)\\). These results suggested that a greater sepal length was positively associated with a greater petal length.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a detailed recap of all things correlation (including further details and examples), revisit the Correlation lecture from DAPR1."
  },
  {
    "objectID": "1_b1_reading.html#visual-exploration",
    "href": "1_b1_reading.html#visual-exploration",
    "title": "Block 1 Flash Cards",
    "section": "Visual Exploration",
    "text": "Visual Exploration\nVisual exploration of our data allows us to visualize the distributions of our data, and to identify potential associations between variables.\n\n How to Visualise Data\n\n\nTo visualise (i.e., plot) our data, we can use ggplot() from the tidyverse package. Note the key components of the ggplot() code:\n\n\ndata = where we provide the name of the dataframe.\n\naes = where we provide the aesthetics. These are things which we map from the data to the graph. For instance, the \\(x\\)-axis, or if we wanted to colour the columns/bars according to some aspect of the data.\n\n+ geom_... = where we add (using +) some geometry. These are the shapes (e.g., bars, points, etc.), which will be put in the correct place according to what we specified in aes().\n\nlabs() = where we provide labels for our plot (e.g., the \\(x\\)- and \\(y\\)-axis)\n\n\n\n\n\n\n\nNote\n\n\n\nThere are lots of arguments that you can further customise, some of which are specified in the examples below e.g., bins =, alpha =, fill =, linewidth =. linetype =, size = etc. For these, you can look up the helper function to see the range of arguments they can take using ? - e.g., ?fill\nIf you’d like to read more about ggplot(), there is a handy cheatsheet.\n\n\nOne other thing to consider when visualising your data is how you are going to arrange your plots. Some handy tips on this:\n\nUse to wrap text in your titles and or axis labels\n\nThe patchwork package allows us to arrange multiple plots in two ways - | arranges the plots adjacent to one another, and / arranges the plots on top of one another\n\n\n\n\n\n Data Visualisation - Marginal Examples\n\n\n\n\nHistogram\nDensity\n\n\n\nA histogram shows the frequency of values which fall within bins of an equal width.\nBasic:\n\nx-axis: possible values of some variable, grouped into bins\ny-axis: frequency of a given value or values within bins\n\nWhat are bins?: A bin represents a range of scores\n\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram() +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nUpdating Bins:\nWithin geom_histogram(), we can specify bins = to specify the number of columns we want (for this example, lets say we want 10):\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram(bins = 10) +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nAlternatively, we can specify binwidth = to specify the width of each bin (it is very helpful to be aware of the scale of your variable here!):\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram(binwidth = 0.1) +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nOutline columns with color:\nWithin geom_histogram(), we can specify color = to set a colored outline of the columns:\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram(color = \"red\") +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nFill columns with color:\nWithin geom_histogram(), we can specify fill = to fill the columns with a color:\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram(fill = \"purple\") +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\n\n\nA visualization of the distribution of a numeric variable.\nBasic:\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_density() +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nFilled:\nWe can fill our plot with colour by specifying fill = within geom_density():\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_density(fill = \"lightblue\") +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nLine Type & Width:\nWe can change the type and width of the line by specifying linetype = and linewidth = within geom_density():\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_density(linetype = 6, linewidth = 3) +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Visualisation - Bivariate Examples\n\n\nUnlike in our marginal plots where we specified our x-axis variable within aes(), to visualise bivariate associations, we need to specify what variables we want on both our x- and y-axis.\n\nScatterplot\n\n\nWe can use a scatterplot (since the variables are numeric and continuous) to visualise the association between the two numeric variables - these will be our x- and y-axis values.\nWe plot these values for each row of our dataset, and we should end up with a cloud of scattered points.\nHere we will want to comment on any key observations that we notice, including if we detect outliers or points that do not fit with the pattern in the rest of the data. Outliers are extreme observations that are not possible values of a variable or that do not seem to fit with the rest of the data. This could either be:\n\n\nmarginally along one axis: points that have an unusual (too high or too low) x-coordinate or y-coordinate;\n\njointly: observations that do not fit with the rest of the point cloud\n\nBasic:\nWe need to specify + geom_point() to get a scatterplot:\n\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point() +\n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nFill points with color:\nWithin geom_point(), we can specify color = to fill the points with a color:\n\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point(color = \"orange\") +\n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nChange size and opacity:\nWe can change the size (using size =) and the opacity (using alpha =) of our geom elements on the plot. Let’s include this below:\n\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point(size = 3, alpha = 0.5) +\n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nAdd a line of best fit:\nWe can superimpose (i.e., add) a line of best fit by including the argument + geom_smooth(). Since we want to fit a straight line, we want to use method = \"lm\". We can also specify whether we want to display confidence intervals around our line by specifying se = TRUE / FALSE.\n\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")"
  },
  {
    "objectID": "1_b1_reading.html#latex-symbols-equations",
    "href": "1_b1_reading.html#latex-symbols-equations",
    "title": "Block 1 Flash Cards",
    "section": "LaTeX Symbols & Equations",
    "text": "LaTeX Symbols & Equations\nBy embedding LaTeX into RMarkdown, you can accurately and precisely format mathematical expressions, ensuring that they are not only technically correct but also visually appealing and easy to interpret.\n\n LaTeX Guide\n\n\nFor an overview of how to integrate LaTeX symbols and equations, review Lesson 9 of the RMD bootcamp."
  },
  {
    "objectID": "1_b1_reading.html#apa-formatting",
    "href": "1_b1_reading.html#apa-formatting",
    "title": "Block 1 Flash Cards",
    "section": "APA Formatting",
    "text": "APA Formatting\nAPA format is a writing/presentation style that is often used in psychology to ensure consistency in communication. APA formatting applies to all aspects of writing - from formatting of papers (including tables and figures), citation of sources, and reference lists. This means that it also applies to how you present results in your Psychology courses, including DAPR2.\n\n APA Formatting Guides\n\n\nAll results should be presented following APA guidelines.\nYou also need to follow APA style rules for tables and figures.\nMake sure to familiarise yourself with the above guides, and practice presenting your results following these rules."
  },
  {
    "objectID": "1_b1_reading.html#tables",
    "href": "1_b1_reading.html#tables",
    "title": "Block 1 Flash Cards",
    "section": "Tables",
    "text": "Tables\nWe want to ensure that we are presenting results in a well formatted table. To do so, there are lots of different packages available (see Lesson 4 of the RMD bootcamp).\nOne of the most convenient ways to present results from regression models is to use the tab_model() function from sjPlot\n\n Creating tables via tab_model\n\n\nWithin tab_model(), there are lots of different ways that you can customise your table. The most common arguments that you should use are dv.labels, pred.labels, and title.\nYou can rename your DV and IV labels by specifying dv.labels and pred.labels. To do so, specify your variable name on the left, and what you would like this to be named in the table on the right. For title, you can simply specify in ““’s what you want your title to be e.g., title = \"This is my title\".\nHere’s an example if I had fitted a model with the following information:\n\nModel name = mdl_test\n\nModel DV = cognitive_score\n\nModel IVs = SES and age\n\n\n\nmdl_test &lt;- lm(cognitive_score ~ SES + age, data = data_name)\n\nI want to change the names of SES and age to be socio-economic status and age - in years respectively. What we need to pay attention to here is the ordering of the IVs - the ordering in our lm() must match that in tab_model(). I also want to name my table Regression Table for Cognitive Scores Model. Here is how we would do this in R:\n\nlibrary(sjPlot)\ntab_model(mdl_test,\n          pred.labels = c('Intercept', 'socio-economic status', 'age - in years'),\n          title = \"Regression Table for Cognitive Scores Model\")\n\nSee here for another short example."
  },
  {
    "objectID": "1_b1_reading.html#cross-referencing",
    "href": "1_b1_reading.html#cross-referencing",
    "title": "Block 1 Flash Cards",
    "section": "Cross Referencing",
    "text": "Cross Referencing\nCross-referencing is a very helpful way to direct your reader through your document, and the good news is that this can be done automatically in RMarkdown.\n\n Cross Referencing\n\n\nThere are three key components to allow you to successfully cross-reference within your RMarkdown document:\n\nA bookdown output format\nA caption to your figure or table\nA named/labeled code chunk\n\nOnce you have the above, you will be able to cross-reference using the syntax @ref(type:label), where label is the chunk name/label, and type is the environment being referenced (e.g. tab for table, fig for figure, etc.).\nFor an in-depth overview and example of how to cross-reference, see Lesson 7 of the RMD bootcamp."
  },
  {
    "objectID": "1_b1_reading.html#footnotes",
    "href": "1_b1_reading.html#footnotes",
    "title": "Block 1 Flash Cards",
    "section": "Footnotes",
    "text": "Footnotes\n\nYes, the error term is gone. This is because the line of best-fit gives you the prediction of the average recall accuracy for a given age, and not the individual recall accuracy of an individual person, which will almost surely be different from the prediction of the line.↩︎"
  },
  {
    "objectID": "1_b2_reading.html",
    "href": "1_b2_reading.html",
    "title": "Blocks 1 & 2 Flash Cards",
    "section": "",
    "text": "The purpose of these flashcards is to complement your Semester 1 Weeks 1 - 11 core learning materials i.e., your lecture and lab materials, by offering additional guidance and examples on key concepts/topics. It’s designed to deepen your understanding, clarify complex concepts, and help you make connections between different areas of study. Think of it as an extra resource that supports what you’re learning in the classroom.\nYou may want to consider using the below as a supporting document whilst your work through lab exercises, and/or refer to in order to aid revision.\n\nWithin this reading, the following packages are used:\n\ntidyverse\nsjPlot\nkableExtra\npsych\nemmeans\nperformance\ncar\n\nNote that you must not copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more here."
  },
  {
    "objectID": "1_b2_reading.html#numeric-exploration",
    "href": "1_b2_reading.html#numeric-exploration",
    "title": "Blocks 1 & 2 Flash Cards",
    "section": "Numeric Exploration",
    "text": "Numeric Exploration\nNumeric exploration of data involves examining key statistics like mean, median, and standard deviation via descriptives tables; and assessing the associations among variables through correlation coefficients. Exploring our data numerically helps us to identify patterns and associations in the data.\nDescriptives\n\n Descriptives Tables\n\n\nThere are numerous packages available that allow us to pull out descriptive statistics from our dataset such as tidyverse and psych.\nWhen we pull out descriptive statistics, it is useful to present these in a well formatted table for your reader. There are lots of different ways of doing this, but one of the most common (and straightforward!) is to use the kable() function from the package kableExtra.\nThis allows us to give our table a clear caption (via caption = \"insert caption here\", align values within columns e.g., center aligned via align = \"??\"), and we can also round to however many decimal places we desire (standard for APA is 2 dp; via digits = ??).\nWe can also add in the function kable_styling(). This is really helpful for customising your table e.g., the font size, position, and whether or not you want the table full width (as well as lots of other things - check out the helper function!).\nFor an overview of how to make tables in RMarkdown, see Lesson 4 of the RMD bootcamp\n\n\n\n\n Descriptives Tables - Examples\n\n\n\n\nThe tidyverse way\nThe psych way\n\n\n\nWe can use the summarise() function to numerically summarise/describe our data. Some key values we may want to consider extracting are (though not limited to): the mean (via mean(), standard deviation (via sd()), minimum value (via min()), maximum value (via max()), standard error (via se()), and skewness (via skew()).\n\n\nNumeric values only example:\nCategorical and numeric values example:\n\n\n\n\nlibrary(tidyverse)\nlibrary(kableExtra)\n\n# using the pre-loaded iris dataset\n# taking the mean and standard deviation of sepal length via the summarize function\n# returning a table with a caption, where numbers are rounded to 2 dp\n# asking for a table that is not the full width of the window display\niris %&gt;%\n    summarize(\n        M_Length = mean(Sepal.Length),\n        SD_Length = sd(Sepal.Length)\n    ) %&gt;%\n    kable(caption = \"Sepal Length Descriptives (in cm)\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\nSepal Length Descriptives (in cm)\n\nM_Length\nSD_Length\n\n\n5.84\n0.83\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(kableExtra)\n\n# using the pre-loaded iris dataset\n# grouping by Species\n# taking the mean and standard deviation of sepal length via the summarize function\n# returning a table of sepal length grouped by species with a caption, where numbers are rounded to 2 dp\n# asking for a table that is not the full width of the window display\niris %&gt;%\n    group_by(Species) %&gt;%\n    summarize(\n        M_Length = mean(Sepal.Length),\n        SD_Length = sd(Sepal.Length)\n    ) %&gt;%\n    kable(caption = \"Sepal Length (in cm) Grouped by Species Descriptives Table\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\nSepal Length (in cm) Grouped by Species Descriptives Table\n\nSpecies\nM_Length\nSD_Length\n\n\n\nsetosa\n5.01\n0.35\n\n\nversicolor\n5.94\n0.52\n\n\nvirginica\n6.59\n0.64\n\n\n\n\n\n\n\n\n\n\nThe describe() function will produce a table of descriptive statistics. If you would like only a subset of this output (e.g., mean, sd), you can use select() after calling describe() e.g., describe() %&gt;% select(mean, sd).\n\n\nNumeric values only example:\nCategorical and numeric values options:\n\n\n\n\nlibrary(psych)\nlibrary(kableExtra)\n\n# using the pre-loaded iris dataset\n# we want to get descriptive statistics of the iris dataset, specifically the sepal length column\n# we specifically want to select the mean and standard deviation from the descriptive statistics available (try this without including this argument to see what values you all get out)\n# returning a table with a caption, where numbers are rounded to 2 dp\n# asking for a table that is not the full width of the window display\ndescribe(iris$Sepal.Length) %&gt;%\n    select(mean, sd) %&gt;%\n    kable(caption = \"Sepal Length Descriptives (in cm)\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\nSepal Length Descriptives (in cm)\n\n\nmean\nsd\n\n\nX1\n5.84\n0.83\n\n\n\n\n\n\nNote that this is quite an overly complex way to return these summary statistics - using the tidyverse() way is much more intuitive and straightforward!\n\nlibrary(psych)\nlibrary(kableExtra)\n\n# using the pre-loaded iris dataset\n# we want to get descriptive statistics of the iris dataset, specifically the sepal length column by Species\n# we want to return a matrix (hence mat = TRUE), then convert this to a dataframe\n# we specifically want to select the mean and standard deviation from the descriptive statistics available (try this without including this argument to see what values you all get out)\n# returning a table with a new column names of Group, Mean, SD; adding a caption; numbers are rounded to 2 dp\n# asking for a table that is not the full width of the window display\n\n\ndescribeBy(Sepal.Length ~ Species, data = iris, mat = TRUE, digits = 2) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column() %&gt;% \n  select(group1, mean, sd) %&gt;%\n    kable(col.names = c(\"Group\", \"Mean\", \"SD\"), caption = \"Sepal Length Descriptives (in cm)\", digits = 2) %&gt;%\n    kable_styling(full_width = FALSE)\n\n\nSepal Length Descriptives (in cm)\n\nGroup\nMean\nSD\n\n\n\nsetosa\n5.01\n0.35\n\n\nversicolor\n5.94\n0.52\n\n\nvirginica\n6.59\n0.64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation\n\n Correlation Coefficient\n\n\nThe correlation coefficient is a standardised number which quantifies the strength and direction of the linear association between two variables. In a population it is denoted by \\(\\rho\\), and in a sample it is denoted by \\(r\\).\nValues of \\(r\\) fall between \\(-1\\) and \\(1\\). How to interpret:\n\nSize: More extreme values (i.e., the The closer \\(r\\) is to \\(+/- 1\\)) the stronger the linear association, and the closer to \\(0\\) a weak/no association. Commonly used cut-offs are:\n\nWeak = \\(.1 &lt; |r| &lt; .3\\)\n\nModerate = \\(.3 &lt; |r| &lt; .5\\)\n\nStrong = \\(|r| &gt; .5\\)\n\n\n\nDirection: The sign of \\(r\\) says nothing about the strength of the association, but its nature and direction:\n\nPositive association means that values of one variable tend to be higher when values of the other variable are higher\nNegative association means that values of one variable tend to be lower when values of the other variable are higher\n\n\n\n\n\n\n\n Correlation Matrix\n\n\nA correlation matrix is a table showing the correlation coefficients - \\(r_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\\) - between variables. Each cell in the table shows the association between two variables. The diagonals show the correlation of a variable with itself (and are therefore always equal to 1).\n\nIn R\nWe can create a correlation matrix by giving the cor() function a dataframe. It is important to remember that all variables must be numeric.\n\nLet’s check the structure of the iris dataset to ensure that all variables are numeric:\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nWe can see that the variable Species in column 5 is a factor - this means that we cannot include this in our correlation matrix. Therefore, we need to subset, or, in other words, select specific columns. We can do this either giving the column numbers inside [], or using select(). In our case, we want the variables in columns 1 - 4, just not 5.\nIf you had NA values within your dataset, you could choose to remove these NAs using na.rm = TRUE inside the cor() function.\n\n\nIndex dataframe ([])\nVariable selection (select())\n\n\n\n\nround(cor(iris[,c(1:4)]), digits = 2)\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n\n\n\n\n\n# select only the columns we want by variable name, and pass this to cor()\niris %&gt;% \n  select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) %&gt;%\n  cor() %&gt;%\n  round(digits = 2)\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n\n\n\n\n\n\n\n\n\n Correlation - Hypothesis Testing\n\n\nThe hypotheses of the correlation test are, as always, statements about the population parameter (in this case the correlation between the two variables in the population - i.e., \\(\\rho\\)).\nIf we are conducting a two tailed test, then…\n\n\n\\(H_0: \\rho = 0\\). There is no linear association between \\(x\\) and \\(y\\) in the population.\n\n\n\\(H_1: \\rho \\neq 0\\) There is a linear association between \\(x\\) and \\(y\\).\n\nIf we instead conduct a one-tailed test, then we are testing either…\n\n\n\\(H_0: \\rho \\leq 0\\) There is a negative or no linear association between \\(x\\) and \\(y\\)\n\n\n\\(H_1: \\rho &gt; 0\\) There is a positive linear association between \\(x\\) and \\(y\\).\n\nOR\n\n\n\\(H_0: \\rho \\geq 0\\) There is a positive or no linear association between \\(x\\) and \\(y\\)\n\n\n\\(H_1: \\rho &lt; 0\\) There is a negative linear association between \\(x\\) and \\(y\\).\n\nTest Statistic\nThe test statistic for this test is the \\(t\\) statistic, the formula for which depends on both the observed correlation (\\(r\\)) and the sample size (\\(n\\)):\n\\[t = r \\sqrt{\\frac{n-2}{1-r^2}}\\]\np-value\nWe calculate the \\(p\\)-value for our \\(t\\)-statistic as the long-run probability of a \\(t\\)-statistic with \\(n-2\\) degrees of freedom being less than, greater than, or more extreme in either direction (depending on the direction of our alternative hypothesis) than our observed \\(t\\)-statistic.\nAssumptions\nFor a test of Pearson’s correlation coefficient \\(r\\), we need to make sure a few conditions are met:\n\nBoth variables are quantitative (i.e., numeric)\nBoth variables are drawn from normally distributed populations\nThe association between the two variables is linear\n\n\n\n\n\n Correlation - Hypothesis Testing in R\n\n\n\nIn R\nWe can test the significance of the correlation coefficient really easily with the function cor.test():\n\ncor.test(iris$Sepal.Length, iris$Petal.Length)\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Petal.Length\nt = 21.646, df = 148, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8270363 0.9055080\nsample estimates:\n      cor \n0.8717538 \n\n\nNote, by default, cor.test() will include only observations that have no missing data on either variable.\nWe can specify whether we want to conduct a one- or two-tailed test by adding the argument alternative = and specifying alternative = \"less\", alternative = \"greater\", or alternative = \"two.sided\" (the latter being the default).\n\n\n\n\n\n\n\nExample Interpretation\nThere was a strong positive association between sepal length and petal length \\((r = .87, t(148) = 21.65, p &lt; .001)\\). These results suggested that a greater sepal length was positively associated with a greater petal length.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a detailed recap of all things correlation (including further details and examples), revisit the Correlation lecture from DAPR1."
  },
  {
    "objectID": "1_b2_reading.html#visual-exploration",
    "href": "1_b2_reading.html#visual-exploration",
    "title": "Blocks 1 & 2 Flash Cards",
    "section": "Visual Exploration",
    "text": "Visual Exploration\nVisual exploration of our data allows us to visualize the distributions of our data, and to identify potential associations between variables.\n\n How to Visualise Data\n\n\nTo visualise (i.e., plot) our data, we can use ggplot() from the tidyverse package. Note the key components of the ggplot() code:\n\n\ndata = where we provide the name of the dataframe.\n\naes = where we provide the aesthetics. These are things which we map from the data to the graph. For instance, the \\(x\\)-axis, or if we wanted to colour the columns/bars according to some aspect of the data.\n\n+ geom_... = where we add (using +) some geometry. These are the shapes (e.g., bars, points, etc.), which will be put in the correct place according to what we specified in aes().\n\nlabs() = where we provide labels for our plot (e.g., the \\(x\\)- and \\(y\\)-axis)\n\n\n\n\n\n\n\nNote\n\n\n\nThere are lots of arguments that you can further customise, some of which are specified in the examples below e.g., bins =, alpha =, fill =, linewidth =. linetype =, size = etc. For these, you can look up the helper function to see the range of arguments they can take using ? - e.g., ?fill\nIf you’d like to read more about ggplot(), there is a handy cheatsheet.\n\n\nOne other thing to consider when visualising your data is how you are going to arrange your plots. Some handy tips on this:\n\nUse to wrap text in your titles and or axis labels\n\nThe patchwork package allows us to arrange multiple plots in two ways - | arranges the plots adjacent to one another, and / arranges the plots on top of one another\n\n\n\n\n\n Data Visualisation - Marginal Examples\n\n\n\n\nHistogram\nDensity\n\n\n\nA histogram shows the frequency of values which fall within bins of an equal width.\nBasic:\n\nx-axis: possible values of some variable, grouped into bins\ny-axis: frequency of a given value or values within bins\n\nWhat are bins?: A bin represents a range of scores\n\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram() +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nUpdating Bins:\nWithin geom_histogram(), we can specify bins = to specify the number of columns we want (for this example, lets say we want 10):\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram(bins = 10) +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nAlternatively, we can specify binwidth = to specify the width of each bin (it is very helpful to be aware of the scale of your variable here!):\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram(binwidth = 0.1) +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nOutline columns with color:\nWithin geom_histogram(), we can specify color = to set a colored outline of the columns:\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram(color = \"red\") +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nFill columns with color:\nWithin geom_histogram(), we can specify fill = to fill the columns with a color:\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram(fill = \"purple\") +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\n\n\nA visualization of the distribution of a numeric variable.\nBasic:\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_density() +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nFilled:\nWe can fill our plot with colour by specifying fill = within geom_density():\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_density(fill = \"lightblue\") +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nLine Type & Width:\nWe can change the type and width of the line by specifying linetype = and linewidth = within geom_density():\n\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_density(linetype = 6, linewidth = 3) +\n    labs(x = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Visualisation - Bivariate Examples\n\n\nUnlike in our marginal plots where we specified our x-axis variable within aes(), to visualise bivariate associations, we need to specify what variables we want on both our x- and y-axis.\n\n\nScatterplot\nBoxplot\nFacets\n\n\n\nWe can use a scatterplot (since the variables are numeric and continuous) to visualise the association between the two numeric variables - these will be our x- and y-axis values.\nWe plot these values for each row of our dataset, and we should end up with a cloud of scattered points.\nHere we will want to comment on any key observations that we notice, including if we detect outliers or points that do not fit with the pattern in the rest of the data. Outliers are extreme observations that are not possible values of a variable or that do not seem to fit with the rest of the data. This could either be:\n\n\nmarginally along one axis: points that have an unusual (too high or too low) x-coordinate or y-coordinate;\n\njointly: observations that do not fit with the rest of the point cloud\n\nBasic:\nWe need to specify + geom_point() to get a scatterplot:\n\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point() +\n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nFill points with color:\nWithin geom_point(), we can specify color = to fill the points with a color:\n\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point(color = \"orange\") +\n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nChange size and opacity:\nWe can change the size (using size =) and the opacity (using alpha =) of our geom elements on the plot. Let’s include this below:\n\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point(size = 3, alpha = 0.5) +\n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nAdd a line of best fit:\nWe can superimpose (i.e., add) a line of best fit by including the argument + geom_smooth(). Since we want to fit a straight line, we want to use method = \"lm\". We can also specify whether we want to display confidence intervals around our line by specifying se = TRUE / FALSE.\n\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\n\n\nWe can use a boxplot to visualise the association between one numeric variable and one categorical variable - these will be our y- and x-axis values respectively.\nBasic:\nWe need to specify + geom_boxplot() to get a boxplot:\n\nggplot(data = iris, aes(x = Species, y = Sepal.Length)) +\n    geom_boxplot() +\n    labs(x = \"Species\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nChange boxplot fill colours by group:\nWithin aes(), we can specify fill = to fill the boxes with a color:\n\nggplot(data = iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n    geom_boxplot() +\n    labs(x = \"Species\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nChange boxplot line colours by group:\nWithin aes(), we can specify color = to colour the lines with a color:\n\nggplot(data = iris, aes(x = Species, y = Sepal.Length, color = Species)) +\n    geom_boxplot() +\n    labs(x = \"Species\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nAdding jitter:\nWe can add jittered points to a boxplot to better see the underlying distribution of the data (by adding a little random variation to each data point) via geom_jitter():\n\nggplot(data = iris, aes(x = Species, y = Sepal.Length, color = Species)) +\n    geom_boxplot() +\n    geom_jitter() + \n    labs(x = \"Species\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nChange legend position:\nWe can add the argument + theme(legend.position = ) to move (or even remove) the legend by specifying, for example, \"right\", \"left\", \"top\", \"bottom\", or \"none\" to remove.\n\n# legend at bottom of plot\nggplot(data = iris, aes(x = Species, y = Sepal.Length, color = Species)) +\n    geom_boxplot() +\n    labs(x = \"Species\", y = \"Sepal Length (in cm)\") + \n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nWhen we have two numeric variables, as well as categorical variables, we can use facet_wrap() / facet_grid() to help divide/arrange our plots. If we had two categorical variables, by simply stringing them together to further group our plots by specifying facet_wrap( ~ cat_variable1 + cat_variable2)\nBasic:\nWe need to specify + geom_point() to get a scatterplot, and either + facet_wrap() or + facet_grid() to separate by your categorical variable:\n\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point() +\n    facet_wrap(~Species) + \n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nAdd a line of best fit:\nWe can superimpose (i.e., add) a line of best fit by including the argument + geom_smooth(). Since we want to fit a straight line, we want to use method = \"lm\". We can also specify whether we want to display confidence intervals around our line by specifying se = TRUE / FALSE. Note that a line is fitted for every level of your categorical variable:\n\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n        facet_wrap(~Species) + \n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n\n\n\n\n\n\n\nSubplot layout:\nYou can change the overall layout of the subplots by specifying dir = within the facet_wrap() argument, where “h” will return a horizontal layout (this is the default) and “v” for vertical.\nYou can also change the layout of the subplot labels by specifying strip.position = within the facet_wrap() argument, where labels can be arranged to display at the “top” (this is the default), “bottom”, “left” or “right”.\n\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point() +\n    facet_wrap(~Species, dir = \"v\", strip.position = \"right\") + \n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")"
  },
  {
    "objectID": "1_b2_reading.html#numeric-outcomes-predictors",
    "href": "1_b2_reading.html#numeric-outcomes-predictors",
    "title": "Blocks 1 & 2 Flash Cards",
    "section": "Numeric Outcomes & Predictors",
    "text": "Numeric Outcomes & Predictors\n\n Simple Linear Regression Models - Example\n\n\nImagine that you were tasked to investigate whether there was an association between recall accuracy and age. You have been provided with data from twenty participants who studied passages of text (c500 words long), and were tested a week later. The testing phase presented participants with 100 statements about the text. They had to answer whether each statement was true or false, as well as rate their confidence in each answer (on a sliding scale from 0 to 100). The dataset contains, for each participant, the percentage of items correctly answered, their age (in years), and their average confidence rating.\nThe data are available at https://uoepsy.github.io/data/recalldata.csv\nWe could specify our model as:\n\\[\n\\text{Recall Accuracy}_i = \\beta_0 + \\beta_1 \\cdot \\text{Age}_i + \\epsilon_i    \n\\]\nand fit our model in R as\n\n#specify model\nrecall_simp &lt;- lm(recall_accuracy ~ age, data = recalldata)\n\n#look at model coefficients\nrecall_simp\n\n\nCall:\nlm(formula = recall_accuracy ~ age, data = recalldata)\n\nCoefficients:\n(Intercept)          age  \n    84.0153      -0.3026  \n\n\nWhen we call the name of the fitted model, recall_simp, you can see the estimated regression coefficients \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). The line of best-fit is thus given by:1\n\\[\n\\widehat{Recall~Accuracy} = 84.02 - 0.31 \\cdot \\text{Age}\n\\]\n\n\n\n\n Multiple Linear Regression Models - Description & Specification\n\n\nMultiple linear regression involves looking at one continuous outcome (i.e., DV), with two or more independent variables (i.e., IVs).\nA multiple linear regression model takes the following form:\n\\[\ny_i = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\epsilon_i\n\\] \\[\n\\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\n\\] So, for example, we could extend our recall accuracy model to include recall confidence as a predictor:\n\\[\n\\text{Recall Accuracy}_i = \\beta_0 + \\beta_1 \\cdot \\text{Recall Confidence}_i + \\beta_2 \\cdot \\text{Age}_i + \\epsilon_i    \n\\]\n\nIn R:\nMultiple and simple linear regression follow the same structure within the lm() function - the logic scales up to however many predictor variables we want to include in our model. You simply add (using the + sign) more independent variables. For example, if we wanted to build a multiple linear regression that included three independent variables, we could fit one of the following via the lm() function:\n\n\nOption A\nOption B\n\n\n\n\nmodel_name &lt;- lm(DV ~ IV1 + IV2 + IV3, data = data_name)\n\n\n\n\nmodel_name &lt;- lm(data_name$DV ~ data_name$IV1 + data_name$IV2 + data_name$IV3)\n\n\n\n\n\nInterpretation of Multiple Regression Coefficients\nYou’ll hear a lot of different ways that people explain multiple regression coefficients.\nFor the model \\(y = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\epsilon\\), the estimate \\(\\hat \\beta_1\\) will often be reported as:\n“the increase in \\(y\\) for a one unit increase in \\(x_1\\) when…”\n\n“holding the effect of \\(x_2\\) constant.”\n“controlling for differences in \\(x_2\\).”\n“partialling out the effects of \\(x_2\\).”\n“holding \\(x_2\\) equal.”\n“accounting for effects of \\(x_2\\).”\n\nFor models with 3+ predictors, just like building the model in R, the logic of the above simply extends.\nFor example “the increase in [outcome] for a one unit increase in [predictor] when…”\n\n“holding [other predictors] constant.”\n\n“accounting for [other predictors].”\n\n“controlling for differences in [other predictors].”\n\n“partialling out the effects of [other predictors].”\n\n“holding [other predictors] equal.”\n\n“accounting for effects of [other predictors].”\n\n\n\n\n\n Simple Linear Regression Models - Visualisation\n\n\nAs we saw above, the line representing the relationship between side and perimeter of squares is able to predict the actual perimeter value from the measurement of the side of a square.\nThis is possible because the association between side and perimeter is an exact one. That is, any squares having the same side will have the same perimeter, and there will be no variation in those values.\nThe line that best fits the association between recall accuracy and age (see Figure 2), instead, is only able to predict the average accuracy for a given value of age.\nThis is because there will be a distribution of recall accuracy at each value of age. The line will fit the trend/pattern in the values, but there will be individual-to-individual variability that we must accept around that average pattern.\n\nggplot(recalldata, aes(x = age, y = recall_accuracy)) + \n    geom_point(size = 3, alpha = 0.5) +\n    geom_smooth(method = lm, se = FALSE) + \n    labs(x = \"Age (in years)\", y = \"Recall Accuracy (%)\")\n\n\n\nFigure 2: Association between Recall Accuracy and Age\n\n\n\n\n\n\n\n Multiple Linear Regression Models - Visualisation\n\n\nWhen we have 2+ predictors, we can’t just plot our data an add geom_smooth(method=lm), because that would give a visualisation of a linear model with just one predictor (whichever one is on the x-axis).\nInstead, we can use the function plot_model() from sjPlot. Here’s an example with our recall accuracy model, with two predictors (recall confidence and age):\n\nplot_model(recall_mdl,\n           type = \"eff\",\n           terms = \"recall_confidence\",\n           show.data = TRUE)\n\nplot_model(recall_mdl,\n           type = \"eff\",\n           terms = \"age\",\n           show.data = TRUE)\n\n\n\nFigure 3: Association between Recall Accuracy, Recall Confidence and Age\n\n\n\n\n\nFigure 4: Association between Recall Accuracy, Recall Confidence and Age"
  },
  {
    "objectID": "1_b2_reading.html#numeric-outcomes-categorical-predictors",
    "href": "1_b2_reading.html#numeric-outcomes-categorical-predictors",
    "title": "Blocks 1 & 2 Flash Cards",
    "section": "Numeric Outcomes & Categorical Predictors",
    "text": "Numeric Outcomes & Categorical Predictors\n\n Categorical Predictors - Overview\n\n\nWe can include categorical predictors in a linear regression, but the interpretation of the coefficients is very specific. Whereas we talked about coefficients being interpreted as “the change in \\(y\\) associated with a 1-unit increase in \\(x\\)”, for categorical explanatory variables, coefficients can be considered to examine differences in group means.\n\n\n\n\n Binary Predictors\n\n\nBinary variables have two categories (more commonly referred to as levels), and these levels (e.g., Yes/No, Dog/Cat, Right/Left, Smoker/Non-Smoker) are simply entered in the model as a series of 0s and 1s. Numeric variables that represent categorical data are typically referred to as dummy variables.\nOur coefficients are just the same as before. The intercept is where our predictor equals zero, and the slope is the change in our outcome variable associated with a 1-unit change in our predictor.\nHowever, “zero” for this predictor variable now corresponds to a whole level. This is known as the “reference level”. Accordingly, the 1-unit change in our predictor (the move from “zero” to “one”) corresponds to the difference between the two levels.\nWhen used as predictors in multiple regression models, binary variables behave much the same way. The coefficient will give us the estimated change in \\(y\\) when moving from one level to the other, whilst holding other predictors constant (for more info, see the Multiple Linear Regression Models - Description & Specification flashcard, specifically the section on interpretation of multiple regression coefficients).\n\n\n\n\n Categorical Predictors with k levels\n\n\nWhen we have a categorical explanatory variable with more than 2 levels, our model gets a bit more complex - it needs not just one, but a number of dummy variables. For a categorical variable with \\(k\\) levels, we can express it in \\(k-1\\) dummy variables.\nFor example, the “species” column below has three levels, and can be expressed by the two variables “species_dog” and “species_parrot”:\n\n\n  species species_dog species_parrot\n1     cat           0              0\n2     cat           0              0\n3     dog           1              0\n4  parrot           0              1\n5     dog           1              0\n6     cat           0              0\n7     ...         ...            ...\n\n\n\nThe “cat” level is expressed whenever both the “species_dog” and “species_parrot” variables are 0.\nThe “dog” level is expressed whenever the “species_dog” variable is 1 and the “species_parrot” variable is 0.\nThe “parrot” level is expressed whenever the “species_dog” variable is 0 and the “species_parrot” variable is 1.\n\nR will do all of this re-expression for us. If we include in our model a categorical explanatory variable with 3 different levels, the model will estimate 2 parameters - one for each dummy variable. We can interpret the parameter estimates (the coefficients we obtain using coefficients(),coef() or summary()) as the estimated increase in the outcome variable associated with an increase of one in each dummy variable (holding all other variables equal).\n\n\n\n\n\n\n\n\n\n\n\n \nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n(Intercept)\n60.28\n1.209\n49.86\n5.273e-39\n\n\nspeciesdog\n-11.47\n1.71\n-6.708\n3.806e-08\n\n\nspeciesparrot\n-4.916\n1.71\n-2.875\n0.006319\n\n\n\n\n\nRecall that the intercept is the estimated outcome when all predictors are zero. In our example then, this represents the cat. We think of the “cat” category in this example as the reference level - it is the category against which other categories are compared against. Therefore, in the above example, an increase in 1 of “species_dog” is the difference between a “cat” and a “dog”. An increase in one of “species_parrot” is the difference between a “cat” and a “parrot”.\n\n\n\n\n\n\n\n\n\n\n\n\n Categorical Predictors - Interpretation\n\n\nLet’s apply this to a different example - the iris dataset, and specifically, the Species categorical variable:\n\n#check what levels we have\nlevels(iris$Species)\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n\nFrom the above, we can see that Species has 3 levels - “setosa”, “versicolor”, and “virginica”. If we put these into a model, assuming R’s default ordering, we know that “setosa” will be taken as our reference group:\n\n#fit model\nspec_model &lt;- lm(Sepal.Length ~ Species, data = iris)\nsummary(spec_model)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         5.0060     0.0728  68.762  &lt; 2e-16 ***\nSpeciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\nSpeciesvirginica    1.5820     0.1030  15.366  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s first map our coefficients and estimates:\n\n\n\n\n\n\n\nCoefficient\nEstimate\nCorresponds to\n\n\n\n(Intercept)\n5.0060\n\\(\\beta_0 = \\hat \\mu_1\\)\n\n\nSpeciesversicolor\n0.9300\n\\(\\beta_0 + \\beta_1 = \\hat \\mu_2\\)\n\n\nSpeciesvirginica\n1.5820\n\\(\\beta_0 + \\beta_2 = \\hat \\mu_3\\)\n\n\n\n\nThe estimate corresponding to (Intercept) contains \\(\\hat \\beta_0 = \\hat \\mu_1 = 5.01\\). The estimated average sepal length for the species setosa was approximately 5.01.\nThe second estimate corresponds to Speciesversicolor and was \\(\\hat \\beta_1 = 0.93\\). The difference in mean sepal length between setosa and versicolor species was estimated to be \\(0.93~cm\\). Thus, \\(\\hat \\mu_2 = 5.01 + 0.93 = 5.94\\). We could say - the species iris versicolor had a sepal length of approximately 5.94cm, and this was approximately 0.93cm longer than the iris setosa. This difference was statistically significant \\((p &lt; .001)\\).\nThe third estimate corresponds to Speciesvirginica and was \\(\\hat \\beta_2 = 1.58\\). The difference in mean sepal length between setosa and virginica species was estimated to be \\(1.58~cm\\). Thus, \\(\\hat \\mu_2 = 5.01 + 1.58 = 6.59\\). We could say - the species iris virginica had a sepal length of approximately 6.59cm, and this was approximately 1.58cm longer than the iris setosa. This difference was statistically significant \\((p &lt; .001)\\).\n\n\n\n\n\n Specifying Reference Levels\n\n\nWhen you have a categorical variable coded as a factor, R will default to using alphabetical ordering - hence in our example above “cat” was the reference level (as opposed to “dog” or “parrot”), and in the other “setosa” was the reference level (as opposed to “versicolor” or “virginica”).\nWe could override this by making it a factor with an ordering to it’s levels (see the use of factor() and levels()). Functions like fct_relevel() might be handy too.\nFor example, let’s say we wanted to change the reference level in our “spec_model” above, there are a few different ways that we can do this.\nFirst we should check the current ordering of the levels:\n\nlevels(iris$Species)\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n\nWe can then change the reference level to be versicolor (using one of the below methods):\n\n\nfct_relevel()\nrelevel()\nfactor()\n\n\n\n\niris &lt;- iris %&gt;% \n  mutate(\n    Species = fct_relevel(Species, \"versicolor\")\n  )\n\n\n\n\niris$Species &lt;- relevel(iris$Species, \"versicolor\")\n\n\n\n\niris$Species &lt;- iris$Species %&gt;% \n  factor(., levels = c(\"versicolor\", \"setosa\", \"virginica\"))\n\n\n\n\nAnd then re-run our model:\n\n#fit model\nspec_model2 &lt;- lm(Sepal.Length ~ Species, data = iris)\nsummary(spec_model2)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        5.9360     0.0728  81.536  &lt; 2e-16 ***\nSpeciessetosa     -0.9300     0.1030  -9.033 8.77e-16 ***\nSpeciesvirginica   0.6520     0.1030   6.333 2.77e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n Dummy vs Effects Coding\n\n\nPossible side-constraints on the parameters are:\n\n\n\n\n\n\n\n\nName\nConstraint\nMeaning of \\(\\beta_0\\)\n\nR\n\n\n\nSum to zero (Effects Coding)\n\\(\\beta_1 + \\beta_2 + \\beta_3 = 0\\)\n\\(\\beta_0 = \\mu\\)\ncontr.sum\n\n\nReference group (Dummy Coding)\n\\(\\beta_1 = 0\\)\n\\(\\beta_0 = \\mu_1\\)\ncontr.treatment\n\n\n\nDummy Coding\nBy default R uses the reference group constraint - i.e., dummy coding (sometimes called treatment contrast coding). If your factor has \\(g\\) levels, your regression model will have \\(g-1\\) dummy variables (R creates them for you, as we’ve seen in the examples above).\nWhen we use this approach, the intercept is the estimated \\(y\\) when all predictors (i.e., \\(x\\)’s) are zero. Because the reference level is kind of like “0” in our contrast matrix, this is part of the intercept estimate. We get out a coefficient for each subsequent level, which are the estimated differences from each level to the reference group.\nEffects Coding\nEffects coding (sometimes called sum-to-zero coding) is the next most commonly used in psychological research. These are a way of comparing each level to the overall (or grand) mean. This involves a bit of trickery that uses -1s and 1s rather than 0s and 1s, in order to make “0” be mid-way between all the levels - the average of the levels.\nWhen we use this approach, the intercept is the estimated average \\(y\\) when averaged across all levels of the predictor variable. In other words, it is the estimated grand mean of \\(y\\). The coefficients represent the estimated difference for that level from the overall grand mean.\n\nIn R\nIf we want to use effects coding, we can apply:\n\ncontrasts(iris$Species) &lt;- \"contr.sum\"\n\nWe can switch back to the default reference group constraint by applying either of these:\n\n# Option 1\ncontrasts(iris$Species) &lt;- NULL\n\n# Option 2\ncontrasts(iris$Species) &lt;- \"contr.treatment\""
  },
  {
    "objectID": "1_b2_reading.html#general",
    "href": "1_b2_reading.html#general",
    "title": "Blocks 1 & 2 Flash Cards",
    "section": "General",
    "text": "General\n\n Extracting Information\n\n\n\n\nModel Call\nResiduals\nModel Coefficients\n\\(\\sigma\\)\n\n\n\n\n\n\n\nMultiple regression output in R, model formula highlighted\n\n\n\nThe call section at the very top of the summary() output shows us the formula that was specified in R to fit the regression model.\nIn the above, we can see that recall accuracy is our DV, recall confidence and age were our two IVs, and our dataset was named recalldata.\n\n\n\n\n\n\nMultiple regression output in R, residuals highlighted\n\n\n\nResiduals are the difference between the observed values and model predicted values of the DV.\nIdeally, for the model to be unbiased, we want our median value to be around 0, as this would show that the errors are random fluctuations around the true line. When this is the case, we know that our model is doing a good job predicting values at the high and low ends of our dataset, and that our residuals were somewhat symmetrical.\n\n\n\n\n\n\nMultiple regression output in R, model coefficients highlighted\n\n\n\n\n\nEstimates\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\nOur model estimates help us to build our best fitting equation of the line that represents the association between our DV and our IV(s).\nIn the above example, we can build our equation for our model from this information:\n\\[\n\\text{Recall Accuracy}_i = \\beta_0 + \\beta_1 \\cdot \\text{Recall Confidence}_i + \\beta_2 \\cdot \\text{Age}_i + \\epsilon_i\n\\] \\[\n\\widehat{\\text{Recall Accuracy}} = 36.16 + 0.90 \\cdot \\text{Recall Confidence} - 0.34 \\cdot \\text{Age}\n\\]\n\nIn R\nThere are numerous equivalent ways to obtain the estimated regression coefficients — that is, \\(\\hat \\beta_0\\), \\(\\hat \\beta_1\\), …., \\(\\hat \\beta_k\\) — from the fitted model (for this below example, our fitted model has been named mdl):\n\nmdl\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\n\n\n\n\nThe standard error of the coefficient is an estimate of the standard deviation of the coefficient (i.e., how much uncertainty there is in our estimated coefficient).\n\nIn R\nIf you wanted to obtain just the standard error for each estimated regression coefficient, you could do the following (for this below example, our fitted model has been named mdl):\n\nsummary(mdl)$coefficients[,2]\n\n\nUsing the standard error, we can create confidence intervals to estimate a plausible range of values for the true population parameter. Recall the formula for obtaining a confidence interval for the population slope is:\n\\[\n\\hat \\beta_j \\pm t^* \\cdot SE(\\hat \\beta_j)\n\\] where \\(t^*\\) denotes the critical value chosen from t-distribution with \\(n-k-1\\) degrees of freedom (where \\(k\\) = number of predictors and \\(n\\) = sample size) for a desired \\(\\alpha\\) level of confidence.\n\nIn R\nWe can obtain the confidence intervals for the regression coefficients using the command confint()\n\n\n\nThe t-statistic is the \\(\\beta\\) coefficient divided by the standard error:\n\\[\nt = \\frac{\\hat \\beta_j - 0}{SE(\\hat \\beta_j)}\n\\]\nwhich follows a \\(t\\)-distribution with \\(n-k-1\\) degrees of freedom (where \\(k\\) = number of predictors and \\(n\\) = sample size).\nWith this, we can test the the null hypothesis \\(H_0: \\beta_j = 0\\).\nGenerally speaking, you want your model coefficients to have large \\(t\\)-statistics as this would indicate that the standard error was small in comparison to the coefficient. The larger our \\(t\\)-statistic, the more confident we can be that the coefficient is not 0.\n\nIn R\nIf you wanted to obtain just the \\(t\\)-values for each estimated regression coefficient, you could do the following (for this below example, our fitted model has been named mdl):\n\ncoef(summary(mdl))[, \"t value\"]\nsummary(mdl)$coefficients[,3]\n\n\n\n\nFrom our \\(t\\)-value, we can compute our \\(p\\)-value. The \\(p\\)-value help us to understand whether our coefficient(s) are statistically significant (i.e., that the coefficient is statistically different from 0). The \\(p\\)-value of each estimate indicates the probability of observing a \\(t\\)-value at least as extreme as, or more extreme than, the one calculated from the sample data when assuming the null hypothesis to be true.\nIn Psychology, a \\(p\\)-value &lt; .05 is usually used to make statements regarding statistical significance (it is important that you always state your \\(\\alpha\\) level to help your reader understand any statements regarding statistical significance).\nThe number of asterisks marks corresponds with the significance of the coefficient (see the ‘Signif. codes’ legend just under the coefficients section).\n\nIn R\nIf you wanted to obtain just the \\(p\\)-values for each estimated regression coefficient, you could do the following (for this below example, our fitted model has been named mdl):\n\nsummary(mdl)$coefficients[,4]\n\n\n\n\n\n\n\n\n\n\n\nMultiple regression output in R, model standard deviation of the errors highlighted\n\n\n\nThe standard deviation of the errors, denoted by \\(\\sigma\\), is an important quantity that our model estimates. It represents how much individual data points tend to deviate above and below the regression line - in other words, it tells us how well the model fits the data.\nA small \\(\\sigma\\) indicates that the points hug the line closely and we should expect fairly accurate predictions, while a large \\(\\sigma\\) suggests that, even if we estimate the line perfectly, we can expect individual values to deviate from it by substantial amounts.\nThe estimated standard deviation of the errors is denoted \\(\\hat \\sigma\\), and is estimated by essentially averaging squared residuals (giving the variance) and taking the square-root:\n\\[\n\\begin{align}\n& \\hat \\sigma = \\sqrt{\\frac{SS_{Residual}}{n - k - 1}} \\\\\n\\qquad \\\\\n& \\text{where} \\\\\n& SS_{Residual} = \\textrm{Sum of Squared Residuals} = \\sum_{i=1}^n{(\\epsilon_i)^2}\n\\end{align}\n\\]\n\nIn R\nThere are a couple of equivalent ways to obtain the estimated standard deviation of the errors — that is, \\(\\hat \\sigma\\) — from the fitted model (for this example, our fitted model has been named mdl):\n\nsigma(mdl)\nsummary(mdl)"
  },
  {
    "objectID": "1_b2_reading.html#manual-contrasts",
    "href": "1_b2_reading.html#manual-contrasts",
    "title": "Blocks 1 & 2 Flash Cards",
    "section": "Manual Contrasts",
    "text": "Manual Contrasts\nDummy and effects coding allow us to test the significance of the difference between means of groups and some other mean (either reference group or grand mean respectively). However, in some cases, we may want to test more specific hypotheses that require us to test the difference between particular combinations of groups. In such cases, we can use manual contrasts.\n\n Manual Contrasts - Rules\n\n\nThere are some rules that we need to follow when using manual contrasts:\n\n\nRule 1: Weights are -1 \\(\\geq\\) x \\(\\leq\\) 1\n\nRule 2: The group(s) in one chunk are given negative weights, the group(s) in the other get positive weights\n\nRule 3: The sum of the weights of the comparison must be 0\n\nRule 4: If a group is not involved in the comparison, weight is 0\n\nRule 5: For a given comparison, weights assigned to group(s) are equal to 1 divided by the number of groups in that chunk.\n\nRule 6: Restrict yourself to running \\(k\\) - 1 comparisons (where \\(k\\) = number of groups)\n\nRule 7: Each contrast can only compare 2 chunks of variance\n\nRule 8: Once a group singled out, it can not enter other contrasts\n\n\n\n\n\n Manual Contrasts - In R\n\n\nAfter specifying our hypotheses, to test our contrasts, we can use the emmeans package and follow the below structure:\n\n\nStep 1: Fit and run the model using `lm()\n\n\nStep 2: Use the emmeans() function to obtain the estimated means of each group. You can visualise these by using plot() on the obtained estimated means of the groups\n\nStep 3: Check the order of your levels via levels()\n\n\nStep 4: Define the contrast by specifying the weights following the rules outlined above (as well as paying attention to the ordering of the levels)\n\nStep 5: Test the pre-specified group contrast(s) via contrast()\n\n\nStep 6: Obtain confidence intervals via confint()\n\n\nAfter completing these steps, the last task would be to interpret the results of the contrast analysis in the context of the hypothesis.\n\n\n\n\n Manual Contrasts - Example\n\n\nLet’s apply this to an example. Suppose we wanted to want to address the following question:\n\nDoes the sepal length of an Iris grown in Western states (i.e., Iris setosa) differs from the sepal length of an Iris grown in Eastern states (i.e., iris versicolor and virginica).\n\nWe could specify our hypothesis as:\n\\[\n\\begin{aligned}\n    \\quad H_0 &: \\mu_\\text{Western} = \\mu_\\text{Eastern} \\\\\n    \\quad H_0 &: \\mu_\\text{Western} = \\frac{1}{2} (\\mu_\\text{Versicolor} + \\mu_\\text{Virginica}) \\\\\n\\\\\n    \\quad H_1 &: \\mu_\\text{Western} \\neq \\mu_\\text{Eastern} \\\\\n    \\quad H_1 &: \\mu_\\text{Western} \\neq \\frac{1}{2} (\\mu_\\text{Versicolor} + \\mu_\\text{Virginica}) \\\\\n\\\\\n\\end{aligned}\n\\]\nAnd then conduct our manual contrast analysis:\n\n# Step 1: Fit and run the model \nspec_model &lt;- lm(Sepal.Length ~ Species, data = iris)\n\n# Step 2: Use`emmeans()`& `plot()`\nseplength_mean &lt;- emmeans(spec_model, ~ Species)\nseplength_mean \n\n Species    emmean     SE  df lower.CL upper.CL\n setosa       5.01 0.0728 147     4.86     5.15\n versicolor   5.94 0.0728 147     5.79     6.08\n virginica    6.59 0.0728 147     6.44     6.73\n\nConfidence level used: 0.95 \n\nplot(seplength_mean)\n\n\n\n\n\n\n# Step 3: Check levels order via `levels()`\nlevels(iris$Species)\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n# Step 4: Define contrast & weights - want to compare Iris setosa to iris versicolor and iris virginica\nseplength_comp &lt;- list(\"Western State Iris - Eastern State Iris\" = c(-1, 1/2, 1/2))\n\n# Step 5: run contrast analysis\nseplength_comp_test &lt;- contrast(seplength_mean, method = seplength_comp)\nseplength_comp_test\n\n contrast                                estimate     SE  df t.ratio p.value\n Western State Iris - Eastern State Iris     1.26 0.0892 147  14.086  &lt;.0001\n\n# Step 6: confidence intervals\nconfint(seplength_comp_test)\n\n contrast                                estimate     SE  df lower.CL upper.CL\n Western State Iris - Eastern State Iris     1.26 0.0892 147     1.08     1.43\n\nConfidence level used: 0.95 \n\n\nAnd write up our findings in the context of the hypothesis / research question:\nWe performed a test against \\(H_0: \\mu_1 - \\frac{1}{2}(\\mu_2 + \\mu_3) = 0\\). At the 5% significance level, there was evidence that iris sepal length was significantly different between Western and Eastern states in the US \\((t(147) = 14.09, p &lt; .001, \\text{two-sided})\\), and this difference was estimated to be 1.26cm. We are 95% confident that an Iris grown in an Eastern state, on average, would be between 1.08cm and 1.43cm longer than those grown in a Western state \\((CI_{95}[1.08, 1.43])\\)."
  },
  {
    "objectID": "1_b2_reading.html#latex-symbols-equations",
    "href": "1_b2_reading.html#latex-symbols-equations",
    "title": "Blocks 1 & 2 Flash Cards",
    "section": "LaTeX Symbols & Equations",
    "text": "LaTeX Symbols & Equations\nBy embedding LaTeX into RMarkdown, you can accurately and precisely format mathematical expressions, ensuring that they are not only technically correct but also visually appealing and easy to interpret.\n\n LaTeX Guide\n\n\nFor an overview of how to integrate LaTeX symbols and equations, review Lesson 9 of the RMD bootcamp."
  },
  {
    "objectID": "1_b2_reading.html#apa-formatting",
    "href": "1_b2_reading.html#apa-formatting",
    "title": "Blocks 1 & 2 Flash Cards",
    "section": "APA Formatting",
    "text": "APA Formatting\nAPA format is a writing/presentation style that is often used in psychology to ensure consistency in communication. APA formatting applies to all aspects of writing - from formatting of papers (including tables and figures), citation of sources, and reference lists. This means that it also applies to how you present results in your Psychology courses, including DAPR2.\n\n APA Formatting Guides\n\n\nAll results should be presented following APA guidelines.\nYou also need to follow APA style rules for tables and figures.\nMake sure to familiarise yourself with the above guides, and practice presenting your results following these rules."
  },
  {
    "objectID": "1_b2_reading.html#tables",
    "href": "1_b2_reading.html#tables",
    "title": "Blocks 1 & 2 Flash Cards",
    "section": "Tables",
    "text": "Tables\nWe want to ensure that we are presenting results in a well formatted table. To do so, there are lots of different packages available (see Lesson 4 of the RMD bootcamp).\nOne of the most convenient ways to present results from regression models is to use the tab_model() function from sjPlot\n\n Creating tables via tab_model\n\n\nWithin tab_model(), there are lots of different ways that you can customise your table. The most common arguments that you should use are dv.labels, pred.labels, and title.\nYou can rename your DV and IV labels by specifying dv.labels and pred.labels. To do so, specify your variable name on the left, and what you would like this to be named in the table on the right. For title, you can simply specify in ““’s what you want your title to be e.g., title = \"This is my title\".\nHere’s an example if I had fitted a model with the following information:\n\nModel name = mdl_test\n\nModel DV = cognitive_score\n\nModel IVs = SES and age\n\n\n\nmdl_test &lt;- lm(cognitive_score ~ SES + age, data = data_name)\n\nI want to change the names of SES and age to be socio-economic status and age - in years respectively. What we need to pay attention to here is the ordering of the IVs - the ordering in our lm() must match that in tab_model(). I also want to name my table Regression Table for Cognitive Scores Model. Here is how we would do this in R:\n\nlibrary(sjPlot)\ntab_model(mdl_test,\n          pred.labels = c('Intercept', 'socio-economic status', 'age - in years'),\n          title = \"Regression Table for Cognitive Scores Model\")\n\nSee here for another short example."
  },
  {
    "objectID": "1_b2_reading.html#cross-referencing",
    "href": "1_b2_reading.html#cross-referencing",
    "title": "Blocks 1 & 2 Flash Cards",
    "section": "Cross Referencing",
    "text": "Cross Referencing\nCross-referencing is a very helpful way to direct your reader through your document, and the good news is that this can be done automatically in RMarkdown.\n\n Cross Referencing\n\n\nThere are three key components to allow you to successfully cross-reference within your RMarkdown document:\n\nA bookdown output format\nA caption to your figure or table\nA named/labeled code chunk\n\nOnce you have the above, you will be able to cross-reference using the syntax @ref(type:label), where label is the chunk name/label, and type is the environment being referenced (e.g. tab for table, fig for figure, etc.).\nFor an in-depth overview and example of how to cross-reference, see Lesson 7 of the RMD bootcamp."
  },
  {
    "objectID": "1_b2_reading.html#footnotes",
    "href": "1_b2_reading.html#footnotes",
    "title": "Blocks 1 & 2 Flash Cards",
    "section": "Footnotes",
    "text": "Footnotes\n\nYes, the error term is gone. This is because the line of best-fit gives you the prediction of the average recall accuracy for a given age, and not the individual recall accuracy of an individual person, which will almost surely be different from the prediction of the line.↩︎\nQQplots plot the values against the associated percentiles of the normal distribution. So if we had ten values, it would order them lowest to highest, then plot them on the y against the 10th, 20th, 30th.. and so on percentiles of the standard normal distribution (mean 0, SD 1)↩︎\nBelsley, D. A., Kuh, E., & Welsch, R. E. (2005). Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley & Sons. DOI: 10.1002/0471725153↩︎\nThis method finds an appropriate value for \\(\\lambda\\) such that the transformation \\((sign(x) |x|^{\\lambda}-1)/\\lambda\\) results in a close to normal distribution.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr Umberto Noe\nDr Josiah King\nDr Emma Waterston\nDepartment of Psychology, The University of Edinburgh"
  },
  {
    "objectID": "about.html#the-team",
    "href": "about.html#the-team",
    "title": "About",
    "section": "",
    "text": "Dr Umberto Noe\nDr Josiah King\nDr Emma Waterston\nDepartment of Psychology, The University of Edinburgh"
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterprtation interprtation interprtation\n\n\nQuestion\n\n\nquestion\nwhat is your name?\nwhat is your favourite colour?\n\n\n\n\n Solution \n\n\nsolution\nhello\n\n2+2\n\n[1] 4\n\n\n\n\n\n\n Optional hello my optional friend\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 2 (DAPR2) lab workbook. Using the menu above, you can find lab materials for each week."
  },
  {
    "objectID": "index.html#help-support-feedback",
    "href": "index.html#help-support-feedback",
    "title": "Home",
    "section": "Help, Support & Feedback",
    "text": "Help, Support & Feedback\n\nWithin Lab Workbook\n\nHints, Notes, and Example Write-Up / Interpretation\nHints are shown in a green box, with the title ‘Hint’. If you are unsure what to do, check the collapsible hint provided (note that these are only present for some questions).\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is an example of a hint.\n\n\n\nNotes are displayed in blue boxes, with the title ‘Note’. These are occasionally used to draw your attention to a specific point.\n\n\n\n\n\n\nNote\n\n\n\nThis is an example of a note.\n\n\nExample write-ups and/or interpretations are shown with a red right border. These are helpful to check your interpretation against, and a useful guide to use in future when interpreting and/or writing up results.\n\n\n\n\n\n\nThis is an example write-up / interpretation block.\n\n\n\n\n\nSolutions\nSolutions are made available immediately below each exercise. To view solutions, click the drop down ‘Solution’ button.\nImportant  Before checking the solution you should attempt the question. You should also avoid copying and pasting code from the solutions. If you do check solutions right away and simply copy the answer, you will struggle to learn from the exercises.\nInstead, you should:\n\nTry to figure out the answer yourself or with your peers (and/or ask for help from a staff member if needed)\nType the code out yourself (and annotate your R code chunks so you know what your code is doing & why - future you will thank you for this).\n\n\n\n\nAsking Questions\n\nDuring labs, if you have a question, please ask one of the tutors for support.\nOutside of labs, we encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#tips-on-googling-statistics-and-r",
    "href": "index.html#tips-on-googling-statistics-and-r",
    "title": "Home",
    "section": "Tips on Googling Statistics and R",
    "text": "Tips on Googling Statistics and R\nSearching online for help with statistics and R can be both a help and a hindrance. If you have an error message in R, copy the error message into Google. The results returned can sometimes just cause more confusion, but sometimes something might jump out at you and help you solve the problem. The same applies with searching the internet for help with statistics - search for “what is a p-value”, and you’ll find many many different articles and forum discussions etc. Some of them you will find too technical, but don’t be scared - the vast majority of people work in statistics will find these too technical too. Some of them you might feel are too simple/not helpful. As a general guide, keep clicking around the search responses, and you may end up finding that someone, somewhere, has provided an explanation at the right level. If you find something during your search which you don’t quite understand, feel free to link it in a post on the discussion forum!"
  },
  {
    "objectID": "index.html#feedback-on-labs",
    "href": "index.html#feedback-on-labs",
    "title": "Home",
    "section": "Feedback on Labs",
    "text": "Feedback on Labs\nIf you wish to make suggestions for improvements to these workbooks (or if you spot any typos!), please email ppls.psych.stats@ed.ac.uk making sure to include the course name in the subject."
  }
]