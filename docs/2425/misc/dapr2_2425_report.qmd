---
title: "DAPR2 Assessed Group-Report 2024/25"
format: 
  html:
    toc: true
    number_sections: false
    toc-location: left
editor_options: 
  chunk_output_type: console
---

```{css echo=FALSE}
div.blue, div.red, div.green, div.yellow, div.frame{ 
    border-radius: 5px; 
    padding: 20px 20px 10px 20px; 
    margin-top: 20px; 
    margin-bottom: 20px; 
}
.blue { background-color:#d9edf7 !important; }
.green { background-color:#dff0d8 !important; }
.yellow { background-color:#fcf8e3 !important; }
.red { background-color:#F3E3E5 !important; 
.frame {border: 1px solid #333333 !important; }
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval=FALSE, warning=FALSE, message=FALSE)
```

---

# General Info

### Key Dates

**Coursework set**: 13th February 2025 (12 noon)    
**Coursework due**: 6th March 2025 (12 noon)    
  
# Group Work Policy

__Please note that:__ 

+ Each group should submit a __single report__ (two documents: see [Files to Submit](#Files-to-Submit))
+ As this is a group-based report, __no extensions are permitted__
+ All group members are expected and encouraged to contribute to the report, but __you should not work with other groups__. Similarity checks will be performed, and further investigations will be carried out if assignments from different groups are similar.

# Use of AI

Academic integrity is an underlying principle of research and academic practice. All submitted work is expected to be your own. AI tools (e.g., ELM) should not be used for assessments on DAPR2. Using AI would constitute [academic misconduct](https://registryservices.ed.ac.uk/academic-services/students/conduct/academic-misconduct).  

# Instructions  {#instructions}

Your task is to describe and analyse the data provided in order to answer a set of research question(s). Analyses will draw on the methodologies we have discussed in lectures and labs. The specific study contexts and research questions can be found below in the [data & research questions section](#task). 

## Structure & Rubric {#rubric}

Your report should **include three sections** - analysis strategy, results, and discussion.   
  
Your report is **marked based on four criteria** - the analysis strategy, results, discussion, and writing and formatting. The associated grade percentage shown in ()'s below. 

### 1. Analysis Strategy (40%)
In this section you should describe how you are going to address the research aim(s) of the study described. Note, you may not need as many analyses as there appear to be questions as some may use different pieces of information from the same analysis. **The marking of this section will be based on the completeness of your descriptions of**:

- data cleaning and variable recoding  
- any descriptive statistics or visualizations you will use prior to running models  
- the analyses you undertook (description of the models)  
- how you will check your model (assumptions and diagnostics including the criteria you have used to evaluate each)  
- what specific information from each model provides the answer to the questions including details of your $\alpha$ levels and any required corrections  
- rationales for all choices

*Important:*   
  
- You do not need to include an introduction to the study unless you feel it is helpful in writing your analysis strategy  
- Your analysis strategy should not contain any results

### 2. Results (40%)
The results section should follow logically from your analysis strategy and present the results of all aspects of your approach. A typical structure would begin by presenting descriptive statistics and move on to inferential tests. **The marking of this section will be based on the completeness of your descriptions of**:

- All key model results should be presented (tables very useful) in the main body of the report
- You should provide full interpretation of key results
- Model assumption and diagnostic checks should be noted (where you can refer the reader to both decision rules from your analysis strategy, and to figures included in the appendix)

### 3. Discussion (10%)
The Discussion section should be very brief (1-2 sentence). **The marking of this section will be based on the completeness of your descriptions of**:

- Concise, coherent and accurate summary statements linking the formal results to each of the research questions. 

*Important:*  
  
- This should not introduce any new statistical results, nor repeat detailed statistical results, but should refer to information presented in the other two sections of the report  

### Assumption and Diagnostics Appendix (Optional)
This optional section has no page limit. You may use this to present assumption and diagnostic plots.

Please note:

- You must still describe your assumption tests in your strategy, including how you will evaluate them
- You must still summarise the results in the results section of the main report
- You must refer accurately to the figures and tables labels presented in the appendix

For example:   
  
- good use of the appendix:  
_"The model met the assumptions of linear regression with residuals showing a constant mean of approximately zero across the fitted values (see Appendix X Fig X), and ..."_
  
- not so good use of the appendix:  
_"The model met assumptions (see Appendix X Fig X)."_

*Important:*  
  
+ The assumption appendix is only for assumption and diagnostic figures and results. Any results from your main models including in the appendix will not be marked  
  
### Writing & Formatting (10%)
The focus of this report is on your ability to create reproducible results, implementing analyses to answer research questions and interpreting the results. However, we do require that the reports are neatly formatted and written clearly. This is especially important since the compiled report will not contain visible `R` code, and so a large part of the challenge comes in clearly describing all aspects of the analysis procedure. A reader of your compiled report should be able to more or less replicate your analyses **without** referring to your R code. Below are some pointers:  

+ Ensure writing is clear and well structured, use appropriate language avoiding unnecessary jargon or overly complex explanations  
+ Figures and tables should be numbered and captioned, and referred to in the text (including those contained within the Appendix); important statistical outcomes should be summarised in the text.
+ Reporting should be clear and consistent. If in doubt, follow [APA 7th Edition guidelines](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf). 
+ Your report should be a **maximum of 4 sides of A4** when the default formatting and font settings within RStudio are used when knitting your file. Please note, that __no content beyond the 4th page will be read or marked (with the exception of the Appendix).__  
+ __Do not__ include a cover page - this will be included in the page limit   
+ __Code chunks should be hidden__ in the pdf produced by your rmd file. To tell RMarkdown to not show your code when knitting to HTML, add `echo=FALSE` next to the `r` that [appears after the backticks](https://uoepsy.github.io//rmd-bootcamp/05-echoeval.html). There should be no visible R code or direct output in your report. 

*Important:*    
  
- The above also applies to your Appendix  

# Report Formatting  
  
We expect pretty much all groups to do their analyses using RMarkdown (**.Rmd**; this is the type of file you've been working with in DAPR1 and DAPR2).  However, you are welcome to write the report in a separate word-processor in order to format the report easily without having to remember the nuances of formatting in RMarkdown. This has the additional benefit of being able to use a collaborative word-processor such as Google Docs.  

Unfortunately there are no straightforward ways to live edit **.Rmd**/**.R** documents collaboratively; we suggest that each group ensures it knows who holds the "master copy" of the script that will be submitted.

You should then export your word-processor document to **.pdf** format for submission. The code should remain as a file with a **.Rmd** or **.R** suffix.  

We don't mind which of these approaches each group takes: **The important thing to remember is that the data analysis (cleaning, models, and plots) that are presented in the report should match those produced by your code.**  

:::yellow 

For a guide on writing and formatting in RMarkdown, you may find these resources helpful:

- [UoEPsy Rmd-Bootcamp](https://uoepsy.github.io/scs/rmd-bootcamp){target="_blank"}  
- [RMarkdown CheatSheet](https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf){target="_blank"}  
- [Writing Math in Rmd](https://rpruim.github.io/s341/S19/from-class/MathinRmd.html#:~:text=Math%20inside%20RMarkdown,10n%3D1n2.){target="_blank"}
:::

# Files to Submit {#Files-to-Submit}

## File Types

You are required to submit __two files__:

1. a complete report knitted to PDF (i.e., in **.pdf** format)
2. and the associated **.Rmd**/**.R** file that reproduces your report

:::yellow
__Knitting .Rmd to PDF__  

Please note that to knit .Rmd directly to pdf, you should:

1. Make sure the tinytex package is installed.  
2. Makes sure the 'yaml' (bit at the very top of your document) looks something like this. in `title`, replace _(GROUP NAME)_ with your group name (e.g., Asymptotic Antelopes), and in `author`, specify the exam number of each individual within the group (for example: B000001, B000002, B000003, ...):  
```{}
---
title: "DAPR2 Group Report (GROUP NAME)"
author: "B000001, B000002, B000003, ..."
date: "06/03/2025"
output: bookdown::pdf_document2
---
```

__If you cannot knit directly to pdf, then try the following steps:__   
  1) Knit to .html file  <br>
  2) Open your html in a web-browser (e.g. Chrome, Firefox)  <br>
  3) Print to pdf (Ctrl+P, then choose to save to pdf)  <br>
  4) Submit the pdf you just saved.  <br>

:::

## File Names  

For both files which you submit, the filename should be your group name with the appropriate extension, and nothing else. 
  
For example, the group **Asymptotic Antelopes** would submit two files:  
  
  - one of **AsymptoticAntelopes.Rmd** / **AsymptoticAntelopes.R**
  - **AsymptoticAntelopes.pdf** 

::: {.callout-note collapse=true}

#### Working Individually?

For anyone who has obtained permission to complete the task individually, in the 'yaml' section of your .Rmd, name the `title` 'DAPR2 Report', and in `author`, specify your exam number.

Please name each file with your exam number (the letter "B" followed by a six digit number - which can be found on your student card: [See here for more information](https://buddy.vet.ed.ac.uk/help/how-to-find-your-exam-number/)). For example, if your exam number was B123456 you would submit:

- one of **B123456.Rmd** / **B123456.R**  
- **B123456.pdf**

You should also write your exam number in the "Submission Title" box prior to submission.

:::

# Submitting Files

## Who Submits & Where   

:::red

**ONLY ONE PERSON FROM EACH GROUP NEEDS TO SUBMIT**

We suggest that you do this together in-person/on a call, so that all group members are able to confirm that they are happy to submit. 

Please submit both files online via the Turnitin links on the LEARN page for DAPR2 (Go to the Report (Group Based) folder within the Assessments section on the Learn page). There will be two links, clearly labelled, as the files need to be submitted individually. 

For **each file** you should complete the "Submit File" popup by entering the exam numbers of _all of your group members_ in the "Submission Title" box (see below).

![](submissionbox.png){width="50%" fig-align="center"}

:::

# Grading

## Report Grading

As mentioned above (see [Structure & Rubric](#rubric)), compiled reports will be assessed according to the following components, with the following weightings:  

- Analysis Strategy = 40%
- Results = 40%
- Discussion = 10%
- Writing and Formatting = 10%

The overall mark will be rounded to the nearest value on the [Psychology 20-point marking scale](https://uoe.sharepoint.com/sites/hss/ppls/PPLS-Undergraduate-Student-Hub-home/SitePages/Assessment-%26-Marking--Psychology.aspx).  

__We are primarily marking each group's *report*, and *not* your code__  
Grades and feedback are provided for the finished reports, with marks awarded for providing evidence of the ability to:  

  - understand and execute appropriate statistical methods to answer each of the questions
  - provide clear explanation of the methods undertaken
  - provide clear and accurate presentation and interpretation of results and conclusions.  

__Why we still want your code__  
We still require your code so that we can assess the reproducibility of your work. The code provided must successfully conduct the analysis described in your report, and return the exact results, figures and tables that are detailed in your report. If it does not, then the [code panalties outlined below](#Code-Pen) will apply. 

## Peer-Adjusted Marking 

Once the group project has been submitted, every member of the group will complete the peer-assessment, in which you will be asked to award "mark adjustments" to yourself and to each of the other members of your group. This will be done through Learn; details will be made available over the next couple of weeks. Each submitted mark should be understood as follows: Relative to the group as a whole, how much did each member contribute? If someone made an average contribution, you should award that person the middle score. If they contributed substantially more, you may choose to give a higher mark; if they genuinely contributed less, you might choose a lower mark. Marks for each group member are scaled then averaged together, and then used as “weights” to adjust the overall project mark. You can see an example of how this logic works by visiting <https://uoe-psy.shinyapps.io/peer_adj/> where there is a "live demo" of a peer-adjustment system.

If you don’t contribute any peer-adjustment marks, the relative weight of the group members who do complete the peer-assessment will be higher. 

**The assessed group reports are marked on the [Psychology 20-point marking scale](https://uoe.sharepoint.com/sites/hss/ppls/PPLS-Undergraduate-Student-Hub-home/SitePages/Assessment-%26-Marking--Psychology.aspx), and peer-assessment ratings adjust grades by up to 20% of the report mark**. 

# Penalties

## Code {#Code-Pen}

We will apply penalties for code errors or non-reproducibility.  

Prior to submitting, check the following:  

+ Does the code run line-by-line without throwing any errors? 
+ Does the code provided lead to the __exact same results__ that you have reported?
+ Does the file knit successfully? 

If any of the lines in your report generate an error, or the code doesn't match the reported results, your grade will be penalised by deducting 10 points from your final grade.

In other words, a 62 becomes 52, and a 42 becomes 32.

If your submission files are not either .Rmd & PDF, you might lose all points and get a grade of zero.

If you have any technical issues with the Turnitin submission, please contact the Teaching Office, and cc the PPLS Psych Stats email address.

## Late

Submissions are considered late until __both__ files (in .Rmd & PDF format) are submitted on Turnitin.   

# Questions

We are here to help and to clarify anything we can, however we will not answer direct questions such as "Is this [part of my coursework] correct?" 

What we would like you to do is think about why you are asking the question. If it is because you are unsure about a section of the material, look back over it, come and discuss the examples from class, and then apply that to the coursework.  

Please ask questions on the discussion forum (Piazza) so that all students may benefit from the answer (please also check that your question has not already been posted!)

<div style="height: 100px"></div>
---

# Data & Research Questions {#task}

> Conduct and report on an analysis that addresses the research aims.  
The data is available at: <https://uoepsy.github.io/data/head_face_dominance.csv>   


### Study Background and Aims 

The data used for the current report are simulated, drawing on recent work on head position and social judgements. The simulated data acted to expand upon the methods and results reported in the following paper:

Witkower, Z., & Tracy, J. L. (2019). A facial-action imposter: How head tilt influences perceptions of dominance from a neutral face. *Psychological Science, 30*(6), 893–906. [https://doi.org/10.1177/0956797619838762](https://doi.org/10.1177/0956797619838762)

*Note you are not expected to provide an introduction, you do not have to read this article.*

### Method & Procedure

A total of 600 participants (aged 18-60) took part in a study examining the role of head tilt on the perception of dominance.  
  
After entering the lab, all participants completed two questionnaires. The first questionnaire assessed generalised anxiety symptoms, measured by the self-report Generalised Anxiety Disorder Assessment (GAD-7). The second questionnaire assessed positive and negative affect (i.e., mood/emotion), measured by the self-report Positive and Negative Affect Schedule (PANAS).  

Next, all participants were randomly allocated to view one of six target images. In all six target images, the emotion displayed was neutral, with the eyes directed straight ahead. The target images varied in their head tilt angle and in which part of the face was occluded. The target image had the face portrayed with either a 10° upward head tilt, a 10° downward head tilt, or no (i.e., 0°) head tilt, where either the upper or lower face was occluded. Participants were asked to rate the dominance of the presented target image.    


### Research Questions

After accounting for anxiety, positive and negative affect, the researchers want to know whether:  

a.	differences in dominance ratings between face occlusion conditions depend on the head tilt condition.  
b.	the difference in dominance ratings between face occlusion conditions differed between those that saw tilted and non-tilted heads.  

```{r eval=F,echo=FALSE}
library(tidyverse)

# eseed=round(runif(1,1e3,1e5))
set.seed(29330)
N = 600
age = round(runif(N,18,60))

# think of these as latent "propensities for anxiety, positive and negative affect"
# based on distributions i can find, gad and panas_neg should be skewed right. 
# panas_pos fairly centered.  
# it feels sensible to make gad and panas_neg positively correlated, 
# and they are both negatively correlated with panas_pos
pGPN = MASS::mvrnorm(N, mu=c(-1.5,-.5,0), 
              Sigma = matrix(c(
                1,.4,-.3,
                .4,1,-.5,
                -.3,-.5,1
              ), nrow=3))

# observed scores (sneaky way of using binomial to sim likert like data)
anxiety = sapply(pGPN[,1], \(x) sum(replicate(7, rbinom(1,3,plogis(x)))))
negative_affect= sapply(pGPN[,2], \(x) sum(replicate(10, rbinom(1,4,plogis(x))+1)))
positive_affect = sapply(pGPN[,3], \(x) sum(replicate(10, rbinom(1,4,plogis(x))+1)))

# distributions:  
par(mfrow=c(3,1))
hist(anxiety);hist(positive_affect);hist(negative_affect)
pairs(cbind(anxiety,positive_affect,negative_affect))

# experimental grid
expgrid = tidyr::expand_grid(
  tilt_condition = c("up","down","none"),
  occlusion_condition = c("upper","lower"),
  n = 1:100
) 

# data 
dat = data.frame(
  anxiety,negative_affect,positive_affect,
  expgrid[,-3]
)
dat$tilt_condition = factor(dat$tilt_condition, levels=c("none","down","up"))

# model matrix for outcome
Xmat = model.matrix(rnorm(N) ~ scale(anxiety) + scale(negative_affect) + scale(positive_affect) + tilt_condition * occlusion_condition, 
                    data = dat)

# we need coefficients for population model, in this order:
dimnames(Xmat)[[2]]

# linear predictor: 
lp = Xmat %*% c(0, .1, .1, 0, 
           .4, .4, 0,
           -.2, .1)
# outcome, then scaled somehow
dat$dominance = rnorm(N, lp, 1)

# dominanceinance is based on https://www.sciencedirect.com/science/article/pii/S1090513810000267#app1
# which is apparently 8 x 7-point likerts, so range 8-56
dat$dominance = round(28.4 + scale(dat$dominance)[,1]*6.2)
range(dat$dominance)

# make sure data is randominancely shuffled 
# (otherwise students will panick with DWT of autocorrelation)
dat = dat[sample(1:N), ]
row.names(dat) <- NULL
dat$PID = paste0("PPT_",1:N)
head(dat)

dat$age <- age

dat <- dat %>% relocate(PID, .before=anxiety)
dat <- dat %>% relocate(age, .before=anxiety)

# test:
lm(dominance ~ anxiety + negative_affect + positive_affect + tilt_condition * occlusion_condition, 
   data = dat) |>
  summary()
```


```{r eval=F,echo=FALSE}
# write_csv(dat, "head_face_dominance.csv")
```

```{r eval=F,echo=FALSE}
###### basic solution ##########
# library(tidyverse)
# library(emmeans)
# library(psych)
# library(kableExtra)
# library(interactions)
# library(sjPlot)
# 
# str(dat)
# 
# ###make conditions factors
# dat <- dat %>%
#   mutate(
#     tilt_condition = factor(tilt_condition),
#     occlusion_condition = factor(occlusion_condition)
#     )
# 
# ##check levels - set reference groups & justify
# levels(dat$tilt_condition)
# levels(dat$occlusion_condition)
# 
# dat$tilt_condition <- relevel(dat$tilt_condition, "none")
# 
# ##check for missing data
# table(is.na(dat))
# 
# ##descriptives
# desc <- dat %>%
#   group_by(tilt_condition, occlusion_condition) %>%
#   summarise(n = n(),
#             M = mean(dominance),
#             SD = sd(dominance)) %>%
#   kable(., caption = "Descriptives", digits = 2) %>%
#   kable_styling()
# desc
# 
# ##basic plot
# p1 <- ggplot(data = dat, aes(x = tilt_condition, y = dominance, color = occlusion_condition)) +
#   geom_boxplot() +
#   labs(x = 'Tilt Condition', y = 'Dominance Rating') +
#     theme_classic()
# p1
# 
# ##run model
# m1 <-lm(dominance ~ anxiety + negative_affect + positive_affect + tilt_condition * occlusion_condition, data = dat)
# summary(m1)
# 
# ##check assumptions
# par(mfrow=c(2,2))
# plot(m1)
# par(mfrow=c(1,1))
# 
# ##table results
# library(sjPlot)
# tab_model(m1,
#           show.stat = TRUE,
#           dv.labels = "Dominance Ratings",
#           title = "Regression Results")
# 
# 
# ##plot model
# library(interactions)
# plt_m1 <- cat_plot(model = m1,
#                    pred = occlusion_condition,
#                    modx = tilt_condition)
# plt_m1
# 
# 
# emm <- emmeans(m1,~tilt_condition * occlusion_condition)
# emm
# pairs(emm)
# 
# ## whether the difference in dominance ratings between face occlusion conditions differed between those that saw tilted and non-tilted heads
# occlusion_coef  <- c('lower' = 1, 'upper' = -1)
# tilt_coef  <- c('down' = .5, 'up' = .5, 'none' = -1)
# contr = tilt_coef %o% occlusion_coef
# contr
# 
# comp <- contrast(emm, method = list('Hyp' = c(0.5, -1, 0.5, -0.5, 1, -0.5)))
# summary(comp, infer=T)
```

### Data Dictionary

```{r echo=FALSE, eval=TRUE}
library(tidyverse)
tibble(
  variable = names(read_csv("head_face_dominance.csv")),
  description = c("Participant ID.", "Participant age (in years).", "Score on the GAD-7 scale (7 items scored on a 0-3 Likert scale) measuring generalised anxiety. Scores were calculated as the sum of the 7 items. Higher scores represented higher levels of anxiety.", "Score on the negative subscale (10 items scored on a 1-5 Likert scale) of the PANAS Questionnaire. Negative affect scores were calculated as the sum of the 10 items within the subscale. Higher scores represented higher levels of negative affect.", "Score on the positive subscale (10 items scored on a 1-5 Likert scale) of the PANAS Questionnaire. Positive affect scores were calculated as the sum of the 10 items within the subscale. Higher scores represented higher levels of positive affect.", "Head tilt condition - upwards = 10° upwards tilt; downwards = 10° downwards tilt; neutral = 0° no tilt.", "Face occlusion condition - lower = lower occluded, only upper section of the face visible; upper = upper occluded, only lower section of the face visible.", "Dominance rating (scored on a 8-56 scale). Higher scores represented higher levels of perceived dominance.")
    ) %>% knitr::kable() %>%
    kableExtra::kable_styling(bootstrap_options = "striped")
```


