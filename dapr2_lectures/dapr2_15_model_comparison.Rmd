---
title: "<b> Case Diagnostics 2 </b>"
subtitle: "Data Analysis for Psychology in R 2<br><br> "
author: "dapR2 Team"
institute: "Department of Psychology<br>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  #base_color = "#0F4C81", # DAPR1
   base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(car)
library(patchwork)
library(kableExtra)

salary2 <- read_csv("./salary2.csv")
```

# Week's Learning Objectives


---
# Topics for today

+ Statistical tools for selection/comparison
  + Incremental $F$-test
  + Nested vs. non-nested models
  + AIC
  + BIC


---
# Example 1: Categorical Variables & Interactions
+ The data comes from a study into patient care in a paediatric wards. 

+ A researcher was interested in whether the subjective well-being of patients differed dependent on the post-operation treatment schedule they were given, and the hospital in which they were staying. 

+ **Condition 1**: `Treatment` (Levels: TreatA, TreatB, TreatC).
  
+ **Condition 2**: `Hosp` (Levels: Hosp1, Hosp2). 
  
+ Total sample n = 180 (30 patients in each of 6 groups).
  + Between person design. 

+ **Outcome**: Subjective well-being (SWB)
  + An average of multiple raters (the patient, a member of their family, and a friend). 
  + SWB score ranged from 0 to 20.
  
---
# The data
```{r}
hosp_tbl <- read_csv("hospital.csv", col_types = "dff")
hosp_tbl %>%
  slice(1:10)
```

---
# The questions

---
# Example 2: Controlling for covariates
+ Our example for today uses data from the Midlife In United States (MIDUS2) study.

+ Outcome: self-rated health

+ Covariates: Age, sex

+ Predictors: Big Five traits and Purpose in Life.


---
# The data
```{r, warning=FALSE, message=FALSE}
midus <- read_csv("MIDUS2.csv")
midus2 <- midus %>%
  select(1:4, 31:42) %>%
  mutate(
    PIL = rowMeans(.[grep("PIL", names(.))],na.rm=T)
  ) %>%
  select(1:4, 12:17) %>%
  drop_na(.)
slice(midus2, 1:3)

```


---
# The questions

---
class: center, middle
# Time for a break

**Grab a cup of tea/coffee....a few equations on the way.**

---
class: center, middle
# Welcome Back!

---
# Incremental F-test
+ Recall, the F-ratio for a single model tests the statistical significance of a linear model

+ The incremental F-test evaluates the statistical significance of the improvement in variance explained in an outcome with the addition of further predictor(s)

+ It is based on the difference in F-values between two models.
  + Note: The F-test as we have already seen it could also be viewed in this way.
  + As the difference between a model with predictors and an "empty model" (intercept only)

+ We call the model with the additional predictor(s) model 1 or full model

+ We call the model without model 0 or restricted model

---
# Incremental F-test

.pull-left[
$$\Delta F_{(\Delta k, N- k_1 - 1)} = \frac{(N-k_1 - 1)\Delta R^2}{\Delta k(1-R_1^2)}$$

$$
\begin{align}
& \text{Where:} \\
& k_1 = \text{no. predictors from model 1} \\
& \Delta R^2 = \text{difference in R2 between model 0 and 1} \\
& \Delta k = \text{difference in no. predictors model 0 and 1} \\
& R_1^2 = \text{R2 value from model 1} \\
\end{align}
$$


]


.pull-right[
$$F_{(df_R-df_F),df_F} = \frac{(SSR_R-SSR_F)/(df_R-df_F)}{SSR_F / df_F}$$



$$
\begin{align}
& \text{Where:} \\
& SSR_R = \text{residual sums of squares for the restricted model} \\
& SSR_F = \text{residual sums of squares for the full model} \\
& df_R = \text{residual degrees of freedom from the restricted model} \\
& df_F = \text{residual degrees of freedom from the full model} \\
\end{align}
$$
]


---
# Models
```{r}
m0 <- lm(health ~ age + sex, data = midus2)
m0res <- summary(m0)

m1 <- lm(health ~ age + sex + O + C + E + A + N, data = midus2)
m1res <- summary(m1)
```


---
# Incremental F-test: calculation
$$\Delta F_{(\Delta k, N- k_1 - 1)} = \frac{(N-k_1 - 1)\Delta R^2}{\Delta k(1-R_1^2)}$$

+ $N$ = `r length(midus2$ID)`
+ $k_0$ = 2
+ $k_1$ = 7
+ $\Delta k$ = 5
+ $R_{m0}^2$ = `r m0res$r.squared`
+ $R_{m1}^2$ = `r m1res$r.squared`
+ $\Delta R^2$ = `r m1res$r.squared - m0res$r.squared`


---
# Incremental F-test: calculation
$$\Delta F_{(\Delta k, N- k_1 - 1)} = \frac{(N-k_1 - 1)\Delta R^2}{\Delta k(1-R_1^2)}$$

+ Plug in our values

$$\Delta F_{(5, 1753)} = \frac{(1761-7 - 1)0.1438}{5(1-0.1484)}$$

+ Work them through:

$$\Delta F_{(5, 1753)} = \frac{252.0814}{4.2595}$$

+ And there we go (we have rounded the numbers above, so this will not match perfectly)

$$\Delta F_{(5, 1753)} = 59.18098$$

---
# Incremental F-test: Significance
+ Having calculated $\Delta F$, we then compare it against an F-distribution with $\Delta k$ and $(N-k_1 - 1)$ degrees of freedom. 

+ This provides a p-value for the change in variance explained by model 1 versus model 0 for a given $\alpha$

+ In our example:

```{r}
pf(.95, 5, 1753)
```



---
# In R

```{r}
anova(m0, m1)
```


---
# Nested vs non-nested models
+ The F-ratio depends on the models being compared being nested

+ Nested means that the predictors in one model are a subset of the predictors in the other

+ We also require the models to be computed on the same data


---
# Nested vs non-nested models

.pull-left[
**Nested**

```{r, eval=FALSE}

m0 <- lm(outcome ~ x1 + x2 , data = data)

m1 <- lm(outcome ~ x1 + x2 + x3, data = data)

```

+ These models are nested.

+ `x1` and `x2` appear in both models
]


.pull-right[
**Non-nested**

```{r, eval=FALSE}

m0 <- lm(outcome ~ x1 + x2 + x4, data = data)

m1 <- lm(outcome ~ x1 + x2 + x3, data = data)

```

+ These models are non-nested

+ There are unique variables in both models
  + `x4` in `m0`
  + `x3` in `m1`

]


---
class: center, middle
# Time for a break

**Time to have a look at a few model specifications and decide if they are nested.**

---
class: center, middle
# Welcome Back!

**What happens when models are non-nested?**


---
# AIC
$$AIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + 2k$$

$$
\begin{align}
& \text{Where:} \\
& SS_{residual} = \text{sum of squares residuals} \\
& n = \text{sample size} \\
& k = \text{number of explanatory variables} \\
& \text{ln} = \text{natural log function} 
\end{align}
$$


+ Unlike the incremental F-test AIC does not require two models to be nested

+ Smaller (more negative) values of AIC indicate better fitting models.
  + So we compare values and choose the model with the smaller AIC


---
# AIC parsimony correction

$$AIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + 2k$$


+ Main point to note is that the term $2k$ applies a penalty for having more predictors

+ When you add more predictors, fit will improve ( $SSE$ will get smaller)

+ The decrease is partially offset by the $+2k$

+ This makes AIC a parsimony-corrected statistic

+ Parsimony-corrected statistics help us avoid over-fitting

---
# In R

```{r}
AIC(m0, m1)
```

---
# BIC
$$BIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + k\,\text{ln}(n)$$
$$
\begin{align}
& \text{Where:} \\
& SS_{residual} = \text{sum of squares residuals} \\
& n = \text{sample size} \\
& k = \text{number of explanatory variables} \\
& \text{ln} = \text{natural log function} 
\end{align}
$$


+ Like AIC... 
  + BIC doesn’t require nested models
  + Smaller (more negative) BIC values mean better models
  + We can compare the BICs for two models and choose the one with the smaller BIC as the better model


---
# In R

```{r}
BIC(m0, m1)
```

---
# Parsimony corrections

$$AIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + 2k$$

$$BIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + k\,\text{ln}(n)$$

+ BIC has a ‘harsher’ parsimony penalty for typical sample sizes when applying linear models than AIC
  
+ When $\text{ln}(n) > 2$ BIC will have a more severe parsimony penalty (i.e. essentially all the time!)


---
# Considerations for use of AIC and BIC
+ The AIC and BIC for a model are not meaningful on their own 
  + They only make sense for model comparisons

+ For AIC, there are no cut-offs to suggest how big a difference in two models is needed to conclude that one is substantively better than the other

+ For BIC, a difference of 10 can be used as a rule of thumb to suggest that one model is substantively better than another


---
# Summary of today


---
class: center, middle
# Thanks for listening!
