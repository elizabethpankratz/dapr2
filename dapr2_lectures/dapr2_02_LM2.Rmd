---
title: "<b> Linear Model 2 </b>"
subtitle: "Data Analysis for Psychology in R 2<br><br> "
author: "dapR2 Team"
institute: "Department of Psychology<br>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  #base_color = "#0F4C81", # DAPR1
   base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(patchwork)
library(kableExtra)
library(MASS)

knitr::opts_chunk$set(fig.retina = 1.5,
                      out.width = '90%')

theme_set(theme_gray(base_size = 15))
```


# Week's Learning Objectives
1. Understand the key principles of least squares. 

2. Be able to interpret the coefficients from a simple linear model.

3. Understand how these interpretations change when we add more predictors


---
# Principle of least squares

+ The numbers $\beta_0$ and $\beta_1$ are typically **unknown** and need to be estimated in order to fit a line through the point cloud.

+ We denote the "best" values as $\hat \beta_0$ and $\hat \beta_1$

+ The best fitting line is found using **least squares**
    + Minimizes the distances between the actual values of $y$ and the model-predicted values of $\hat y$
    + Specifically minimizes the sum of the *squared* deviations

---
# Principle of least squares

+ Actual value = $y_i$

+ Model-predicted value = $\hat y_i = \hat \beta_0 + \hat \beta_1 x_i$

+ Deviation or residual = $y_i - \hat y_i$

+ Minimize the **residual sum of squares**, $SS_{Residual}$, which is

$$SS_{Residual} = \sum_{i=1}^{n} [y_i - (\hat \beta_0 + \hat \beta_1 x_{i})]^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$

---
# Principle of least squares

+ **Why do you think we square the deviations? **

    + HINT: Look back to the "What is $\epsilon_i$?" slide
    
--

+ We have positive and negative residual terms

+ If we simply added them, they would cancel out.

---
# Data, predicted values and residuals

+ Data = $y_i$
    + This is what we have measured in our study. 
    + For us, the test scores.

+ Predicted value = $\hat{y}_i = \hat \beta_0 + \hat \beta_1 x_i$ = the y-value on the line at specific values of $x$
    + Or, the value of the outcome our model predicts given someone's values for predictors.
    + In our example, given you study for 4 hrs, what test score does our model predict you will get.

+ Residual = Difference between $y_i$ and $\hat{y}_i$. So;

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

???
+ these are important distinctions for understanding linear models
+ return to them a lot.

---
# Fitting the line
+ Calculations for slope:

$$\hat \beta_1 = \frac{SP_{xy}}{SS_x}$$


+ $SP_{xy}$ = sum of cross-products:


$$SP_{xy} = \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})$$


+ $SS_x$ = sums of squared deviations of $x$:


$$SS_x = \sum_{i=1}^{n}(x_i - \bar{x})^2$$


<!-- --- -->
<!-- # Equivalent formula -->

<!-- $$\hat \beta_1 =  -->
<!-- \frac{SP_{xy}}{SS_x} =  -->
<!-- r \frac{s_y}{s_x}$$ -->

<!-- where -->

<!-- - $r = \frac{SP_{xy}}{\sqrt{SS_x \times SS_y}}$ -->

<!-- - $s_y = \sqrt{ \frac{SS_y}{n - 1} } = \sqrt{ \frac{\sum_{i=1}^{n}(y_i - \bar{y})^2}{n - 1} }$ -->

<!-- - $s_x = \sqrt{ \frac{SS_x}{n - 1} } = \sqrt{ \frac{\sum_{i=1}^{n}(x_i - \bar{x})^2}{n - 1} }$ -->


---
# Fitting the line
+ Calculations for intercept:

$$\hat \beta_0 = \bar{y} - \hat \beta_1 \bar{x}$$

+ $\hat \beta_1$ = slope estimate

+ $\bar{y}$ = mean of $y$

+ $\bar{x}$ = mean of $x$


---
class: center, middle
# Time for a break

This would be a good time to take a look at the lecture 3 worked example

Here we show these calculations for our example.

---
class: center, middle
# Welcome Back!

**Where we left off... **

Calculated the intercept and slope

Now let's think about error...

---
# What is $\sigma$?

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(patchwork)

n = 300
df1 <- tibble(
  x = runif(n, 1, 6),
  y = 3 + 2 * x + rnorm(n, 0, 0.4)
)

p1 <- ggplot(df1, aes(x, y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  ylim(4, 20)


n = 300
df2 <- tibble(
  x = runif(n, 1, 6),
  y = 3 + 2 * x + rnorm(n, 0, 2.5)
)

p2 <- ggplot(df2, aes(x, y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  ylim(4, 20)
```

.pull-left[
<center>**Small $\sigma$**</center>
```{r echo = FALSE, message = FALSE, warning = FALSE, out.width = '90%', fig.align='center'}
p1
```
]

.pull-right[
<center>**Large $\sigma$**</center>
```{r echo = FALSE, message = FALSE, warning = FALSE, out.width = '90%', fig.align='center'}
p2
```
]

---
# What is $\sigma$?

+ The less scatter around the line, the smaller the standard deviation of the errors

+ The less scatter around the line, the stronger the relationship between $y$ and $x$. 

--

+ We estimate $\sigma$ using the residuals

+ The estimated standard deviation of the errors is:
$$\hat \sigma = \sqrt{\frac{SS_{Residual}}{n - k - 1}} = \sqrt{\frac{\sum_{i=1}^n(y_i - \hat y_i)^2}{n - k - 1}}$$

+ In simple linear regression we only have one $x$, so $k = 1$ and the denominator becomes $n - 2$.


---
# Recap linear model
+ The linear model for a single predictor is written as:

$$y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i$$

+ $\beta_0$ = **Intercept**: the point where the line cross $y$, and $x$ = 0

+ $\beta_1$ = **Slope**: the gradient of the line, or rate of change

+ In our worked example we saw how to calculate the intercept and slope from the raw data.


---
# `lm` in R
+ We do not generally calculate by hand like this, so we also briefly introduced the `lm()` function.

```{r, echo=FALSE}
test <- tibble(
  student = paste(rep("ID",10),1:10, sep=""),
  hours = seq(0.5,5,.5),
  score = c(1,3,1,2,2,6,3,3,4,8)
)
```

```{r}
lm(score ~ hours, data = test)
```

---
# Interpretation

+ **Slope is the number of units by which Y increases, on average, for a unit increase in X.**

--
    + Unit of Y = 1 point on the test
    + Unit of X = 1 hour of study
    
--

+ So, for every hour of study, test score increases on average by 1.055 points.

--

+ **Intercept is the expected value of Y when X is 0.**

--

    + X = 0 is a student who does not study.

--

+ So, a student who does no study would be expected to score 0.40 on the test.

???
+ So we know in a general sense what the intercept and slope are, but what do they mean with respect to our data and question?

---
# Note of caution on intercepts
+ In our example, 0 has a meaning.
    + It is a student who has studied for 0 hours.
    
+ But it is not always the case that 0 is meaningful.

+ Suppose our predictor variable was not hours of study, but age.

+ **Look back at the interpretation of the intercept, and instead of hours of study, insert age. Read this aloud a couple of times.**

--

+ This is the first instance of a very general lesson about interpreting statistical tests. 
    + The interpretation is always in the context of the constructs and how we have measured them.


---
class: center, middle
# Time for a break

---
#  Multiple regression 
+ The aim of a linear model is to explain variance in an outcome

+ In simple linear models, we have a single predictor, but the model can accommodate (in principle) any number of predictors. 

+ However, when we include multiple predictors, those predictors are likely to correlate

+ Thus, a linear model with multiple predictors finds the optimal prediction of the outcome from several predictors, **taking into account their redundancy with one another**


---
#  Uses of multiple regression 
+ **For prediction:** multiple predictors may lead to improved prediction. 

+ **For theory testing:** often our theories suggest that multiple variables together contribute to variation in an outcome

+ **For covariate control:** we might want to assess the effect of a specific predictor, controlling for the influence of others.
	+ E.g., effects of personality on health after removing the effects of age and sex


---
#  Extending the regression model 

+ Our model for a single predictor:

$$y_i = \beta_0 + \beta_1 x_{1i} + \epsilon_i$$ 

+ is extended to include additional $x$'s:

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \epsilon_i$$  

+ For each $x$, we have an additional $b$
  + $\beta_1$ is the coefficient for the 1st predictor
  + $\beta_2$ for the second etc.


---
#  Interpreting coefficients in multiple regression 

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_j x_{ji} + \epsilon_i$$

+ Given that we have additional variables, our interpretation of the regression coefficients changes a little

+ $\beta_0$ = the predicted value for $y$ **all** $x$ are 0.
	
+ Each $\beta_j$ is now a **partial regression coefficient**
	+ It captures the change in $y$ for a one unit change in , $x$ **when all other x's are held constant**

+ What does holding constant mean? 
  + Refers to finding the effect of the predictor when the values of the other predictors are fixed
		+ It may also be expressed as the effect of **controlling for**, or **partialling out**, or **residualizing for** the other $x$'s

+ With multiple predictors `lm` isolates the effects and estimates the unique contributions of predictors. 

---
#  Visualizing models

.pull-left[

```{r, warning=FALSE, echo=FALSE, message=FALSE}
df <- read_csv("./salary2.csv")
m1 <- lm(salary ~ perf, data = df)
m2 <- lm(salary ~ perf + serv, data = df)

ggplot(df, aes(x=perf, y=salary)) +
  geom_point() +
  geom_smooth(method = "lm")+
  xlab("") +
  ylab("")

```

]

.pull-right[

```{r, echo=FALSE}
knitr::include_graphics("./lm_surface.png")
```

]

???
+ In simple linear models, we could visualise the model as a straight line in 2D space
	+ Least squares finds the coefficients that produces the *regression line* that minimises the vertical distances of the observed y-values from the line

+ In a regression with  2 predictors, this becomes a regression plane in 3D space
	+ The goal now becomes finding the set of coefficients that minimises the vertical distances between the *regression*  *plane* and the observed y-values

+ The logic extends to any number of predictors
	+ (but becomes very difficult to visualise!)

---
#  Example: lm with 2 predictors 

+ Imagine we were interested in examining predictors of school performance.  

+ we get a teacher rating of child's performance, a self-report measure of self control, and also measure teacher rated class interaction. 

+ We collect data on a sample of n=650 12 year old and fit a linear model.

+ We'll fit the model to $z$-scores for all variables. 
  + Remember $z$-scores have a mean of 0, and a SD of 1
  + So "1 unit" of a $z$-score is 1 SD


---
#  `lm` code

.pull-left[
```{r, echo=FALSE, message=FALSE, warning=FALSE}
data <- read_csv("./aggression.csv")
data <- data %>%
  transmute(
    z_perf = scale(aggression),
    z_SC = scale(self_control),
    z_interaction = scale(resting_HR)
    )
#write_csv(data, "./school.csv")
```
]

```{r}
perf <- lm(z_perf ~ z_SC + z_interaction, #<<
          data = data)
```


+ Multiple predictors are separated by `+`



---
#  Multiple regression coefficients 

```{r}
summary(perf)
```


---
#  Multiple regression coefficients 

```{r}
res <- summary(perf)
res$coefficients
```

+ **Controlling for class interaction, for every SD unit increase in self-control, there is a `r round(res$coefficients[[2,1]],2)` SD unit increase in academic performance**


---
#  Multiple regression coefficients 

```{r}
res <- summary(perf)
res$coefficients
```

+ **Controlling for self-control, for every SD unit increase in rating of class interaction, there is a `r round(res$coefficients[[3,1]],2)` SD unit decrease in academic performance**



---
# Summary of today



---
class: center, middle
# Thanks for listening!
      