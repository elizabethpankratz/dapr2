<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title> Assumptions &amp; Diagnostics </title>
    <meta charset="utf-8" />
    <meta name="author" content="dapR2 Team" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <b> Assumptions &amp; Diagnostics </b>
## Data Analysis for Psychology in R 2<br><br>
### dapR2 Team
### Department of Psychology<br>The University of Edinburgh

---









# Week's Learning Objectives
1. Be able to state the assumptions underlying a linear model.

2. Understand how to test linear model assumptions.


---
# Topics for today
+ What are the assumptions of linear model and how can we assess them?
	+ Linearity
	+ Independence of errors
	+ Normality of errors
	+ Equal variance (Homoscedasticity)


---
# Linear model assumptions 
+ So far, we have discussed evaluating linear models with respect to:
	+ Overall model fit ( `\(F\)` -ratio, `\(R^2\)`)
	+ Individual predictors

+ However, the linear model is also built on a set of assumptions.

+ If these assumptions are violated, the model will not be very accurate.

+ Thus, we also need to assess the extent to which these assumptions are met.


---
# Some data for today

.pull-left[
+ Let's look again at our data predicting salary from years or service and performance ratings (no interaction).

`$$y_i = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \epsilon_i$$`

+ `\(y\)` = Salary (unit = thousands of pounds ).

+ `\(x_1\)` = Years of service.

+ `\(x_2\)` = Average performance ratings.
 
]

.pull-right[

&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; id &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; salary &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; serv &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; perf &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID101 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80.18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID102 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 123.98 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID103 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80.55 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID104 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 84.35 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID105 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 83.76 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID106 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 117.61 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID107 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 96.38 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID108 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 96.49 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID109 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 88.23 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID110 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 143.69 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


]

---
# Our model

```r
m1 &lt;- lm(salary ~ perf + serv, data = salary2)
```

+ We will run all our assumptions based on the object `m1`

---
# Visualizations vs tests
+ There exist a variety of ways to assess assumptions, which broadly split into statistical tests and visualizations.

+ We will focus on visualization:
	+ Easier to see the nature and magnitude of the assumption violation
	+ There is also a very useful function for producing them all.

+ Statistical tests often suggest assumptions are violated when problem is small.
  + This is to do with the statistical power of the tests.
  + Give no information on what the actual problem is.
  + A summary table of tests will be given at the end of the lecture.


---
# Visualizations made easy
+ For a majority of assumption and diagnostic plots, we will make use of the `plot()` function.
  + If we give `plot()` a linear model object (e.g. `m1` or `m2`), we can automatically generate assumption plots.

+ We will also make use of some individual functions for specific visualizations.

+ Alternatively, we can also use `check_model()` from the `performance` package.
  + This provides `ggplot` figures as well as some notes to aid interpretation.
  + Caution that these plots are **not in a format to use directly in reports**

---
#  Linearity 
+ **Assumption**: The relationship between `\(y\)` and `\(x\)` is linear.
  + Assuming a linear relation when the true relation is non-linear can result in under-estimating that relation


+ **Investigated with**:
  + Scatterplots with loess lines (single variables)
  + Component-residual plots (when we have multiple predictors)


---
# Linear vs non-linear

.pull-left[

&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-4-1.png" width="90%" /&gt;

]

.pull-right[

&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-5-1.png" width="90%" /&gt;

]

---
#  What is a loess line?

+ Method for helping visualize the shape of relationships:

+ Stands for...
  + **LO**cally
  + **E**stimated
  + **S**catterplot
  + **S**moothing

+ Essentially produces a line with follows the data.

+ Useful for single predictors.

---
# Visualization

.pull-left[

```r
lin_m1 &lt;- salary2 %&gt;%
  ggplot(., aes(x=serv, y=perf)) +
  geom_point()+
  geom_smooth(method = "lm", se=F) + # &lt;&lt;
* geom_smooth(method = "loess", se=F,
              col = "red") +
  labs(x= "Years of Service", y="Performance", 
       title = "Scatterplot with linear (blue) 
       and loess (red) lines")
```
]

.pull-right[
&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-7-1.png" width="90%" /&gt;

]

---
#  Non-linearity

+ With multiple predictors, we need to know whether the relations are linear between each predictor and outcome, controlling for the other predictors

+ This can be done using **component-residual plots**
  + Also known as partial-residual plots
		
+ Component-residual plots have the `\(x\)` values on the X-axis and partial residuals on the Y-axis

+ *Partial residuals* for each X variable are:

`$$\epsilon_i + B_jX_{ij}$$`

+ Where :
	+ `\(\epsilon_i\)` is the residual from the linear model including all the predictors
	+ `\(B_jX_{ij}\)` is the partial (linear) relation between `\(x_j\)` and `\(y\)`

---
#  `crPlots()` 

+ Component-residual plots can be obtained using the `crPlots()` function from `car` package


```r
m1 &lt;- lm(salary ~ perf + serv, data = salary2)
crPlots(m1)
```

+ The plots for continuous predictors show a linear (dashed) and loess (solid) line

+ The loess line should follow the linear line closely, with deviations suggesting non-linearity

---
#  `crPlots()`
&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-9-1.png" width="90%" /&gt;


???
+ Here the relations look pretty good.

+ Deviations of the line are minor

---
# Normally distributed errors 
+ **Assumption**: The errors ( `\(\epsilon_i\)` ) are normally distributed around each predicted value.

+ **Investigated with**:
  + QQ-plots
  +	Histograms

	
---
# Visualizations 
+ **Histograms**: Plot the frequency distribution of the residuals.


```r
hist(m1$residuals)
```

--

+ **Q-Q Plots**: Quantile comparison plots.
	+ Plot the standardized residuals from the model against their theoretically expected values.
	+ If the residuals are normally distributed, the points should fall neatly on the diagonal of the plot.
	+ Non-normally distributed residuals cause deviations of points from the diagonal.
		+ The specific shape of these deviations are characteristic of the distribution of the residuals.


```r
*plot(m1, which = 2)
```


---
# Visualizations

.pull-left[

&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-12-1.png" width="90%" /&gt;

]


.pull-right[

&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-13-1.png" width="90%" /&gt;

]


---
#  Equal variance (Homoscedasticity) 

+ **Assumption**: The equal variances assumption is constant across values of the predictors `\(x_1\)`, ... `\(x_k\)`, and across values of the fitted values `\(\hat{y}\)`
	+ Heteroscedasticity refers to when this assumption is violated (non-constant variance)

+ **Investigated with**:
  + Plot residual values against the predicted values ( `\(\hat{y}\)` ).

---
#  Residual-vs-predicted values plot 
+ In R, we can plot the residuals vs predicted values using `residualPlot()` function in the `car` package.
  + Categorical predictors should show a similar spread of residual values across their levels
  + The plots for continuous predictors should look like a random array of dots
	  + The solid line should follow the dashed line closely


```r
residualPlot(m1)
```

+ We can also get this plot using:


```r
plot(m1, which = 1)
```

---
#  Residual-vs-predicted values plot 

.pull-left[
&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-16-1.png" width="90%" /&gt;

]

.pull-right[
&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-17-1.png" width="90%" /&gt;
]



---
#  Independence of errors 
+ **Assumption**: The errors are not correlated with one another

+ Difficult to test unless we know the potential source of correlation between cases.
  + We will see more of this in year 3.

+ Essentially, for now, we will evaluate this based on study design.
  + If a design is between person, we will assume the errors to be independent.


---
#  Multi-collinearity 
+ This is **not an assumption of linear model**, but it is something we need to consider. 
  + It sits between assumptions and case diagnostics.
  
+ Multi-collinearity refers to the correlation between predictors
  + We saw this in the formula for the standard error of model slopes for an `lm` with multiple predictors.

+ When there are large correlations between predictors, the standard errors are increased
	+ Therefore, we don't want our predictors to be too correlated

---
#  Variance Inflation Factor 
+ The **Variance Inflation Factor** or VIF quantifies the extent to which standard errors are increased by predictor inter-correlations

+ It can be obtained in R using the `vif()` function:


```r
vif(m1)
```

```
##     perf     serv 
## 1.001337 1.001337
```

+ The function gives a VIF value for each predictor

+ Ideally, we want values to be close to 1

+ VIFs&gt; 10 indicate a problem

---
#  What to do about multi-collinearity 

+ In practice, multi-collinearity is not often a major problem

+ When issues arise, consider:
	+ Combining highly correlated predictors into a single composite
		  + E.g. create a sum or average of the two predictors
	+ Dropping an IV that is obviously statistically and conceptually redundant with another from the model

---
class: center, middle
# Time for a break

**And a quiz...identify the plot and the assumption**


---
class: center, middle
# Violated Assumptions
What do we do about non-normality of residuals, heteroscedasticity and non-linearity? 


---
# Fixing violations

1. Model misspecification (predictors): add predictors
2. If the outcome is not continuous, use generalized linear model (more later in course)
3. Transformations
4. Bootstrapped inference

---
# Model misspecification
+ Sometimes assumptions appear violated because our model is not correct.

+ Typically we have:
  + Failed to include an interaction
  + Failed to include a non-linear (higher order) effect
  
+ Usually detected by observing violations of linearity or normality of residuals.

+ Solved by including the terms in our linear model.

---
# Non-linear transformations 
+ Another approach is a non-linear transformation of the outcome and/or predictors.
  + Often related to non-normal residuals, heteroscedasticity and non-linearity. 

+ This involves applying a function (see first week) to the values of a variable. 
  + This changes the values and overall shape of the distribution

+ For non-normal residuals and heteroscedasticity, skewed outcomes can be transformed to normality

+ Non-linearity may be helped by a transformation of both predictors and outcomes

---
#  Transforming variables to normality 
+ Positively skewed data can be made more normally distributed using a log-transformation.

+ Negatively skewed data can be made more normally distributed using same procedure but first reflecting the variable (make biggest values the smallest and smallest the biggest) and then applying the log-transform

+ What does skew look like?


---
# Visualizing Skew

.pull-left[


&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-19-1.png" width="90%" /&gt;

]

.pull-right[
&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-20-1.png" width="90%" /&gt;

]

---
#  Log-transformations 
+ Log-transformations can be implemented in R using the `log()` function.

+ If your variable contains zero or negative values, you need to first add a constant to make all your values positive
	+ A good strategy is to add a constant so that your minimum value is one
	+ E.g., if your minimum value is -1.5, add 2.5 to all your values


---
# Log-transformation in action


```r
df_skew &lt;- df_skew %&gt;%
  mutate(
*   log_pos = log(pos),
*   neg_ref = ((-1)*neg) + (max(neg)+1),
*   log_neg = log(neg_ref)
  )
```


---
# Log-transformation in action

.pull-left[
&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-22-1.png" width="90%" /&gt;

]

.pull-right[
&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-23-1.png" width="90%" /&gt;

]


---
# Log-transformation in action

.pull-left[
&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-24-1.png" width="90%" /&gt;

]

.pull-right[
&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-25-1.png" width="90%" /&gt;

]


---
# Generalised linear model
+ All the models we have been discussing are suitable for continuous outcome variables.

+ Sometimes our outcomes are not continuous or normally distributed not because of an error in measurement, but because they would not be expected to be.
  + E.g. Reaction time, counts, binary variables.

+ For such data, we need a slightly different version of a linear model.
  + More on this to come later in the course.
  
  
---
# Bootstrapped inference
+ One of the concerns when we have violated assumptions is that we make poor inferences.

+ This is because with violated assumptions, the building blocks of our inferences may be unreliable.

+ Bootstrapping as a tool can help us here.
  + We will cover this in detail later in the course.

---
# Summary of assumptions
+ **Linearity**: The relationship between `\(y\)` and `\(x\)` is linear.
  + Assuming a linear relation when the true relation is non-linear can result in under-estimating that relation

+ **Normally distributed errors**: The errors ( `\(\epsilon_i\)` ) are normally distributed around each predicted value.

+ **Homoscedasticity**: The equal variances assumption is constant across values of the predictors `\(x_1\)`, ... `\(x_k\)`, and across values of the fitted values `\(\hat{y}\)`

+ **Independence of errors**: The errors are not correlated with one another


---
# Summary of today

+ Looked at the third set of model evaluations, assumptions.

+ Described and considered how to assess:
  + Linearity
	+ Independence of errors
	+ Normality of errors
	+ Equal variance (Homoscedasticity)

+ Key take home point:
  + There are no hard and fast rules for assessing assumptions
  + It takes practice to consider if violations are a problem

---
class: center, middle
# Thanks for listening!

# Week's Learning Objectives

1. Understand the difference between outliers and influential points. 

2. Test and assess the effect of influential cases on LM coefficients and overall model evaluations.


---
# Topics for today

+ Linear model outliers

+ Leverage

+ Influence

+ Dealing with problematic cases

---
#  Linear model diagnostics

+ In previous lectures we have discussed evaluating linear models:
	+ Evaluating model fit ( `\(R^2\)` and `\(F\)`-ratio )
	+ Evaluating individual predictors
	+ Evaluating assumptions

+ Another important aspect of evaluating linear models pertains to the cases (individuals):
	+ Does the model fit poorly for some individuals?
	+ Do some individuals have a lot of influence on the results?

+ Linear model diagnostics allow us to explore individual cases in the context of a model

---
#  Three important features

+ Model outliers
	+ Cases for which there is a large discrepancy between their predicted value ( `\(\hat{y_i}\)` ) and their observed value ( `\(y_i\)` )

--

+ High leverage cases
	+ Cases with an unusual value of the predictor ( `\(x_i\)` )

--

+ High influence cases
	+ Cases who are having a large impact on the estimation of model

---
#  Model outliers

+ Linear model outliers are cases that have unusual outcome values given their predictor values
	+ They will show a large difference between the predicted ( `\(\hat{y_i}\)` ) and the observed value ( `\(y_i\)` )

+ **What quantity have we calculated that would summarise this difference?**

--

+ Outliers will demonstrate large **residuals** ( `\(\epsilon_i\)` )
  + If you got stuck here, look back at the introduction to linear models

---
#  Why are we interested in linear model outliers?

.pull-left[
+ They can (but do not necessarily) have a strong influence on the model
	+ We'll get to this.

+ It may be informative to follow-up and investigate outliers
	+ Is it a data entry error?
	+ Does the case somehow not belong with the rest of the data? (E.g., a male in a sample of females)
]

.pull-right[
&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-26-1.png" width="90%" /&gt;

]


---
#  How do we determine if a case is an outlier?

+ We judge linear model outlying-ness of a case on the basis of the size of its residual

+ Unstandardised residuals are:

`$$y_i - \hat{y_i}$$`

+ They are in the same units as the DV

+ Fine for comparison across cases within the same linear model model

+ Difficult to compare across models where the DVs will have different units

---
#  Standardised residuals

+ **Standardised residuals**
	+ Divide the unstandardised residuals by an estimate of their standard deviation
	+ Converts the residuals to z-score units
	+ However, their calculation includes the potential outlier

+ **Studentised residuals**
	+ Provide a version of the standardised residual excluding the case
	+ Values **&gt;+2 or &lt;-2** indicate potential outlyingness

---
#  Identifying studentised residuals &gt;2 

.pull-left[

```r
m1 &lt;- lm(salary ~ serv + perf, data = salary2)

salary2 %&gt;%
  mutate(
*   resid = rstudent(m1)
  ) %&gt;%
* dplyr::filter(resid &gt; 2 | resid &lt; -2) %&gt;%
  kable(.)  %&gt;%
  kable_styling(., full_width = F)
```
]


.pull-right[

+ Steps:
  + Extract the studentized residuals from our model.
  + Identify those outside `\(\pm &gt; 2\)`
]


---
#  Identifying studentised residuals &gt;2 

.pull-left[

```r
m1 &lt;- lm(salary ~ serv + perf, data = salary2)

salary2 %&gt;%
  mutate(
*   resid = rstudent(m1)
  ) %&gt;%
* dplyr::filter(resid &gt; 2 | resid &lt; -2) %&gt;%
  kable(.)  %&gt;%
  kable_styling(., full_width = F)
```
]


.pull-right[

&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; id &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; salary &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; serv &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; perf &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; resid &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID114 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 132.12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.090732 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID125 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 51.78 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -2.324626 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID127 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 144.33 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.028044 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID133 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 41.60 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -2.238126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID159 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 129.42 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.939648 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID161 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 148.05 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.194388 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID173 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 17.86 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -2.880247 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]



---
#  Leverage 
.pull-left[
+ High leverage cases are those with an unusual predictor value or combination of predictor values
	+ In simple linear model: an `\(x\)` value far from the `\(\bar{x}\)` 

+ **Why are we interested in leverage?** 
+ High leverage cases have potential to influence `\(\hat \beta_0\)` and `\(\hat \beta_1\)` 
]

.pull-right[
&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-30-1.png" width="90%" /&gt;
]

---
#  Finding a case with high leverage

+ **Hat values** ( `\(h_i\)` ) are used to assess leverage in linear model

+ For a simple linear model, the hat value for case `\(i\)` would be:

`$$h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^n(x_i - \bar{x})^2}$$`

+ Where:
  + `\(n\)` is the sample size
  + `\((x_i - \bar{x})^2\)` is the squared deviation of the predictor value for that case, `\(x_i\)`, from the mean `\(\bar x\)`
  + `\(\sum_i^n(x_i - \bar{x})^2\)` is the sum of all these squared deviations, for all cases



---
#  Finding a case with high leverage

+ The mean of hat values ( `\(\bar{h}\)` ) is then:

`$$\bar{h} = (k+1)/n$$`

  + `\(k\)` is the number of predictors
  + `\(n\)` is the sample size

+ In simple linear regression `\(k = 1\)` as there is just one predictor, hence `\(\bar h = (1 + 1) / n = 2 /n\)`

+ As a rough heuristic, values more than `\(2\bar{h}\)` are considered high leverage

---
#  Hat values in R 

.pull-left[

```r
salary2 %&gt;%
  mutate(
*   hat = hatvalues(m1)
  ) %&gt;%
* dplyr::filter(., hat &gt; 2*((1+1)/150)) %&gt;%
  kable(.)  %&gt;%
  kable_styling(., full_width = F)
```

]

.pull-right[

+ Steps to identify large `\(h_i\)` values:
  + Extract the `\(h_i\)` from our model.
  + Identify those outside `\(2\bar{h}\)`
  
]


---
#  Hat values in R 

.pull-left[

```r
salary2 %&gt;%
  mutate(
*   hat = hatvalues(m1)
  ) %&gt;%
* dplyr::filter(., hat &gt; 2*((1+1)/150)) %&gt;%
  kable(.)  %&gt;%
  kable_styling(., full_width = F)
```

]

.pull-right[

&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; id &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; salary &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; serv &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; perf &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; hat &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID108 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 96.49 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0321553 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID110 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 143.69 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0471197 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID113 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 88.27 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0408792 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID114 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 132.12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0559020 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID116 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 70.90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0342800 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID117 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 149.24 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0439770 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID119 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 119.03 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0321553 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID120 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 105.12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0421530 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID121 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 103.61 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0334091 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID122 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 89.50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0694283 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID123 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 157.10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0410090 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID124 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 81.67 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0676132 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID126 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 93.39 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0344470 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID129 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 91.89 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0494827 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID132 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 137.68 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0413662 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID133 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 41.60 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0420739 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID134 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 125.90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0768679 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID137 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 106.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0326665 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID138 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 97.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0333394 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID139 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 108.75 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0382477 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 70.51 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0310413 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID141 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 142.55 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0413662 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID145 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 134.55 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0527432 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID149 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 62.60 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0376451 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID151 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 64.59 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0659745 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID155 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 74.80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0565876 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID156 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 108.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0525868 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID158 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 103.59 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0268633 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID159 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 129.42 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0314559 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 74.27 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0352835 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID161 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 148.05 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0368975 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID162 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 66.57 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0376451 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID163 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 84.19 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0333946 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID164 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 94.09 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0307448 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID170 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 88.72 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0591296 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID171 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 113.39 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0530959 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID172 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 94.74 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0303591 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID173 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 17.86 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0735795 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 111.36 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0364259 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID182 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 110.15 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0659204 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID184 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 71.32 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0518582 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID185 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 116.17 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0268633 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID186 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 138.37 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0278048 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID191 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 101.32 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0333394 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID192 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 136.89 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0513663 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID198 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 99.85 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0321148 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID199 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 95.12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0736123 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
  
]


---
class: center, middle
# Time for a break

Let's have a R task.

Look at the code of the previous slide

Write in words what each line is doing

---
class: center, middle
# Welcome Back!

**Where we left off... **

We had discussed outliers and leverage.

Next up, influence.

---
#  Influence

+ High leverage cases, when they are also linear model outliers, will have high **influence**

+ Cases with high influence, have a strong effect on the coefficients

+ If we deleted such a case, the linear model coefficients would change substantially


---
# Influence

+ If a handful of influential cases are responsible for the linear model results, the conclusions might not generalise very well

+ Multiple ways to consider influence.
  + Here we will discuss Cook's distance.
  + In a few weeks we will consider some additional measures
  
+ Cook's Distance of a data point `\(i\)` (can be written many ways):


`$$D_i = \frac{(\text{StandardizedResidual}_i)^2}{k+1} \times \frac{h_i}{1-h_i}$$`

---
#  Cooks Distance 
`$$\frac{(\text{StandardizedResidual}_i)^2}{k+1} = \text{Outlyingness}$$`


`$$\frac{h_i}{1-h_i} = \text{Leverage}$$`

+ So `\(D_i = \text{Outlyingness} \times \text{Leverage}\)`


+ Cook's distance refers to **the average distance the `\(\hat{y}\)` values will move if a given case is removed.**
  + If removing one case changes the predicted values a lot (moves the regression line), then that case is influencing our results.

---
#  Cooks Distance 

+ Many different suggestions for cut-off's:
  + `\(D_i &gt; 1\)` 
  + `\(D_i &gt; \frac{4}{n-k-1}\)`
  + Or size relative all values in data set

---
#  Cook's distance in R

.pull-left[

```r
salary2 %&gt;%
  mutate(
*   cook = cooks.distance(m1)
  ) %&gt;%
* dplyr::filter(., cook &gt; 4/(150-1-1)) %&gt;%
  kable(.)  %&gt;%
  kable_styling(., full_width = F)
```

&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; id &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; salary &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; serv &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; perf &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; cook &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID114 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 132.12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0833774 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID125 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 51.78 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0407764 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID133 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 41.60 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0704271 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID134 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 125.90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0545706 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID157 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 155.65 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0285518 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID159 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 129.42 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0867202 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID161 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 148.05 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0591664 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID171 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 113.39 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0287219 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID173 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 17.86 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.2042640 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 111.36 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0432211 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ID199 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 95.12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0288245 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

.pull-right[
&lt;img src="dapr2_09_assumptions_diagnostics_files/figure-html/unnamed-chunk-35-1.png" width="90%" /&gt;

]


---
# Influence of coefficients
+ Cook's distance is a single value summarizing the total influence of a case

+ In the context of a lm with 2+ predictors, we may want to look in a little more detail.

+ **DFFit**: The difference between the predicted outcome value for a case with versus without a case included

+ **DFbeta**: The difference between the value for a coefficient with and without a case included

+ **DFbetas**: A standardised version of DFbeta
  + Obtained by dividing by an estimate of the standard error of the regression coefficient with the case removed

---
# In R

+ We can extract these measures using the `influence.measures()` function:


```r
dfs_m1 &lt;- influence.measures(m1)
round(dfs_m1$infmat[1:10,],3)
```

```
##    dfb.1_ dfb.perf dfb.serv  dffit cov.r cook.d   hat
## 1  -0.091    0.042    0.066 -0.106 1.041  0.004 0.023
## 2  -0.005    0.005    0.004  0.008 1.058  0.000 0.025
## 3  -0.083    0.041    0.054 -0.097 1.038  0.003 0.020
## 4   0.057   -0.018   -0.120 -0.184 0.991  0.011 0.018
## 5  -0.004    0.042   -0.065 -0.098 1.046  0.003 0.025
## 6  -0.016    0.007    0.035  0.061 1.040  0.001 0.015
## 7   0.127   -0.140   -0.091 -0.227 0.986  0.017 0.023
## 8   0.183   -0.141   -0.177 -0.278 0.992  0.025 0.032
## 9  -0.030    0.015    0.020 -0.036 1.051  0.000 0.020
## 10 -0.056    0.062    0.029  0.078 1.078  0.002 0.047
```


---
#  Influence on standard errors 
+ Influential cases can impact the `\(SE\)` as well as the estimation of coefficients.
  + This means it impacts our inferences.

+ Recall, the standard error for a regression slope (single predictor):

`$$SE(\beta_1) = \sqrt{\frac{\frac{SSE}{(N-k-1)}}{\sum(x_i  -\bar{x})^2}}$$`

+ Where:
	+ `\(SSE\)` is the sum of squared error (i.e. `\(SS_{residual}\)` )
	+ `\(N\)` is the sample size
	+ `\(k\)` is the number of predictors

---
#  Influence on standard errors 

`$$SE(\beta_1) = \sqrt{\frac{\frac{SSE}{(N-k-1)}}{\sum(x_i  -\bar{x})^2}}$$`

+ `\(\sum(x_i  -\bar{x})^2\)` is the sum of squared deviations from each `\(X\)` value from the mean of `\(X\)`

+ This term implies that increasing the variance in `\(X\)` will decrease the standard error of

+ High leverage cases (which are far from the mean of X) can increase `\(X\)` variance

---
#  COVRATIO 
+ Influence on standard errors can be measured using the **COVRATIO** statistic
	+ COVRATIO value &lt;1 show that precision is decreased (SE increased)  by a case
	+ COVRATIO value &gt;1 show that precision is increased (SE decreased) by a case

+ Cases with COVRATIOS `\(&gt; 1+[3(k +1)/n]\)` or `\(&lt; 1-[3( k +1)/ n ]\)` can be considered to have a strong influence on the standard errors

---
#  COVRATIO in R 
+ COVRATIO values can be extracted using the `covratio()` function:


```r
cr &lt;- covratio(m1)
cr[1:5]
```

```
##        1        2        3        4        5 
## 1.040592 1.057786 1.038253 0.990737 1.045659
```


---
#  COVRATIO in R 

+ And we can check cases using the cuts above:


```r
which(cr &gt; (1+(3*2)/100))
```

```
## 10 17 19 20 22 24 29 32 39 41 45 51 55 56 60 63 70 72 82 84 91 92 99 
## 10 17 19 20 22 24 29 32 39 41 45 51 55 56 60 63 70 72 82 84 91 92 99
```

---
class: center, middle
# Time for a break

---
class: center, middle
# Welcome Back!


---
#  What should be done about outliers, high leverage and high influence values?

+ Easier to identify unusual cases than it is to decide what to do about them?

+ In general, not a good idea to delete cases automatically for exceeding a threshold. 
	+ Especially if they don't show large influence
	
+ Instead, try to investigate why a case is unusual

+ Think of linear model diagnostics more as a way of learning about the limitations of the model
	+ Which cases can't it explain very well?
	+ Why is that?
	+ Do results depend quite strongly on certain cases?

---
#  Investigating and dealing with unusual cases

+ Is there a data entry error?
	+ Is the value within the plausible range of values for a variable?
	+ Can it be corrected? If not, the incorrect value should be deleted

+ Is the data legitimate but extreme?
	+ Consider ways to reduce its influence rather than delete (e.g. **windsorizing**)
	+ May be model specification problems (e.g. missing variables, interactions - coming soon)

+ Are there unusual values as a result of skewness?

???
+ Replacing the extreme value with the next largest value for that variable
+ Avoids missingness/ preserves information
+ Note that deleting or winsorising values can change the model, therefore, different cases might then show up as outlying, high leverage, or influential
+ Iterative approach to removing/ winsorising cases is needed


---
#  Sensitivity Analyses

+ Sensitivity analysis refers to the idea of checking whether you get similar results irrespective of the methodological decisions you make

+ Sensitivity analysis can be used to check whether you get the same pattern of results. Do the estimated regression coefficients change substantially:
	+ With versus without including certain unusual cases?
	+ With versus without transforming a variable?

+ If results are highly similar, you have more confidence that the results aren't very dependent on those decisions

+ If they differ a lot, this should be reported as a limitation
  
---
# Summary

+ We have looked at case diagnostics for lm

+ Outliers can be assessed via studentized residuals.

+ Leverage can be assessed via hat values

+ Influence can be assessed via Cook's distance
  + As well as dfbeta and COVRATIO

+ In short, we do not want a small number of observations having undue influence over our results, so we should always look closely at diagnostics.

---
class: center, middle
# Thanks for listening!

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
