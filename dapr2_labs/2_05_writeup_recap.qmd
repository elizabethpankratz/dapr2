---
title: "Write Up Example & Block 3 Recap"
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source('assets/setup.R')

set.seed(953)

library(DT)
library(pander)
library(sjPlot)
library(tidyverse)
library(sandwich)
library(interactions)
```

:::lo

### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand how to write-up and provide interpretation of a 2x2 factorial ANOVA

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1. Be up to date with lectures
2. Have completed Labs 1-4

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse** 
* **patchwork**
* **kableExtra**

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/lietraining.csv) or read it in via this link https://uoepsy.github.io/data/lietraining.csv. 

:::

# Section A: Write-Up



# Section B: Weeks 1-5 Recap

In the second part of the lab, there is no new content - the purpose of the recap section is for you to revisit and revise the concepts you have learned over the last 4/5 weeks. 

:::red

Before you expand each of the boxes below, think about how comfortable you feel with each concept.  

:::

`r optbegin("F-Ratio", olabel=FALSE,toggle=params$TOGGLE)`

As in simple linear regression, the $F$-ratio is used to test the null hypothesis that all regression slopes are zero.  

It is called the $F$-ratio because it is the ratio of the how much of the variation is explained by the model (per paramater) versus how much of the variation is unexplained (per remaining degrees of freedom). 

$$
F_{df_{model},df_{residual}} = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\
\quad \\
\begin{align}
& \text{Where:} \\
& df_{model} = k \\
& df_{error} = n-k-1 \\
& n = \text{sample size} \\
& k  = \text{number of explanatory variables} \\
\end{align}
$$

---

:::blue

In **R**, at the bottom of the output of `summary(<modelname>)`,:

```{r mlroutputrf, echo=FALSE, fig.cap="Multiple regression output in R, summary.lm(). F statistic highlighted", fig.align = 'left'}
knitr::include_graphics("images/mlroutputf.png")
```

The $F$-ratio you see at the bottom of `summary(model)` is actually a comparison between two models: your model (with some explanatory variables in predicting $y$) and the *null model*. 

In regression, the null model can be thought of as the model in which all explanatory variables have zero regression coefficients. It is also referred to as the __intercept-only model__, because if all predictor variable coefficients are zero, then the only we are only estimating $y$ via an intercept (which will be the mean - $\bar y$). 

Alongside viewing the $F$-ratio, you can see the results from testing the null hypothesis that all of the coefficients are $0$ (the alternative hypothesis is that at least one coefficient is $\neq 0$. Under the null hypothesis that all coefficients = 0, the ratio of explained:unexplained variance should be approximately 1)

:::


:::{.callout-note}

See Week 1 [lab](https://uoepsy.github.io/dapr2/2223/labs/1_07_int1_nc.html) and [lectures](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_06_interactions1#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Nested vs Non-Nested Models", olabel=FALSE,toggle=params$TOGGLE)`

**Nested Models**

Consider that you have two regression models where Model 1 contains a subset of the predictors contained in the other Model 2 and is fitted to the same data. More simply, Model 2 contains all of the predictors included in Model 1, **plus** additional predictor(s). This means that Model 1 is *nested* within Model 2, or that Model 1 is a *submodel* of Model 2. These two terms, at least in this setting, are interchangeable - it might be easier to think of Model 1 as your null and Model 2 as your alternative.

**Non-Nested Models**

Consider that you have two regression models where Model 1 contains different variables to those contained in Model 2, where both models are fitted to the same data. More simply, Model 1 and Model 2 contain unique variables that are not shared. This means that Model 1 and Model 2 are **not** nested.

`r optend()`

`r optbegin("Incremental F-test", olabel=FALSE,toggle=params$TOGGLE)`

If (*and only if*) two models are __nested__, can we compare them using an __incremental F-test__.  

This is a formal test of whether the additional predictors provide a better fitting model.  
Formally this is the test of:  

+ $H_0:$ coefficients for the added/omitted variables are all zero.
+ $H_1:$ at least one of the added/omitted variables has a coefficient that is not zero. 

The $F$-ratio for comparing the residual sums of squares between two models can be calculated as:

$$
F_{(df_R-df_F),df_F} = \frac{(SSR_R-SSR_F)/(df_R-df_F)}{SSR_F / df_F} \\
\quad \\
\begin{align}
& \text{Where:} \\
& SSR_R = \text{residual sums of squares for the restricted model} \\
& SSR_F = \text{residual sums of squares for the full model} \\
& df_R = \text{residual degrees of freedom from the restricted model} \\
& df_F = \text{residual degrees of freedom from the full model} \\
\end{align}
$$
`r optend()`

`r optbegin("Comparing regression models with `anova()`", olabel=FALSE,toggle=params$TOGGLE)`

Remember that you want your models to be *parsimonious*, or in other words, only as complex as they need to be in order to describe the data well. This means that you need to be able to justify your model choice, and one way to do so is by comparing models via `anova()`. If your model with multiple IVs does not provide a significantly better fit to your data than a more simplistic model with less IVs, then the more simplistic model should be preferred. 

:::blue

In **R**, we can conduct an incremental $F$-test by constructing two linear regression models, and passing them to the `anova()` function: `anova(model1, model2)`. 

If the $p$-value is sufficiently low (i.e., below your predetermined significance level - usually .05), then you would conclude that model 2 is significantly better fitting than model 1. If $p$ is not < .05, then you should favor the more simplistic model. 

:::

`r optend()`

`r optbegin("AIC & BIC", olabel=FALSE,toggle=params$TOGGLE)`

AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) combine information about the sample size, the number of model parameters, and the residual sums of squares ($SS_{residual}$). Models do not *need* to be nested to be compared via AIC and BIC, __but__ they need to have been fit to the same dataset.  

For both of these fit indices, lower values are better, and both include a penalty for the number of predictors in the model (although BIC's penalty is harsher):

$$
AIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + 2k \\
\quad \\
BIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + k\,\text{ln}(n) \\
\quad \\
\begin{align}
& \text{Where:} \\
& SS_{residual} = \text{sum of squares residuals} \\
& n = \text{sample size} \\
& k = \text{number of explanatory variables} \\
& \text{ln} = \text{natural log function} 
\end{align}
$$

:::blue

In **R**, we can calculate AIC and BIC by using the `AIC()` and `BIC()` functions respectively.

:::

`r optend()`

`r optbegin("When to use Which Correction", olabel=FALSE,toggle=params$TOGGLE)`

**Bonferroni**  

- Use Bonferroni’s method when you are interested in a small number of planned contrasts (or pairwise comparisons).
- Bonferroni's method is to divide alpha by the number of tests/confidence intervals.
- Assumes that all comparisons are independent of one another.  
- It sacrifices slightly more power than Tukey’s method (discussed below), but it can be applied to any set of contrasts or linear combinations (i.e., it is useful in more situations than Tukey).
- It is usually better than Tukey if we want to do a small number of planned comparisons.  

**Šídák**  

- (A bit) more powerful than the Bonferroni method.
- Assumes that all comparisons are independent of one another. 
- Less common than Bonferroni method, largely because it is more difficult to calculate (not a problem now we have computers).  

**Tukey**

- It specifies an exact family significance level for comparing all pairs of treatment means.  
- Use Tukey’s method when you are interested in all (or most) pairwise comparisons of means.  

**Scheffe**

- It is the most conservative (least powerful) of all tests.
- It controls the family alpha level for testing all possible contrasts.
- It should be used if you have not planned contrasts in advance.
- For testing pairs of treatment means it is too conservative (you should use Bonferroni or Šídák).

:::blue

In **R**, you can easily change which correction you are using via the `adjust = ` argument.

:::

`r optend()`