---
title: "Write Up Example & Block 3 Recap"
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source('assets/setup.R')

set.seed(953)

library(DT)
library(pander)
library(sjPlot)
library(tidyverse)

```

:::lo

### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand how to write-up and provide interpretation of a 2x2 factorial ANOVA

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1. Be up to date with lectures
2. Have completed Labs 1-4

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse** 
* **patchwork**
* **kableExtra**
* **emmeans**
* **pander**

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/lietraining.csv) or read it in via this link https://uoepsy.github.io/data/lietraining.csv. 

:::

# Section A: Write-Up

In this section of the lab you will be be presented with a research question, and tasked with writing up and presenting your analyses.

The aim in writing should be that a reader is able to more or less replicate your analyses **without** referring to your R code. This requires detailing all of the steps you took in conducting the analysis. The point of using RMarkdown is that you can pull your results **directly** from the code. If your analysis changes, so does your report!  

Make sure that your final report doesn't show any R functions or code. Remember you are interpreting and reporting your results in text, tables, or plots, targeting a generic reader who may use different software or may not know R at all. If you need a reminder on how to hide code, format tables, etc., make sure to review the [rmd bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

:::{.callout-note}

## Important - Write-Up Examples & Plagiarism

The example write-up sections included below are not **perfect** - they instead should give you a good example of what information you should include within each section, and how to structure this. For example, some information is missing (e.g., interpretation of descriptive statistics, what type of interaction is present), some information could be presented more clearly (e.g., variable names in tables, table/figure titles/captions, and rationales for choices), and writing could be more concise in places (e.g., discussion section is quite long).  

Further, **you must not copy any of the write-up included below for future reports** - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more [here](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct).

:::

## Study Overview 

> **Research Question** 
>
> Do Police training materials and the mode of communication influence the accuracy of veracity judgements?  

`r optbegin("Lie detectors: Data Codebook", olabel=FALSE, toggle=params$TOGGLE)`

__Description__

A total of 120 participants took part in a study in which they were presented with 100 recordings, and were tasked with guessing whether the speaker in each recording was lying or whether they were telling the truth.  

Participants scored points every time they correctly identified a truth or a lie, and lost points whenever they mistook a lie for a truth (or vice versa). The maximum possible points to be scored was 100. 

Half of the participants ($n$ = 60) were shown recordings in audio and video, the other half were presented with only the audio track. 

Prior to taking part in the experiment, participants were given material to read for 10 minutes. Half of the participants in each condition (30 in the audio-only condition, and 30 in the audiovideo condition) were given instructional material used by the Police Force to train detectives to pick up on dishonesty during interrogations via various verbal and non-verbal cues. The remaining 30 participants in each condition were given a series of [cartoon strips to read](https://external-preview.redd.it/OY7xRuBBRkx8JFrjYoh3PswM2VMBDHBv4oXbpIEcB3Q.jpg?auto=webp&s=1a12316259fdfd314d4060f44f30260735741da5).


The data in `lietraining.csv` contain seven attributes collected from a sample of $n=120$ participants: 

- `pid`: Participant ID
- `age`: Age (in years) of participant
- `trained`: Whether participants were given instructional material used by the Police Force to train detectives to pick up on dishonesty during interrogations via various verbal and non-verbal cues (yes = `y` / no = `n`) 
- `audiovideo`: Audio-Video recording condition - either `audio+video` for those participants shown recordings in audio and video, or `ausio-only` for those participants presented with only the audio track
- `points`: Points scored for identifying a truth or a lie (range: 0-100)

__Preview__

The first six rows of the data are:

```{r echo=FALSE, message=FALSE}
read_csv('https://uoepsy.github.io/data/lietraining.csv') %>%  head %>% gt::gt()
```

`r optend()`

### Setup

`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read the lietraining dataset into R, assigning it to an object named `liedat`

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r message=FALSE}
library(tidyverse)
library(psych) 
library(kableExtra)
library(emmeans)
library(pander)
library(interactions)

#read in data
liedat <- read_csv("https://uoepsy.github.io/data/lietraining.csv")
```

`r solend()`

### Analysis Code

Try to answer the research question above without referring to the provided analysis code below, and then check how your script matches up - is there anything you missed or done differently? If so, discuss the differences with a tutor - there are lots of ways to code to the same solution!

`r optbegin("Provided Analysis Code", olabel=FALSE,toggle=params$TOGGLE)`  

```{r}
######Step 1 is always to read in the data, then to explore, check, describe, and visualise it.

#check coding of variables - are they coded as they should be?
str(liedat)
head(liedat)

#make variables factors & label
liedat$audiovideo <- factor(liedat$audiovideo, labels=c("audio","audio+video"))
liedat$trained <- factor(liedat$trained, labels = c("untrained","trained"))

#create descriptives table
descript <- liedat %>% 
    group_by(audiovideo, trained) %>%
   summarise(
       meanpoints = round(mean(points), 2),
       se = round(sd(points)/sqrt(n()), 2)
    )
descript

#plot showing the mean points for each condition
p1 <- ggplot(descript, aes(x = audiovideo, y = meanpoints, color = trained)) + 
  geom_point(size = 3) +
  geom_linerange(aes(ymin = meanpoints - 2 * se, ymax = meanpoints + 2 * se)) +
  geom_path(aes(x = as.numeric(audiovideo)))
p1

######Step 2 is to run your model(s) of interest to answer your research question, and make sure that the data meet the assumptions of your chosen test

#build model
lie_mdl <- lm(points ~ audiovideo * trained, data = liedat)

#check assumptions
par(mfrow=c(2,2))
plot(lie_mdl)
par(mfrow=c(1,1))

# look at model output - summary() and anova()
summary(lie_mdl)
anova(lie_mdl)


#####Step 3 somewhat depends on the outcomes of step 2. Here, you may need to consider conducting further analyses before writing up / describing your results in relation to the research question. 

#Perform a pairwise comparison of the mean accuracy (as measured by points accrued) across the 2Ã—2 factorial design, making sure to adjust for multiple comparisons. 

emms_lie <- emmeans(lie_mdl, ~ audiovideo * trained)

lie_con <- contrast(emms_lie, method = "pairwise", adjust="tukey")
lie_con

# confidence intervals
confint(lie_con)

#plot
plot(lie_con)
```

`r optend()`

### The 3-Act Structure: Analysis Strategy, Results, & Discussion

Recall that we need to present our report in three clear sections - think of your sections like the 3 key parts of a play or story - we need to (1) provide some background and scene setting for the reader, (2) present our results in the context of the research question, and (3) present a resolution to our story - relate our findings back to the question we were asked and provide our answer. 

If you need a reminder of what to include within each section, refer to [Lab 11](https://uoepsy.github.io/dapr2/2223/labs/1_11_writeup_recap.html), and read through the 'what to include' sections for Analysis Strategy, Results, and Discussion. 

#### Act I: Analysis Strategy

`r qbegin(1)`

Attempt to draft a discussion section based on the above research question and analysis provided.

`r qend()`

`r optbegin("Example Write-Up of Analysis Strategy Section", olabel=FALSE, toggle = params$TOGGLE)`

The `lietraining` dataset contained information on 120 participants who took part in a study concerning lie detection. Participants were each presented with 100 recordings (half were shown recordings in audio and video, and the other half audio only), and were tasked with judging whether the speaker in each recording was lying or whether they were telling the truth. Participants scored 1 point each time they correctly identified a truth or a lie, and lost 1 point whenever they mistook a lie for a truth (or vice versa). The maximum score was 100, where higher scores reflected higher levels of accuracy. Prior to taking part in the experiment participants were given materials to read. Half of the participants in each condition were given instructional material used by the Police Force (used to train detectives to pick up on dishonesty during interrogations via various verbal and non-verbal cues) and the remaining 30 participants in each condition were given a series of [cartoon strips to read](https://external-preview.redd.it/OY7xRuBBRkx8JFrjYoh3PswM2VMBDHBv4oXbpIEcB3Q.jpg?auto=webp&s=1a12316259fdfd314d4060f44f30260735741da5).  

All participant data was complete, and accuracy scores (points) within range i.e., 0-100. Categorical variables were coded as factors, where audio was designated as the reference level for mode of communication, and untrained as the reference level for training materials.

To investigate whether police training materials (trained vs untrained) and the mode of communication (audio vs audiovideo) interacted to influence the accuracy of veracity judgements, a two-way ANOVA model was used. Effects were considered statistically significant at $\alpha = 0.05$. Using dummy coding, the following model specification was used:

$$
\begin{aligned}
\text{Accuracy Scores} &= \beta_0 \\
&+ \beta_1 A_\text{AudioVideo} + \beta_2 T_\text{Trained} \\
&+ \beta_3 (A_\text{AudioVideo} * T_\text{Trained}) \\
&+ \epsilon
\end{aligned}
$$
To address the research question of whether the interaction between training materials and mode of communication was statistically significant, this formally corresponded to testing whether the interaction coefficient was equal to zero:

$$
H_0: \beta_3 = 0 \\
H_1: \beta_3 \neq 0
$$
The following assumptions were assessed visually using diagnostic plots: independence (with the previous plot and a plot of residuals vs index; no dependence should be indicated), equal variances (via a scale-location plot; residuals should be evenly spread across the range of fitted values, where the spread should be constant across the range of fitted values), and normality (via a qqplot of the residuals; points should follow along the diagonal line). Outliers were assessed via Cooks Distance, where values >2 indicated influential points. 

`r optend()`

#### Act II: Results

`r qbegin(2)`

Attempt to draft a results section based on your detailed analysis strategy and the analysis provided.

`r qend()`

`r optbegin("Example Write-Up of Results Section", olabel=FALSE, toggle = params$TOGGLE)`

Descriptive statistics are displayed in @tbl-descript. 

```{r descript, echo = FALSE, message = FALSE, caption = "Descriptives Table"}
#| label: tbl-descript
#| tbl-cap: "Descriptive Statistics"
# the kable() function makes tables nice for html:
liedat %>% 
    group_by(audiovideo, trained) %>%
    summarise(
       meanpoints = round(mean(points), 2),
       se = round(sd(points)/sqrt(n()), 2)) %>%
    knitr::kable(., caption = "Descriptive Statistics")  %>%
  kableExtra::kable_styling()
```

In the audio condition, there did not appear to be a difference between trained and non-trained scores. However, untrained scored higher than trained in the audio+video condition. There appeared to be an interaction (see @fig-lieplot). 

```{r lieplot, echo = FALSE, fig.cap = "", fig.align = "center"}
#| label: fig-lieplot
#| fig-cap: "Interaction Plot"
ggplot(descript, aes(x = audiovideo, y = meanpoints, color = trained)) + 
  geom_point(size = 3) +
  geom_linerange(aes(ymin = meanpoints - 2 * se, ymax = meanpoints + 2 * se)) +
  geom_path(aes(x = as.numeric(audiovideo)))
```

Accuracy of veracity judgements (measured by points scored in lie-detecting game) were analysed with a 2 (audio vs audiovideo) $\times$ 2 (untrained vs trained) between-subjects ANOVA. 

The model met assumptions of linearity and independence (see top left panel of @fig-assumpt; residuals were randomly scattered with a mean of zero and there was no clear dependence), homoscedasticity (see bottom left panel of @fig-assumpt; there was a constant spread of residuals), and normality (see top right panel of @fig-assumpt; the QQplot showed very little deviation from the diagonal line). 

```{r assumpt, echo = FALSE, fig.cap = "Assumption Checks", fig.align = "center"}
#| label: fig-assumpt
#| fig-cap: "Diagnostic Plots"
par(mfrow=c(2,2))
plot(lie_mdl)
par(mfrow=c(1,1))
```

There was a significant interaction between presentation mode and whether or not participants had received training for detecting lies $F(1, 116) = 17.37, p <. 001$ (see @tbl-aovout). 

```{r aovout, echo = FALSE, caption = "ANOVA Output"}
#| label: tbl-aovout
#| tbl-cap: "Model Results"
pander(anova(lie_mdl))
```

As displayed in @fig-intplot, results suggested that the difference in points did not differ significantly in the audio condition regardless of training, but that there were significant differences in the audio video condition.

```{r int, warning=FALSE, message=FALSE, echo = FALSE}
#interaction plot:
plt_lie_mdl <- cat_plot(model = lie_mdl, 
                  pred = audiovideo, 
                  modx = trained, 
                  main.title = "Interaction Model",
                  y.label = "Accuracy Scores",
                  legend.main = "Training Condition")

```

```{r}
#| label: fig-intplot
#| fig-cap: "Interaction Plot"
plt_lie_mdl
```

To explore the interaction further, pairwise comparisons were conducted. Tukeyâ€™s Honestly Significant Difference comparisons (see @fig-contrastplot) indicated that, contrary to what one might expect, participants who were presented with audiovisual recordings scored on average 9.5 points lower when they had read the police training materials compared to when they had received no training (95\% CI [4.87 â€” 14.14]). The presentation mode (audio vs audio-video) was not found to result in a significantly different average score for those who were untrained (95\% CI [-1.80 â€” 7.47]), and nor did training appear to have any effect on detecting lies in the audio-only condition (95\% CI [-5.61 â€” 3.66]).

```{r contrastplot, echo = FALSE, out.width="80%", fig.cap = "Tukey HSD Pairwise Comparisons", fig.align = "center"}
#| label: fig-contrastplot
#| fig-cap: "Comparisons"
plot(lie_con)
```

`r optend()`

#### Act III: Discussion

`r qbegin(3)`

Attempt to draft a discussion section based on your results and the analysis provided.

`r qend()`

`r optbegin("Example Write-Up of Discussion Section", olabel=FALSE, toggle = params$TOGGLE)`

The findings indicated that, in general, people were inaccurate when trying to distinguish between a lie and the truth, as the overall mean score was `r mean(liedat$points) %>% round(2)` out of 100 (SD = `r sd(liedat$points) %>% round(2)`), where a series of completely random guesses expected to score 50/100. The police training materials did not appear to be associated with the ability to accurately detect a lie - trained participants performed more poorly (compared to untrained) in the audio-visual condition. This may indicate that the training materials focus too heavily on visual cues, which perhaps are not actually associated with dishonesty in the appropriate way. 

MAKE STATEMENT IN RELATION TO THE NULL HERE TOO. 

`r optend()`

# Section B: Weeks 1-5 Recap

In the second part of the lab, there is no new content - the purpose of the recap section is for you to revisit and revise the concepts you have learned over the last 4/5 weeks. 

:::red

Before you expand each of the boxes below, think about how comfortable you feel with each concept.  

:::

`r optbegin("F-Ratio", olabel=FALSE,toggle=params$TOGGLE)`

As in simple linear regression, the $F$-ratio is used to test the null hypothesis that all regression slopes are zero.  

It is called the $F$-ratio because it is the ratio of the how much of the variation is explained by the model (per paramater) versus how much of the variation is unexplained (per remaining degrees of freedom). 

$$
F_{df_{model},df_{residual}} = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\
\quad \\
\begin{align}
& \text{Where:} \\
& df_{model} = k \\
& df_{error} = n-k-1 \\
& n = \text{sample size} \\
& k  = \text{number of explanatory variables} \\
\end{align}
$$

---

:::blue

In **R**, at the bottom of the output of `summary(<modelname>)`,:

```{r mlroutputrf, echo=FALSE, fig.cap="Multiple regression output in R, summary.lm(). F statistic highlighted", fig.align = 'left'}
knitr::include_graphics("images/mlroutputf.png")
```

The $F$-ratio you see at the bottom of `summary(model)` is actually a comparison between two models: your model (with some explanatory variables in predicting $y$) and the *null model*. 

In regression, the null model can be thought of as the model in which all explanatory variables have zero regression coefficients. It is also referred to as the __intercept-only model__, because if all predictor variable coefficients are zero, then the only we are only estimating $y$ via an intercept (which will be the mean - $\bar y$). 

Alongside viewing the $F$-ratio, you can see the results from testing the null hypothesis that all of the coefficients are $0$ (the alternative hypothesis is that at least one coefficient is $\neq 0$. Under the null hypothesis that all coefficients = 0, the ratio of explained:unexplained variance should be approximately 1)

:::


:::{.callout-note}

See Week 1 [lab](https://uoepsy.github.io/dapr2/2223/labs/1_07_int1_nc.html) and [lectures](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_06_interactions1#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Nested vs Non-Nested Models", olabel=FALSE,toggle=params$TOGGLE)`

**Nested Models**

Consider that you have two regression models where Model 1 contains a subset of the predictors contained in the other Model 2 and is fitted to the same data. More simply, Model 2 contains all of the predictors included in Model 1, **plus** additional predictor(s). This means that Model 1 is *nested* within Model 2, or that Model 1 is a *submodel* of Model 2. These two terms, at least in this setting, are interchangeable - it might be easier to think of Model 1 as your null and Model 2 as your alternative.

**Non-Nested Models**

Consider that you have two regression models where Model 1 contains different variables to those contained in Model 2, where both models are fitted to the same data. More simply, Model 1 and Model 2 contain unique variables that are not shared. This means that Model 1 and Model 2 are **not** nested.

`r optend()`

`r optbegin("Incremental F-test", olabel=FALSE,toggle=params$TOGGLE)`

If (*and only if*) two models are __nested__, can we compare them using an __incremental F-test__.  

This is a formal test of whether the additional predictors provide a better fitting model.  
Formally this is the test of:  

+ $H_0:$ coefficients for the added/omitted variables are all zero.
+ $H_1:$ at least one of the added/omitted variables has a coefficient that is not zero. 

The $F$-ratio for comparing the residual sums of squares between two models can be calculated as:

$$
F_{(df_R-df_F),df_F} = \frac{(SSR_R-SSR_F)/(df_R-df_F)}{SSR_F / df_F} \\
\quad \\
\begin{align}
& \text{Where:} \\
\\
& SSR_R = \text{residual sums of squares for the restricted model} \\
& SSR_F = \text{residual sums of squares for the full model} \\
& df_R = \text{residual degrees of freedom from the restricted model} \\
& df_F = \text{residual degrees of freedom from the full model} \\
\end{align}
$$
`r optend()`

`r optbegin("Comparing regression models with anova()", olabel=FALSE,toggle=params$TOGGLE)`

Remember that you want your models to be *parsimonious*, or in other words, only as complex as they need to be in order to describe the data well. This means that you need to be able to justify your model choice, and one way to do so is by comparing models via `anova()`. If your model with multiple IVs does not provide a significantly better fit to your data than a more simplistic model with less IVs, then the more simplistic model should be preferred. 

:::blue

In **R**, we can conduct an incremental $F$-test by constructing two linear regression models, and passing them to the `anova()` function: `anova(model1, model2)`. 

If the $p$-value is sufficiently low (i.e., below your predetermined significance level - usually .05), then you would conclude that model 2 is significantly better fitting than model 1. If $p$ is not < .05, then you should favor the more simplistic model. 

:::

`r optend()`

`r optbegin("AIC & BIC", olabel=FALSE,toggle=params$TOGGLE)`

AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) combine information about the sample size, the number of model parameters, and the residual sums of squares ($SS_{residual}$). Models do not *need* to be nested to be compared via AIC and BIC, __but__ they need to have been fit to the same dataset.  

For both of these fit indices, lower values are better, and both include a penalty for the number of predictors in the model (although BIC's penalty is harsher):

$$
AIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + 2k \\
\quad \\
BIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + k\,\text{ln}(n) \\
\quad \\
\begin{align}
& \text{Where:} \\
& SS_{residual} = \text{sum of squares residuals} \\
& n = \text{sample size} \\
& k = \text{number of explanatory variables} \\
& \text{ln} = \text{natural log function} 
\end{align}
$$

:::blue

In **R**, we can calculate AIC and BIC by using the `AIC()` and `BIC()` functions respectively.

:::


`r optend()`

`r optbegin("Side Contraints", olabel=FALSE,toggle=params$TOGGLE)`

Possible side-constraints on the parameters are:

|       Name      |             Constraint            |             Meaning of $\beta_0$            |         R         |
|:---------------:|:---------------------------------:|:-------------------------------------------:|:-----------------:|
| Sum to zero (Effects Coding) | $\beta_1 + \beta_2 + \beta_3 = 0$ | $\beta_0 = \mu$   |    `contr.sum`    |
| Reference group (Dummy Coding) |           $\beta_1 = 0$           | $\beta_0 = \mu_1$ | `contr.treatment` |


:::blue
**IMPORTANT**

- By default `R` uses the reference group constraint. If your factor has $g$ levels, your regression model will have $g-1$ dummy variables (`R` creates them for you)

- We can switch back to the default reference group constraint by applying either of these:

```{r}
# Option 1
#contrasts(rest_spend$music) <- NULL
# Option 2
#contrasts(rest_spend$music) <- "contr.treatment"
```
:::



`r optend()`


`r optbegin("When to use Which Correction", olabel=FALSE,toggle=params$TOGGLE)`

**Bonferroni**  

- Use Bonferroniâ€™s method when you are interested in a small number of planned contrasts (or pairwise comparisons).
- Bonferroni's method is to divide alpha by the number of tests/confidence intervals.
- Assumes that all comparisons are independent of one another.  
- It sacrifices slightly more power than Tukeyâ€™s method (discussed below), but it can be applied to any set of contrasts or linear combinations (i.e., it is useful in more situations than Tukey).
- It is usually better than Tukey if we want to do a small number of planned comparisons.  

**Å Ã­dÃ¡k**  

- (A bit) more powerful than the Bonferroni method.
- Assumes that all comparisons are independent of one another. 
- Less common than Bonferroni method, largely because it is more difficult to calculate (not a problem now we have computers).  

**Tukey**

- It specifies an exact family significance level for comparing all pairs of treatment means.  
- Use Tukeyâ€™s method when you are interested in all (or most) pairwise comparisons of means.  

**Scheffe**

- It is the most conservative (least powerful) of all tests.
- It controls the family alpha level for testing all possible contrasts.
- It should be used if you have not planned contrasts in advance.
- For testing pairs of treatment means it is too conservative (you should use Bonferroni or Å Ã­dÃ¡k).

:::blue

In **R**, you can easily change which correction you are using via the `adjust = ` argument.

:::

`r optend()`