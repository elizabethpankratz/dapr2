---
title: "Simple Effects, Pairwise Comparisons, & Corrections"
link-citations: TRUE
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
source('assets/setup.R')

set.seed(3)

library(DT)
library(tidyverse)
```

:::lo

### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand how to interpret simple effects for experimental designs
2. Understand how to conduct pairwise comparisons
3. Understand how to apply corrections available for multiple comparisons

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1. Be up to date with lectures
2. Have completed previous lab exercises from [Week 1](https://uoepsy.github.io/dapr2/2223/labs/2_01_model_comps.html), [Week 2](https://uoepsy.github.io/dapr2/2223/labs/2_02_effects.html), and [Week 3](https://uoepsy.github.io/dapr2/2223/labs/2_03_contrasts.html)

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse** 
* **psych** 
* **kableExtra**
* **sjPlot**
* **interactions**
* **patchwork**
* **emmeans**

### <i class="fa fa-pencil-square-o" aria-hidden="true"></i> Presenting Results
All results should be presented following [APA guidelines](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf). If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the [rmd bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

The example write-up sections included as part of the solutions are **not perfect** - they instead should give you a good example of what information you should include and how to structure this. Note that you must **not** copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more [here](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct). 

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/cognitive_experiment.csv) or read it in via this link https://uoepsy.github.io/data/cognitive_experiment.csv.  
  
Note, you have already worked with *some* of this data last week - see [Semester 2 Week 3 lab](), but we now have a third Task condition - Classification. 

:::

# Study Overview

> **Research Question** 
>
> Are there differences in types of memory deficits for those experiencing different cognitive impairment(s)?

In this week's exercises, we will further explore questions such as:

- Does level $i$ of the first factor have an effect on the response?
- Does level $j$ of the second factor have an effect on the response?
- Is there a combined effect of level $i$ of the first factor and level $j$ of the second factor on the response? In other words, is there interaction of the two factors so that the combined effect is not simply the additive effect of level $i$ of the first factor plus the effect of level $j$ of the second factor?

`r optbegin("Cognitive Exp 3x3 data codebook.", olabel=FALSE, toggle=params$TOGGLE)` 
  
__Description__

The researchers designed a study yielding a $3 \times 3$ factorial design to test whether there are differences in types of memory deficits for those experiencing different cognitive impairment(s).

The tasks chosen by the researchers have been picked to map onto the theoretical differences between the three types of research participants. The Grammar and Classification tasks are known to reflect *implicit* memory processes, whereas the recognition task is known to reflect *explicit* memory processes. If the theory is correct, we would expect the difference in scores between the recognition and grammar/classification tasks to be relatively similar for the control and amnesiac groups, but relatively larger for the Huntingtons group compared to controls.

__Data Dictionary__

The data in `cognitive_experiment.csv` contain three attributes collected from $n=45$ participants, and includes: 

```{r echo=FALSE, message=FALSE, warning=FALSE}
cog <- read_csv("https://uoepsy.github.io/data/cognitive_experiment.csv")
tibble(
variable = names(cog),
description = c("Diagnosis classifies the three types of individuals: 1 = Amnesic patients, 2 = Huntingtons patients, and 3 = Control group of individuals with no known neurological disorder", "Task tells us to which one of three tasks each study participant was randomly assigned to: 1 = Grammar (which consists of classifying letter sequences as either following or not following grammatical rules), 2 = Classification (which consists of classifying stimuli into certain groupings, based on previously indicated information about the groups characteristics), and 3 = Recognition (which consists of recognising particular stimuli as stimuli that have previously been presented during the task)", "Score")) %>% gt::gt()
```
  
__Data Overview__

We have data from the 45 participants (15 amnesiacs, 15 Huntington individuals, and 15 controls). Recall that study involves two factors, now with three levels each. For each combination of factor levels we have 5 observations:

```{r echo=FALSE, message=FALSE}
library(tidyverse)
df <- read_csv('https://uoepsy.github.io/data/cognitive_experiment.csv')
# head(df)

df$Diagnosis <- factor(df$Diagnosis, 
                       labels = c("amnesic", "huntingtons", "control"),
                       ordered = FALSE)

df$Task <- factor(df$Task, 
                  labels = c("grammar", "classification", "recognition"), 
                  ordered = FALSE)

library(kableExtra)

df %>% 
    pivot_wider(names_from = 'Task', values_from = 'Y', values_fn = list) %>% 
    kable() %>%
    kable_styling(full_width = FALSE) %>%
    add_header_above(c(" " = 1, "Task" = 3))
```

The five observations are assumed to come from a population having a specific mean. The population means corresponding to each combination of factor levels can be schematically written as:

$$
\begin{matrix}
                   &         &         & \textbf{Task} & \\
                   &         &  (j=1) & (j=2) & (j=3) & \\
                   &         &  \text{ grammar} & \text{ classification} & \text{ recognition} \\
                   & (i=1)\text{ control} & \mu_{1,1} & \mu_{1,2} & \mu_{1,3} \\
\textbf{Diagnosis} & (i=2)\text{ amnesic} & \mu_{2,1} & \mu_{2,2} & \mu_{2,3} \\
                   & (i=3)\text{ huntingtons} & \mu_{3,1} & \mu_{3,2} & \mu_{3,3}
\end{matrix}
$$
  
  
__Preview__

The first six rows of the data are:

```{r echo=FALSE, message=FALSE}
read_csv('https://uoepsy.github.io/data/cognitive_experiment.csv') %>% head %>% gt::gt()
```

`r optend()`

<div class="divider div-transparent div-dot"></div>

# Setup

`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read the cognitive_experiment dataset into R, assigning it to an object named `cog` (recall that you have already worked with *some* of this data last week - see [Semester 2 Week 3 lab](), but we now have a third Task condition - Classification) 
 
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r message=FALSE}
#load packages
library(tidyverse)
library(psych)
library(kableExtra)
library(emmeans)
library(sjPlot)
library(interactions)
library(patchwork)

#read in data
cog <- read_csv('https://uoepsy.github.io/data/cognitive_experiment.csv')
```

`r solend()`

<div class="divider div-transparent div-dot"></div>

# Exercises 

## Study & Analysis Plan Overview 

`r qbegin(1)`

Firstly, examine the dataset, and perform any necessary and appropriate data management steps.

Next, consider would be the most appropriate coding constraint to apply in order to best address the research question - i.e., are we interested in whether group X (e.g., Amnesic) differed from group Y (e.g., Huntingtons), or whether group X (e.g., Amnesic) differed from the grand mean? 

Choose appropriate reference levels for the Diagnosis and Task variables based on your decision above. 

:::{.callout-tip appearance="simple" collapse="true"}

### Hint
If you think you'd benefit from a refresher on coding constraints, it might be best to revisit the materials from Semester 1 Block 2. 

In terms of data management:  
- Convert categorical variables to factors  
- Label appropriately factors to aid with your model interpretations  
- If needed, provide better variable names  

Note that all of these steps can be done in combination - the `mutate()` and `factor()` functions will likely be useful here. 

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Let's have a look at the data to see what we're working with - `str()` or `head()` are a good place to start - and then we should check for any missing data (`NA` values):
```{r}
#first look at dataset structure
str(cog)

#now lets look at top 6 rows (or the head) of the dataset
head(cog)

#check for NAs 
table(is.na(cog))
# there are none - all FALSE
```

Next, lets convert `Diagnosis` and `Task` into factors, making the labels of each factor level more meaningful. According to the data description, the encoding of the factor `Diagnosis` is: 1 = amnesic patients, 2 = Huntingtons patients, and 3 are control patients. The encoding for the factor `Task` is: 1 = grammar task, 2 = classification task, and 3 = recognition task.

```{r}
cog <- cog %>%
    mutate(
        Diagnosis = factor(Diagnosis, 
                           levels = c(1, 2, 3),
                           labels = c('amnesic', 'huntingtons', 'control'),
                           ordered = FALSE),
        Task = factor(Task, 
                      levels = c(1, 2, 3),
                      labels = c('grammar', 'classification', 'recognition'),
                      ordered = FALSE)) %>%
    rename(Score = Y)
```

Since we are interested in comparing groups, we should use dummy coding. By default, `R` uses dummy coding, so we do not need to make any changes to the coding constraint. 

However, for our reference groups, we're likely to want it to be the Control group for Diagnosis, and recognition for Task: 

```{r}
cog$Diagnosis <- fct_relevel(cog$Diagnosis, "control")
cog$Task <- fct_relevel(cog$Task, "recognition")
```

`r solend()`

<br>  

`r qbegin(2)`

Provide a brief overview of the study design and data, before detailing your analysis plan to address the research question.

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

- Give the reader some background on the context of the study (you might be able to re-use some of the content you wrote for [Semester 2 Week 3 lab]() here, but note that we now have an extra condition within Task)
- Outline data checks / data cleaning
- State what type of analysis you will conduct in order to address the research question
- Specify the model to be fitted to address the research question (note that you will need to specify the reference level of your categorical variables. This will be somewhat similar to last week, but with the addition of Classification in Task, our model will contain a different number of parameters)
- Specify your chosen significance ($\alpha$) level
- State your hypotheses

Much of the information required can be found in the [Study Overview] codebook.

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

The `cog` dataset contained information on 45 hypothetical participants from a between-subjects study. Participants belonged to one of three 'Diagnosis' groups, which had 15 participants in each - Control, Amnesic, or Huntingtons. Participants from each of the Diagnosis groups were equally and randomly assigned to one of three 'Tasks' to measure different memory processes - Grammar, Classification, or Recognition - the former two measuring implicit memory and the latter explicit. This resulted in 5 participants from each Diagnosis group in each of the three Task conditions. 

All participant data was complete, and categorical variables were coded as factors. For the purpose of this analysis, 'Control' was designated as the reference group for Diagnosis, since it was the only group of participants with no known neurological disorder. For Task, the recognition task measures explicit memory whereas the other two measure implicit memory, so this was specified as the reference group.   

Boxplots will be used to visualise the associations among Diagnosis and Task conditions. To address the research question of whether the difference in performance between explicit and implicit memory tasks will be greatest for Huntington patients in comparison to controls, we first need to define the dummy variables:

For Diagnosis:

$$
\begin{gather*}
D_\text{Amnesic} = \begin{cases}
1 & \text{if Diagnosis is Amnesic}\\  
0 & \text{otherwise}
\end{cases}
\\  
\\  
D_\text{Huntingtons} = \begin{cases}
1 & \text{if Diagnosis is Huntingtons}\\
0 & \text{otherwise}
\end{cases}
\quad   
\\  
\\  
(\text{Control is base level})  
\end{gather*}
$$

And for Task:  
  
$$
\begin{gather*}    
T_\text{Grammar} = \begin{cases}
1 & \text{if Task is Grammar}\\
0 & \text{otherwise}\\  
\end{cases}\
\\  
\\
T_\text{Classification} = \begin{cases}  
1 & \text{if Task is Classification}\\   
0 & \text{otherwise}\\ 
\end{cases}\\\  
\quad    
\\    
\\  
(\text{Recognition is base level})\\ 
\end{gather*}   
$$

Based on the above dummy coding, we are going to fit the following interaction model:


$$
\begin{align}
\text{Interaction Model}: \text{Score} &= \beta_0  \\
      &+ \beta_1 \cdot D_\text{Amnseic} + \beta_2 \cdot  D_\text{Huntingtons}  \\
      &+ \beta_3 \cdot  T_\text{Grammar}  + \beta_4 \cdot  T_\text{Classification}  \\
      &+ \beta_5 \cdot  (D_\text{Amnseic} \cdot  T_\text{Grammar})  \\
      &+ \beta_6 \cdot  (D_\text{Huntingtons} \cdot  T_\text{Grammar})  \\
      &+ \beta_7 \cdot  (D_\text{Amnseic} \cdot  T_\text{Classification})  \\
      &+ \beta_8 \cdot  (D_\text{Huntingtons} \cdot  T_\text{Classification})  \\
      &+ \epsilon  
\end{align}
$$

Effects will be considered statistically significant at $\alpha = .05$

Our hypotheses are:

$H_0:$ All $\beta_j = 0$ (for $j = 5, 6, 7, 8$)

There are no significant differences in performance between explicit and implicit memory tasks for patients with different cognitive impairment(s). 

$H_1:$ At least one $\beta_j \neq  0$ (for $j = 5, 6, 7, 8$)

There are significant differences in performance between explicit and implicit memory tasks for patients with different cognitive impairment(s). 

`r solend()`

<div class="divider div-transparent div-dot"></div>

## Descriptive Statistics & Visualisations

`r qbegin(3)`

Provide a table of descriptive statistics and visualise your data.

Interpret the descriptive statistics and visualisations in the context of the study (i.e., comment on any observed differences among groups). 

:::{.callout-tip appearance="simple" collapse="true"}

### Hint

1. For your table of descriptive statistics, both the `group_by()` and `summarise()` functions will come in handy here  
2. Recall that when visualising a continuous outcome across several groups, `geom_boxplot()` may be most appropriate to use  
3. Make sure to comment on any observed differences among the sample means of the different conditions  

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

::: {.panel-tabset}

## Numeric

Descriptive statistics presented in a well formatted table:

```{r message=FALSE, warning=FALSE}
#| label: tbl-cog-descript
#| tbl-cap: Descriptive Statistics
cog_stats <- cog %>% 
    group_by(Diagnosis, Task) %>%
    summarise(
        Mean = mean(Score), 
        SD = sd(Score),
        SE = sd(Score) / sqrt(n()),
        Min = min(Score),
        Max = max(Score)) %>%
    kable(caption = "Descriptive Statistics of Score", digits = 2) %>%
    kable_styling()

cog_stats
```

## Visual 

Since we have a continuous outcome and 2 categorical predictors - a boxplot would be most appropriate for visualisations:

```{r}
#| label: fig-cog-desc
#| fig-cap: "Association between Task Condition, Diagnosis, and Score"
cog_plt <- ggplot(data = cog, aes(x = Diagnosis, y = Score, color = Task)) +
  geom_boxplot() +
  labs(x = 'Diagnosis', y = 'Score')
cog_plt
```

:::

::: {.callout-important icon=false appearance="minimal"}

+ Control patients consistently performed best across all tasks. They doid not appear to differ substantially in their scores between grammar and classification tasks, but they clearly performed better in the recognition task in comparison to both the grammar and classification ones.

+ Amnesic patients appeared to perform better than Huntingtons patients in grammar and classification tasks (reflecting intrinsic memory processes), and performed worse than Huntingtons patients in the recognition task (reflecting extrinsic memory processes).

:::

`r solend()`

<div class="divider div-transparent div-dot"></div>

## Model Fitting & Interpretation 

`r qbegin(4)`

Fit the specified model using `lm()`, and store the model in an object named "mdl_int".

Provide key model results in a formatted table and plot the interaction model before reporting in-text the overall model fit.

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

*Table tips*
- Use `tab_model()` from the **sjPlot** package. Remember that you can rename your DV and IV labels by specifying `dv.labels` and `pred.labels`.

*Plotting tips*
- Using the `cat_plot()` function from the **interactions** package, visualise the interaction effects from your model.

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

::: {.panel-tabset}

## Build & Check Model

Fit model & check summary:

```{r}
#fit interaction model
mdl_int <- lm(Score ~ Diagnosis * Task, data = cog)

#check model output
summary(mdl_int)
```

## Results Table

Table containing the results from model:

```{r}
#| label: tbl-w4-modresults
#| tbl-cap: Regression Table for Scores Model
tab_model(mdl_int,
          dv.labels = "Scores",
          pred.labels = c("Diagnosisamnesic" = "Diagnosis - Amnesic",
                          "Diagnosishuntingtons" = "Diagnosis - Huntingtons",
                          "Taskgrammar" = "Task - Grammar",
                          "Taskclassification" = "Task - Classification",
                          "Diagnosisamnesic:Taskgrammar" = "Diagnosis - Amnesic : Task - Grammar",
                          "Diagnosishuntingtons:Taskgrammar" = "Diagnosis - Huntingtons : Task - Grammar",
                          "Diagnosisamnesic:Taskclassification" = "Diagnosis - Amnesic : Task - Classification",
                          "Diagnosishuntingtons:Taskclassification" = "Diagnosis - Huntingtons : Task - Classification"),
          title = "Regression Table for Scores Model")
```

## Interaction Plot

Plot interaction model: 

```{r}
#| label: fig-cog-4
#| fig-cap: "Interaction Plot"
plt_cog_mdl <- cat_plot(model = mdl_int, 
                  pred = Diagnosis, 
                  modx = Task, 
                  main.title = "Scores across Diagnosis and Task",
                  x.label = "Diagnosis",
                  y.label = "Score",
                  legend.main = "Task")
plt_cog_mdl
```

:::

::: {.callout-important icon=false appearance="minimal"}

Full regression results including 95% Confidence Intervals are shown in @tbl-w4-modresults, and the interaction model is visually presented in @fig-cog-4. The $F$-test for model utility was significant $(F(8, 36) = 12.28, p < .001)$, and the model explained approximately 67% of the variability in Scores.  

:::

`r solend()`

<div class="divider div-transparent div-dot"></div>

## Contrast Analysis

We will now begin by looking at each factor separately. 

`r qbegin(5)`

In terms of the diagnostic groups, we want to compare the individuals with amnesia to those with Huntingtons. This corresponds to a contrast with coefficients of 0, 1, and −1, for control, amnesic, and Huntingtons, respectively. 

Similarly, in terms of the tasks, we want to compare the average of the two implicit memory tasks with the explicit memory task. This corresponds to a contrast with coefficients of 0.5, 0.5, and −1 for the three tasks. 

When we are in presence of a significant interaction, the coefficients for a contrast between the means are found by multiplying each row coefficient with all column coefficients as shown below:

```{r echo=FALSE, out.width = '100%'}
knitr::include_graphics('images/contr_interaction.png')
```

Specify the coefficients to be used in the contrast analysis.

Formally state the contrast that the researchers were interested in as testable hypotheses. 

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

This can be done in `R` using:

::: {.panel-tabset}

## Option 1

```{r}
diag_coef  <- c('control' = 0, 'amnesic' = 1, 'huntingtons' = -1)
task_coef  <- c('grammar' = 0.5, 'classification' = 0.5, 'recognition' = -1)
contr_coef <- outer(diag_coef, task_coef)
contr_coef
```

## Option 2

```{r}
diag_coef  <- c('control' = 0, 'amnesic' = 1, 'huntingtons' = -1)
task_coef  <- c('grammar' = 0.5, 'classification' = 0.5, 'recognition' = -1)
contr_coef <- diag_coef %o% task_coef
contr_coef
```

:::

The above coefficients correspond to the following hypotheses:

$$
H_0: \quad \left(\frac{\mu_{2,1} + \mu_{2,2}}{2} - \mu_{2,3} \right) - \left( \frac{\mu_{3,1} + \mu_{3,2}}{2} - \mu_{3,3} \right) = 0
$$

$$
H_1: \quad \left(\frac{\mu_{2,1} + \mu_{2,2}}{2} - \mu_{2,3} \right) - \left( \frac{\mu_{3,1} + \mu_{3,2}}{2} - \mu_{3,3} \right) \neq 0
$$

which can be equivalently specified as:

$$
H_0: \quad \frac{\mu_{2,1} + \mu_{2,2}}{2} - \mu_{2,3} \quad = \quad \frac{\mu_{3,1} + \mu_{3,2}}{2} - \mu_{3,3}
$$

$$  
H_1: \quad \frac{\mu_{2,1} + \mu_{2,2}}{2} - \mu_{2,3} \quad \neq \quad  \frac{\mu_{3,1} + \mu_{3,2}}{2} - \mu_{3,3}
$$

both statements state that, in the population, the difference between the mean implicit memory and the explicit memory score is the same for individuals with amnesia and Huntingtons. 

Note that the scores for the grammar and classification tasks have been averaged to obtain a single measure of 'implicit memory' score.

`r solend()`

<br>

`r qbegin(6)`

Firstly, use `emmeans()` to obtain the estimated means and uncertainties for your factors. 

Next, specify the coefficients of the comparison and run the contrast analysis, obtaining 95% confidence intervals. 


`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Now that we have the coefficients, let's firstly call the `emmeans` function to get the estimated means of our groups (this is also helpful to look at the ordering of the groups):

```{r}
diag_task_mean <- emmeans(mdl_int, ~ Diagnosis*Task)
diag_task_mean
```

Next, from `contr_coef`, insert the coefficients **following the order specified by the rows of** `diag_task_mean` above. That is, the first one should be for `control` `recognition` and have a value of 0, the second for `amnesic` `recognition` with a value of -1, and so on...

Let's specify our weights, and give a name to this contrast (in this example 'Research Hyp'):

```{r}
diag_task_comp <- contrast(diag_task_mean, 
                     method = list('Research Hyp' = c(0, -1, 1, 0, 0.5, -0.5, 0, 0.5, -0.5))
                     )
```

Next, let's look at the output via one of two ways:

::: {.panel-tabset}

## Option 1

```{r}
#examine output
diag_task_comp

#obtain confidence intervals
confint(diag_task_comp)
```

## Option 2

```{r}
#examine summary output and state `infer = TRUE` to include confidence intervals
summary(diag_task_comp, infer = TRUE)
```

:::

::: {.callout-important icon=false appearance="minimal"}

We performed a test against $H_0: \quad \frac{\mu_{2,1} + \mu_{2,2}}{2} - \mu_{2,3} \quad = \quad \frac{\mu_{3,1} + \mu_{3,2}}{2} - \mu_{3,3}$. At the 5\% significance level, there was evidence that individuals with Amnesia and Huntingtons did differ in the difference between implicit and explicit memory tasks $t(36) = 5.40, p < .001, \text{two-sided})$, and this difference was estimated to be 52.50. We are 95\% confident that the difference in implicit and explicit memory scores between individuals with Amnesia and Huntingtons was between 32.80 to 72.20 points. Therefore, we can reject the null hypothesis that the difference in differences was equal to zero. 

:::

`r solend()`

<div class="divider div-transparent div-dot"></div>

## Simple Effects

By considering the simple effects^[simple effects are the effects of one variable *at a specific level of another variable*], we can identify at which levels of the interacting condition we see different effects. 

`r qbegin(7)`

Examine the simple effects for Task at each level of Diagnosis; and then the simple effects for Diagnosis at each level of Task. 

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`


::: {.panel-tabset}

## Simple Effects of Task

From `mdl_int_simple1` we can see the differences between between tasks for each diagnosis group:

```{r}
mdl_int_simple1 <- pairs(diag_task_mean, simple = "Task")
mdl_int_simple1
```

## Simple Effects of Diagnosis

From `mdl_int_simple2` we can see the differences between diagnoses for each task group:

```{r}
mdl_int_simple2 <- pairs(diag_task_mean, simple = "Diagnosis")
mdl_int_simple2
```

:::

`r solend()`

<br>

`r qbegin(8)`

Visualise the interaction, displaying two plots - one with Diagnosis on the x-axis, and the other with Task on the x-axis.

Considering the simple effects that you noted above, identify the significant effects and match them to the corresponding points of your interaction plot.

:::{.callout-tip appearance="simple" collapse="true"}

### Hint

To visualise the interaction, you can use `emmip()`.

Recall that the **patchwork** package allows us to arrange multiple plots using either `/` or `|` or `+`

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

::: {.panel-tabset}

## Simple Effects of Task

```{r}
plt_1 <- emmip(mdl_int, Diagnosis ~ Task, CIs = TRUE)
plt_1
```

For the simple effects of task (see `plt_1`), we saw the significant differences (those for which $p<.05$):  

+ Only in the Huntingtons group, between recognition & grammar and recognition & classification tasks  
    + left-most blue point compared to the middle blue point, and then compared to the right-most blue point  

## Simple Effects of Diagnosis

```{r}
plt_2 <- emmip(mdl_int, Task ~ Diagnosis, CIs = TRUE)
plt_2
```


For the simple effects of Diagnosis (see `plt_2`), we saw significant differences (those for which $p<.05$):  

+ in the recognition task, between control & amnesic   
    + left-most red point to middle red point   
    
+ in the recognition task, between amnesic & huntingtons   
    + middle red-point to right-most red point  
    
+ in the grammar task, between control & amnesic  
    + left-most green point to middle green point  
    
+ in the grammar task, between control & huntingtons   
    + left-most green point to right-most green point  
    
+ in the grammar task, between amnesic & huntingtons   
    + middle green point to right-most green point  

+ in the classification task, between control & huntingtons   
    + left-most blue point to right-most blue point  

+ in the classification task, between amnesic & huntingtons     
    + middle blue point to right-most blue point  

:::

`r solend()`

<div class="divider div-transparent div-dot"></div>

## Pairwise Comparisons & Multiple Corrections

`r qbegin(9)`

Conduct exploratory pairwise comparisons to compare all levels of Diagnosis with all levels of Task, applying no correction (note that Tukey will be automatically applied since we are comparing groups of means, so you will need to overwrite this).

Without adjusting our $\alpha$ (or $p$-value), why might any inferences drawn from your output be problematic?

:::{.callout-tip appearance="simple" collapse="true"}

### Hint

You can specify the adjustment using `adjust = " `. Possible options include:

+ `adjust = "none"`
+ `adjust =  "bonferroni"`
+ `adjust = "sidak"`
+ `adjust = "Tukey"`
+ `adjust = scheffe"`

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
pairs_res <- pairs(diag_task_mean, adjust = "none")
pairs_res

#can also plot if you'd like:
plot(pairs_res)
```

From the above, we can see comparisons for all different possible pairs of diagnosis-task combinations^[the differences between the group means for the comparison as labelled]. 

In total, there are 9 different estimates, but comparing them all means that we have 36 comparisons being tested! By not adjusting our $p$-value, we are increasing the experimentwise Type I error rate - we could wrongly reject the null hypothesis at a much higher rate than 5/100 (or 1/20 as is assumed when $\alpha = .05$). To overcome this, we might adjust and determine a result to be statistically significant if $p < .005$, as opposed to $p < .05$, depending on how many tests are in our family of tests).  

`r solend()`

<br>

`r qbegin(10)`

Select an appropriate method to adjust for multiple comparisons, and then obtain confidence intervals.

Comment on how these $p$-values differ from your raw (i.e., unadjusted) $p$-values.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Note what the functions in `R` do is adjust the $p$-value, rather than the $\alpha$. 

Since we're interested in all pairwise comparisons of means, the Tukey adjustment might be a sensible approach. However, we'll also show the Bonferroni to show how it differs (note, in practice you would only apply one correction and justify this choice based on your design - we are only applying two to note how they differ!)

::: {.panel-tabset}

## Tukey

```{r}
contrast(diag_task_mean, method = "pairwise", adjust="Tukey")
```

Note that 8 of the comparisons are no longer significant when using Tukey's adjustment, suggesting that these might have been (when using no adjustment) Type I errors! 


## Bonferroni

```{r}
contrast(diag_task_mean, method = "pairwise", adjust="bonferroni")
```

The first Bonferroni adjusted $p$-value is 0.0207. 

The raw (unadjusted) $p$-value from the previous question was 0.0005759265. 

The Bonferroni method simply multiplies the 'raw' $p$-value by the number of the tests, which we know is 36. 

```{r}
0.0005759265 * 36
```

In terms of differences in Bonferroni to raw $p$-values, they are thus 36 times the size.  

One benefit of Bonferroni is that it can be applied to any set of $p$-values, whereas Tukey only applies when comparing the means of levels of a factor. The downside, however, is that it may be overly conservative (i.e. reduce our power to detect an effect that is truly there).  

:::

`r solend()`

<div class="divider div-transparent div-dot"></div>

# Compile Report

`r qbegin("Compile Report", qlabel = FALSE)`  

Knit your report to PDF, and check over your work. To do so, you should make sure:

- Only the output you want your reader to see is visible (e.g., do you want to hide your code?)
- Check that the **tinytex** package is installed
- Ensure that the ‘yaml’ (bit at the very top of your document) looks something like this:

```{}
---
title: "this is my report title"
author: "B1234506"
date: "07/09/2024"
output: bookdown::pdf_document2
---
```

:::{.callout-tip appearance="simple" collapse="true"}
# What to do if you cannot knit to PDF
If you are having issues knitting directly to PDF, try the following:  

- Knit to HTML file  
- Open your HTML in a web-browser (e.g. Chrome, Firefox)  
- Print to PDF (Ctrl+P, then choose to save to PDF)  
- Open file to check formatting
:::

:::{.callout-tip appearance="simple" collapse="true"}
# Hiding Code and/or Output

:::{.panel-tabset}
## Hiding R Code

To not show the code of an R code chunk, and only show the output, write:

````
```{{r, echo=FALSE}}
# code goes here
```
````

## Hiding R Output

To show the code of an R code chunk, but hide the output, write:

````
```{{r, results='hide'}}
# code goes here
```
````

## Hiding R Code and Output

To hide both code and output of an R code chunk, write:

````
```{{r, include=FALSE}}
# code goes here
```
````
:::

:::

:::{.callout-tip appearance="simple" collapse="true"}

### Tinytex
You must make sure you have **tinytex** installed in R so that you can “Knit” your Rmd document to a PDF file:

```{r eval = FALSE}
install.packages("tinytex")
tinytex::install_tinytex()
```

:::

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

You should end up with a PDF file. If you have followed the above instructions and still have issues with knitting, speak with a Tutor. 

`r solend()`
