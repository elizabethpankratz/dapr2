---
title: "Simple Effects, Pairwise Comparisons, Corrections"
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE, eval=FALSE, echo=FALSE}
source('assets/setup.R')

set.seed(3)

library(DT)
library(tidyverse)
```

:::lo

### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand how to interpret simple effects for experimental designs
2. Understand how to conduct pairwise comparisons
3. Understand how to apply corrections available for multiple comparisons

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1. Be up to date with lectures
2. Have completed previous lab exercises from Semester 1

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse** 
* **psych** 
* **kableExtra**

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/RestaurantSpending.csv) or read it in via this link https://uoepsy.github.io/data/RestaurantSpending.csv

:::

# Study Overview

> **Research Question** 
>
> Do you think the type of background music playing in a restaurant influences the amount of money that diners spend on their meal?

## Factorial ANOVA

Next week, the lab will focus on Experiment 3 described above. You have already worked with *some* of this data before - see [semester 1 week 8 lab](https://uoepsy.github.io/dapr2/labs/1_08_int_cc.html), but we now have a third task condition - Classification. 

__Data download link:__ https://uoepsy.github.io/data/cognitive_experiment.csv

We have data from the 45 participants (15 amnesiacs, 15 Huntington individuals, and 15 controls). Recall that study involves two factors, now with three levels each. For each combination of factor levels we have 5 observations:

```{r echo=FALSE}
df <- read_csv('https://uoepsy.github.io/data/cognitive_experiment.csv')
# head(df)

df$Diagnosis <- factor(df$Diagnosis, 
                       labels = c("amnesic", "huntingtons", "control"),
                       ordered = FALSE)

df$Task <- factor(df$Task, 
                  labels = c("grammar", "classification", "recognition"), 
                  ordered = FALSE)

library(kableExtra)

df %>% 
    pivot_wider(names_from = 'Task', values_from = 'Y') %>% 
    kable() %>%
    kable_styling(full_width = FALSE) %>%
    add_header_above(c(" " = 1, "Task" = 3))
```


The five observations are assumed to come from a population having a specific mean. The population means corresponding to each combination of factor levels can be schematically written as:


$$
\begin{matrix}
                   &         &         & \textbf{Task} & \\
                   &         & (j=1)\text{ grammar} & (j=2)\text{ classification} & (j=3)\text{ recognition} \\
                   & (i=1)\text{ control} & \mu_{1,1} & \mu_{1,2} & \mu_{1,3} \\
\textbf{Diagnosis} & (i=2)\text{ amnesic} & \mu_{2,1} & \mu_{2,2} & \mu_{2,3} \\
                   & (i=3)\text{ huntingtons} & \mu_{3,1} & \mu_{3,2} & \mu_{3,3}
\end{matrix}
$$


`r qbegin(1)`
Repeat the steps outlined in the Semester 1 Week 8 lab, but using the new dataset.

- Read the cognitive experiment data into R. 
- Convert categorical variables into factors, and assign more informative labels to the factor levels according to the data description provided above.
- Relevel the `Diagnosis` factor to have 'Control' as the reference group.
- Relevel the `Task` factor to have 'Recognition' as the reference group. 
- Rename the response variable from `Y` to `Score`.
- Describe the data.
- Visualise the interaction between Diagnosis and Task.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Load the tidyverse library and read the data into R:

```{r}
cog <- read_csv('https://uoepsy.github.io/data/cognitive_experiment.csv')
head(cog)
```

We will now convert `Diagnosis` and `Task` into factors, making the labels of each factor level more meaningful. 

According to the data description, the encoding of the factor `Diagnosis` is: 1 = amnesic patients, 2 = Huntingtons patients, and 3 are control patients.The encoding for the factor `Task` is: 1 = grammar task, 2 = classification task, and 3 = recognition task.

```{r}
cog$Diagnosis <- factor(cog$Diagnosis, 
                        labels = c("amnesic", "huntingtons", "control"), 
                        ordered = FALSE)
cog$Task <- factor(cog$Task, 
                   labels = c("grammar", "classification", "recognition"), 
                   ordered = FALSE)
```

Relevel the `Diagnosis` factor so that the reference group is "Control":

```{r}
cog$Diagnosis <- fct_relevel(cog$Diagnosis, "control")
cog$Task <- fct_relevel(cog$Task, "recognition")
```

Rename the response:

```{r}
cog <- cog %>%
    rename(Score = Y)
```

Look at the data:

```{r}
head(cog)
```

Describe data:
```{r}
cog_stats <- cog %>% 
    group_by(Diagnosis, Task) %>%
    summarise(
        Avg_Score = mean(Score), 
        SD = sd(Score),
        SE = sd(Score) / sqrt(n())
        )

cog_stats
```

Since we have not yet fitted our model, we cannot use the `emmip` function from `emmeans` or `plot_model` from `sjPlot`. We can however use a simple `ggplot` and use our summary scores from above:

```{r}
ggplot(data = cog_stats, aes(x = Task, y = Avg_Score, color = Diagnosis)) +
    geom_point(size = 3) +
    geom_line(aes(x = as.numeric(Task)))
```

:::int

Control patients consistently perform best across all tasks. They don't seem to differ substantially in their scores between grammar and classification tasks, but they clearly perform better in the recognition task than the grammar and classification ones.

Amnesic patients appear to perform better than Huntingtons patients in grammar an classification tasks (reflecting intrinsic memory processes) and perform worse than Huntingtons patients in the recognition task (reflecting extrinsic memory processes).

:::

`r solend()`

`r qbegin(2)`

The model with interaction is:


\begin{aligned}
Score &= \beta_0 \\
      &+ \beta_1 D_\text{Control} + \beta_2 D_\text{Amnesic}   \\
      &+ \beta_3 T_\text{Recognition}  + \beta_4 D_\text{Grammar} \\
      &+ \beta_5 (D_\text{Control} * T_\text{Recognition}) + \beta_6 (D_\text{Amnesic} * T_\text{Recognition})  \\
      &+ \beta_7 (D_\text{Control} * T_\text{Grammar}) + \beta_8 (D_\text{Amnesic} * T_\text{Grammar})  \\
      &+ \epsilon 
\end{aligned}


Fit the above model, and set the the sum to zero constraint for `Diagnosis` of 'Control' and `Task` of 'Recognition'.

Applying the sum to zero constraint, we would have:

\begin{aligned}
\text{Intercept (global mean)} &= \beta_0 \frac{\mu_{1,1} + \mu_{1,2} + \cdots + \mu_{3,3}}{9} \\
\beta_{Huntingtons} &= -(\beta_1 + \beta_2) \\
\beta_{Classification} &= -(\beta_3 + \beta_4) \\
\beta_{Huntingtons:Classification} &= -(\beta_5 + \beta_6 + \beta_7 + \beta_8)
\end{aligned}

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

The fitted model should be:

```{r}
contrasts(cog$Diagnosis) <- "contr.sum"
contrasts(cog$Task) <- "contr.sum"
mdl_int1 <- lm(Score ~ Diagnosis * Task, data = cog)
summary(mdl_int1)
anova(mdl_int1)
```

Next week, we will explore the `summary()` and `anova()` output in detail. If you have time, note down what you think these estimates might be telling us. Remember, the interpretation of the estimates is now *different* from what you had in Semester 1 Week 8 - now each beta coefficient is the difference between a group mean and the **overall** mean. 

`r solend()`


# Recap

You have (hopefully) already made a head start on this weeks exercises if you completed the Factorial ANOVA section of [last week's lab](https://uoepsy.github.io/dapr2/labs/2_03_cont_design_factanova.html#Factorial_ANOVA). If you haven't yet completed these two questions, do so before reading any further. 

In this week's exercises, we will further explore questions such as:

- Does level $i$ of the first factor have an effect on the response?
- Does level $j$ of the second factor have an effect on the response?
- Is there a combined effect of level $i$ of the first factor and level $j$ of the second factor on the response? In other words, is there interaction of the two factors so that the combined effect is not simply the additive effect of level $i$ of the first factor plus the effect of level $j$ of the second factor?

## Research question and data

As a reminder, we are working with data from a study yielding a $3 \times 3$ factorial design to test whether there are differences in types of memory deficits for those experiencing different cognitive impairment(s).

```{r echo=FALSE}
library(tidyverse)
cog <- read_csv('https://uoepsy.github.io/data/cognitive_experiment.csv')
# head(df)

cog$Diagnosis <- factor(cog$Diagnosis, 
                       labels = c("amnesic", "huntingtons", "control"),
                       ordered = FALSE)

cog$Task <- factor(cog$Task, 
                  labels = c("grammar", "classification", "recognition"), 
                  ordered = FALSE)

cog$Diagnosis <- fct_relevel(cog$Diagnosis, "control")
cog$Task <- fct_relevel(cog$Task, "recognition")

cog <- cog %>%
    rename(Score = Y)

library(kableExtra)

cog %>% 
    pivot_wider(names_from = 'Task', values_from = 'Score') %>% 
    kable() %>%
    kable_styling(full_width = FALSE) %>%
    add_header_above(c(" " = 1, "Task" = 3))
```


# Interaction Model

`r qbegin(1)`
Let's look at the `summary()` and `anova()` output in detail from the model you should have previously fitted with the sum to zero constraint. As a reminder, the model with interaction is:

\begin{aligned}
Score &= \beta_0 \\
      &+ \beta_1 D_\text{Control} + \beta_2 D_\text{Amnesic}   \\
      &+ \beta_3 T_\text{Recognition}  + \beta_4 T_\text{Grammar} \\
      &+ \beta_5 (D_\text{Control} * T_\text{Recognition}) + \beta_6 (D_\text{Amnesic} * T_\text{Recognition})  \\
      &+ \beta_7 (D_\text{Control} * T_\text{Grammar}) + \beta_8 (D_\text{Amnesic} * T_\text{Grammar})  \\
      &+ \epsilon 
\end{aligned}

Applying the sum to zero constraint (for `Diagnosis` of 'Control' and `Task` of 'Recognition'), we would have:

\begin{aligned}
\text{Intercept (global mean)} &= \beta_0 \frac{\mu_{1,1} + \mu_{1,2} + \cdots + \mu_{3,3}}{9} \\
\beta_{Huntingtons} &= -(\beta_1 + \beta_2) \\
\beta_{Classification} &= -(\beta_3 + \beta_4) \\
\beta_{Huntingtons:Classification} &= -(\beta_5 + \beta_6 + \beta_7 + \beta_8)
\end{aligned}
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Let's look at the `anova()` and `summary()` output:
```{r}
contrasts(cog$Diagnosis) <- "contr.sum"
contrasts(cog$Task) <- "contr.sum"
mdl_int <- lm(Score ~ Diagnosis * Task, data = cog)
anova(mdl_int)
```

:::int
Scores significantly differed by both Diagnosis ($F(2, 36)=16.63, p < .001$) and by Task ($F(2, 36)=16.63, p < .001$). The interaction between Diagnosis and Task was significant ($F(4, 36)=7.92, p < 0.001$). This provides evidence against the null hypothesis that effect of Task is constant across the different levels of Diagnosis.
:::

```{r}
summary(mdl_int)
```

:::int
The F-test for model utility is again significant at the 5\% level: $F(8,36) = 12.28, p < .001$. An F-statistic this large or larger occurring by chance only is very small. In the presence of a significant interaction we **do not** interpret the main effects as their interpretation changes with the level of the other factor.
:::

`r solend()`

`r qbegin(2)`
Based on previous plotting, it does seem that there is a clear interaction between diagnosis and task (as well as from our output above). However, we have not statistically compared our interaction model to an additive model (same model but without the interaction). Until we do so, we cannot confidently progress on the *assumption* that the interaction model is the most suitable for answering the research question.

We also want to consider the best coding constraint to apply in order to best answer the research question  - are we interested in whether group X (e.g., Amnesic) differed from group Y (e.g., Huntingtons), or whether group X (e.g., Amnesic) differed from the overall group mean? 


Since we are interested in comparing groups, we should reset to dummy coding, and thus should re-run our interaction model. Next, we need to perform a model comparison between the additive model and the interaction model using the `anova()` function. 

After re-running your model with dummy coding, interpret the result of the model comparison.


\begin{aligned}
Additive Model: Score &= \beta_0 \\
      &+ \beta_1 D_\text{Amnseic} + \beta_2 D_\text{Huntingtons}   \\
      &+ \beta_3 T_\text{Grammar}  + \beta_4 T_\text{Classification} \\
      &+ \epsilon 
\end{aligned}

\begin{aligned}
Interaction Model: Score &= \beta_0 \\
      &+ \beta_1 D_\text{Amnseic} + \beta_2 D_\text{Huntingtons}   \\
      &+ \beta_3 T_\text{Grammar}  + \beta_4 T_\text{Classification} \\
      &+ \beta_5 (D_\text{Amnseic} * T_\text{Grammar}) + \beta_6 (D_\text{Huntingtons} * T_\text{Grammar})  \\
      &+ \beta_7 (D_\text{Amnseic} * T_\text{Classification}) + \beta_8 (D_\text{Huntingtons} * T_\text{Classification})  \\
      &+ \epsilon 
\end{aligned}




`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Switch back to dummy coding:
```{r}
contrasts(cog$Diagnosis) <- "contr.treatment"
contrasts(cog$Task) <- "contr.treatment"
```

Build additive model and re-run interaction model:
```{r}
mdl_add <- lm(Score ~ Diagnosis + Task, data = cog)
mdl_int <- lm(Score ~ Diagnosis * Task, data = cog)
```


The relevant function is `anova()` with the two models as inputs to conduct a model comparison:
```{r}
anova(mdl_add, mdl_int)
```

:::int
We performed an F-test to compare two nested models: an additive two-factor ANOVA against a two-factor model with interaction. The test results are $F(4, 36) = 7.92, p < .001$.

At the 5\% significance level, the probability of obtaining an F-statistic as large as 7.92 or larger is <.001. 

Hence, the comparison of nested models provides evidence against the additive effects model, suggesting that we should use the interaction model as each factor has a different effect on the response depending the level of the other factor.
:::
`r solend()`


`r qbegin(3)`
Using `plot_model()` (note that this function is from `sjPlot` package - make sure that you load this), generate a plot showing the predicted mean scores for each combination of levels of the diagnosis and task factors.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

To visualize the interaction between Diagnosis and Task, we will use `plot_model()`, and ask for a plot of predicted values by specifying `type = "pred"`. Because the interaction involves the two variables, we need to specify a list of the `terms` (or names) from the model that should be plotted.

```{r}
library(sjPlot)
plot_model(mdl_int, type = "pred", terms = c("Diagnosis", "Task"))
```
`r solend()`

# Contrast analysis

We will begin by looking at each factor separately. 

In terms of the diagnostic groups, recall that we want to compare the amnesiacs to the Huntington individuals. This corresponds to a contrast with coefficients of 0, 1, and −1, for control, amnesic, and Huntingtons, respectively. 

Similarly, in terms of the tasks, we want to compare the average of the two implicit memory tasks with the explicit memory task. This corresponds to a contrast with coefficients of 0.5, 0.5, and −1 for the three tasks. 

When we are in presence of a significant interaction, the coefficients for a contrast between the means are found by multiplying each row coefficient with all column coefficients as shown below:

```{r echo=FALSE, out.width = '100%'}
knitr::include_graphics('images/contr_interaction.png')
```

This can be done in R using:

```{r}
diag_coef  <- c('control' = 0, 'amnesic' = 1, 'huntingtons' = -1)
task_coef  <- c('grammar' = 0.5, 'classification' = 0.5, 'recognition' = -1)
contr_coef <- outer(diag_coef, task_coef)   # or: diag_coef %o% task_coef
contr_coef
```

The above coefficients correspond to testing the null hypothesis

$$
H_0 : \frac{\mu_{2,1} + \mu_{2,2}}{2} - \mu_{2,3} - \left( \frac{\mu_{3,1} + \mu_{3,2}}{2} - \mu_{3,3} \right) = 0
$$

or, equivalently,

$$
H_0 : \frac{\mu_{2,1} + \mu_{2,2}}{2} - \mu_{2,3} = \frac{\mu_{3,1} + \mu_{3,2}}{2} - \mu_{3,3}
$$

which says that, in the population, the difference between the mean implicit memory and the explicit memory score is the same for amnesic patients and Huntingtons individuals. Note that the scores for the grammar and classification tasks have been averaged to obtain a single measure of 'implicit memory' score.


Now that we have the coefficients, let's call the `emmeans` function (this is helpful to look at the ordering of the groups):
```{r}
library(emmeans)
emm <- emmeans(mdl_int, ~ Diagnosis*Task)
emm
```

Next, from `contr_coef`, insert the coefficients following the order specified by the rows of `emm` above. That is, the first one should be for `control` `recognition` and have a value of 0, the second for `amnesic` `recognition` with a value of -1, and so on...

We also give a name to this contrast, such as 'Research Hyp'.

```{r}
comp_res <- contrast(emm, method = list('Research Hyp' = c(0, -1, 1, 0, 0.5, -0.5, 0, 0.5, -0.5)))
comp_res
confint(comp_res)

```

or:

```{r}
summary(comp_res, infer = TRUE)
```

`r qbegin(4)`

Interpret the results of the contrast analysis.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

The contrast analysis yielded a $t$-value of 5.4 and a corresponding $p$-value < .001. Thus, there is evidence that the contrast is not zero in the population. In other words, Amnesics and Huntingtons patients differ in the difference between implicit and explicit recognition memory tasks. 

The contrast analysis shows that the 95\% confidence interval for our contrast stretches from 32.8 to 72.2. This interval does not contain zero. Thus, we can be 95\% confident that the task difference is not the same for amnesiacs as for Huntingtons, which is why we can reject the null hypothesis that the difference in differences is zero. 

`r solend()`

# Simple Effects

By considering the simple effects, we can identify at which levels of the interacting condition we see different effects. 

`r qbegin(5)`

Since we have a significant interaction, we should also look at the simple main effects. Simple effects are the effect of one factor (e.g., Task) at each level of another factor (e.g., Diagnosis - Control, Huntingtons, and Amnesic).

Examine the simple effects for Task at each level of Diagnosis; and then the simple effects for Diagnosis at each level of Task. 


`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
mdl_int_simple1 <- pairs(emm, simple = "Task")
mdl_int_simple1

mdl_int_simple2 <- pairs(emm, simple = "Diagnosis")
mdl_int_simple2
```

From `mdl_int_simple1` we can see the differences between between tasks for each diagnosis group, and from  `mdl_int_simple2` the differences between diagnoses for each task group.

`r solend()`

`r qbegin(6)`
There are various ways we can create an interaction plot, for instance, try this code: 
```{r eval=F}
emmip(mdl_int, Diagnosis ~ Task, CIs = TRUE)
```
Considering the simple effects that we just saw in Question 5, identify the significant effects and match them to the parts of an interaction plot. 

`r optbegin("Optional: You can change what is plotted on the x-axis", olabel=F,toggle=params$TOGGLE)`
We can switch what is on the x-axis pretty easily: 
```{r, eval=F, out.width="90%"}
library(patchwork) #loading this package so that I can display plots in a grid
plt_1 <- emmip(mdl_int, Diagnosis ~ Task, CIs = TRUE)
plt_2 <- emmip(mdl_int, Task ~ Diagnosis, CIs = TRUE)
plt_1 / plt_2
```
`r optend()`
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
For the simple effects of Task, the significant differences (those for which $p<.05$) are only seen in the Huntingtons group, between recognition & grammar and recognition & classification tasks.   
In the plot below, we can see this is the left-most blue point compared to the middle blue point, and then compared to the right-most blue point.  
```{r}
emmip(mdl_int, Diagnosis ~ Task, CIs = TRUE)
```

<hr> 

For the simple effects of Diagnosis, we saw significant differences

- in the recognition task, between control & amnesic   
  (in the plot below: left-most red point to middle red-point)  
- in the recognition task, between amnesic & huntingtons   
  (in the plot below: middle red-point to right-most red point)  
- in the grammar task, between control & huntingtons   
  (in the plot below: left-most green point to right-most green point)
- in the classification task, between control & huntingtons   
  (in the plot below: left-most blue point to right-most blue point)

```{r}
emmip(mdl_int, Task ~ Diagnosis, CIs = TRUE)
```


`r solend()`


# Pairwise Comparisons

`r qbegin(7)`

Conduct exploratory pairwise comparisons to compare all levels of Diagnosis with all levels of Task.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
pairs_res <- pairs(emm)
pairs_res

#can also plot if you'd like:
plot(pairs_res)
```

From the above, we can see comparisons for all different possible pairs of diagnosis-task combinations. As well as recapping assumption checks, next week we will also explore how we can adjust our pairwise comparisons (*think* about how many comparisons were conducted above - without adjusting our $\alpha$ (or $p$-value), why might any inferences drawn be problematic?)


`r solend()`

# Multiple Comparisons  

In last week's exercises we began to look at how we compare different groups, by using contrast analysis to conduct tests of specific comparisons between groups. We also saw how we might conduct *pairwise comparisons*, where we test all possible pairs of group means within a given set.  

For instance, we can compare the means of the different diagnosis groups for each task: 
```{r}
emm_task <- emmeans(mdl_int, ~ Diagnosis | Task)
contr_task <- contrast(emm_task, method = 'pairwise')
contr_task
```

or we can test all different combinations of task and diagnosis group (if that was something we were theoretically interested in, which is unlikely!) which would equate to conducting 36 comparisons!  

```{r}
emm_task <- emmeans(mdl_int, ~ Diagnosis * Task)
contr_task <- contrast(emm_task, method = 'pairwise')
contr_task
```

`r optbegin("36? how do we know there are 36?", olabel=FALSE, toggle=params$TOGGLE)`
There are 3 diagnosis groups, and 3 tasks, meaning there are 9 different group means. All possible pairwise comparisons would is all different possible combinations of 2 from a set of 9.  We can work this out using the rule: 

$$
_nC_r = \frac{n!}{r!(n-r)!} \\
\begin{align} \\
& \text{Where:} \\
& n = \text{total number in the set} \\
& r = \text{number chosen} \\
& _nC_r = \text{number of combinations of r from n} \\
\end{align}
$$

In **R**:
```{r}
factorial(9)/(factorial(2)*(factorial(9-2)))
```

Or, easier still:

```{r}
dim(combn(9, 2))
```
`r optend()`

## Why does the number of tests matter?  

`r optbegin("refresher on making errors in hypothesis tests", olabel=FALSE,toggle=params$TOGGLE)`  
Think back to ["Type 1 errors" from DAPR1](https://uoe-psychology.github.io/uoe_psystats/dapr1/book/chap-typeerror.html) - when we conduct an hypothesis test, and we set $\alpha=0.05$, we will reject the null hypothesis $H_0$ when we find a $p < .05$. Now remember what a $p$-value represents - it is the chance of observing a statistic at least as extreme as the one we do have, assuming the null hypothesis to be true. This means that *if* $H_0$ **is** true, then we will still observe a $p < .05$ 5\% of the time. So our chance of making this error = the threshold ($\alpha$) at which below a p-value results in us rejecting $H_0$. 
`r optend()` 

But this error-rate applies to each statistical hypothesis we test. So if we conduct an experiment in which we plan on conducting lots of tests of different comparisons, the chance of an error being made increases substantially. Across the family of tests performed that chance will be much higher than 5\%.^[what defines a 'family' of tests is debateable.]

Each test conducted at $\alpha = 0.05$ has a 0.05 (or 5%) probability of Type I error (wrongly rejecting the null hypothesis). If we do 9 tests, that experimentwise error rate is $\alpha_{ew} \leq 9 \times 0.05$, where 9 is the number of comparisons made as part of the experiment.

Thus, if nine **independent** comparisons were made at the $\alpha = 0.05$ level, the experimentwise Type I error rate $\alpha_{ew}$ would be at most $9 \times 0.05 = 0.45$. That is, we could wrongly reject the null hypothesis on average 45 times out of 100. 
To make this more confusing, many of the tests in a family are not **independent** (see the lecture slides for the calculation of error rate for dependent tests).  

Here, we go through some of the different options available to us to control, or 'correct' for this problem. 

# Corrections

## Bonferroni

`r qbegin(2)`

Load the data from last week, and re-acquaint yourself with it. 

Provide a plot of the Diagnosis*Task group mean scores.  

The data is at https://uoepsy.github.io/data/cognitive_experiment.csv. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r eval=FALSE}
cog <- read_csv('https://uoepsy.github.io/data/cognitive_experiment.csv')
# head(df)

cog$Diagnosis <- factor(cog$Diagnosis, 
                       labels = c("amnesic", "huntingtons", "control"),
                       ordered = FALSE)

cog$Task <- factor(cog$Task, 
                  labels = c("grammar", "classification", "recognition"), 
                  ordered = FALSE)

cog$Diagnosis <- fct_relevel(cog$Diagnosis, "control")
cog$Task <- fct_relevel(cog$Task, "recognition")

cog <- cog %>%
    rename(Score = Y)
```
`r solend()`

`r qbegin(3)`

Fit the interaction model, using `lm()`. 
Pass your model to the `anova()` function, to remind yourself that there is a significant interaction present.  

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
mdl_int <- lm(Score ~ Task*Diagnosis, data = cog)
anova(mdl_int)
```
`r solend()`

`r qbegin(4)`
There are various ways to make nice tables in RMarkdown.  
Some of the most well known are:

- The **knitr** package has `kable()`  
- The **pander** package has `pander()`  

Pick one (or find go googling and find a package you like the look of), install the package (if you don't already have it), then try to create a nice pretty ANOVA table rather than the one given by `anova(model)`.  

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
library(knitr)
kable(anova(mdl_int))

library(pander)
pander(anova(mdl_int))
```

`r solend()`

`r qbegin(5)`
As in the previous week's exercises, let us suppose that we are specifically interested in comparisons of the mean score *across the different diagnosis groups for a given task*.  

Edit the code below to obtain the pairwise comparisons of diagnosis groups for each task. Use the Bonferroni method to adjust for multiple comparisons, and then obtain confidence intervals.  

```{r eval=FALSE}
library(emmeans)
emm_task <- emmeans(mdl_int, ? )
contr_task <- contrast(emm_task, method = ?, adjust = ? )
```

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
emm_task <- emmeans(mdl_int, ~ Diagnosis | Task)
contr_task <- contrast(emm_task, method = "pairwise", adjust="bonferroni")
contr_task
confint(contr_task)
```
`r solend()`

:::frame
**adjusting $\alpha$, adjusting p**

In the lecture we talked about adjusting the $\alpha$ level (i.e., instead of determining significance at $p < .05$, we might adjust and determine a result to be statistically significant if $p < .005$, depending on how many tests are in our family of tests).  

Note what the functions in R do is adjust the $p$-value, rather than the $\alpha$. The Bonferroni method simply multiplies the 'raw' p-value by the number of the tests. 

:::


`r qbegin(6)`
In question 4 above, there are 9 tests being performed, but there are 3 in each 'family' (each `Task`). 

Try changing your answer to question 4 to use `adjust = "none"`, rather than `"bonferroni"`, and confirm that the p-values are 1/3 of the size. 
  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

The first Bonferroni adjusted p-value is 0.0492. 

```{r}
0.0492/3
```

Let's check that this is the raw p-value:
```{r}
contrast(emm_task, method = "pairwise", adjust="none")
```

`r solend()`

## Šídák

`r qbegin(7)`
The Sidak approach is slightly less conservative than the Bonferroni adjustment. Doing this with the **emmeans** package is easy, can you figure out how?  

**Hint:** you just have to change the `adjust` argument in `contrast()` function. 

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
contrast(emm_task, method = "pairwise", adjust = "sidak")
```

`r solend()`


## Tukey

`r qbegin(8)`

Like with Šídák, in R we can easily change to Tukey. Conduct pairwise comparisons of the scores of different Diagnosis groups on different Task types (i.e., the interaction), and use the Tukey adjustment.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
emm_task <- emmeans(mdl_int, ~ Diagnosis*Task)
contr_task <- contrast(emm_task, method = "pairwise", adjust="tukey")
contr_task
```

We can also use the following, which doesn't require the **emmeans** package. You might see this when you look online for resources. The `aov()` function is fitting an ANOVA model, and then `TukeyHSD()` compares between Diagnosis group; between Task type; and between Diagnosis*Task.  

Run the code below yourself to see the output. 

```{r eval=FALSE}
TukeyHSD(aov(Score ~ Diagnosis * Task, data = cog))
```

`r solend()`

## Scheffe  

`r qbegin(9)`

Run the same pairwise comparison as above, but this time with the Scheffe adjustment.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
emm_task <- emmeans(mdl_int, ~ Diagnosis * Task)
contr_task <- contrast(emm_task, method = "pairwise", adjust="scheffe")
contr_task
```

`r solend()`
