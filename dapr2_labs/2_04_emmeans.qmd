---
title: "Simple Effects, Pairwise Comparisons, Corrections"
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
source('assets/setup.R')

set.seed(3)

library(DT)
library(tidyverse)
```

:::lo

### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand how to interpret simple effects for experimental designs
2. Understand how to conduct pairwise comparisons
3. Understand how to apply corrections available for multiple comparisons

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1. Be up to date with lectures
2. Have completed previous lab exercises from [Week 1](https://uoepsy.github.io/dapr2/2223/labs/2_01_model_comps.html), [Week 2](https://uoepsy.github.io/dapr2/2223/labs/2_02_effects.html), and [Week 3](https://uoepsy.github.io/dapr2/2223/labs/2_03_contrasts.html)

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse** 
* **psych** 
* **kableExtra**
* **stargazer**
* **patchwork**
* **emmeans**

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/cognitive_experiment.csv) or read it in via this link https://uoepsy.github.io/data/cognitive_experiment.csv. Note, you have already worked with *some* of this data before - see [Semester 1 week 8 lab](https://uoepsy.github.io/dapr2/labs/1_08_int_cc.html), but we now have a third Task condition - Classification. 

:::

# Study Overview

> **Research Question** 
>
> Are there differences in types of memory deficits for those experiencing different cognitive impairment(s)?

In this week's exercises, we will further explore questions such as:

- Does level $i$ of the first factor have an effect on the response?
- Does level $j$ of the second factor have an effect on the response?
- Is there a combined effect of level $i$ of the first factor and level $j$ of the second factor on the response? In other words, is there interaction of the two factors so that the combined effect is not simply the additive effect of level $i$ of the first factor plus the effect of level $j$ of the second factor?

`r optbegin("Cognitive Exp 3x3 Codebook", olabel=FALSE, toggle=params$TOGGLE)` 

__Description__

The researchers designed a study yielding a $3 \times 3$ factorial design to test whether there are differences in types of memory deficits for those experiencing different cognitive impairment(s).

 The first factor, "Diagnosis", classifies the three types of individuals:

- 1 denotes amnesic patients;
- 2 denotes Huntingtons patients; and
- 3 denotes a control group of individuals with no known neurological disorder.

The second factor, "Task", tells us to which of two tasks each study participant was randomly assigned to:

- 1 = grammar task, which consists of classifying letter sequences as either following or not following grammatical rules;
- 2 = classification task, which consists of classifying stimuli into certain groupings, based on previously indicated information about the groups characteristics; and
- 3 = recognition task, which consists of recognising particular stimuli as stimuli that have previously been presented during the task.

The tasks chosen by the researchers have been picked to map onto the theoretical differences between the three types of research participants. The Grammar and Classification tasks are known to reflect *implicit* memory processes, whereas the recognition task is known to reflect *explicit* memory processes. If the theory is correct, we would expect the difference in scores between the recognition and grammar/classification tasks to be relatively similar for the control and amnesiac groups, but relatively larger for the Huntingtons group compared to controls.

__Preview__

We have data from the 45 participants (15 amnesiacs, 15 Huntington individuals, and 15 controls). Recall that study involves two factors, now with three levels each. For each combination of factor levels we have 5 observations:

The first ten rows of the data are:

```{r echo=FALSE, message=FALSE}
library(tidyverse)
df <- read_csv('https://uoepsy.github.io/data/cognitive_experiment.csv')
# head(df)

df$Diagnosis <- factor(df$Diagnosis, 
                       labels = c("amnesic", "huntingtons", "control"),
                       ordered = FALSE)

df$Task <- factor(df$Task, 
                  labels = c("grammar", "classification", "recognition"), 
                  ordered = FALSE)

library(kableExtra)

df %>% 
    pivot_wider(names_from = 'Task', values_from = 'Y', values_fn = list) %>% 
    kable() %>%
    kable_styling(full_width = FALSE) %>%
    add_header_above(c(" " = 1, "Task" = 3))
```

The five observations are assumed to come from a population having a specific mean. The population means corresponding to each combination of factor levels can be schematically written as:

$$
\begin{matrix}
                   &         &         & \textbf{Task} & \\
                   &         & (j=1)\text{ grammar} & (j=2)\text{ classification} & (j=3)\text{ recognition} \\
                   & (i=1)\text{ control} & \mu_{1,1} & \mu_{1,2} & \mu_{1,3} \\
\textbf{Diagnosis} & (i=2)\text{ amnesic} & \mu_{2,1} & \mu_{2,2} & \mu_{2,3} \\
                   & (i=3)\text{ huntingtons} & \mu_{3,1} & \mu_{3,2} & \mu_{3,3}
\end{matrix}
$$
`r optend()`

# Setup

`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read the cognitive_experiment dataset into R, assigning it to an object named `cog` 
 
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r message=FALSE}
#load packages
library(tidyverse)
library(psych)
library(kableExtra)
library(emmeans)
library(stargazer)
library(patchwork)

#read in data
cog <- read_csv('https://uoepsy.github.io/data/cognitive_experiment.csv')
```

`r solend()`

<br>

`r qbegin(1)`

Firstly, examine the dataset, and perform any necessary and appropriate data management steps.

Next, consider would be the most appropriate coding constraint to apply in order to best address the research question - i.e., are we interested in whether group X (e.g., Amnesic) differed from group Y (e.g., Huntingtons), or whether group X (e.g., Amnesic) differed from the global/grand mean?

Make any appropriate changes to reference levels if required based on your decision above. 

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

First step is to always look at the data we are working with - `str()` or `head()` are a good place to start

```{r}
str(cog)
```

Convert `Diagnosis` and `Task` into factors, making the labels of each factor level more meaningful. 

According to the data description, the encoding of the factor `Diagnosis` is: 1 = amnesic patients, 2 = Huntingtons patients, and 3 are control patients. The encoding for the factor `Task` is: 1 = grammar task, 2 = classification task, and 3 = recognition task.

```{r}
cog$Diagnosis <- factor(cog$Diagnosis, 
                        labels = c("amnesic", "huntingtons", "control"), 
                        ordered = FALSE)
cog$Task <- factor(cog$Task, 
                   labels = c("grammar", "classification", "recognition"), 
                   ordered = FALSE)
```

Rename the response so that we have a more meaningful name than "Y":

```{r}
cog <- cog %>%
    rename(Score = Y)
```

Re-examine the data:

```{r}
head(cog)
```

Check for NAs:
```{r}
#check for NAs - there are none - all FALSE
table(is.na(cog))
```

Since we are interested in comparing groups, we should use dummy coding. By default, `R` uses dummy coding, so we do not need to make any changes to the coding constraint. 
However, if we're going to have a reference group, we're likely to want it to be the Control group. Likewise for the Tasks, the recognition task measures 'explicity memory' whereas the other two measure 'implicit memory', so it arguably makes sense to use that as the reference group.  

```{r}
cog$Diagnosis <- fct_relevel(cog$Diagnosis, "control")
cog$Task <- fct_relevel(cog$Task, "recognition")
```

Now, let's summarise our data checks and cleaning: 

::: {.callout-important icon=false appearance="minimal"}

All participant data was complete, and categorical variables were coded as factors. In terms of reference groups, we re-leveled the `Diagnosis` factor so that the reference group was Control (since the other two groups have some form of cognitive impairment), and the `Task` factor to have Recognition as the reference group (since it measures explicit memory whilst the other two task types implicit). 

:::

`r solend()`

<br>

`r qbegin(2)`

Provide a table of descriptive statistics and visualise your data.

Remember to interpret your plot in the context of the study (i.e., comment on any observed differences among treatment groups). 

:::{.callout-tip appearance="simple" collapse="true"}

### Hint

1. For your table of descriptive statistics, both the `group_by()` and `summarise()` functions will come in handy here.
2. Recall that when visualising a continuous outcome across several groups, `geom_boxplot()` may be most appropriate to use.
3. Make sure to comment on any observed differences among the sample means of the four treatment conditions.

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Let's first produce a descriptive statistics table:

```{r message=FALSE, warning=FALSE}
#| label: tbl-cog-descript
#| tbl-cap: Descriptive Statistics
cog_stats <- cog %>% 
    group_by(Diagnosis, Task) %>%
    summarise(
        Avg_Score = mean(Score), 
        SD = sd(Score),
        SE = sd(Score) / sqrt(n()),
        Min = min(Score),
        Max = max(Score)) %>%
    kable(caption = "Descriptive Statistics", digits = 2) %>%
    kable_styling()

cog_stats
```

Since we have a continuous outcome and 2 categorical predictors - a boxplot would be most appropriate for visualisations:

```{r}
#| label: fig-cog-desc
#| fig-cap: "Association between Task Condition, Diagnosis, and Average Score"
cog_plt <- ggplot(data = cog, aes(x = Diagnosis, y = Score, color = Task)) +
  geom_boxplot() +
  labs(x = 'Diagnosis', y = 'Average Score')
cog_plt
```

::: {.callout-important icon=false appearance="minimal"}

+ Control patients consistently perform best across all tasks. They don't seem to differ substantially in their scores between grammar and classification tasks, but they clearly perform better in the recognition task than the grammar and classification ones.

+ Amnesic patients appear to perform better than Huntingtons patients in grammar an classification tasks (reflecting intrinsic memory processes) and perform worse than Huntingtons patients in the recognition task (reflecting extrinsic memory processes).

:::

`r solend()`

<br>

`r qbegin(3)`

Formally state:

+ the linear model(s) to investigate whether there are differences in Score based on Diagnosis and Task conditions
+ your chosen significance level
+ the null and alternative hypotheses

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Based on previous plotting, it does seem that there is a clear interaction between diagnosis and task. However, we have not **statistically compared our interaction model to an additive model** (same model but without the interaction). This is what the RQ warrants, and until we make this comparison, we cannot confidently progress on the *assumption* that the interaction model is the most suitable for answering the research question. Thus, we need to fit both models:

$$
\begin{aligned}
Additive Model: Score &= \beta_0 \\
      &+ \beta_1 D_\text{Amnseic} + \beta_2 D_\text{Huntingtons}   \\
      &+ \beta_3 T_\text{Grammar}  + \beta_4 T_\text{Classification} \\
      &+ \epsilon 
\end{aligned}
$$

$$
\begin{aligned}
Interaction Model: Score &= \beta_0 \\
      &+ \beta_1 D_\text{Amnseic} + \beta_2 D_\text{Huntingtons}   \\
      &+ \beta_3 T_\text{Grammar}  + \beta_4 T_\text{Classification} \\
      &+ \beta_5 (D_\text{Amnseic} * T_\text{Grammar}) + \beta_6 (D_\text{Huntingtons} * T_\text{Grammar})  \\
      &+ \beta_7 (D_\text{Amnseic} * T_\text{Classification}) + \beta_8 (D_\text{Huntingtons} * T_\text{Classification})  \\
      &+ \epsilon 
\end{aligned}
$$

Effects will be considered statistically significant at $\alpha=.05$

Our hypotheses are:

$H_0:$ All $\beta_j = 0$ (for $j = 5, 6, 7, 8$)

There are no significant differences in performance between explicit and implicit memory tasks for patients different cognitive impairment(s). 

$H_1:$ At least one $\beta_j \neq  0$ (for $j = 5, 6, 7, 8$)

There are significant differences in performance between explicit and implicit memory tasks for patients different cognitive impairment(s). 

We will statistically compare the additive and interaction model using `anova()` in order to determine which model best explains our data (i.e., the assocation between memory deficits and different cognitive impairments)

`r solend()`

<br>

`r qbegin(4)`

Fit the specified models, and assign them the names "mdl_add" and "mdl_int" as appropriate. 

Provide key model results from the two models in a single formatted table, and report the results of the model comparison in APA format.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Let's first fit our two models, and compare via `anova()`:

```{r}
#Additive Model
mdl_add <- lm(Score ~ Diagnosis + Task, data = cog)
summary(mdl_add)

#Interaction Model
mdl_int <- lm(Score ~ Diagnosis * Task, data = cog)
summary(mdl_int)

#Compare models using anova
anova(mdl_add, mdl_int)
```

Use the **stargazer** package to create table (note, you can also use `tab_model()` if you prefer:

```{r, results = 'asis', message = FALSE}
stargazer(mdl_add, mdl_int,
          title="Regression Model",
          dep.var.labels=c("Score"),
          covariate.labels=c("Intercept", "Diagnosis - Amnesic", "Diagnosis - Huntingtons", "Task - Grammar", "Task - Classification", "Diagnosis - Amnesic : Task - Grammar", "Diagnosis - Huntingtons : Task - Grammar", "Diagnosis - Amnesic : Task - Classification", "Diagnosis - Huntingtons : Task - Classification"),
          type = "html", 
          digits = 2,
          single.row=TRUE, 
          align=TRUE,
          intercept.bottom = FALSE)
```

::: {.callout-important icon=false appearance="minimal"}

We performed an $F$-test to compare two nested models: an additive two-factor model against a two-factor model with interaction. The test results are $F(4, 36) = 7.92, p < .001$.

At the 5\% significance level, the probability of obtaining an $F$-statistic as large as 7.92 or larger is <.001. 

Hence, the comparison of nested models provided evidence against the additive effects model, suggesting that we should use the interaction model as each factor has a different effect on the response depending the level of the other factor.

:::

`r solend()`

## Contrast analysis

We will now begin by looking at each factor separately. 

`r qbegin(5)`

In terms of the diagnostic groups, we want to compare the amnesiacs to the Huntington individuals. This corresponds to a contrast with coefficients of 0, 1, and −1, for control, amnesic, and Huntingtons, respectively. 

Similarly, in terms of the tasks, we want to compare the average of the two implicit memory tasks with the explicit memory task. This corresponds to a contrast with coefficients of 0.5, 0.5, and −1 for the three tasks. 

When we are in presence of a significant interaction, the coefficients for a contrast between the means are found by multiplying each row coefficient with all column coefficients as shown below:

```{r echo=FALSE, out.width = '100%'}
knitr::include_graphics('images/contr_interaction.png')
```

Specify the contrast weights in `R`, and state the null hypothesis of your contrast analysis. 

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

This can be done in `R` using:

::: {.panel-tabset}

## Option 1

```{r}
diag_coef  <- c('control' = 0, 'amnesic' = 1, 'huntingtons' = -1)
task_coef  <- c('grammar' = 0.5, 'classification' = 0.5, 'recognition' = -1)
contr_coef <- outer(diag_coef, task_coef)
contr_coef
```

## Option 2

```{r}
diag_coef  <- c('control' = 0, 'amnesic' = 1, 'huntingtons' = -1)
task_coef  <- c('grammar' = 0.5, 'classification' = 0.5, 'recognition' = -1)
contr_coef <- diag_coef %o% task_coef
contr_coef
```

:::

The above coefficients correspond to testing the null hypothesis

$$
H_0 : \frac{\mu_{2,1} + \mu_{2,2}}{2} - \mu_{2,3} - \left( \frac{\mu_{3,1} + \mu_{3,2}}{2} - \mu_{3,3} \right) = 0
$$

or, equivalently,

$$
H_0 : \frac{\mu_{2,1} + \mu_{2,2}}{2} - \mu_{2,3} = \frac{\mu_{3,1} + \mu_{3,2}}{2} - \mu_{3,3}
$$

which says that, in the population, the difference between the mean implicit memory and the explicit memory score is the same for amnesic patients and Huntingtons individuals. Note that the scores for the grammar and classification tasks have been averaged to obtain a single measure of 'implicit memory' score.

`r solend()`

<br>

`r qbegin(6)`

Run the contrast analysis, and interpret your output in the context of the research question. 

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Now that we have the coefficients, let's firstly call the `emmeans` function (this is helpful to look at the ordering of the groups):
```{r}
emm <- emmeans(mdl_int, ~ Diagnosis*Task)
emm
```

Next, from `contr_coef`, insert the coefficients **following the order specified by the rows of** `emm` above. That is, the first one should be for `control` `recognition` and have a value of 0, the second for `amnesic` `recognition` with a value of -1, and so on...

We also give a name to this contrast, such as 'Research Hyp'.

```{r}
comp_res <- contrast(emm, 
                     method = list('Research Hyp' = c(0, -1, 1, 0, 0.5, -0.5, 0, 0.5, -0.5))
                     )
```

Next, let's look at the output via one of two ways:

::: {.panel-tabset}

## Option 1

```{r}
comp_res
confint(comp_res)
```

## Option 2

```{r}
summary(comp_res, infer = TRUE)
```

:::

::: {.callout-important icon=false appearance="minimal"}

The contrast analysis yielded a $t$-value of 5.4 and a corresponding $p$-value < .001. Thus, there is evidence that the contrast is not zero in the population. In other words, Amnesics and Huntingtons patients differ in the difference between implicit and explicit recognition memory tasks. 

The contrast analysis shows that the 95\% confidence interval for our contrast stretches from 32.8 to 72.2. This interval does not contain zero. Thus, we can be 95\% confident that the task difference is not the same for amnesiacs as for Huntingtons, and therefore can reject the null hypothesis that the difference in differences is equal to zero. 

:::

`r solend()`

## Simple Effects

By considering the simple effects^[simple effects are the effects of one variable *at a specific level of another variable*], we can identify at which levels of the interacting condition we see different effects. 

`r qbegin(7)`

Examine the simple effects for Task at each level of Diagnosis; and then the simple effects for Diagnosis at each level of Task. 

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
mdl_int_simple1 <- pairs(emm, simple = "Task")
mdl_int_simple1

mdl_int_simple2 <- pairs(emm, simple = "Diagnosis")
mdl_int_simple2
```

From `mdl_int_simple1` we can see the differences between between tasks for each diagnosis group, and from  `mdl_int_simple2` the differences between diagnoses for each task group.

`r solend()`

<br>

`r qbegin(8)`

Visualise the interaction, displaying two plots - one with Diagnosis on the x-axis, and the other with Task on the x-axis.

Considering the simple effects that we noted above, identify the significant effects and match them to the parts of your interaction plot.

:::{.callout-tip appearance="simple" collapse="true"}

### Hint

To visualise the interaction, you can use `emmip()`.

Recall that the **patchwork** package allows us to arrange multiple plots using either `/` or `|` or `+`

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

First create our plots:
```{r}
plt_1 <- emmip(mdl_int, Diagnosis ~ Task, CIs = TRUE)
plt_2 <- emmip(mdl_int, Task ~ Diagnosis, CIs = TRUE)
plt_1 / plt_2
```

* Simple Effects of Task

For the simple effects of task (see `plt_1`), we saw the significant differences (those for which $p<.05$):

  * Only in the Huntingtons group, between recognition & grammar and recognition & classification tasks
  (top plot: left-most blue point compared to the middle blue point, and then compared to the right-most blue point)

* Simple Effects of Diagnosis

For the simple effects of Diagnosis (see `plt_2`), we saw significant differences: 

  * in the recognition task, between control & amnesic   
  (bottom plot: left-most red point to middle red point)  
  * in the recognition task, between amnesic & huntingtons   
  (bottom plot: middle red-point to right-most red point)  
  * in the grammar task, between control & amnesic  
  (bottom plot: left-most green point to middle green point)
  * in the grammar task, between control & huntingtons   
  (bottom plot: left-most green point to right-most green point)
  * in the grammar task, between amnesic & huntingtons   
  (bottom plot: middle green point to right-most green point)
  * in the classification task, between control & huntingtons   
  (bottom plot: left-most blue point to right-most blue point)
  * in the classification task, between amnesic & huntingtons   
  (bottom plot: middle blue point to right-most blue point)

`r solend()`

## Pairwise Comparisons & Multiple Corrections

`r qbegin(9)`

Conduct exploratory pairwise comparisons to compare all levels of Diagnosis with all levels of Task, applying no correction (note that Tukey will be automatically applied since we are comparing groups of means, so you will need to overwrite this).

Without adjusting our $\alpha$ (or $p$-value), why might any inferences drawn from your output be problematic?

:::{.callout-tip appearance="simple" collapse="true"}

### Hint

You can specify the adjustment using `adjust = " `. Possible options include:

+ `adjust = "none"`
+ `adjust =  "bonferroni"`
+ `adjust = "sidak"`
+ `adjust = "Tukey"`
+ `adjust = scheffe"`

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
pairs_res <- pairs(emm, adjust = "none")
pairs_res

#can also plot if you'd like:
plot(pairs_res)
```

From the above, we can see comparisons for all different possible pairs of diagnosis-task combinations. 

In total, there are 9 different estimates, but comparing them all means that we have 36 comparisons being tested! By not adjusting our $p$-value, we are increasing the experimentwise Type I error rate - we could wrongly reject the null hypothesis at a much higher rate than 5/100 (or 1/20 as is assumed when $\alpha = .05$). To overcome this, we might adjust and determine a result to be statistically significant if $p < .005$, as opposed to $p < .05$, depending on how many tests are in our family of tests).  

`r solend()`

<br>

`r qbegin(10)`

Select an appropriate method to adjust for multiple comparisons, and then obtain confidence intervals.

Comment on how these $p$-values differ from your raw (i.e., unadjusted) $p$-values.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Note what the functions in `R` do is adjust the $p$-value, rather than the $\alpha$. 

Since we're interested in all pairwise comparisons of means, the Tukey adjustment might be a sensible approach. However, we'll also show the Bonferroni to show how it differs (note, in practice you would only apply one correction and justify this choice based on your design - we are only applying two to note how they differ!)

Let's start with Tukey:

```{r}
contrast(emm, method = "pairwise", adjust="Tukey")
```

Note that 8 of the comparisons are no longer significant when using Tukey's adjustment, suggesting that these might have (when using no adjustment) been Type I errors! 


Next, lets look at Bonferroni:

```{r}
contrast(emm, method = "pairwise", adjust="bonferroni")
```

The first Bonferroni adjusted $p$-value is 0.0207. 
The raw (unadjusted) $p$-value from the previous question was 0.0005759265. 
The Bonferroni method simply multiplies the 'raw' $p$-value by the number of the tests, which we know is 36. 

```{r}
0.0005759265 * 36
```

In terms of differences in Bonferroni to raw $p$-values, they are thus 36 times the size.  

One benefit of Bonferroni is that it can be applied to any set of $p$-values, whereas Tukey only applies when comparing the means of levels of a factor. The downside, however, is that it may be overly conservative (i.e. reduce our power to detect an effect that is truly there).  

`r solend()`
