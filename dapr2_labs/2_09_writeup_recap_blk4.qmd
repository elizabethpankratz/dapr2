---
title: "Write Up Example & Block 4 Recap"
link-citations: TRUE
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source('assets/setup.R')

set.seed(953)

library(DT)
library(pander)
library(sjPlot)
library(tidyverse)

```

::: lo
### <i class="fa fa-graduation-cap"></i> Learning Objectives

At the end of this lab, you will:

1.  Understand how to write-up and provide interpretation of a binary logistic regression model

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1.  Be up to date with lectures
2.  Have completed previous lab exercises from [Semester 2 Week 7](https://uoepsy.github.io/dapr2/2324/labs/2_07_glm1.html) and [Semester 2 Week 8](https://uoepsy.github.io/dapr2/2324/labs/2_08_glm2.html) 

### <i class="fab fa-r-project"></i> Required R Packages

Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs).

For this lab, you will need to load the following package(s):

* **tidyverse** 
* **patchwork**
* **kableExtra**
* **psych**
* **sjPlot**

### <i class="fa fa-file"></i> Lab Data

You can download the data required for this lab [here](https://uoepsy.github.io/data/SenilityWAIS.csv) or read it in via this link https://uoepsy.github.io/data/SenilityWAIS.csv.
:::

# Section A: Write-Up

In this section of the lab you will be be presented with a research question, and tasked with writing up and presenting your analyses.

The aim in writing should be that a reader is able to more or less replicate your analyses **without** referring to your `R` code. This requires detailing all of the steps you took in conducting the analysis. The point of using RMarkdown is that you can pull your results **directly** from the code. If your analysis changes, so does your report!

Make sure that your final report doesn't show any R functions or code. Remember you are interpreting and reporting your results in text, tables, or plots, targeting a generic reader who may use different software or may not know R at all. If you need a reminder on how to hide code, format tables, etc., make sure to review the [rmd bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

::: callout-note
## Important - Write-Up Examples & Plagiarism

The example write-up sections included below are not **perfect** - they instead should give you a good example of what information you should include within each section, and how to structure this. For example, some information is missing (e.g., interpretation of descriptive statistics, what type of interaction is present), some information could be presented more clearly (e.g., variable names in tables, table/figure titles/captions, and rationales for choices), and writing could be more concise in places (e.g., discussion section is quite long).

Further, **you must not copy any of the write-up included below for future reports** - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more [here](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct).
:::

## Study Overview

> **Research Aim**
> 
> Explore the associations among the ability to delay gratification and age, visibility, and time of day.

> **Research Question**
> 
> + RQ: Does the probability of delaying gratification change as a function of age, marhsmallow visibility, and time of day?

`r optbegin("Marshmallows: Data Codebook", olabel=FALSE, toggle=params$TOGGLE)`

**Description**

The data used for this write-up exercise are simulated, drawing on the Stanford marshmallow experiment - a study on delayed gratification in children. The simulated data are loosely based on the findings of this work, and acted to expand upon the methods and results reported in the paper:

Mischel, W., Ebbesen, E. B., & Raskoff Zeiss, A. (1972). Cognitive and attentional mechanisms in delay of gratification. *Journal of Personality and Social Psychology, 21*(2), 204–218. [https://doi.org/10.1037/h0032198](https://doi.org/10.1037/h0032198)

In the current study, a sample of 304 children, ranging in ages from 3 to 10 years old, took part. Each child was shown a marshmallow, and it was explained that they were about to be left alone for 10 minutes. They were told that they were welcome to eat the marshmallow while they were waiting, but if the marshmallow was still there after 10 minutes, they would be rewarded with **two** marshmallows.

For half of the children who took part, the marshmallow was visible for the entire 10 minutes (or until they ate it!). For the other half, the marshmallow was placed under a plastic cup.

The experiment took part at various times throughout the working day, and researchers were worried about children being more hungry at certain times of day, so they kept track of whether each child completed the task in the morning or the afternoon, so that they could control for this in their analyses.

For an example of the marshmallow experiment, watch the following video:

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/watch?v=QX_oy9614HQ&t=60s" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>

__Data Dictionary__

The data in `mallow2.csv` contain five attributes collected from a simulated sample of $n=340$ hypothetical individuals, and includes:

```{r echo=FALSE, message=FALSE, warning=FALSE}
marshmallow <- read_csv("https://uoepsy.github.io/data/mallow2.csv")
tibble(
Variable = names(marshmallow),
Description = c("Participant name", "Age (in months)", "Time of day that the experiment took place ('am' = morning, 'pm' = afternoon)", "	Experimental condition - whether the marshmallow was 'visible' or 'hidden' for the 10 minutes", "Whether or not the participant took the marshmallow within the 10 minutes")
) %>% gt::gt()
```


__Preview__

The first six rows of the data are:

```{r echo=FALSE, message=FALSE}
read_csv('https://uoepsy.github.io/data/mallow2.csv') %>%  head %>% gt::gt()
```

`r optend()`

### Setup

`r qbegin("Setup", qlabel = FALSE)`

1.  Create a new RMarkdown file
2.  Load the required package(s)
3.  Read the mallow2 dataset into R, assigning it to an object named `marshmallow`

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

```{r message=FALSE}
library(tidyverse)
library(patchwork)
library(kableExtra)
library(psych) 
library(sjPlot)

#read in data
marshmallow <- read_csv("https://uoepsy.github.io/data/mallow2.csv")
```

`r solend()`

### Analysis Code

Try to answer the research question above without referring to the provided analysis code below, and then check how your script matches up - is there anything you missed or done differently? If so, discuss the differences with a tutor - there are lots of ways to code to the same solution!

`r optbegin("Provided Analysis Code", olabel=FALSE,toggle=params$TOGGLE)`

```{r}
######Step 1 ######
## TASKS: read in the data, then to check, clean, describe, and visualise it.


############## CHECK ###################

#check coding of variables - are they coded as they should be?
str(marshmallow)
head(marshmallow)
#time of day, visibility, and taken all currently coded as character variables when should be factors, need to fix this.
# age also in months - likely more useful to have in years given age included up to 10 years old.


### check range of values, NAs, etc. for each variable 

# age - should range 36-120 months
describe(marshmallow$agemonths)
# ISSUE - we have values WAY above what should be included from the sample

#visibility - should either be hidden or visible, with same number of ppts in each condition
table(marshmallow$visibility)
# All good

#time of day - should either be am or pm
table(marshmallow$timeofday)
# ISSUE - we have one mis-coded as “5 pm”, which I’m guessing should just be “pm”. And we have one at “noon”. There’s no way of knowing whether that was morning or afternoon, so safest option is to just remove

#taken - shold either be taken or waited
table(marshmallow$taken)
# All good

############## CLEAN ###################
# We can do all cleaning in one long command.
# First create age (in years) variable
# Second fix time of day - make factor and specify correct levels
# Third make visibility a factor and specify levels
# Fourth make taken a factor and specify levels
# Lastly we want to remove anyone over 120 months (i.e., 10 years) and those with NA values for time of day (i.e., the value of noon)

marshmallow <- marshmallow %>%
    mutate(
      age = agemonths/12,
      timeofday = factor(timeofday, 
                         levels=c("am","pm","5 pm"), # possible levels
                         labels = c("am","pm","pm")), # make the levels these
      visibility = factor(visibility, 
                          levels=c("visible","hidden")),
      taken = factor(taken, 
                     levels=c("waited","taken"))
      ) %>%
    filter(agemonths <= 120, !is.na(timeofday))

#check cleaning has worked:
describe(marshmallow$agemonths) #min over 36, and max 120 - all good
describe(marshmallow$age) #min over 3 yo, and max 10 yo - all good
table(marshmallow$timeofday) #now only am or pm as should be 

############## DESCRIBE ###################

#create descriptives table
descript <- marshmallow %>% 
    group_by(visibility) %>%
   summarise(
       M_Age = mean(age),
       SD_Age = sd(age),
       percent_pm = sum(timeofday=="pm")/n()*100,
       percent_taken = sum(taken=="taken")/n()*100) %>%
  kable(caption = "Descriptive Statistics", digits = 2) %>%
  kable_styling()
descript

############## VISUALISE ###################

#bar plot - senility presence
mallow_plt1 <- ggplot(data = marshmallow, aes(x = as_factor(taken), fill = as_factor(taken))) + 
  geom_bar() + 
    labs(x = "Taken Marshmallow (0 = No, 1 = Yes)", fill = "Marshmallow Taken \n(0 = No, 1 = Yes)", y = "Frequency")
mallow_plt1 

#density plot - senility & WAIS
sen_plt2 <- ggplot(data = marshmallow, aes(x = age, fill = as_factor(taken))) + 
  geom_density() + 
    labs(x = "WAIS Score", fill = "Senility Symptoms Present \n(0 = No, 1 = Yes)")
sen_plt2


######Step 2 ######
## TASKS: run your model(s) of interest to answer your research question, and make sure that the data meet the assumptions of your chosen test

############## BUILD MODEL & EXAMINE OUTPUT ###################

mm1 <- glm(taken ~ timeofday + age + visibility, data = marshmallow, family = binomial)
summary(mm1)
exp(coefficients(mm1))

############## ASSUMPTIONS ###################

#std deviance residuals
plot(rstandard(mm1, type = "deviance"), ylab = "Standardised Deviance Residuals")

#cooks D
plot(cooks.distance(mm1), ylab = "Cook's Distance")


############## MODEL FIT ###################

#compare to null - conduct model comparison
#fit null
mm0 <- glm(taken ~ 1, family = "binomial", data = marshmallow)

#compare models - models are nested
anova(mm0, mm1, test = "Chisq")
AIC(mm0, mm1)
BIC(mm0, mm1)

############## PLOT / TABLE MODEL RESULTS ###################

#plot model
plt_mdl1 <- plot_model(mm1, type = "pred") 
plt_mdl1

#results in formatted table
tab_model(mm1,
          dv.labels = "Marshmallow Taken",
          pred.labels = c("Intercept", "Time of Day - PM", "Age (in years)", "Visibility - Hidden"),
          title = "Regression Table for Marshmallow Model")

```

`r optend()`

### The 3-Act Structure: Analysis Strategy, Results, & Discussion

We need to present our report in three clear sections - think of your sections like the 3 key parts of a play or story - we need to (1) provide some background and scene setting for the reader, (2) present our results in the context of the research question, and (3) present a resolution to our story - relate our findings back to the question we were asked and provide our answer. 

If you need a reminder of what to include within each section, refer to [Semester 1 Lab 11](https://uoepsy.github.io/dapr2/2223/labs/1_11_writeup_recap.html), and read through the 'what to include' sections for Analysis Strategy, Results, and Discussion.

#### Act I: Analysis Strategy

`r qbegin(1)`

Attempt to draft a discussion section based on the above research question and analysis provided.


`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Analysis Strategy - What to Include

Your analysis strategy will contain a number of different elements detailing plans and changes to your plan. Remember, your analysis strategy should **not** contain any results. You may wish to include the following sections:  

-  Very brief data and design description:
     - Give the reader some background on the context of your write-up. For example, you may wish to describe the data source, data collection strategy, study design, number of observational units.
     - Specify the variables of interest in relation to the research question, including their unit of measurement, the allowed range (for Likert scales), and how they are scored. If you have categorical data, you will need to specify the levels and coding of your variables, and what was specified as your reference level and the justification for this choice.

-  Data management:  
     - Describe any data cleaning and/or recoding.
     - Are there any observations that have been excluded based on pre-defined criteria? How/why, and how many? 
     - Describe any transformations performed to aid your interpretation (i.e., mean centering, standardisation, etc.)

-  Model specification:  
     -  Clearly state your hypotheses and specify your chosen significance level.
     -  What type of statistical analysis do you plan to use to answer the research question? (e.g., simple linear regression, multiple linear regression, binary logistic regression, etc.)
     - In some cases, you may wish to include some visualisations and descriptive tables to motivate your model specification. 
     -  Specify the model(s) to be fitted to answer your given research question and analysis structure. Clearly specify the response and explanatory variables included in your model(s). This includes specifying the type of coding scheme applied if using categorical data. 
     - Specify the assumption and diagnostic checks that you will conduct. Specify what plots you will use, and how you will evaluate these. 
     
As noted and encouraged throughout the course, one of the main benefits of using RMarkdown is the ability to include inline R code in your document. Try to incorporate this in your write up so you can automatically pull the specified values from your code. If you need a reminder on how to do this, see [Lesson 4 of the Rmd Bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

:::

`r optbegin("Example Write-Up of Analysis Strategy Section", olabel=FALSE, toggle = params$TOGGLE)`

The `mallow2` dataset contained information on 304 participants who took part in a study concerning the presence of senility symptoms (scored dichotomously as present (1) or not present (0). The sample of older adults also completed the Wechsler Adult Intelligence Scale (WAIS), and scores were available for a subset of these items. All participant data was complete (no missing values).

To investigate whether the probability of having senility symptoms change as a function of WAIS score, a binary logistic regression model was used. Effects were considered statistically significant at $\alpha = .05$. The following model specification was used:

$$
\begin{aligned}
M_1 &: \qquad \log \left( \frac{p}{1 - p}\right) = \beta_0 + \beta_1 \cdot \text{Time of day}_{PM} +  \beta_2 \cdot \text{Age} + \beta_3 \cdot \text{Visibility}_{Hidden}
\end{aligned}
$$

$$
\begin{aligned}
\text{where}~{p}~ &=~ \text{probability of taking the marshmallow}
\end{aligned}
$$


To address the research question of whether the probability of taking the marshmallow changed as a function of time of day, age, and marshmallow visibility, this formally corresponded to:

$$
H_0: \text{All} ~~ \beta_j ~~ = ~~ 0 ~~ (for ~~ j ~~ = 1, 2, 3)
$$

$$
H_1: \text{At least one} ~~ \beta_j ~~ = ~~ 0 ~~ (for ~~ j ~~ = 1, 2, 3)
$$

To assess model fit, we visually assessed the standardized deviance residuals and Cook's Distance. We expected the former to identify outliers (or extreme values), and we expected residuals to fall within the range of -2 to 2. We used the latter to check for influential observations, and visually assessed if any of our 302 observations had a Cook's distance > than 0.5 (moderately influential) or > 1 (highly influential).

`r optend()`

#### Act II: Results

`r qbegin(2)`

Attempt to draft a results section based on your detailed analysis strategy and the analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Results - What To Include

The results section should follow from your analysis strategy. This is where you would present the evidence and results that will be used to answer the research questions and can support your conclusions. Make sure that you address all aspects of the approach you outlined in the analysis strategy (including the evaluation of assumptions and diagnostics). 

In this section, it is useful to include tables and plots to clearly present your findings to your reader. It is important, however, to carefully select what is the key information that should be presented. You don't want to overload the reader with unnecessary or duplicate information, and you also want to save space in case there is a page limit. Make use of figures with multiple panels where you can.

As a broad guideline, you want to start with the results of any exploratory data analysis, presenting tables of summary statistics and exploratory plots. You may also want to visualise associations between/among variables and report covariances or correlations. Then, you should move on to the results from your model.

:::

`r optbegin("Example Write-Up of Results Section", olabel=FALSE, toggle = params$TOGGLE)`

Descriptive statistics are displayed in @tbl-descript.

```{r descript, echo = FALSE, message = FALSE, caption = "Descriptives Table"}
#| label: tbl-descript
#| tbl-cap: "Descriptive Statistics"
# the kable() function makes tables nice for html:
marshmallow %>% 
    group_by(visibility) %>%
   summarise(
       M_Age = mean(age),
       SD_Age = sd(age),
       percent_pm = sum(timeofday=="pm")/n()*100,
       percent_taken = sum(taken=="taken")/n()*100) %>%
  kable(caption = "Descriptive Statistics", digits = 2) %>%
  kable_styling()
```

It appeared that there those without senility symptoms present had higher WAIS scores than those with symptoms (see @fig-senwais-plot).

```{r lieplot, echo = FALSE, fig.cap = "", fig.align = "center"}
#| label: fig-senwais-plot
#| fig-cap: "Association between Delayed Gratification, Age, Visibility of Marshmallow, and Time of Day"
ggplot(data = marshmallow, aes(x = age, fill = as_factor(taken))) + 
  geom_density() + 
    facet_grid(visibility ~ timeofday) +
    labs(x = "Age", fill = "Marshmallow Taken \n(0 = No, 1 = Yes)")
```

A binary logistic regression model was fitted to determine whether the probability of having senility symptoms change as a function of WAIS score.

Our model did not raise any concerns regarding fit. Though there appeared to be a few residuals with a value slightly larger than 2 in absolute value (see left-hand plot in @fig-assumpt), they were not influential points (see right-hand plot in @fig-assumpt), since none of our observations had a Cook's distance value > 0.5.

```{r assumpt, echo = FALSE, fig.cap = "Assumption Checks", fig.align = "center"}
#| label: fig-assumpt
#| fig-cap: "Model Fit Plots"
par(mfrow=c(1,2))
plot(rstandard(mm1, type = "deviance"), ylab = "Standardised Deviance Residuals", ylim=c(-3.5, 3.5))
plot(cooks.distance(mm1), ylab = "Cook's Distance", ylim=c(0, 0.04))
par(mfrow=c(1,1))
```

```{r}
#| include: false
res <- exp(car::Confint(mm1))
res <- round(res,2)
```

WAIS scores were a significant predictor of whether or not individuals experienced senility symptoms (see @tbl-res), where for every additional point scored on the WAIS, the odds of having senility symptoms decreased by a factor of `r res[2,1]` ($95\%\, CI\, [`r paste(res[2,2:3],collapse=", ")`]$).

```{r tableres, echo = FALSE, message = FALSE, caption = "Model Results"}
#| label: tbl-res
#| tbl-cap: "Regression Table for Marshmallow Model"
tab_model(mm1,
          dv.labels = "Marshmallow Taken",
          pred.labels = c("Intercept", "Time of Day - PM", "Age (in years)", "Visibility - Hidden"),
          title = "Regression Table for Marshmallow Model")
```

We visualised the predicted probability based on our model estimates (see @fig-sen-out), which suggested that higher WAIS scores were associated with a lower probability of endorsing senility symptoms.

```{r figureres, echo = FALSE, , message = FALSE, warning = FALSE, caption = "Predicted Probability"}
#| label: fig-sen-out
#| fig-cap: "Model Results"
 plot_model(mm1, type = "eff") 
```

We performed a deviance goodness-of-fit test to compare our fitted model to the null. At the 5% significance level, the addition of information about the participants’ WAIS resulted in a significant decrease in model deviance $\chi^2(1) = 10.78, p = .001$. Hence, we have strong evidence that the subjects’ WAIS was a helpful predictor of whether or not participants will experience symptoms of senility.

`r optend()`

#### Act III: Discussion

`r qbegin(3)`

Attempt to draft a discussion section based on your results and the analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Discussion - What To Include

In the discussion section, you should summarise the key findings from the results section and provide the reader with a few take-home sentences drawing the analysis together and relating it back to the original question. 

The discussion should be relatively brief, and should not include any statistical analysis - instead think of the discussion as a conclusion, providing an answer to the research question(s).

:::

`r optbegin("Example Write-Up of Discussion Section", olabel=FALSE, toggle = params$TOGGLE)`

Our results led us to reject the null hypothesis, as the direction of the association was clear from the results of our binary logistic regression model - lower WAIS scores increased the odds of endorsing senility symptoms. In conclusion, the presence of senility symptoms varied as a function of WAIS score.

`r optend()`

# Section B: Weeks 6-10 Recap

In the second part of the lab, there is no new content - the purpose of the recap section is for you to revisit and revise the concepts you have learned over the last 4 weeks.

::: red
Before you expand each of the boxes below, think about how comfortable you feel with each concept.
:::

`r optbegin("Errors and Power in Hypothesis Testing",olabel=FALSE,toggle=params$TOGGLE)`

When testing an hypothesis, we reach one of the following two decisions:

- failing to reject $H_0$ as the evidence against it is not sufficient
- rejecting $H_0$ as we have enough evidence against it

However, irrespective of our decision, the underlying truth can either be that

- $H_0$ is actually false
- $H_0$ is actually true

Hence, we have four possible outcomes following an hypothesis test:

1. We failed to reject $H_0$ when it was true, meaning we made a __Correct__ decision
1. We rejected $H_0$ when it was false, meaning we made a __Correct__ decision
1. We rejected $H_0$ when it was true, committing a __Type I error__ 
1. We failed to reject $H_0$ when it was false, committing a __Type II error__ 

In the first two cases we are correct, while in the latter two we committed an error.

When we reject the null hypothesis, we never know if we were correct or committed a Type I error, however we can control the chance of us committing a Type I error.
Similarly, when we fail to reject the null hypothesis, we never know if we are correct or we committed a Type II error, but we can also control the chance of us committing a Type II error.

The following table summarises the two types of errors that we can commit:

```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/ht-errors-table-2.png')
```

A Type I error corresponds to a false discovery, while a type II error corresponds to a failed discovery/missed opportunity.

Each error has a corresponding probability:

- The probability of incorrectly rejecting a true null hypothesis is $\alpha = P(\text{Type I error})$
- The probability of incorrectly not rejecting a false null hypothesis is $\beta = P(\text{Type II error})$

A related quantity is __Power__, which is defined as the probability of correctly rejecting a false null hypothesis.

:::yellow
__Power__

Power is the probability of rejecting a false null hypothesis. That is, it is the probability that we will find an effect when it is in fact present.
$$
\text{Power} = 1 - P(\text{Type II error}) = 1 - \beta
$$

:::

::: callout-note
See [S2 Week 6 lecture](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_17_intropower.html#1), [S2 Week 6 lab](https://uoepsy.github.io/dapr2/2223/labs/2_08_power.html) for further details, examples, and to revise these concepts further.
:::

`r optend()`

`r optbegin("Factors affecting power",olabel=FALSE,toggle=params$TOGGLE)`

In practice, it is ideal for studies to have high power while using a relatively small significance level such as .05 or .01. For a fixed $\alpha$, the power increases in the same cases that P(Type II error) decreases, namely as the sample size increases and as the parameter value moves farther into the H values away from
the H0 value.

The __power__ of a test is affected by the following factors:

- __sample size.__ Power increases as the sample size increases.
- __effect size.__ Power increases as the parameter value moves farther into the $H_1$ values away from the $H_0$ value.
- __significance level.__ Power increases as the significance level increases.

Out of these, increasing the significance level $\alpha$ is never an acceptable way to increase power as it leads to more Type I errors, i.e. a higher chance of incorrectly rejecting a true null hypothesis (false discoveries). 

As you can see the four quantities above are linked in some complex way, and in this exercises you will see:

a. How to compute the minimum sample size required to correctly reject a false null hypothesis with a given degree of confidence.

b. Given sample size constraints, to find what is the probability that you test will correctly reject a false null hypothesis. If it's too low, you might want to reconsider your study or perhaps even abandon it.

::: callout-note
See [S2 Week 6 lecture](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_17_intropower.html#1), [S2 Week 6 lab](https://uoepsy.github.io/dapr2/2223/labs/2_08_power.html) for further details, examples, and to revise these concepts further.
:::

`r optend()`

`r optbegin("Effect Size",olabel=FALSE,toggle=params$TOGGLE)`

Effect size refers to the "detectability" of your alternative hypothesis. In simple terms, it compares the distance between the alternative and the null hypothesis to the variability in your data.

For simplicity consider comparing a mean: $H_0: \mu = 0$ vs $H_1: \mu \neq 0$. If the sample mean were really 0.1 and the null hypothesis is 0, the distance is 0.1 - 0 = 0.1.

Now, a distance of 0.1 has a different weight in the following two scenarios.

__Scenario 1.__ Data vary between -1 and 1.

__Scenario 2.__ Data vary between -1000 and 1000.

Clearly, in Scenario 1 a distance of of 0.1 is a big difference.
Conversely, in Scenario 2 a distance of 0.1 is not an interesting difference, it's negligible compared to the magnitude of the data.

```{r echo=FALSE, out.width='85%'}
x = factor(rep(c('A', 'B'), each = 50))
y = c(rnorm(50, 0.1, 0.08), rnorm(50, 0.1, 1))
plt.df = tibble(x, y)
stripchart(y ~ x, data = plt.df, pch = 1, 
           col = 'black', frame.plot = FALSE)
points(0.1, 1.05, col = 'red', pch = 6, cex = 1, lwd = 2)
points(0.1, 1.95, col = 'red', pch = 2, cex = 1, lwd = 2)
abline(v = 0, col = 'blue', lty = 2)
```

::: callout-note
See [S2 Week 6 lecture](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_17_intropower.html#1), [S2 Week 6 lab](https://uoepsy.github.io/dapr2/2223/labs/2_08_power.html) for further details, examples, and to revise these concepts further.
:::

`r optend()`

`r optbegin("The pwr package",olabel=FALSE,toggle=params$TOGGLE)`

You will perform power analysis using the `pwr` package. To install it, run `install.packages("pwr")` in your console, then run `library(pwr)` in your RMarkdown file.

The following functions are available.

Function | Description 
:--------|:------------
 `pwr.2p.test    ` | Two proportions (equal n)
 `pwr.2p2n.test  ` | Two proportions (unequal n)
 `pwr.anova.test ` | Balanced one-way ANOVA
 `pwr.chisq.test ` | Chi-square test
 `pwr.f2.test    ` | General linear model
 `pwr.p.test     ` | Proportion (one sample)
 `pwr.r.test     ` | Correlation
 `pwr.t.test     ` | t-tests (one sample, two samples, paired)
 `pwr.t2n.test   ` | t-test (two samples with unequal n)

For each function, you can specify three of four arguments (sample size, alpha, effect size, power) and the fourth argument will be calculated for you.

Of the four quantities, effect size is often the most difficult to specify. Calculating effect size typically requires some experience with the measures involved and knowledge of past research. 

Typically, specifying effect size requires you to read published literature or past papers on your research topic, to see what effect sizes were found and what significant results were reported. Other times, this might come from previous collected data or subject-knowledge from your colleagues.

But what can you do if you have no clue what effect size to expect in a given study? Cohen (1988) provided guidelines for what a small, medium, or large effect typically is in the behavioral sciences.

Type of test | Small | Medium | Large
:---|:---:|:---:|:---:|:---:
t-test | 0.20 | 0.50 | 0.80
ANOVA | 0.10 | 0.25 | 0.40 
Linear regression | 0.02 | 0.15 | 0.35

::: callout-note
See [S2 Week 6 lecture](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_17_intropower.html#1), [S2 Week 6 lab](https://uoepsy.github.io/dapr2/2223/labs/2_08_power.html) for further details, examples, and to revise these concepts further.
:::

`r optend()`

`r optbegin("Power for t-tests",olabel=FALSE,toggle=params$TOGGLE)`

We compare the mean of a response variable between two groups using a t-test. For example, if you are comparing the mean response between two groups, say treatment and control, the null and alternative hypotheses are:
$$
H_0 : \mu_t - \mu_c = 0 \\
H_1 : \mu_t - \mu_c \neq 0
$$

The effect size in this case is Cohen's $D$:
$$
D = \frac{(\bar x_t - \bar x_c) - 0}{s_p}
$$
where

- $\bar x_t$ and $\bar x_c$ are the sample means in the treatment and control groups, respectively
- $s_p$ is the "pooled" standard deviation


Cohen's $D$ measures the distance between (a) the observed difference in means from (b) the hypothesised value 0, and compares this to the variability in the data.

:::blue 

In **R** we use the function

```{r, eval = FALSE}
pwr.t.test(n = , d = , sig.level = , power = , type = , alternative = )
```

where

- `n` = the sample size
- `d` = the effect size
- `sig.level` = the significance level $\alpha$ (the default is 0.05)
- `power` = the power level.
- `type` = the type of t-test to perform: either a two-sample t-test ("two.sample"), a one-sample t-test ("one.sample"), or a dependent sample t-test ("paired"). A two-sample test is the default.
- `alternative` = whether the alternative hypothesis is two-sided ("two.sided")
or one-sided ("less" or "greater"). A two-sided test is the default.

:::

::: callout-note
See [S2 Week 6 lecture](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_17_intropower.html#1), [S2 Week 6 lab](https://uoepsy.github.io/dapr2/2223/labs/2_08_power.html) for further details, examples, and to revise these concepts further.
:::

`r optend()`

`r optbegin("Power for linear regression",olabel=FALSE,toggle=params$TOGGLE)`

In linear regression, the relevant function in **R** is:

:::blue

```{r, eval=FALSE}
library(pwr)
pwr.f2.test(u = , v = , f2 = , sig.level = , power = )
```

where
-   `u` = numerator degrees of freedom
-   `v` = denominator degrees of freedom
-   `f2` = effect size

:::

::: {.panel-tabset}

The formula for the effect size $f^2$ is different depending on your goal.

### Goal 1. Test on all slopes

Here you have one model, 

$$
y = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k + \epsilon
$$

and you wish to find the minimum sample size required to answer the following test of hypothesis with a given power: 

$$
\begin{aligned}
H_0 &: \beta_1 = \beta_2 = \dots = \beta_k = 0 \\
H_1 &: \text{At least one } \beta_i \neq 0
\end{aligned}
$$

The appropriate formula for the effect size is: 

$$
f^2 = \frac{R^2}{1 - R^2}
$$

And the numerator degrees of freedom are $\texttt u = k$, the number of predictors in the model.

The denominator degrees of freedom returned by the function will give you: 

$$
\texttt v = n - (k + 1) = n - k - 1
$$

From which you can infer the sample size as 

$$
n = \texttt v + k + 1
$$

### Goal 2. Test on a subset of slopes

In this case you would have a smaller model $m$ with $k$ predictors and a larger model $M$ with $K$ predictors: 

$$
\begin{aligned}
m &: \quad y = \beta_0 + \beta_1 x_1 + \dots + \beta_{k} x_k 
+ \epsilon \\
M &: \quad y = \beta_0 + \beta_1 x_1 + \dots + \beta_{k} x_k 
+ \underbrace{\beta_{k + 1} x_{k+1} + \dots + \beta_{K} x_K}_{\text{extra predictors}}
+ \epsilon
\end{aligned}
$$

This case is when you wish to find the minimum sample size required to answer the following test of hypothesis: 

$$
\begin{aligned}
H_0 &: \beta_{k+1} = \beta_{k+2} = \dots = \beta_K = 0 \\
H_1 &: \text{At least one of the above } \beta \neq 0
\end{aligned}
$$

You need to use the R-squared from the larger model $R^2_M$ and the R-squared from the smaller model $R^2_m$. The appropriate formula for the effect size is: 

$$
f^2 = \frac{R^2_{M} - R^2_{m}}{1 - R^2_M}
$$

Here, the numerator degrees of freedom are the extra predictors: $\texttt u = K - k$.

The denominator degrees of freedom returned by the function will give you (here you use $K$ the number of all predictors in the larger model): 

$$
\text v = n - (K + 1) = n - K - 1
$$

From which you can infer the sample size as 

$$
n = \text v + K + 1
$$

:::

::: callout-note
See [S2 Week 6 lecture](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_17_intropower.html#1), [S2 Week 6 lab](https://uoepsy.github.io/dapr2/2223/labs/2_08_power.html) for further details, examples, and to revise these concepts further.
:::


`r optend()`

`r optbegin("Probability, Odds, Log-Odds", olabel=FALSE,toggle=params$TOGGLE)`

-   The **probability** $p$ ranges from 0 to 1.

-   The **odds** $\frac{p}{1-p}$ ranges from 0 to $\infty$.

-   The **log-odds** $\log \left( \frac{p}{1-p} \right)$ ranges from $-\infty$ to $\infty$. 

::: callout-note
In the labs, we used "log" to denote log-odds. In the lectures, you will have seen this denoted as "ln". 
:::

```{r plodd, echo=FALSE, out.width = '95%', fig.cap="Probability, Odds and Log-odds"}
knitr::include_graphics("images/glm/plo.png")
```

In order to understand the connections among these concepts, lets work with an example where the *probability* of an event occurring is 0.2:

-   Odds of event occurring:

$$
\text{odds} = (\frac{0.2}{0.8}) = 0.25
$$

-   Log-odds of the event occurring:

$$
log(\frac{0.2}{0.8}) = -1.3863 \\ \ \\
$$

<center>
OR
</center>

$$
log(0.25) = -1.3863
$$

-   Probability can be reconstructed as:

$$
(\frac{odds}{1+odds}) = (\frac{0.25}{1+0.25}) = 0.2 \\ \ \\
$$
<center>
OR
</center>

$$
(\frac{exp(log(odds))}{1+exp(log(odds))}) = (\frac{exp(-1.3863)}{1+exp(-1.3683)}) = (\frac{0.25}{1.25}) = 0.2
$$

::: blue
In **R**:

-   obtain the odds

    `odds <- 0.2 / (1 - 0.2)`

-   obtain the log-odds for a given probability

    `log_odds <- log(0.25)`

    **OR**

    `log_odds <- qlogis(0.2)`

-   obtain the probability from the odds

    `prob_O <- odds / (1 + odds)`

-   obtain the probability from the log-odds

    `prob_LO <- exp(log_odds) / (1 + exp(log_odds))`

    **OR**

    `prob_LO <- plogis(log_odds)`
:::

::: callout-note
See [S2 Week 7 Lecture](https://uoepsy.github.io/dapr2/2223/lectures/dapR2_16_binarylogistic.html#1), [S2 Week 7 lab](https://uoepsy.github.io/dapr2/2223/labs/2_06_glm1.html), and [S2 Week 7 lab](https://uoepsy.github.io/dapr2/2223/labs/2_07_glm2.html) for further details, examples, and to revise these concepts further.
:::

`r optend()`

`r optbegin("Binary Logistic Regression", olabel=FALSE,toggle=params$TOGGLE)`

When a response (y) is binary coded (e.g., 0/1; failure/success; no/yes; fail/pass; unemployed/employed) we must use logistic regression. The predictors can either be continuous or categorical.

$$
{\log(\frac{p_i}{1-p_i})} = \beta_0 + \beta_1 \ x_{i1} 
$$

::: blue
**In R:**
```{r, eval=FALSE}
glm(y ~ x1 + x2, data = data, family = binomial)
```
:::

::: callout-note
See [S2 Week 6 lecture](https://uoepsy.github.io/dapr2/2223/lectures/dapR2_16_binarylogistic.html#1), [S2 Week 6 lab](https://uoepsy.github.io/dapr2/2223/labs/2_06_glm1.html), and [S2 Week 7 lab](https://uoepsy.github.io/dapr2/2223/labs/2_07_glm2.html) for further details, examples, and to revise these concepts further.
:::

`r optend()`

`r optbegin("Interpretation of coefficients", olabel=FALSE,toggle=params$TOGGLE)`

To interpret the fitted coefficients, we first exponentiate the model:
$$
\begin{aligned}
\log \left( \frac{p_x}{1-p_x} \right) &= \beta_0 + \beta_1 x \\
e^{ \log \left( \frac{p_x}{1-p_x} \right) } &= e^{\beta_0 + \beta_1 x } \\
\frac{p_x}{1-p_x} &= e^{\beta_0} \ e^{\beta_1 x}
\end{aligned}
$$

and recall that the probability of success divided by the probability of failure is the odds of success. 
$$
\frac{p_x}{1-p_x} = \text{odds}
$$

Let's apply this to our lab example from Section A above:

__Intercept__

$$
\text{If }x = 0, \qquad \frac{p_0}{1-p_0} = e^{\beta_0} \ e^{\beta_1 x} = e^{\beta_0}
$$

That is, $e^{\beta_0}$ represents the odds of having symptoms of senility for individuals with a WAIS score of 0. 

In other words, for those with a WAIS score of 0, the probability of having senility symptoms is $e^{\beta_0}$ times that of non having them.

__Slope__

$$
\text{If }x = 1, \qquad \frac{p_1}{1-p_1} = e^{\beta_0} \ e^{\beta_1 x} = e^{\beta_0} \ e^{\beta_1}
$$

Now consider taking the ratio of the odds of senility symptoms when $x=1$ to the odds of senility symptoms when $x = 0$:

$$
\frac{\text{odds}_{x=1}}{\text{odds}_{x=0}} = \frac{p_1 / (1 - p_1)}{p_0 / (1 - p_0)} 
= \frac{e^{\beta_0} \ e^{\beta_1}}{e^{\beta_0}}
= e^{\beta_1}
$$

So, $e^{\beta_1}$  represents the odds ratio for a 1 unit increase in WAIS score.

It is typically interpreted by saying that for a one-unit increase in WAIS score the odds of senility symptoms increase by a factor of $e^{\beta_1}$.

Equivalently, we say that $e^{\beta_1}$ represents the multiplicative increase (or decrease) in the odds of success when $x$ is increased by 1 unit.

::: blue
**In R:**

To translate log-odds to odds in order to aid interpretation, we can exponentiate (i.e., by using exp()) the coefficients from your model using R via the following command:

```{r, eval = FALSE}
exp(coef(modelname))
```

We can also use `R` to extract predicted probabilities for us from our models.

-   Calculate the predicted log-odds (probabilities on the logit scale): `predict(model, type="link")`
-   Calculate the predicted probabilities: `predict(model, type="response")`
:::

::: callout-note
See [S2 Week 6 lecture](https://uoepsy.github.io/dapr2/2223/lectures/dapR2_16_binarylogistic.html#1), [S2 Week 6 lab](https://uoepsy.github.io/dapr2/2223/labs/2_06_glm1.html), and [S2 Week 7 lab](https://uoepsy.github.io/dapr2/2223/labs/2_07_glm2.html) for further details, examples, and to revise these concepts further.
:::

`r optend()`

`r optbegin("Generalized Linear Models", olabel=FALSE,toggle=params$TOGGLE)`

Generalized linear models can be fitted in R using the `glm` function, which is similar to the `lm` function for fitting linear models. However, we also need to specify the family (i.e., link) function. There are three key components to consider:

-   Random component / probability distribution - The distribution of the response/outcome variable. Can be from any family of distributions as listed below.
-   Systematic component / linear predictor - the explanatory/predictor variable(s) (can be continuous or discrete).
-   Link function - specifies the link between a random and systematic components.

**Formula:**

$y_i = \beta_0 + \beta_1 x_i + + \beta_2 x_i + \epsilon_i$

::: blue
**In R:**

```{r, eval=FALSE}
glm(y ~ x1 + x2, data = data, family = <INSERT_FAMILY>)
```

:::

The family argument takes (the name of) a family function which specifies the link function and variance function (as well as a few other arguments not entirely relevant to the purpose of this course).

The exponential family functions available in **R** are:

-   `binomial` (link = "logit")
-   `gaussian` (link = "identity")
-   `poisson` (link = "log")
-   `Gamma` (link = "inverse")
-   `inverse.gaussian` (link = "1/mu2")

See `?glm` for other modeling options. See `?family` for other allowable link functions for each family.

::: callout-note
See [S2 Week 6 lecture](https://uoepsy.github.io/dapr2/2223/lectures/dapR2_16_binarylogistic.html#1), [S2 Week 6 lab](https://uoepsy.github.io/dapr2/2223/labs/2_06_glm1.html), and [S2 Week 7 lab](https://uoepsy.github.io/dapr2/2223/labs/2_07_glm2.html) for further details, examples, and to revise these concepts further.
:::

`r optend()`

`r optbegin("Drop-in-deviance test to compare nested models",olabel=FALSE,toggle=params$TOGGLE)`

When moving from linear regression to more advanced and flexible models, testing of goodness of fit is more often done by comparing a model of interest to a simpler one.

The only caveat is that the two models need to be **nested**, i.e. one model needs to be a simplification of the other, and all predictors of one model needs to be within the other.

We want to compare the model we previously fitted against a model where all slopes are 0, i.e. a baseline model: 

$$
\begin{aligned}
M_1 : \qquad\log \left( \frac{p}{1 - p} \right) &= \beta_0 \\
M_2 : \qquad \log \left( \frac{p}{1 - p} \right) &= \beta_0 + \beta_1 x
\end{aligned}
$$ 

:::blue 
**In R:**
We do the comparison as follows:

```{r, eval = FALSE}
mdl_reduced <- glm(DV ~ 1, family = "binomial", data = dataset)
mdl_main <- glm(DV ~ IV1 + IV2 ... + IV4, family = "binomial", data = dataset)

anova(mdl_red, mdl_main, test = 'Chisq')
```

In the output we can see the residual deviance of each model. Remember the deviance is the equivalent of residual sum of squares in linear regression.
:::

::: callout-note
See [S2 Week 6 lecture](https://uoepsy.github.io/dapr2/2223/lectures/dapR2_16_binarylogistic.html#1), [S2 Week 6 lab](https://uoepsy.github.io/dapr2/2223/labs/2_06_glm1.html), and [S2 Week 7 lab](https://uoepsy.github.io/dapr2/2223/labs/2_07_glm2.html) for further details, examples, and to revise these concepts further.
:::

`r optend()`

`r optbegin("Akaike and Bayesian Information Criteria",olabel=FALSE,toggle=params$TOGGLE)`

Deviance measures lack of fit, and it can be reduced to zero by making the model more and more complex, effectively estimating the value at each single data point. However, this involves adding more and more predictors, which makes the model more complex (and less interpretable).

Typically, simpler model are preferred when they still explain the data almost as well. This is why information criteria were devised, exactly to account for both the model misfit but also its complexity.

$$
\text{Information Criterion} = \text{Deviance} + \text{Penalty for model complexity}
$$

Depending on the chosen penalty, you get different criteria. Two common ones are the Akaike and Bayesian Information Criteria, AIC and BIC respectively: 

$$
\begin{aligned}
\text{AIC} &= \text{Deviance} + 2 p \\
\text{BIC} &= \text{Deviance} + p \log(n)
\end{aligned}
$$

where $n$ is the sample size and $p$ is the number of regression coefficients in the model. **Models that produce smaller values of these fitting criteria should be preferred.**

AIC and BIC differ in their degrees of penalization for number of regression coefficients, with BIC usually favouring models with fewer terms.

::: callout-note
See [S2 Week 6 lecture](https://uoepsy.github.io/dapr2/2223/lectures/dapR2_16_binarylogistic.html#1), [S2 Week 6 lab](https://uoepsy.github.io/dapr2/2223/labs/2_06_glm1.html), and [S2 Week 7 lab](https://uoepsy.github.io/dapr2/2223/labs/2_07_glm2.html) for further details, examples, and to revise these concepts further.
:::

`r optend()`

`r optbegin("Exploratory vs Confirmatory Analyses",olabel=FALSE,toggle=params$TOGGLE)`

*Exploratory* analyses are conducted when either (1) you have a hypothesis but no clear analysis plan/strategy; (2) you have lots of variables that might be associated with an outcome variable, but you're not sure which or in what ways i.e., no clear predictions. It also provides tools for hypothesis generation - particularly via visualisation of data.

*Confirmatory* analyses are conducted when you have a specific research question to test. You can think of this type of analysis as putting your hypotheses to trial - does your data support or fail to support your argument?

`r optend()`

`r optbegin("Steps in Exploratory Analyses",olabel=FALSE,toggle=params$TOGGLE)` There are a number of steps involved in exploratory analyses:

-   Step 1: Check coding of data and visualise variables of interest
-   Step 2: Compare models of interest to find if your variables of interest are good predictors of your outcome variable
-   Step 3: Compute the k-fold cross validation MSE for each of your models
-   Step 4: Identify best fitting model 


`r optend()`

