[
  {
    "objectID": "1_01_function.html",
    "href": "1_01_function.html",
    "title": "Functions and Models",
    "section": "",
    "text": "At the end of this lab, you will:\n\nHave reviewed the main concepts from introductory statistics.\nUnderstand the concept of a function.\nBe able to discuss what a statistical model is.\nUnderstand the link between models and functions.\n\n\n\n\n\nHave attended and/or watched Week 1 lectures.\nHave installed R and RStudio on your own computer (unless you have a Chromebook where you may continue to use the PPLS RStudio Server).\n\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nggExtra\nkableExtra\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/handheight.csv."
  },
  {
    "objectID": "1_01_function.html#functions-and-mathematical-models-plots",
    "href": "1_01_function.html#functions-and-mathematical-models-plots",
    "title": "Functions and Models",
    "section": "Functions and Mathematical Models: Plots",
    "text": "Functions and Mathematical Models: Plots\n\nQuestion 4\n\n\nCreate a data set called squares containing the perimeter of four squares having sides of length \\(0, 2, 5, 9\\) metres, and then plot the squares data as points\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember that to combine multiple numbers together we use the function c().\n\n\n\n\n\n\n\n Solution \n\n\n\n#Create 'squares' dataset using tibble()\nsquares <- tibble(\n  side = c(0, 2, 5, 9),\n  perimeter = 4 * side\n)\n\nsquares\n\n# A tibble: 4 × 2\n   side perimeter\n  <dbl>     <dbl>\n1     0         0\n2     2         8\n3     5        20\n4     9        36\n\n\n\n#Create Plot\nggplot(data = squares, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\n\nFigure 1: ?(caption)\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nGenerate one hundred data points, and use them to visualise the relationship between side and perimeter of squares. To do so, you need to complete four steps:\n\nCreate a sequence of one hundred side lengths (x) going from 0 to 3 metres.\nCompute the corresponding perimeters (y).\nPlot the side and perimeter data as points on a graph.\nVisualise the functional relationship between side and perimeter of squares. To do so, use the function geom_line() to connect the computed points with lines.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember that to create a sequence of numbers, we can use the function seq().\n\n\n\n\n\n\n\n Solution \n\n\nLet’s start by creating the side and perimeter data:\n\n#step 1 & 2\nsquares_grid <- tibble(\n  side = seq(0, 3, length.out = 100),\n  perimeter = 4 * side\n)\n\nPlot the individual points:\n\n# step 3\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4*Side')\n\n\n\n\nVisualise the functional relationship by connecting the individual points with lines:\n\n#step 4\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_line(colour = 'blue') +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4*Side')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function \\(y = 4 \\ x\\) that you plotted above is an example of a function representing a mathematical model.\nWe typically validate a model using experimental data. However, we all know how squares work and that two squares with the same side will have the same perimeter (more on this later).\n\n\n\n\nQuestion 6\n\n\nThe Scottish National Gallery kindly provided us with measurements of side and perimeter (in metres) for a sample of 10 square paintings.\nThe data are provided below:\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nPlot the mathematical model of the relationship between side and perimeter for squares, and superimpose on top the experimental data from the Scottish National Gallery.\n\n\n\n\n Solution \n\n\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nggplot() +\n  geom_line(data = squares_grid, aes(x = side, y = perimeter), colour = 'blue') +\n  geom_point(data = sng, aes(x = side, y = perimeter), colour = 'black', \n             alpha = 0.5, size = 3) +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\n\nFigure 2: The exact relationship between side and perimeter of squares\n\n\n\n\nThe above plot shows perfect agreement between the observed data and the model.\n\n\n\n\n\nQuestion 7\n\n\nUse the mathematical model to predict the perimeter of a painting with a side of 1.5 metres.\n\n\n\n\n\n\nHint\n\n\n\n\n\nDon’t forget to always include the measurement units when reporting/writing-up results!\n\n\n\n\n\n\n\n Solution \n\n\nWe do not have a painting with a side of 1.5 metres within the random sample of paintings from the Scottish National Gallery. However, we can predict the perimeter of an unobserved squared painting having a 1.5 metre side using the mathematical model.\nYou can obtain this prediction using either a visual approach or an algebraic one.\n\nVisual ApproachAlgebraic Approach\n\n\n\n\n\n\n\nSometimes we can directly read a predicted value from the graph of the functional relationship.\nConsider the plot created in the previous question. First, we need to check where x = 1.5. Then, we draw a vertical dashed line until it meets the blue line. The y value corresponding to x = 1.5 can be read off the y-axis.\nHowever, in this case it is not that easy to read it from the drawing…\n\n\nYou can substitute the x value in the formula and calculate the corresponding y value.\n\\[\ny = 4 * x = 4 * 1.5 = 6\n\\]\n\n\n\n\n\n\n\n\n\nThe predicted perimeter of squared paintings having a 1.5m side is 6m."
  },
  {
    "objectID": "1_01_function.html#study-overview",
    "href": "1_01_function.html#study-overview",
    "title": "Functions and Models",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Question\nHow does handspan vary as a function of height?\n\nConsider now the relationship between height (in inches) and handspan (in cm). Utts and Heckard (2015) provided data for a sample of 167 students which reported their height and handspan as part of a class survey.\nUsing the handheight data you already loaded at the start of the lab, your task is to investigate how handspan varies as a function of height for the students in the sample.\n\n Handheight codebook.\n\n\nDescription\nThe data set records the height and handspan reported by a random sample of 167 students as part of a class survey.\nThe variables are:\n\nheight, measured in inches\nhandspan, measured in centimetres\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n \n  \n    height \n    handspan \n  \n \n\n  \n    68 \n    21.5 \n  \n  \n    71 \n    23.5 \n  \n  \n    73 \n    22.5 \n  \n  \n    64 \n    18.0 \n  \n  \n    68 \n    23.5 \n  \n  \n    59 \n    20.0 \n  \n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nUsing a scatterplot (since the variables are numeric and continuous) to visualise the relationship between the two numeric variables, comment on any main differences you notice with the relationship between side and perimeter of squares. Note if you detected outliers or points that do not fit with the pattern in the rest of the data.\n\n\n\n\n Solution \n\n\n\nplt <- ggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\nplt\n\n\n\n\nFigure 3: Simple Scatterplot\n\n\n\n\nWe can also add marginal boxplots for each variable using the package ggExtra.\n\nggMarginal(plt, type = 'boxplot')\n\n\n\n\nFigure 4: The statistical relationship between height and handspan\n\n\n\n\nOutliers are extreme observations that are not possible values of a variable or that do not seem to fit with the rest of the data. This could either be:\n\nmarginally along one axis: points that have an unusual (too high or too low) x-coordinate or y-coordinate;\njointly: observations that do not fit with the rest of the point cloud.\n\nThe boxplots in Figure 4 do not highlight any outliers in the marginal distributions of height and handspan. Furthermore, from the scatterplot we do not notice any extreme observations or points that do not fit with the rest of the point cloud.\nWe notice a moderate, positive linear relationship between height and handspan.\nRecall Figure 2, displaying the relationship between side and perimeters of squares. In the plot we notice two points on top of each other, reflecting the fact that two squares having the same side will always have the same perimeter. In fact, the data from the Scottish National Gallery include two squared paintings with a side of 1.1m, both having a measured perimeter of 4.4m.\nFigure 4, instead, displays the relationship between height and handspan of a sample of students. The first thing that grabs our attention is the fact that students having the same height do not necessarily have the same handspan. Rather, we clearly see a variety of handspan values for students all having a height of, for example, 70in. To be more precise, the seven students who are 70 in. tall all have differing handspans.\n\n\n\n\n\nQuestion 9\n\n\nUsing the following command, superimpose on top of the scatterplot a best-fit line describing how handspan varies as a function of height. For the moment, the argument se = FALSE tells R to not display uncertainty bands.\n\ngeom_smooth(method = lm, se = FALSE)\n\nComment on any differences between the lines representing the linear relationship between (a) the side and perimeter of square and (b) height and handspan.\n\n\n\n\n Solution \n\n\n\nggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_smooth(method = lm, se = FALSE) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\n\n\n\nFigure 5: The best-fit line\n\n\n\n\nThe line representing the relationship between side and perimeter of squares is able to predict the actual perimeter value from the measurement of the side of a square. This is possible because the relationship between side and perimeter is an exact one.\nThat is, any squares having the same side will have the same perimeter, and there will be no variation in those values.\nThe line that best fits the relationship between height and handspan (see Figure 5), instead, is only able to predict the average handspan for a given value of height.\nThis is because there will be a distribution of handspans at each value of height. The line will fit the trend/pattern in the values, but there will be individual-to-individual variability that we must accept around that average pattern.\n\n\n\n\n\nQuestion 10\n\n\n\n\n\nThe line of best-fit is given by:1\n\\[\n\\widehat{Handspan} = -3 + 0.35 \\ Height\n\\]\nCalculate the predicted handspan of a student who is (a) 73in tall, and (b) 5in tall.\n\n\n\n\n Solution \n\n\n\nThe predicted average handspan for students who are 73in tall is:  \\(-3 + 0.35 * 73 = 22.55\\)cm.  \nThe predicted average handspan for students who are 5in tall is:  \\(-3 + 0.35 * 5 = -1.25\\)cm.\n\nBut wait, handspan can not be negative… This does not make any sense! That’s right, we went too far off the range of the available data on heights, which were between 57in and 78in. We extrapolated. This is very dangerous…\n\n\n\n\n\nSource: Randall Munroe, xkcd.com"
  },
  {
    "objectID": "1_02_slr.html",
    "href": "1_02_slr.html",
    "title": "Intro to Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to specify a simple linear model.\nUnderstand what fitted values and residuals are.\nBe able to interpret the coefficients of a fitted model.\nBe able to test hypotheses and construct confidence intervals for the regression coefficients.\n\n\n\n\n\nBe up to date with lectures\nHave completed previous lab exercises\n\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/riverview.csv."
  },
  {
    "objectID": "1_02_slr.html#data-exploration",
    "href": "1_02_slr.html#data-exploration",
    "title": "Intro to Linear Regression",
    "section": "Data Exploration",
    "text": "Data Exploration\nThe common first port of call for almost any statistical analysis is to explore the data, and we can do this visually and/or numerically.\n\n\n\n\n\n\n\n\n\nMarginal Distributions\nBivariate Associations\n\n\n\n\nDescription\nThe distribution of each variable (e.g., employee incomes and education levels) without reference to the values of the other variables\nDescribing the relationship between two numeric variables\n\n\nVisually  \n \nPlot each variable individually.   You could use, for example, geom_density() for a density plot or geom_histogram() for a histogram to comment on and/or examine:   - The shape of the distribution. Look at the shape, centre and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal? - Identify any unusual observations. Do you notice any extreme observations (i.e., outliers)?\nPlot associations among two variables.   You could use, for example, a scatterplot to comment on and/or examine:   - The direction of the association indicates whether there is a positive or negative association - The form of association refers to whether the relationship between the variables can be summarized well with a straight line or some more complicated pattern - The strength of association entails how closely the points fall to a recognizable pattern such as a line - Unusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail\n\n\nNumerically  \nCompute and report summary statistics e.g., mean, standard deviation, median, min, max, etc.   You could, for example, calculate summary statistics such as the mean (mean()) and standard deviation (sd()), etc. within summarize()\nCompute and report the correlation coefficient.    You can use the cor() function to calculate this\n\n\n\n\nMarginal Distributions\n\nQuestion 1\n\n\nVisualise and describe the marginal distribution of (a) employee incomes, and (b) education level.\n\n\n\n\n Solution \n\n\nWe can plot the marginal distributions of employee incomes and education level as density curves, and add a boxplot underneath to check for the presence of outliers.\n\nEmployee Incomes\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = income)) +\n  geom_density() +\n  geom_boxplot(width = 1/300) +\n  labs(x = \"Income (in thousands of U.S. dollars)\", \n       y = \"Probability density\")\n\n\n\n\nFigure 1: Density plot and boxplot of employee incomes\n\n\n\n\nThe plot suggested that the distribution of employee incomes was unimodal, and most of the incomes were between roughly $45,000 and $70,000. The lowest income in the sample was approximately $25,000 and the highest over $80,000. This suggested there was a fair high degree of variation in the data. Furthermore, the boxplot did not highlight any outliers in the data.\nTo further summarize the distribution, it is typical to compute and report numerical summary statistics such as the mean and standard deviation:\n\ndesc_income <- riverview %>% \n  summarize(\n    M = mean(income), \n    SD = sd(income)\n    )\ndesc_income\n\n# A tibble: 1 × 2\n      M    SD\n  <dbl> <dbl>\n1  53.7  14.6\n\n\nFollowing the exploration above, we can describe the income variable as follows:\n\n\n\n\n\n\nThe marginal distribution of income was unimodal with a mean of approximately $53,700. There was variation in employees’ salaries (SD = $14,553).\n\n\n\n\n\nEducation Level\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = education)) +\n  geom_density() +\n  geom_boxplot(width = 1/100) +\n  labs(x = \"Education (in years)\", \n       y = \"Probability density\")\n\n\n\n\nFigure 2: Density plot and boxplot of employee education levels\n\n\n\n\nSummary statistics for the employees’ level of education:\n\ndesc_education <- riverview %>%\n  summarize(\n    M = mean(education),\n    SD = sd(education)\n    )\ndesc_education\n\n# A tibble: 1 × 2\n      M    SD\n  <dbl> <dbl>\n1    16  4.36\n\n\n\n\n\n\n\n\n\nThe marginal distribution of education was unimodal with an average of of 16 years. There was variation in employees’ level of education (SD = 4.4 years).\n\n\n\n\n\n\n\n\n\n\nAssociations among Variables\n\nQuestion 2\n\n\nCreate a scatterplot of income and education level before calculating the correlation between the income and education level. Making reference to both the plot and correlation coefficient, describe the relationship between income and level of education among the employees in the sample.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe are trying to investigate how income varies when varying years of formal education. Hence, income is the dependent variable (on the y-axis), and education is the independent variable (on the x-axis).\n\n\n\n\n\n\n\n Solution \n\n\nLet’s produce a scatterplot:\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Education (in years)\", \n       y = \"Income (in thousands of U.S. dollars)\")\n\n\n\n\nFigure 3: The relationship between employees’ education level and income\n\n\n\n\nTo comment on the strength of the linear association we compute the correlation coefficient:\n\ncorr <- riverview %>%\n  select(education, income) %>%\n  cor()\ncorr\n\n          education    income\neducation 1.0000000 0.7947847\nincome    0.7947847 1.0000000\n\n\nthat is,\n\\[\nr_{\\text{education, income}} = 0.79\n\\]\n\n\n\n\n\n\n\nThere was a strong positive linear association between education level and income for the employees in the sample. High incomes tended to be observed, on average, with more years of formal education (\\(r\\) = .79).\nThe scatterplot did not highlight any outliers."
  },
  {
    "objectID": "1_02_slr.html#model-specification-and-fitting",
    "href": "1_02_slr.html#model-specification-and-fitting",
    "title": "Intro to Linear Regression",
    "section": "Model Specification and Fitting",
    "text": "Model Specification and Fitting\nThe scatterplot highlighted a linear relationship, where the data points were scattered around an underlying linear pattern with a roughly-constant spread as x varied.\nHence, we will try to fit a simple (i.e., one x variable only) linear regression model:\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\\\\n\\quad \\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nwhere “\\(\\epsilon \\sim N(0, \\sigma) \\text{ independently}\\)” means that the errors around the line have mean zero and constant spread as x varies.\n\nQuestion 3\n\n\nUsing the lm() function, fit a simple linear model to predict income (DV) by Education (IV), naming the output mdl.\nWrite down the equation of the fitted line.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe syntax of the lm() function is:\n\n[model name] <- lm([response variable i.e., dependent variable] ~ [explanatory variable i.e., independent variable], data = [dataframe])\n\n\n\n\n\n\n\n\n Solution \n\n\nThe fitted model can be written as\n\\[\n\\widehat{Income} = \\hat \\beta_0 + \\hat \\beta_1 \\ Education\n\\]\nor\n\\[\n\\widehat{Income} = \\hat \\beta_0 \\cdot 1 + \\hat \\beta_1 \\cdot Education\n\\]\nWhen we specify the linear model in R, we include after the tilde sign, ~, the variables that appear to the right of the \\(\\hat \\beta\\)s. That’s why the 1 is included.\nAs the variables are in the riverview dataframe, we would write:\n\nmdl <- lm(income ~ 1 + education, data = riverview)\nmdl\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\nNote that by calling the name of the fitted model, mdl, you can see the estimated regression coefficients \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). The fitted line is\n\n\n\n\n\n\n\\[\n\\widehat{Income} = 11.32 + 2.65 \\ Education \\\\\n\\]\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nExplore the following equivalent ways to obtain the estimated regression coefficients — that is, \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) — from the fitted model:\n\nmdl\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl)\n\n\n\n\n\n Solution \n\n\nThe estimated parameters returned by the below methods are all equivalent. However, summary() returns more information.\n\nmdl()mdl$coefficientscoef(mdl)coefficients(mdl)summary(mdl)\n\n\nSimply invoke the name of the fitted model:\n\nmdl\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\n\n\n\nmdl$coefficients\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\n\ncoef(mdl)\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\n\ncoefficients(mdl)\n\n(Intercept)   education \n  11.321379    2.651297 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\n\n\n\n\n\n\n\n\n\nThe estimated intercept is \\(\\hat \\beta_0 = 11.32\\) and the estimated slope is \\(\\hat \\beta_1 = 2.65\\).\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nExplore the following equivalent ways to obtain the estimated standard deviation of the errors — that is, \\(\\hat \\sigma\\) — from the fitted model mdl:\n\nsigma(mdl)\nsummary(mdl)\n\n\n Huh? What is \\(\\sigma\\)?\n\n\nThe standard deviation of the errors, denoted by \\(\\sigma\\) is an important quantity to estimate because it measures how much individual data points tend to deviate above and below the regression line.\nA small \\(\\sigma\\) indicates that the points hug the line closely and we should expect fairly accurate predictions, while a large \\(\\sigma\\) suggests that, even if we estimate the line perfectly, we can expect individual values to deviate from it by substantial amounts.\nThe estimated standard deviation of the errors is (surprisingly) denoted \\(\\hat \\sigma\\) and is equal to\n\\[\n\\hat \\sigma = \\sqrt{\\frac{SS_{Residual}}{n - 2}}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\nThe estimated standard deviation of the errors can be equivalently obtained by the below methods. However, summary() returns more information.\n\nsigma(mdl)summary(mdl)\n\n\n\nsigma(mdl)\n\n[1] 8.978116\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term “Residual standard error” is a misnomer, as the help page for sigma says (check ?sigma). However, it’s hard to get rid of this bad name as it has been used in too many books showing R output.\n\n\n\n\n\n\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma = 8.98\\).\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nInterpret the estimated intercept and slope, as well as the estimated standard deviation of the errors in the context of the research question.\n\n\n\n\n Solution \n\n\nWe can interpret the estimated intercept as follows,\n\n\n\n\n\n\nThe estimated average income associated to zero years of formal education is $11,321.\n\n\n\nFor the estimated slope we might write,\n\n\n\n\n\n\nThe estimated increase in average income associated to a one year increase in education is $2,651.\n\n\n\nTo interpret the estimated standard deviation of the errors we can use the fact that about 95% of values from a normal distribution fall within two standard deviations of the centre.\n\n\n\n\n\n\nFor any particular level of education, employee incomes should be distributed above and below the regression line with standard deviation estimated to be \\(\\hat \\sigma = 8.98\\). Since \\(2 \\hat \\sigma = 2 (8.98) = 17.96\\), we expect most (about 95%) of the employee incomes to be within about $18,000 from the regression line.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\nPlot the data and the fitted regression line. To do so:\n\nExtract the estimated regression coefficients e.g., via betas <- coef(mdl)\nExtract the first entry of betas (i.e., the intercept) via betas[1]\nExtract the second entry of betas (i.e., the slope) via betas[2]\nProvide the intercept and slope to the function\n\ngeom_abline(intercept = <intercept>, slope = <slope>)\n\n\n\n\n Solution \n\n\nThe function coef(mdl) returns a vector: that is, a sequence of numbers all of the same type. To get the first element of the sequence you append [1], and [2] for the second.\nWe can plot the model as follows:\n\nbetas <- coef(mdl)\nintercept <- betas[1]\nslope <- betas[2]\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = intercept, slope = slope, \n              color = 'blue', size = 1) + \n  labs(x = \"Education (in years)\", \n       y = \"Income (in thousands of U.S. dollars)\")"
  },
  {
    "objectID": "1_02_slr.html#fitted-and-predicted-values",
    "href": "1_02_slr.html#fitted-and-predicted-values",
    "title": "Intro to Linear Regression",
    "section": "Fitted and Predicted Values",
    "text": "Fitted and Predicted Values\nTo compute the model-predicted values for the data in the sample, we can use various funcitons:\n\npredict(<fitted model>)\nfitted(<fitted model>)\nfitted.values(<fitted model>)\nmdl$fitted.values\n\nFor example, this will give us the estimated income (point on our regression line) for each observed value of education level.\n\npredict(mdl)\n\n       1        2        3        4        5        6        7        8 \n32.53175 32.53175 37.83435 37.83435 37.83435 43.13694 43.13694 43.13694 \n       9       10       11       12       13       14       15       16 \n43.13694 48.43953 48.43953 48.43953 51.09083 53.74212 53.74212 53.74212 \n      17       18       19       20       21       22       23       24 \n53.74212 53.74212 56.39342 59.04472 59.04472 61.69601 61.69601 64.34731 \n      25       26       27       28       29       30       31       32 \n64.34731 64.34731 64.34731 66.99861 66.99861 69.64990 69.64990 74.95250 \n\n\nWe can also compute model-predicted values for other (unobserved) data:\n\npredict(<fitted model>, newdata = <dataframe>)\n\nWe first need to remember that the model predicts income using the independent variable education. Hence, if we want predictions for new data, we first need to create a tibble with a column called education containing the years of education for which we want the prediction.\n\nnewdata <- tibble(education = c(11, 23))\nnewdata\n\n# A tibble: 2 × 1\n  education\n      <dbl>\n1        11\n2        23\n\n\nThen we take newdata and add a new column called income_hat, computed as the prediction from the fitted mdl using the newdata above:\n\nnewdata <- newdata %>%\n  mutate(\n    income_hat = predict(mdl, newdata = newdata)\n  )\nnewdata\n\n# A tibble: 2 × 2\n  education income_hat\n      <dbl>      <dbl>\n1        11       40.5\n2        23       72.3"
  },
  {
    "objectID": "1_02_slr.html#residuals",
    "href": "1_02_slr.html#residuals",
    "title": "Intro to Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nThe residuals represent the deviations between the actual responses and the predicted responses and can be obtained either as\n\nmdl$residuals;\nresid(mdl);\nresiduals(mdl);\ncomputing them as the difference between the response and the predicted response."
  },
  {
    "objectID": "1_02_slr.html#inference-for-regression-coefficients",
    "href": "1_02_slr.html#inference-for-regression-coefficients",
    "title": "Intro to Linear Regression",
    "section": "Inference for Regression Coefficients",
    "text": "Inference for Regression Coefficients\nWe have fitted a linear model, and we now know how we interpret our coefficients. But this is only part of the story. Consider again the output of the summary() function:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\nTo quantify the amount of uncertainty in each estimated coefficient that is due to sampling variability, we use the standard error (SE) of the coefficient. Recall that a standard error gives a numerical answer to the question of how variable a statistic will be because of random sampling.\nThe standard errors are found in the column “Std. Error”. That is, the SE of the intercept is 6.1232, and the SE of the slope corresponding to the education variable is 0.3696.\nIn this example the slope, 2.651, has a standard error of 0.37. One way to envision this is as a distribution. Our best guess (mean) for the slope parameter is 2.651. The standard deviation of this distribution is 0.37, which indicates the precision (uncertainty) of our estimate.\n\n\n\n\n\nSampling distribution of the slope coefficient. The distribution is approximately bell-shaped with a mean of 2.651 and a standard error of 0.37.\n\n\n\n\nIt shouldn’t surprise you that the reference distribution in this case is a \\(t\\)-distribution with \\(n-2\\) degrees of freedom, where \\(n\\) is the sample size."
  },
  {
    "objectID": "1_03_mlr.html",
    "href": "1_03_mlr.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Setup\n\nSetup\n\n\n\nCreate a new RMarkdown file\nLoad the required package(s)\nRead the wellbeing dataset into R, assigning it to an object named mwdata\n\n\n\n\n\n Solution \n\n\n\n#Loading the required package(s)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(sjPlot)\n\n# Reading in data and storing to an object named 'mwdata'\nmwdata <- read_csv(file = 'https://uoepsy.github.io/data/wellbeing.csv')\n\n\n\n\n\n\nStudy Overview\n\nResearch Question\nIs there an association between well-being and time spent outdoors after taking into account the relationship between well-being and social interactions?\n\nThe researchers know that other aspects of peoples’ lifestyles such as how much social interaction they have can influence their mental well-being. They are intersted in furthering this understanding, and want to account for the relationship between psychological wellbeing and time spent outdoors.\n\n\n Wellbeing data codebook.\n\n\nDescription\nResearchers interviewed 32 participants, selected at random from the population of residents of Edinburgh & Lothians. They used the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\nThe researchers also asked participants to estimate the average number of hours they spend outdoors each week, the average number of social interactions they have each week (whether on-line or in-person), and whether they believe that they stick to a routine throughout the week (Yes/No).\nThe data in wellbeing.csv contain five attributes collected from a random sample of \\(n=32\\) hypothetical residents over Edinburgh & Lothians, and include:\n\nwellbeing: Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n\noutdoor_time: Self report estimated number of hours per week spent outdoors\n\nsocial_int: Self report estimated number of social interactions per week (both online and in-person)\nroutine: Binary Yes/No response to the question “Do you follow a daily routine throughout the week?”\nlocation: Location of primary residence (City, Suburb, Rural)\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n\n\n  \n  \n    \n      wellbeing\n      outdoor_time\n      social_int\n      location\n      routine\n    \n  \n  \n    30\n7\n8\nSuburb\nRoutine\n    21\n9\n8\nCity\nNo Routine\n    38\n14\n10\nSuburb\nRoutine\n    27\n16\n10\nCity\nNo Routine\n    20\n1\n10\nRural\nNo Routine\n    37\n11\n12\nSuburb\nNo Routine\n  \n  \n  \n\n\n\n\n\n\n\n\nQuestion 1\n\n\nProduce plots of the marginal distributions (the distributions of each variable in the analysis without reference to the other variables) of the wellbeing, outdoor_time, and social_int variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nYou could use, for example, geom_density() for a density plot or geom_histogram() for a histogram.\nLook at the shape, center and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\n\n\n\n\n\n\n\n\n Solution \n\n\nWe should be familiar now with how to visualise a marginal distribution. You might choose histograms, density curves, or boxplots, or a combination:\n\nwellbeing_plot <- \n  ggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  geom_boxplot(width = 1/250) +\n  labs(x = \"Score on WEMWBS (range 14-70)\", y = \"Probability\\ndensity\")\n\noutdoortime_plot <- \n  ggplot(data = mwdata, aes(x = outdoor_time)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Probability\\ndensity\")\n\nsocial_plot <- \n  ggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  geom_boxplot(width = 1/150) +\n  labs(x = \"Number of social interactions per week\", y = \"Probability\\ndensity\")\n\n# the \"patchwork\" library allows us to arrange multiple plots\nwellbeing_plot / outdoortime_plot / social_plot\n\n\n\n\nFigure 1: Marginal distribution plots of wellbeing sores, weekly hours spent outdoors, and social interactions\n\n\n\n\n\n\nThe marginal distribution of scores on the WEMWBS is unimodal with a mean of approximately 43. There is variation in WEMWBS scores (SD = 11.7).\n\nThe marginal distribution of weekly hours spent outdoors is unimodal with a mean of approximately 14.8. There is variation in weekly hours spent outdoors (SD = 6.9).\n\nThe marginal distribution of numbers of social interactions per week is unimodal with a mean of approximately 16. There is variation in numbers of social interactions (SD = 4.4).\n\n\n\n\n\n\n\nQuestion 2\n\n\nProduce plots of the marginal relationships between the outcome variable (wellbeing) and each of the explanatory variables.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThink about:\n\nDirection of association\nForm of association (can it be summarised well with a straight line?)\n\nStrength of association (how closely do points fall to a recognizable pattern such as a line?)\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail.\n\n\n\n\n\n\n\n\n Solution \n\n\n\nwellbeing_outdoor <- \n  ggplot(data = mwdata, aes(x = outdoor_time, y = wellbeing)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_social <- \n  ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Number of social interactions per week\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_outdoor | wellbeing_social\n\n\n\n\nFigure 2: Scatterplots displaying the relationships between scores on the WEMWBS and a) weekly outdoor time (hours), and b) weekly number of social interactions\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nProduce a correlation matrix of the variables which are to be used in the analysis, and write a short paragraph describing the relationships.\n\nCorrelation matrix\nA table showing the correlation coefficients - \\(r_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\\) - between variables. Each cell in the table shows the relationship between two variables. The diagonals show the correlation of a variable with itself (and are therefore always equal to 1).\n\nIn R, we can create a correlation matrix by giving the cor() function a dataframe. However, we only want to give it 3 columns here. Think about how we select specific columns, either using select(), or giving the column numbers inside [].\n\n\n\n\n\n\n Solution \n\n\nWe can either use:\n\n# correlation matrix of the first 3 columns\ncor(mwdata[,1:3])\n\nor:\n\n# select only the columns we want by name, and pass this to cor()\nmwdata %>% \n  select(wellbeing, outdoor_time, social_int) %>%\n  cor()\n\n             wellbeing outdoor_time social_int\nwellbeing    1.0000000    0.5815613  0.7939003\noutdoor_time 0.5815613    1.0000000  0.3394469\nsocial_int   0.7939003    0.3394469  1.0000000\n\n\n\n\n\n\n\n\n\nThere was a moderate, positive, linear association between weekly outdoor time and WEMWBS scores for the participants in the sample (\\(r\\) = .58). Higher number of hours spent outdoors each week was associated, on average, with higher wellbeing scores,\n\nThere was a moderate, positive, linear association between the weekly number of social interactions and WEMWBS scores for the participants in the sample (\\(r\\) = .79). More social interactions were associated, on average, with higher wellbeing scores.\nThere was a weak positive correlation between weekly outdoor time and the weekly number of social interactions (\\(r\\) = .34).\n\n\n\n\n Note that there is a weak correlation between our two explanatory variables (outdoor_time and social_int). We will return to how this might affect our model when later on we look at the assumptions of multiple regression.\n\n\n\n\n\nQuestion 4\n\n\nSpecify the model equation, where \\(x_1\\) = weekly outdoor time, \\(x_2\\) = weekly numbers of social interactions and \\(y\\) = scores on the WEMWBS. What are the parameters of the model. How do we denote parameter estimates?\nNext, fit the linear model in using lm(), assigning the output to an object called mdl1.\n\n\n\n\n\n\nHint\n\n\n\n\n\nAs we did for simple linear regression, we can fit our multiple regression model using the lm() function. We can add as many explanatory variables as we like, separating them with a +.\n\n( <response variable> ~ 1 + <explanatory variable 1> + <explanatory variable 2> + ... , data = <dataframe> )\n\n\n\n\n\n\n\n\n Solution \n\n\nA model for the relationship between \\(x_1\\) = weekly outdoor time, \\(x_2\\) = weekly numbers of social interactions and \\(y\\) = scores on the WEMWBS is given by:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\\\ \\quad \\\\ \\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nIn the model specified above,\n\n\\(\\mu_{y|x_1, x_2} = \\beta_0 + \\beta_1 x + \\beta_2 x_2\\) represents the systematic part of the model giving the mean of \\(y\\) at each combination of values of \\(x_1\\) and \\(x_2\\);\n\\(\\epsilon\\) represents the error (deviation) from that mean, and the errors are independent from one another.\n\nThe parameters of our model are:\n\n\\(\\beta_0\\) (The intercept);\n\\(\\beta_1\\) (The slope across values of \\(x_1\\));\n\\(\\beta_2\\) (The slope across values of \\(x_2\\));\n\n\\(\\sigma\\) (The standard deviation of the errors).\n\nWhen we estimate these parameters from the available data, we have a fitted model (recall that the h\\(\\hat{\\textrm{a}}\\)ts are used to distinguish our estimates from the true unknown parameters):\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Outdoor Time + \\hat \\beta_2 \\cdot Social Interactions\n\\]\nAnd we have residuals \\(\\hat \\epsilon = y - \\hat y\\) which are the deviations from the observed values and our model-predicted responses.\n\nVisual\nNote that for simple linear regression we talked about our model as a line in 2 dimensions: the systematic part \\(\\beta_0 + \\beta_1 x\\) defined a line for \\(\\mu_y\\) across the possible values of \\(x\\), with \\(\\epsilon\\) as the random deviations from that line. But in multiple regression we have more than two variables making up our model.\nIn this particular case of three variables (one outcome + two explanatory), we can think of our model as a regression surface (see Figure 3). The systematic part of our model defines the surface across a range of possible values of both \\(x_1\\) and \\(x_2\\). Deviations from the surface are determined by the random error component, \\(\\hat \\epsilon\\).\n\n\n\n\n\nFigure 3: Regression surface for wellbeing ~ outdoor_time + social_int, from two different angles\n\n\n\n\nDon’t worry about trying to figure out how to visualise it if we had any more explanatory variables! We can only concieve of 3 spatial dimensions. One could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it’s not worth trying!.\n\nFitting the model in R:\n\nmdl1 <- lm(wellbeing ~ 1 + outdoor_time + social_int, data = mwdata)\n\n\n\n\n\n\nQuestion 5\n\n\nUsing any of:\n\nmdl1\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\nWrite out the estimated parameter values of:\n\n\\(\\hat \\beta_0\\), the estimated average wellbeing score associated with zero hours of outdoor time and zero social interactions per week.\n\n\\(\\hat \\beta_1\\), the estimated increase in average wellbeing score associated with one hour increase in weekly outdoor time, holding the number of social interactions constant (i.e., when the remaining explanatory variables are held at the same value or are fixed).\n\\(\\hat \\beta_2\\), the estimated increase in average wellbeing score associated with an additional social interaction per week (an increase of one), holding weekly outdoor time constant.\n\n\n\n\n\n Solution \n\n\n\nmdl1$coefficientscoef(mdl1)coefficients(mdl1)summary(mdl1)\n\n\n\nmdl1$coefficients\n\n (Intercept) outdoor_time   social_int \n   5.3703775    0.5923673    1.8034489 \n\n\n\n\n\ncoef(mdl1)\n\n (Intercept) outdoor_time   social_int \n   5.3703775    0.5923673    1.8034489 \n\n\n\n\n\ncoefficients(mdl1)\n\n (Intercept) outdoor_time   social_int \n   5.3703775    0.5923673    1.8034489 \n\n\n\n\nLook under the “Estimate” column:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + outdoor_time + social_int, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\n\n\n\n\n\\(\\hat \\beta_0\\) = 5.37\n\n\\(\\hat \\beta_1\\) = 0.59\n\n\\(\\hat \\beta_2\\) = 1.8\n\n\n\n\n\n\nQuestion 6\n\n\nWithin what distance from the model predicted values (the regression surface) would we expect 95% of wEMWBS wellbeing scores to be?\n\n\n\n\n\n\nHint\n\n\n\n\n\nEither sigma() or part of the output from summary() will help you here.\n\n\n\n\n\n\n\n Solution \n\n\n\nsigma(mdl1)summary(mdl1)\n\n\n\nsigma(mdl1)\n\n[1] 6.148276\n\n\n\n\nLook at the “Residual standard error” entry of the summary(mdl) output:\n\nsummary(mdl1)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + outdoor_time + social_int, data = mwdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.742 -4.915 -1.255  5.628 10.936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.3704     4.3205   1.243   0.2238    \noutdoor_time   0.5924     0.1689   3.506   0.0015 ** \nsocial_int     1.8034     0.2691   6.702 2.37e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 29 degrees of freedom\nMultiple R-squared:  0.7404,    Adjusted R-squared:  0.7224 \nF-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09\n\n\n\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma\\) = 6.15. We would expect 95% of wellbeing scores to be within about 12.3 (\\(2 \\hat \\sigma\\)) from the model fit.\n\n\n\n\n\nQuestion 7\n\n\nObtain 95% confidence intervals for the regression coefficients, and write a sentence about each one.\n\n\n\n\n Solution \n\n\n\nconfint(mdl1, level = 0.95)\n\n                  2.5 %     97.5 %\n(Intercept)  -3.4660660 14.2068209\noutdoor_time  0.2468371  0.9378975\nsocial_int    1.2530813  2.3538164\n\n\n\n\nThe average wellbeing score for all those with zero hours of outdoor time and zero social interactions per week is between -3.47 and 14.21.\n\nWhen holding the number of social interactions per week constant, each one hour increase in weekly outdoor time is associated with a difference in wellbeing scores between 0.25 and 0.94, on average.\n\nWhen holding weekly outdoor time constant, each increase of one social interaction per week is associated with a difference in wellbeing scores between 1.25 and 2.35, on average.\n\n\n\n\n\n\n\nNumeric + Categorical\nLet’s do that again, but paying careful attention to where and how the process differs when we have a categorical (or “qualitative”) predictor.\n\nSuppose that the group of researchers were instead wanting to study the relationship between well-being and time spent outdoors after taking into account the relationship between well-being and having a routine.\n\n\nQuestion 8\n\n\nWe have already visualised the marginal distribution of weekly outdoor time in an earlier question, as well as its relationship with wellbeing scores.\nProduce visualisations of:\n\nthe distribution of the routine variable\nthe relationship between routine and wellbeing.\n\n\n\n\n\n\n\nNote\n\n\n\nWe cannot visualise the distribution of routine as a density curve or boxplot, because it is a categorical variable (observations can only take one of a set of discrete response values).\n\n\n\n\n\n\n Solution \n\n\ngeom_bar() will count the number of observations falling into each unique level of the routine variable:\n\nggplot(data = mwdata, aes(x = routine)) +\n  geom_bar()+\n  labs(x = \"Routine\", y = \"Frequency\")\n\n\n\n\nFigure 4: Marginal distribution plots of Routine (y/n)\n\n\n\n\nWe might plot the relationship between routine and wellbeing as two boxplots:\n\nggplot(data = mwdata, aes(x = routine, y = wellbeing)) +\n  geom_boxplot()+\n  labs(x = \"Routine\", y = \"Wellbeing score (WEMWBS)\")\n\n{#fig-relation-wb+outine width=672}\n\n\n\n\n\n\n\nQuestion 9\n\n\nFit the multiple regression model below using lm(), and assign it to an object named mdl2.\n\\[\nWellbeing = \\beta_0 + \\beta_1 \\cdot OutdoorTime + \\beta_2 \\cdot Routine + \\epsilon\n\\]\nExamine the summary output of the model.\n\\(\\hat \\beta_0\\) (the intercept) is the estimated average wellbeing score associated with zero hours of weekly outdoor time and zero in the routine variable. What group is the intercept the estimated wellbeing score for when they have zero hours of outdoor time? Why (think about what zero in the routine variable means)?\n\n\n\n\n Solution \n\n\n\nmdl2 <- lm(wellbeing ~ outdoor_time + routine, data = mwdata)\nsummary(mdl2)\n\n\nCall:\nlm(formula = wellbeing ~ outdoor_time + routine, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.3597  -5.7983   0.1047   7.2899  12.5957 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     26.2525     3.9536   6.640  2.8e-07 ***\noutdoor_time     0.9152     0.2358   3.881 0.000552 ***\nroutineRoutine   7.2947     3.2507   2.244 0.032633 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.06 on 29 degrees of freedom\nMultiple R-squared:  0.4361,    Adjusted R-squared:  0.3972 \nF-statistic: 11.21 on 2 and 29 DF,  p-value: 0.0002467\n\n\nAs you can see in the output of the model, we have a coefficient called routineRoutine. This is the parameter estimate for a dummy variable which has been inputted into the model. The lm() function will automatically name the dummy variables (and therefore the coefficients) according to what level is identified by the 1. It names them <variable><Level>, so we can tell that routineRoutine is 1 for “Routine” and 0 for “No Routine”.\nThe intercept is therefore the estimated wellbeing score for those with No Routine and zero hours of outdoor time.\n\n\n\n\n\nQuestion 10\n\n\nWe can visualise the model \\(\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot OutdoorTime + \\hat \\beta_2 \\cdot Routine\\) as two lines.\nEach line represents the model predicted values for wellbeing scores across the range of weekly outdoor time, with one line for those who report having “Routine” and one for those with “No Routine”.\nGet a pen and paper, and sketch out the plot shown in Figure 5.\n\n\n\n\n\nFigure 5: Multiple regression model: Wellbeing ~ Outdoor Time + Routine\n\n\n\n\nAnnotate your plot with labels for each of parameter estimates from your model:\n\n\n\nParameter Estimate\nModel Coefficient\nEstimate\n\n\n\n\n\\(\\hat \\beta_0\\)\n(Intercept)\n26.25\n\n\n\\(\\hat \\beta_1\\)\noutdoor_time\n0.92\n\n\n\\(\\hat \\beta_2\\)\nroutineRoutine\n7.29\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nBelow you can see where to add the labels, but we have not said which is which.\n\n\n\n\n\n\nA is the vertical distance between the red and blue lines (the lines are parallel, so this distance is the same wherever you cut it on the x-axis).\n\nB is the point at which the blue line cuts the y-axis.\n\nC is the vertical increase (increase on the y-axis) for the blue line associated with a 1 unit increase on the x-axis (the lines are parallel, so this is the same for the red line).\n\n\n\n\n\n\n\n\n Solution \n\n\n\nA = \\(\\hat \\beta_2\\) = routineRoutine coefficient = 7.29\nB = \\(\\hat \\beta_0\\) = (Intercept) coefficient = 26.25\nC = \\(\\hat \\beta_1\\) = outdoor_time coefficient = 0.92"
  },
  {
    "objectID": "1_04_model_fit.html",
    "href": "1_04_model_fit.html",
    "title": "Model Fit and Standardization",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand the calculation and interpretation of the coefficient of determination.\nUnderstand the calculation and interpretation of the F-test of model utility.\nUnderstand how to standardize model coefficients and when this is appropriate to do.\nUnderstand the relationship between the correlation coefficient and the regression slope.\n\n\n\n\n\nBe up to date with lectures\nHave completed previous lab exercises\n\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/riverview.csv. Note: this is the same data as Lab 2."
  },
  {
    "objectID": "1_04_model_fit.html#data-recap",
    "href": "1_04_model_fit.html#data-recap",
    "title": "Model Fit and Standardization",
    "section": "Data recap",
    "text": "Data recap\n\n Riverview data codebook\n\n\nDescription\nThe riverview data come from Lewis-Beck and Lewis-Beck (2015) and contain five attributes collected from a random sample of \\(n=32\\) employees working for the city of Riverview, a hypothetical midwestern city in the US. The attributes include:\n\neducation: Years of formal education\nincome: Annual income (in thousands of U.S. dollars)\nseniority: Years of seniority\ngender: Employee’s gender\nmale: Dummy coded gender variable (0 = Female, 1 = Male)\nparty: Political party affiliation\n\nPreview\nThe first six rows of the data are:\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\nRows: 32 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): gender, party\ndbl (4): education, income, seniority, male\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n \n  \n    education \n    income \n    seniority \n    gender \n    male \n    party \n  \n \n\n  \n    8 \n    37.449 \n    7 \n    male \n    1 \n    Democrat \n  \n  \n    8 \n    26.430 \n    9 \n    female \n    0 \n    Independent \n  \n  \n    10 \n    47.034 \n    14 \n    male \n    1 \n    Democrat \n  \n  \n    10 \n    34.182 \n    16 \n    female \n    0 \n    Independent \n  \n  \n    10 \n    25.479 \n    1 \n    female \n    0 \n    Republican \n  \n  \n    12 \n    46.488 \n    11 \n    female \n    0 \n    Democrat"
  },
  {
    "objectID": "1_04_model_fit.html#back-to-regression-coefficients",
    "href": "1_04_model_fit.html#back-to-regression-coefficients",
    "title": "Model Fit and Standardization",
    "section": "Back to regression coefficients",
    "text": "Back to regression coefficients\n\nQuestion 6\n\n\nCompute the average education level and the average income in the sample.\nUse the predict() function to compute the predicted income for those with average education level.\nWhat do you notice?\n\n\n\n\n Solution \n\n\nSample means:\n\nstats <- riverview %>%\n  summarise(\n    mean_education = mean(education),\n    mean_income = mean(income)\n  )\nstats\n\n# A tibble: 1 × 2\n  mean_education mean_income\n           <dbl>       <dbl>\n1             16        53.7\n\n\nPrediction:\n\nquery <- tibble(education = stats$mean_education)\nquery\n\n# A tibble: 1 × 1\n  education\n      <dbl>\n1        16\n\npredict(mdl, newdata = query)\n\n       1 \n53.74212 \n\n\n The predicted average income for those having average education level is equal to the average income in the sample.\n\n\n\n\nQuestion 7\n\n\nLet’s formalise the previous question using symbols. Consider the fitted model \\(\\hat{y} = \\hat \\beta_0 + \\hat \\beta_1 x\\).\nWhat is the predicted response for an individual having an explanatory variable at the average level \\(\\bar{x}\\)?\n\n\n\n\n\n\nHint\n\n\n\n\n\nSubstitute the formula of \\(\\hat \\beta_0\\) into the equation of the fitted model.\n\n\n\n\n\n\n\n Solution \n\n\nWe require the prediction at \\(x = \\bar{x}\\), that is:\n\\[\n\\hat{y} = \\hat \\beta_0 + \\hat \\beta_1 \\bar{x}\n\\]\nRecall the formula for the fitted intercept: \\(\\hat \\beta_0 = \\bar{y} - \\hat \\beta_1 \\bar{x}\\). Let’s substitute it into the equation:\n\\[\n\\begin{split}\n\\hat{y} &= \\hat \\beta_0 + \\hat \\beta_1 \\bar{x} \\\\\n&= \\bar{y} - \\hat \\beta_1 \\bar{x} + \\hat \\beta_1 \\bar{x} \\\\\n&= \\bar{y}\n\\end{split}\n\\]\nThis highlights an important property of the fitted regression line: it always passes through the point of averages \\((\\bar x, \\bar y)\\).\nIntuitively, what would be your prediction \\(\\hat y\\) when \\(x\\) equals \\(\\bar{x}\\)? If you guessed \\(\\bar{y}\\) you’re on track!"
  },
  {
    "objectID": "1_04_model_fit.html#binary-predictors",
    "href": "1_04_model_fit.html#binary-predictors",
    "title": "Model Fit and Standardization",
    "section": "Binary predictors",
    "text": "Binary predictors\nLet’s suppose that instead of having measured education in years, we had data instead on “Obtained College Degree: Yes/No”. Our explanatory variable would be binary categorical (think back to our discussion of types of data).\nLet us pretend that everyone with >18 years of education has a college degree:\n\n#create new variable called 'degree', where those over age 18 are allocated to Yes condition, and those under 18 to No.\nriverview <- \n  riverview %>%\n    mutate(\n      degree = ifelse(education > 18, \"Yes\", \"No\")\n    )\n\nWe may then plot our relationship as a boxplot. If you want to see the individual points, you could always “jitter” them (right-hand plot below)\n\nggplot(riverview, aes(x = degree, y = income)) + \n  geom_boxplot() +\nggplot(riverview, aes(x = degree, y = income)) + \n  geom_jitter(height=0, width=.05)"
  },
  {
    "objectID": "1_04_model_fit.html#standardization",
    "href": "1_04_model_fit.html#standardization",
    "title": "Model Fit and Standardization",
    "section": "Standardization",
    "text": "Standardization\n\nQuestion 8\n\n\nAdd to the riverview dataset two variables called z_education and z_income representing the standardized education and income variables, respectively.\nWithout using R, if you were to fit a linear regression model using the standardized response and standardized predictor, what would the intercept be?\n\n\n\n\n Solution \n\n\n\nriverview <- riverview %>%\n  mutate(\n    z_education = (education - mean(education)) / sd(education),\n    z_income = (income - mean(income)) / sd(income)\n  )\n\nCheck that they are standardized:\n\nriverview %>%\n  summarise(\n    M_z_education = mean(z_education), SD_z_education = sd(z_education), \n    M_z_income = mean(z_income), SD_z_income = sd(z_income)\n  )\n\n# A tibble: 1 × 4\n  M_z_education SD_z_education M_z_income SD_z_income\n          <dbl>          <dbl>      <dbl>       <dbl>\n1      1.73e-18              1  -1.28e-16           1\n\n\nA standardized variable has mean 0 and standard deviation equal to 1. We can substitute this in the formula of the estimated intercept:\n\\[\n\\hat \\beta_0 = \\bar{y} - \\hat \\beta_1 \\bar{x} = 0 - \\hat \\beta_1 0 = 0\n\\]\nThe intercept of the linear model between the standardized variables is zero — that is, the regression line passes through the origin of the graph.\nBut wait… The origin is also the average of \\(z_x\\) and the average of \\(z_y\\) as they are standardized. In symbols \\((0, 0) = (\\bar{z}_x, \\bar{z}_y)\\).\nWe expected the fitted line to pass through the point of averages — in this case, the origin.\n\n\n\n\nQuestion 9\n\n\nUsing R, fit the regression model using the standardized response and explanatory variables.\nWhat is the slope equal to?\n\n\n\n\n Solution \n\n\nLinear model for standardized variables:\n\nmdl_z <- lm(z_income ~ 1 + z_education, data = riverview)\nsummary(mdl_z)\n\n\nCall:\nlm(formula = z_income ~ 1 + z_education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0863 -0.3974  0.1435  0.3523  1.2629 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.055e-18  1.091e-01   0.000        1    \nz_education  7.948e-01  1.108e-01   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6169 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\n Correlation coefficient of original variables:\n\nriverview %>%\n  select(education, income) %>%\n  cor()\n\n          education    income\neducation 1.0000000 0.7947847\nincome    0.7947847 1.0000000\n\n\n The slope of the standardized variables is equal to the correlation between the original variables.\n\n\n\n\nQuestion 10\n\n\nInterpret the slope of the standardized variables.\n\n\n\n\n Solution \n\n\n\n\n\n\n\n\nFor every standard deviation increase in education, income increases on average by 0.79 standard deviations."
  },
  {
    "objectID": "1_05_recap_writeup.html",
    "href": "1_05_recap_writeup.html",
    "title": "Block 1 Recap",
    "section": "",
    "text": "At the end of this lab, you will:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBe up to date with lectures\nHave completed labs 1 - 4\n\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/cogapoe4.csv"
  },
  {
    "objectID": "1_05_recap_writeup.html#in-line-coding",
    "href": "1_05_recap_writeup.html#in-line-coding",
    "title": "Block 1 Recap",
    "section": "In-line Coding",
    "text": "In-line Coding\nWe have seen above how to interpret and write up sections of our results, but now lets use R to help us do this using inline coding (for a refresher, see Lesson 3 of the RMD Bootcamp)."
  },
  {
    "objectID": "1_07_int1_nc+nn.html",
    "href": "1_07_int1_nc+nn.html",
    "title": "Interactions I: Num x Cat & Num x Num",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand the concept of an interaction.\nBe able to interpret the meaning of a numeric \\(\\times\\) categorical interaction.\nBe able to interpret the meaning of a numeric \\(\\times\\) numeric interaction.\nUnderstand the principle of marginality and why this impacts modelling choices with interactions.\nVisualize and probe interactions.\n\n\n\n\nYou will need to have completed [Lab X].\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npsych\nsjPlot\npatchwork\nkableExtra\nplotly\npander\n\n\n\n\nYou can download the data required for Part A of this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv.\nYou can download the data required for Part B of this lab here or read it in via this link https://uoepsy.github.io/data/scs_study.csv."
  },
  {
    "objectID": "1_07_int1_nc+nn.html#exercises",
    "href": "1_07_int1_nc+nn.html#exercises",
    "title": "Interactions I: Num x Cat & Num x Num",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion A1\n\n\n\nSpecify a multiple regression model to address the research question.\nVisually explore the associations among the variables included in your analysis.\n\n\n“Except in special circumstances, a model including a product term for interaction between two explanatory variables should also include terms with each of the explanatory variables individually, even though their coefficients may not be significantly different from zero. Following this rule avoids the logical inconsistency of saying that the effect of \\(X_1\\) depends on the level of \\(X_2\\) but that there is no effect of \\(X_1\\).”\n— Ramsey and Schafer (2012)\n\n\nHint 1: Check the “location” variable. It currently has three levels (Rural/Suburb/City), but we only want two (Rural/Not Rural). You’ll need to fix this. One way to do this would be to use ifelse() to define a variable which takes one value (“Rural”) if the observation meets from some condition, or another value (“Not Rural”) if it does not. Type ?ifelse in the console if you want to see the help function. You can use it to add a new variable either inside mutate(), or using data$new_variable_name <- ifelse(test, x, y) syntax.\nHint 2: The pairs.panels() function from the psych package will plot all variables in a dataset against one another. This will save you the time you would have spent creating individual plots.\n\n\n\n\n\n Solution \n\n\nTo address the research question, we are going to fit the following model, where \\(y\\) = wellbeing; \\(x_1\\) = weekly outdoor time; and \\(x_2\\) = whether or not the respondent lives in a rural location or not.\n\\[\ny = \\beta_0 + \\beta_1  x_1 + \\beta_2  x_2 + \\beta_3 (x_1 \\cdot x_2) + \\epsilon \\\\\n\\quad \\\\ \\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\quad \\text{independently}\n\\]\nFirst let’s create a new variable for Rural/Not Rural\n\nwrdata <- wrdata %>% \n  mutate(\n    isRural = ifelse(location == \"rural\", \"rural\", \"not rural\")\n  )\n\nNow let’s use the pairs.panels() function from the psych package. We could use it on the whole dataset, but for now we’ll just do it on the variables we’re interested in:\n\nwrdata %>% \n  select(wellbeing, social_int, isRural) %>%\n  pairs.panels()\n\n\n\n\n\n\n\n\nQuestion A2\n\n\nProduce a visualisation of the association between weekly number of social interactions and well-being, with separate facets for rural vs non-rural respondents.\n\n\n\n\n Solution \n\n\n\nggplot(data = wrdata, aes(x = social_int, y = wellbeing))\n\n\n\n  geom_point() \n\ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\n  facet_wrap(~isRural) \n\n<ggproto object: Class FacetWrap, Facet, gg>\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  <ggproto object: Class FacetWrap, Facet, gg>\n\n\n\n\n\n\nQuestion A3\n\n\nFit your model using lm(), and assign it as an object with the name “rural_mod”.\nHint: When fitting a regression model in R with two explanatory variables A and B, and their interaction, these two are equivalent:\n\ny ~ A + B + A:B\ny ~ A*B\n\n\n\n\n\n Solution \n\n\n\n#fit model including interaction between social_int and isRural\nrural_mod <- lm(wellbeing ~ 1 + social_int * isRural, data = wrdata)\n\n#check model output\nsummary(rural_mod)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int * isRural, data = wrdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.4845  -2.7975   0.0155   2.4539  15.6743 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              30.9986     1.4284  21.702  < 2e-16 ***\nsocial_int                0.6488     0.1160   5.593 7.42e-08 ***\nisRuralrural              1.3866     2.0510   0.676  0.49981    \nsocial_int:isRuralrural  -0.5176     0.1615  -3.206  0.00157 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.558 on 196 degrees of freedom\nMultiple R-squared:  0.2962,    Adjusted R-squared:  0.2854 \nF-statistic: 27.49 on 3 and 196 DF,  p-value: 6.97e-15\n\n\n\n\n\n\nInterpreting coefficients for A and B in the presence of an interaction A:B\nWhen you include an interaction between \\(x_1\\) and \\(x_2\\) in a regression model, you are estimating the extent to which the effect of \\(x_1\\) on \\(y\\) is different across the values of \\(x_2\\).\nWhat this means is that the effect of \\(x_1\\) on \\(y\\) depends on/is conditional upon the value of \\(x_2\\).\n(and vice versa, the effect of \\(x_2\\) on \\(y\\) is different across the values of \\(x_1\\)).\nThis means that we can no longer talk about the “effect of \\(x_1\\) holding \\(x_2\\) constant”. Instead we can talk about a marginal effect of \\(x_1\\) on \\(y\\) at a specific value of \\(x_2\\).\n\nWhen we fit the model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 \\cdot x_2) + \\epsilon\\) using lm():\n\nthe parameter estimate \\(\\hat \\beta_1\\) is the marginal effect of \\(x_1\\) on \\(y\\) where \\(x_2 = 0\\)\n\nthe parameter estimate \\(\\hat \\beta_2\\) is the marginal effect of \\(x_2\\) on \\(y\\) where \\(x_1 = 0\\)\n\n\n\n\n N.B. Regardless of whether or not there is an interaction term in our model, all parameter estimates in multiple regression are “conditional” in the sense that they are dependent upon the inclusion of other variables in the model. For instance, in \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\) the coefficient \\(\\hat \\beta_1\\) is conditional upon holding \\(x_2\\) constant. \n\n\n\nInterpreting the interaction term A:B\nThe coefficient for an interaction term can be thought of as providing an adjustment to the slope.\nIn the model below, we have a numeric*categorical interaction:\n\\[\n\\begin{align}\n\\text{wellbeing} \\ = \\ &\\beta_0 + \\beta_1 \\text{social_interactions} + \\beta_2 \\text{isRural} + \\\\\n&\\beta_3 (\\text{social_interactions} \\cdot \\text{isRural}) + \\epsilon\n\\end{align}\n\\]\nThe estimate \\(\\hat \\beta_3\\) is the adjustment to the slope \\(\\hat \\beta_1\\) to be made for the individuals in the \\(\\text{isRural}=1\\) group.\n\n\nQuestion A4\n\n\nLook at the parameter estimates from your model, and write a description of what each one corresponds to on the plot shown in Figure @ref(fig:plot-annotate-int) (it may help to sketch out the plot yourself and annotate it).\n\n“The best method of communicating findings about the presence of significant interaction may be to present a table of graph of the estimated means at various combinations of the interacting variables.”\n— Ramsey and Schafer (2012)\n\n\n\nScale for 'colour' is already present. Adding another scale for 'colour',\nwhich will replace the existing scale.\n\n\nScale for 'x' is already present. Adding another scale for 'x', which will\nreplace the existing scale.\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\nMultiple regression model: Wellbeing ~ Social Interactions * is RuralNote that the dashed lines represent predicted values below the minimum observed number of social interactions, to ensure that zero on the x-axis is visible\n\n\n\n\n\n Hints.\n\n\nHere are some options to choose from:\n\nThe point at which the blue line cuts the y-axis (where social_int = 0)\nThe point at which the red line cuts the y-axis (where social_int = 0)\nThe average vertical distance between the red and blue lines.\nThe vertical distance from the blue to the red line at the y-axis (where social_int = 0)\nThe vertical distance from the red to the blue line at the y-axis (where social_int = 0)\nThe vertical distance from the blue to the red line at the center of the plot\nThe vertical distance from the red to the blue line at the center of the plot\nThe slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line\nThe slope of the red line\nHow the slope of the line changes when you move from the blue to the red line\nHow the slope of the line changes when you move from the red to the blue line\n\n\n\n\n\n\n\n\n Solution \n\n\nWe can obtain our parameter estimates using various functions such as summary(rural_mod),coef(rural_mod), coefficients(rural_mod) etc.\n\ncoefficients(rural_mod)\n\n            (Intercept)              social_int            isRuralrural \n             30.9985688               0.6487945               1.3865688 \nsocial_int:isRuralrural \n             -0.5175856 \n\n\n\n\\(\\hat \\beta_0\\) = (Intercept) = 31: The point at which the blue line cuts the y-axis (where social_int = 0).\n\n\\(\\hat \\beta_1\\) = social_int = 0.65: The slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line.\n\\(\\hat \\beta_2\\) = isRuralrural = 1.39: The vertical distance from the blue to the red line at the y-axis (where social_int = 0).\n\n\\(\\hat \\beta_3\\) = social_int:isRuralrural = -0.52: How the slope of the line changes when you move from the blue to the red line.\n\n\n\n\n\nQuestion A5\n\n\nLoad the sjPlot package and try using the function plot_model().\nThe default behaviour of plot_model() is to plot the parameter estimates and their confidence intervals. This is where type = \"est\". Try to create a plot like Figure @ref(fig:plot-annotate-int), which shows the two lines (Hint: what are this weeks’ exercises all about? type = ???.)\n\n\n\n\n Solution \n\n\n\nplot_model(rural_mod, type=\"int\")"
  },
  {
    "objectID": "1_07_int1_nc+nn.html#exercises-1",
    "href": "1_07_int1_nc+nn.html#exercises-1",
    "title": "Interactions I: Num x Cat & Num x Num",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion B1\n\n\nProduce plots of the relevant distributions and associations involved in the research question.\n\n\n\n\n Solution \n\n\n\nggplot(data = scs_study, aes(x=dass)) + \n  geom_density() + \n  geom_boxplot(width = 1/50) +\n  labs(title=\"Marginal distribution of DASS-21 Scores\", \n       x = \"Depression Anxiety and Stress Scale\", y = \"Probability density\")\n\n\n\n\n\nThe marginal distribution of scores on the Depression, Anxiety and Stress Scale (DASS-21) is unimodal with a mean of approximately 45 and a standard deviation of 7.\n\n\nggplot(data = scs_study, aes(x=scs)) + \n  geom_density() + \n  geom_boxplot(width = 1/50) +\n  labs(title=\"Marginal distribution of Social Comparison Scale (SCS) scores\", \n       x = \"Social Comparison Scale Score\", y = \"Probability density\")\n\n\n\n\n\nThe marginal distribution of score on the Social Comparison Scale (SCS) is unimodal with a mean of approximately 36 and a standard deviation of 4. There look to be a number of outliers at the upper end of the scale.\n\n\nggplot(data = scs_study, aes(x=zn)) + \n  geom_density() + \n  geom_boxplot(width = 1/50) +\n  labs(title=\"Marginal distribution of Neuroticism (Z-Scored)\", \n       x = \"Neuroticism (Z-Scored)\", y = \"Probability density\")\n\n\n\n\n\nThe marginal distribution of Neuroticism (Z-scored) is positively skewed, with the 25% of scores falling below -0.8, 75% of scores falling below 0.59.\n\n\np1 <- ggplot(data = scs_study, aes(x=scs, y=dass)) + \n  geom_point()+\n  labs(x = \"SCS\", y = \"DASS-21\")\n\np2 <- ggplot(data = scs_study, aes(x=zn, y=dass)) + \n  geom_point()+\n  labs(x = \"Neuroticism\", y = \"DASS-21\")\n\np1 | p2\n\n\n\n# the kable() function from the kableExtra package can make table outputs print nicely into html.\nscs_study %>%\n  select(dass, scs, zn) %>%\n  cor() %>% \n  kable(digits = 2) %>%\n  kable_styling(full_width = FALSE)\n\n\n\n \n  \n      \n    dass \n    scs \n    zn \n  \n \n\n  \n    dass \n    1.00 \n    -0.23 \n    0.20 \n  \n  \n    scs \n    -0.23 \n    1.00 \n    0.11 \n  \n  \n    zn \n    0.20 \n    0.11 \n    1.00 \n  \n\n\n\n\n\n\nThere is a weak, negative, linear relationship between scores on the Social Comparison Scale and scores on the Depression Anxiety and Stress Scale for the participants in the sample. Severity of symptoms measured on the DASS-21 tend to decrease, on average, the more favourably participants view their social rank.\nThere is a weak, positive, linear relationship between the levels of Neuroticism and scores on the DASS-21. Participants who are more neurotic tend to, on average, display a higher severity of symptoms of depression, anxiety and stress.\n\n\n\n\n\nQuestion B2\n\n\nRun the code below. It takes the dataset, and uses the cut() function to add a new variable called “zn_group”, which is the “zn” variable split into 4 groups.\nRemember: we have to re-assign this output as the name of the dataset (the scs_study <- bit at the beginning) to make these changes occur in our environment (the top-right window of Rstudio). If we didn’t have the first line, then it would simply print the output.\n\nscs_study <-\n  scs_study %>%\n  mutate(\n    zn_group = cut(zn, 4)\n  )\n\nWe can see how it has split the “zn” variable by plotting the two against one another:\n(Note that the levels of the new variable are named according to the cut-points).\n\nggplot(data = scs_study, aes(x = zn_group, y = zn)) + \n  geom_point()\n\n\n\n\nPlot the association between scores on the SCS and scores on the DASS-21, for each group of the variable we just created.\nHow does the pattern differ? Does it suggest an interaction?\nTip: Rather than creating four separate plots, you might want to map some feature of the plot to the variable we created in the data, or make use of facet_wrap()/facet_grid().\n\n\n\n\n Solution \n\n\n\nggplot(data = scs_study, aes(x = scs, y = dass, col = zn_group)) + \n  geom_point() + \n  facet_grid(~zn_group) +\n  theme(legend.position = \"none\") # remove the legend\n\n\n\n\nThe associaiton between SCS scores and DASS-21 scores appears to be different between these groups. For those with a relatively high neuroticism score, the association seems stronger, while for those with a low neuroticism score there is almost no discernable association. This suggests an interaction - the association of DASS-21 ~ SCS differs across the values of neuroticism!\n\n\n\n\nCutting one of the explanatory variables up into groups essentially turns a numeric variable into a categorical one. We did this just to make it easier to visualise how a an association differs across the values of another variable, because we can imagine a separate line for the association between SCS and DASS-21 scores for each of the groups of neuroticism. However, in grouping a numeric variable like this we lose information. Neuroticism is measured on a continuous scale, and we want to capture how the association between SCS and DASS-21 differs across that continuum (rather than cutting it into chunks).\nWe could imagine cutting it into more and more chunks (see Figure @ref(fig:reglinescut)), until what we end up with is a an infinite number of lines - i.e., a three-dimensional plane/surface (recall that in for a multiple regression model with 2 explanatory variables, we can think of the model as having three-dimensions). The inclusion of the interaction term simply results in this surface no longer being necessarily flat. You can see this in Figure @ref(fig:3dint).\n\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nSeparate regression lines DASS ~ SCS for neuroticism when cut into 4 (left) or 6 (center) or 12 (right) groups\n\n\n\n\n\n\n\n\n3D plot of regression surface with interaction. You can explore the plot in the figure below from different angles by moving it around with your mouse.\n\n\n\n\nQuestion B3\n\n\nSpecify the model required in order to answer the research question (e.g., \\(\\text{??} = \\beta_0 + \\beta_1 \\cdot \\text{??} + .... + \\epsilon\\)), and then fit your model using lm().\n\n\n\n\n Solution \n\n\n\n\n\nModel to answer research question:\n\\[\n\\text{DASS-21 Score} = \\beta_0 + \\beta_1 \\cdot \\text{SCS Score} + \\beta_2 \\cdot \\text{Neuroticism} + \\beta_3 \\cdot (\\text{SCS score} \\cdot \\text{Neuroticism}) + \\epsilon\n\\]\n\n#fit interaction model\ndass_mdl <- lm(dass ~ 1 + scs*zn, data = scs_study)\n\nsummary(dass_mdl)\n\n\nCall:\nlm(formula = dass ~ 1 + scs * zn, data = scs_study)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.301  -3.825  -0.173   3.733  45.777 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 60.80887    2.45399  24.780  < 2e-16 ***\nscs         -0.44391    0.06834  -6.495 1.64e-10 ***\nzn          20.12813    2.35951   8.531  < 2e-16 ***\nscs:zn      -0.51861    0.06552  -7.915 1.06e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.123 on 652 degrees of freedom\nMultiple R-squared:  0.1825,    Adjusted R-squared:  0.1787 \nF-statistic:  48.5 on 3 and 652 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "1_08_int2_cc.html",
    "href": "1_08_int2_cc.html",
    "title": "Interactions II: Cat x Cat",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to interpret a categorical \\(\\times\\) categorical interaction.\nVisualize and probe interactions.\nBe able to read interaction plots.\n\n\n\n\nYou will need to have completed [Lab X].\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\nemmeans\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/cognitive_experiment_3_by_2.csv"
  },
  {
    "objectID": "1_08_int2_cc.html#visualising-the-interactions",
    "href": "1_08_int2_cc.html#visualising-the-interactions",
    "title": "Interactions II: Cat x Cat",
    "section": "Visualising the interactions",
    "text": "Visualising the interactions\n\n\n\n\n\nIn the interaction plot above you can see three highlighted differences, where the differences are denoted with the Greek letter \\(\\Delta\\) (“delta”) with a hat on top, \\(\\hat \\Delta\\), to denote that those are estimates for the unknown population differences based on the available sample data. The corresponding population differences are unknown as we don’t have the data for the entire population, and they are denoted with a \\(\\Delta\\) without a hat on top.\nYou can see highlighted:\n\nThe difference in the mean score between Recognition and Grammar for Control patients, \\(\\hat \\Delta_{\\text{Control}}\\)\nThe difference in the mean score between Recognition and Grammar for Amnesic patients, \\(\\hat \\Delta_{\\text{Amnesic}}\\)\nThe difference in the mean score between Recognition and Grammar for Huntingtons patients, \\(\\hat \\Delta_{\\text{Huntingtons}}\\)\n\nAn interaction is present if the effect of Task (i.e. the difference in mean score between Recognition and Grammar tasks) substantially varies across the possible values for Diagnosis. That is, if the difference for Amnesic is not the same as that for Control, or if the difference for Huntingtons is not the same as that for Control, or both.\nThe model summary returns two rows for the interactions. Let’s focus on this row:\n## DiagnosisAmnesic:TaskRecognition      -10.000     11.719  -0.853  0.40192    \nIt returns an estimate for the difference\n\\[\n\\hat \\beta_4 = \\hat \\Delta_{\\text{Amnesic}} - \\hat \\Delta_{\\text{Control}}\n\\]\nand also performs a test for the hypothesis that the differences are equal in the population:\n\\[\nH_0: \\Delta_{\\text{Amnesic}} = \\Delta_{\\text{Control}} \\\\\nH_1: \\Delta_{\\text{Amnesic}} \\neq \\Delta_{\\text{Control}}\n\\]\nor, equivalently, that:\n\\[\nH_0: \\Delta_{\\text{Amnesic}} - \\Delta_{\\text{Control}} = 0 \\\\\nH_1: \\Delta_{\\text{Amnesic}} - \\Delta_{\\text{Control}} \\neq 0\n\\]\nLet’s now focus on the last row:\n## DiagnosisHuntingtons:TaskRecognition   40.000     11.719   3.413  0.00228 ** \nIt returns an estimate for the difference\n\\[\n\\hat \\beta_5 = \\hat \\Delta_{\\text{Huntingtons}} - \\hat \\Delta_{\\text{Control}}\n\\]\nand also performs a test for the hypothesis that the differences are equal in the population:\n\\[\nH_0: \\Delta_{\\text{Huntingtons}} = \\Delta_{\\text{Control}} \\\\\nH_1: \\Delta_{\\text{Huntingtons}} \\neq \\Delta_{\\text{Control}}\n\\]\nor, equivalently, that:\n\\[\nH_0: \\Delta_{\\text{Huntingtons}} - \\Delta_{\\text{Control}} = 0 \\\\\nH_1: \\Delta_{\\text{Huntingtons}} - \\Delta_{\\text{Control}} \\neq 0\n\\]\n\nQuestion 8\n\n\nInterpret the model output in the context of the research hypothesis.\n\n\n\n\n Solution \n\n\nLet’s recall the researchers’ hypothesis:\n\nA group of researchers wants to test an hypothesised theory according to which the difference in performance between explicit and implicit memory tasks will be greatest for Huntington patients in comparison to controls.\nOn the other hand, the difference in performance between explicit and implicit memory tasks will not significantly differ between patients with amnesia in comparison to controls.\n\nWe can get a nice printout of the model summary as follows:\n\n\ntab_model(mdl_int, show.stat = TRUE)\n\n\n\n\n \nScore\n\n\nPredictors\nEstimates\nCI\nStatistic\np\n\n\n(Intercept)\n80.00\n67.91 – 92.09\n13.65\n<0.001\n\n\nDiagnosis [Amnesic]\n-20.00\n-37.10 – -2.90\n-2.41\n0.024\n\n\nDiagnosis [Huntingtons]\n-40.00\n-57.10 – -22.90\n-4.83\n<0.001\n\n\nTask [Recognition]\n15.00\n-2.10 – 32.10\n1.81\n0.083\n\n\nDiagnosis [Amnesic] *Task [Recognition]\n-10.00\n-34.19 – 14.19\n-0.85\n0.402\n\n\nDiagnosis [Huntingtons] *Task [Recognition]\n40.00\n15.81 – 64.19\n3.41\n0.002\n\n\nObservations\n30\n\n\nR2 / R2 adjusted\n0.739 / 0.685\n\n\n\n\n\n\n\n\nWe could interpret it as follows:\n\nThe difference in scores between the recognition and grammar tasks, respectively measuring explicit and implicit memory, for Huntingtons patients in comparison to controls was significant and indicated a difference of \\(\\hat \\beta_5 = 40\\) points in explicit vs implicit memory performance: \\(t(24) = 3.41, p = 0.002\\).\nThe difference in scores between the recognition and grammar tasks, respectively measuring explicit and implicit memory, for amnesiac patients in comparison to controls was estimated to be \\(\\hat \\beta_4 = -10\\) points but it is not found to be significantly different from 0: \\(t(24) = -0.85, p = 0.40\\).\nThis indicates that the researchers’ hypothesis that the difference in performance between explicit and implicit memory tasks does not differ significantly between amnesic and control patients, while it does differ significantly between Huntington and control patients.\n\nWe can also provide an interpretation of the interaction plot:\n\nplot_model(mdl_int, type = \"int\")\n\n\n\n\nCompared to controls, amnesiac patients will have a significant deficit in explicit memory (as measured by the recognition task), but not on implicit memory (as measured by the grammar task).\nCompared to controls, Huntingtons patients will have a significant deficit in implicit memory (as measured by the grammar task) but not in explicit memory (as measured by the recognition task)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr Umberto Noe\nDr Josiah King\nDr Emma Waterston\nDepartment of Psychology, The University of Edinburgh"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 2 (DAPR2) lab workbook. Using the menu above, you can find lab materials for each week."
  },
  {
    "objectID": "index.html#solutions",
    "href": "index.html#solutions",
    "title": "Home",
    "section": "Solutions",
    "text": "Solutions\nSolutions will be made available immediately below each exercise. Do make sure to attempt each question before checking the solution."
  },
  {
    "objectID": "index.html#asking-questions",
    "href": "index.html#asking-questions",
    "title": "Home",
    "section": "Asking Questions",
    "text": "Asking Questions\nWe encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#tips-on-googling-statistics-and-r",
    "href": "index.html#tips-on-googling-statistics-and-r",
    "title": "Home",
    "section": "Tips on Googling Statistics and R",
    "text": "Tips on Googling Statistics and R\nSearching online for help with statistics and R can be both a help and a hindrance. If you have an error message in R, copy the error message into Google. The results returned can sometimes just cause more confusion, but sometimes something might jump out at you and help you solve the problem. The same applies with searching the internet for help with statistics - search for “what is a p-value”, and you’ll find many many different articles and forum discussions etc. Some of them you will find too technical, but don’t be scared - the vast majority of people work in statistics will find these too technical too. Some of them you might feel are too simple/not helpful. As a general guide, keep clicking around the search responses, and you may end up finding that someone, somewhere, has provided an explanation at the right level. If you find something during your search which you don’t quite understand, feel free to link it in a post on the discussion forum!"
  },
  {
    "objectID": "index.html#feedback-on-labs",
    "href": "index.html#feedback-on-labs",
    "title": "Home",
    "section": "Feedback on Labs",
    "text": "Feedback on Labs\nIf you wish to make suggestions for improvements to these workbooks (or if you spot any typos!), please email ppls.psych.stats@ed.ac.uk making sure to include the course name in the subject."
  },
  {
    "objectID": "1_02_slr.html#data-xxploration",
    "href": "1_02_slr.html#data-xxploration",
    "title": "Intro to Linear Regression",
    "section": "Data Xxploration",
    "text": "Data Xxploration\nThe common first port of call for almost any statistical analysis is to explore the data, and we can do this visually and/or numerically.\n\nMarginal Distributions\nMarginal distribution: The distribution of each variable (e.g., employee incomes and education levels) without reference to the values of the other variables. Typical steps when examining the marginal distribution of a numeric variable are:\n\nVisualise the distribution of the variable. You could use, for example, geom_density() for a density plot or geom_histogram() for a histogram.\nComment on the shape of the distribution. Look at the shape, centre and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\nIdentify any unusual observations. Do you notice any extreme observations (i.e., outliers)?\n\n\nQuestion 1\n\n\nVisualise and describe the marginal distribution of (a) employee incomes, and (b) education level.\n\n\n\n\n Solution \n\n\nWe can plot the marginal distributions of employee incomes and education level as density curves, and add a boxplot underneath to check for the presence of outliers.\n\nEmployee Incomes\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = income)) +\n  geom_density() +\n  geom_boxplot(width = 1/300) +\n  labs(x = \"Income (in thousands of U.S. dollars)\", \n       y = \"Probability density\")\n\n\n\n\nFigure 1: Density plot and boxplot of employee incomes\n\n\n\n\nThe plot suggested that the distribution of employee incomes was unimodal, and most of the incomes were between roughly $45,000 and $70,000. The lowest income in the sample was approximately $25,000 and the highest over $80,000. This suggested there was a fair high degree of variation in the data. Furthermore, the boxplot did not highlight any outliers in the data.\nTo further summarize the distribution, it is typical to compute and report numerical summary statistics such as the mean and standard deviation. One way to compute these values is to use the summary() function:\n\ndesc_income <- riverview %>% \n  summarize(\n    M = mean(income), \n    SD = sd(income)\n    )\ndesc_income\n\n# A tibble: 1 × 2\n      M    SD\n  <dbl> <dbl>\n1  53.7  14.6\n\n\nFollowing the exploration above, we can describe the income variable as follows:\n\n\n\n\n\n\nThe marginal distribution of income was unimodal with a mean of approximately $53,700. There was variation in employees’ salaries (SD = $14,553).\n\n\n\n\n\nEducation Level\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = education)) +\n  geom_density() +\n  geom_boxplot(width = 1/100) +\n  labs(x = \"Education (in years)\", \n       y = \"Probability density\")\n\n\n\n\nFigure 2: Density plot and boxplot of employee education levels\n\n\n\n\nSummary statistics for the employees’ level of education:\n\ndesc_education <- riverview %>%\n  summarize(\n    M = mean(education),\n    SD = sd(education)\n    )\ndesc_education\n\n# A tibble: 1 × 2\n      M    SD\n  <dbl> <dbl>\n1    16  4.36\n\n\n\n\n\n\n\n\n\nThe marginal distribution of education was unimodal with an average of of 16 years. There was variation in employees’ level of education (SD = 4.4 years).\n\n\n\n\n\n\n\n\n\n\nAssociations among Variables\nAfter examining the marginal distributions of the variables of interest in the analysis, we typically move on to examining relationships between the variables.\nWhen describing the relationship between two numeric variables, we typically look at their scatterplot and comment on four characteristics of the relationship:\n\nThe direction of the association indicates whether large values of one variable tend to go with large values of the other (positive association) or with small values of the other (negative association).\nThe form of association refers to whether the relationship between the variables can be summarized well with a straight line or some more complicated pattern.\nThe strength of association entails how closely the points fall to a recognizable pattern such as a line.\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail.\n\n\nQuestion 2\n\n\nCreate a scatterplot of income and education level before calculating the correlation between the income and education level.\nMaking reference to both the plot and correlation coefficient, describe the relationship between income and level of education among the employees in the sample.\n\n\n\n\n Solution \n\n\nWe are trying to investigate how income varies when varying years of formal education. Hence, income is the dependent variable (on the y-axis), and education is the independent variable (on the x-axis).\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Education (in years)\", \n       y = \"Income (in thousands of U.S. dollars)\")\n\n\n\n\nFigure 3: The relationship between employees’ education level and income\n\n\n\n\nTo comment on the strength of the linear association we compute the correlation coefficient:\n\ncorr <- riverview %>%\n  select(education, income) %>%\n  cor()\ncorr\n\n          education    income\neducation 1.0000000 0.7947847\nincome    0.7947847 1.0000000\n\n\nthat is,\n\\[\nr_{\\text{education, income}} = 0.79\n\\]\n\n\n\n\n\n\n\nThere was a strong positive linear association between education level and income for the employees in the sample. High incomes tended to be observed, on average, with more years of formal education.\nThe scatterplot did not highlight any outliers."
  },
  {
    "objectID": "index.html#hints-notes-and-example-write-up-interpretation",
    "href": "index.html#hints-notes-and-example-write-up-interpretation",
    "title": "Home",
    "section": "Hints, Notes, and Example Write-Up / Interpretation",
    "text": "Hints, Notes, and Example Write-Up / Interpretation\nHints are shown in a green box, with the title ‘Hint’. If you are unsure what to do, check the collapsible hint provided (note that these are only present for some questions).\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is an example of a hint.\n\n\n\nNotes are displayed in blue boxes, with the title ‘Note’. These are occasionally used to draw your attention to a specific point.\n\n\n\n\n\n\nNote\n\n\n\nThis is an example of a note.\n\n\nExample write-ups and/or interpretations are shown with a red right border. These are helpful to check your interpretation against, and a useful guide to use in future when interpreting and/or writing up results.\n\n\n\n\n\n\nThis is an example write-up / interpretation block."
  },
  {
    "objectID": "index.html#help-feedback",
    "href": "index.html#help-feedback",
    "title": "Home",
    "section": "Help & Feedback",
    "text": "Help & Feedback\n\nWithin Lab Workbook\n\nHints, Notes, and Example Write-Up / Interpretation\nHints are shown in a green box, with the title ‘Hint’. If you are unsure what to do, check the collapsible hint provided (note that these are only present for some questions).\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is an example of a hint.\n\n\n\nNotes are displayed in blue boxes, with the title ‘Note’. These are occasionally used to draw your attention to a specific point.\n\n\n\n\n\n\nNote\n\n\n\nThis is an example of a note.\n\n\nExample write-ups and/or interpretations are shown with a red right border. These are helpful to check your interpretation against, and a useful guide to use in future when interpreting and/or writing up results.\n\n\n\n\n\n\nThis is an example write-up / interpretation block.\n\n\n\n\n\n\nAsking Questions\nDuring labs, if you have a question, please ask one of the tutors for support. Outside of labs, we encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#help-support-feedback",
    "href": "index.html#help-support-feedback",
    "title": "Home",
    "section": "Help, Support & Feedback",
    "text": "Help, Support & Feedback\n\nWithin Lab Workbook\n\nHints, Notes, and Example Write-Up / Interpretation\nHints are shown in a green box, with the title ‘Hint’. If you are unsure what to do, check the collapsible hint provided (note that these are only present for some questions).\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis is an example of a hint.\n\n\n\nNotes are displayed in blue boxes, with the title ‘Note’. These are occasionally used to draw your attention to a specific point.\n\n\n\n\n\n\nNote\n\n\n\nThis is an example of a note.\n\n\nExample write-ups and/or interpretations are shown with a red right border. These are helpful to check your interpretation against, and a useful guide to use in future when interpreting and/or writing up results.\n\n\n\n\n\n\nThis is an example write-up / interpretation block.\n\n\n\n\n\nSolutions\nSolutions are made available immediately below each exercise. To view solutions, click the drop down ‘Solution’ button.\nImportant  Before checking the solution you should attempt the question. You should also avoid copying and pasting code from the solutions. If you do check solutions right away and simply copy the answer, you will struggle to learn from the exercises.\nInstead, you should:\n\nTry to figure out the answer yourself or with your peers (and/or ask for help from a staff member if needed)\nType the code out yourself (and annotate your R code chunks so you know what your code is doing & why - future you will thank you for this).\n\n\n\n\nAsking Questions\n\nDuring labs, if you have a question, please ask one of the tutors for support.\nOutside of labs, we encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "1_04_model_fit.html#options",
    "href": "1_04_model_fit.html#options",
    "title": "Model Fit and Standardization",
    "section": "Options",
    "text": "Options\n\nOption 1\nThe R-squared coefficient is defined as:\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n\\]\nIn R we can write:\n\nriverview_fitted <- riverview %>%\n  mutate(\n    income_hat = predict(mdl),\n    resid = income - income_hat\n  )\nhead(riverview_fitted)\n\n# A tibble: 6 × 8\n  education income seniority gender  male party       income_hat  resid\n      <dbl>  <dbl>     <dbl> <chr>  <dbl> <chr>            <dbl>  <dbl>\n1         8   37.4         7 male       1 Democrat          32.5   4.92\n2         8   26.4         9 female     0 Independent       32.5  -6.10\n3        10   47.0        14 male       1 Democrat          37.8   9.20\n4        10   34.2        16 female     0 Independent       37.8  -3.65\n5        10   25.5         1 female     0 Republican        37.8 -12.4 \n6        12   46.5        11 female     0 Democrat          43.1   3.35\n\nriverview_fitted %>%\n  summarise(\n    SSModel = sum( (income_hat - mean(income))^2 ),\n    SSTotal = sum( (income - mean(income))^2 )\n  ) %>%\n  summarise(\n    RSquared = SSModel / SSTotal\n  )\n\n# A tibble: 1 × 1\n  RSquared\n     <dbl>\n1    0.632\n\n\n\n\nOption 2\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\nThe output of summary() displays the R-squared value in the following line:\nMultiple R-squared:  0.6317\nFor the moment, ignore “Adjusted R-squared”. We will come back to this later in the course.\n\n\nOption 3\nWe can perform an ANalysis Of VAriance or, in short, ANOVA. It simply means that we are examining/partitioning the total variability of a response variable.\nThe anova() function returns the sum of squares of interest in the column Sum Sq:\n\nmdl_anova <- anova(mdl)\nmdl_anova\n\nAnalysis of Variance Table\n\nResponse: income\n          Df Sum Sq Mean Sq F value    Pr(>F)    \neducation  1 4147.3  4147.3  51.452 5.562e-08 ***\nResiduals 30 2418.2    80.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConsider the column Sum Sq. The entry corresponding to education gives \\(SS_{Model}\\) = 4147.3, as education is the explanatory variable. The entry corresponding to Residuals gives \\(SS_{Residual}\\) = 2418.2.\n\n# Because the column name Sum Sq has a space, we need to wrap it with backticks\nSSModel <- mdl_anova$`Sum Sq`[1]\nSSResidual <- mdl_anova$`Sum Sq`[2]\nSSTotal <- SSModel + SSResidual\n\nRSquared <- SSModel / SSTotal\nRSquared\n\n[1] 0.6316828\n\n\n\nInterpretation\n\n\n\n\n\n\nApproximately 63% of the total variability in employee incomes is explained by the linear association with education level."
  },
  {
    "objectID": "1_04_model_fit.html#summary-output",
    "href": "1_04_model_fit.html#summary-output",
    "title": "Model Fit and Standardization",
    "section": "summary() output",
    "text": "summary() output\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\nThe relevant row is the following:\nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08"
  },
  {
    "objectID": "1_04_model_fit.html#anova-output",
    "href": "1_04_model_fit.html#anova-output",
    "title": "Model Fit and Standardization",
    "section": "anova() output",
    "text": "anova() output\n\nanova(mdl)\n\nAnalysis of Variance Table\n\nResponse: income\n          Df Sum Sq Mean Sq F value    Pr(>F)    \neducation  1 4147.3  4147.3  51.452 5.562e-08 ***\nResiduals 30 2418.2    80.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd the relevant entries are:\n\nthe Df column, which contains the degrees of freedom;\nF value = 51.452, which is the F-statistic;\nPr(>F) = 5.562e-08 = \\(5.562 \\times 10^{-8}\\), which is the p-value.\n\n\n\nInterpretation\n\n\n\n\n\n\nWe performed an \\(F\\)-test for the overall significance of the regression, \\(F(1, 30) = 51.45, p < .001\\).\nThe large \\(F\\)-statistic leads to a very small \\(p\\)-value (\\(<.001\\)), meaning that we have very strong evidence against the null hypothesis that the model is ineffective.\nIn other words, the data provide strong evidence that education is an effective predictor of income."
  },
  {
    "objectID": "1_01_function.html#visual-approach",
    "href": "1_01_function.html#visual-approach",
    "title": "Functions and Models",
    "section": "Visual Approach",
    "text": "Visual Approach\n\n\n\n\n\nSometimes we can directly read a predicted value from the graph of the functional relationship.\nConsider the plot created in the previous question. First, we need to check where x = 1.5. Then, we draw a vertical dashed line until it meets the blue line. The y value corresponding to x = 1.5 can be read off the y-axis.\nHowever, in this case it is not that easy to read it from the drawing…"
  },
  {
    "objectID": "1_01_function.html#algebraic-approach",
    "href": "1_01_function.html#algebraic-approach",
    "title": "Functions and Models",
    "section": "Algebraic Approach",
    "text": "Algebraic Approach\nYou can substitute the x value in the formula and calculate the corresponding y value.\n\\[\ny = 4 * x = 4 * 1.5 = 6\n\\]\n\n\n\n\n\n\nThe predicted perimeter of squared paintings having a 1.5m side is 6m."
  }
]