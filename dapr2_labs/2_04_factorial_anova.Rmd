---
title: "Two-way ANOVA"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
nocite: "@Maxwell2017"
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
```

```{r echo=FALSE}
set.seed(3)
```

:::lo
**LEARNING OBJECTIVES**

1. Understand how to interpret interactions with effects coding.  
2. Understand how to interpret simple effects for experimental designs.
3. Understand how to conduct pairwise comparisons. 
:::

# Recap

You have (hopefully) already made a head start on this weeks exercises if you completed the Factorial ANOVA section of [last week's lab](https://uoepsy.github.io/dapr2/labs/2_03_cont_design_factanova.html#Factorial_ANOVA). If you haven't yet completed these two questions, do so before reading any further. 

In this week's exercises, we will further explore questions such as:

- Does level $i$ of the first factor have an effect on the response?
- Does level $j$ of the second factor have an effect on the response?
- Is there a combined effect of level $i$ of the first factor and level $j$ of the second factor on the response? In other words, is there interaction of the two factors so that the combined effect is not simply the additive effect of level $i$ of the first factor plus the effect of level $j$ of the second factor?

## Research question and data

As a reminder, we are working with data from a study yielding a $3 \times 3$ factorial design to test whether there are differences in types of memory deficits for those experiencing different cognitive impairment(s).

```{r echo=FALSE}
library(tidyverse)
library(emmeans)
cog <- read_csv('https://uoepsy.github.io/data/cognitive_experiment.csv')
# head(df)

cog$Diagnosis <- factor(cog$Diagnosis, 
                       labels = c("amnesic", "huntingtons", "control"),
                       ordered = FALSE)

cog$Task <- factor(cog$Task, 
                  labels = c("grammar", "classification", "recognition"), 
                  ordered = FALSE)

cog$Diagnosis <- fct_relevel(cog$Diagnosis, "control")
cog$Task <- fct_relevel(cog$Task, "recognition")

cog <- cog %>%
    rename(Score = Y)

library(kableExtra)

cog %>% 
    pivot_wider(names_from = 'Task', values_from = 'Score') %>% 
    kable() %>%
    kable_styling(full_width = FALSE) %>%
    add_header_above(c(" " = 1, "Task" = 3))
```


# Interaction Model

`r qbegin(1)`
Let's look at the `summary()` and `anova()` output in detail from the model you should have previously fitted with the sum to zero constraint. As a reminder, the model with interaction is:

\begin{aligned}
Score &= \beta_0 \\
      &+ \beta_1 D_\text{Control} + \beta_2 D_\text{Amnesic}   \\
      &+ \beta_3 T_\text{Recognition}  + \beta_4 D_\text{Grammar} \\
      &+ \beta_5 (D_\text{Control} * T_\text{Recognition}) + \beta_6 (D_\text{Amnesic} * T_\text{Recognition})  \\
      &+ \beta_7 (D_\text{Control} * T_\text{Grammar}) + \beta_8 (D_\text{Amnesic} * T_\text{Grammar})  \\
      &+ \epsilon 
\end{aligned}

Applying the sum to zero constraint (for `Diagnosis` of 'Control' and `Task` of 'Recognition'), we would have:

\begin{aligned}
\text{Intercept (global mean)} &= \beta_0 \frac{\mu_{1,1} + \mu_{1,2} + \cdots + \mu_{3,3}}{9} \\
\beta_{Huntingtons} &= -(\beta_1 + \beta_2) \\
\beta_{Classification} &= -(\beta_3 + \beta_4) \\
\beta_{Huntingtons:Classification} &= -(\beta_5 + \beta_6 + \beta_7 + \beta_8)
\end{aligned}
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Let's look at the `summary()` output first:
```{r}
contrasts(cog$Diagnosis) <- "contr.sum"
contrasts(cog$Task) <- "contr.sum"
mdl_int <- lm(Score ~ Diagnosis * Task, data = cog)
summary(mdl_int)
```

:::int
The F-test for model utility is again significant at the 5\% level: $F(8,36) = 12.28, p < .001$. An F-statistic this large or larger occurring by chance only is very small.
:::


In the presence of a significant interaction we **do not** interpret the main effects as their interpretation changes with the level of the other factor.

```{r}
anova(mdl_int)
```

:::int
The interaction between diagnosis and task is significant. At the 5\% level, the probability of obtaining an F-statistic as large as 7.92 or larger, if there was no interaction effect, is <.001. This provides evidence against the null hypothesis that effect of task is constant across the different diagnoses.
:::

`r solend()`

`r qbegin(2)`
Based on previous plotting, it does seem that there is a clear interaction between diagnosis and task. However, we have not statistically compared our interaction model to an additive model (same model but without the interaction). Until we do so, we cannot confidently progress on the *assumption* that the interaction model is the most suitable for answering the research question. We also want to consider the best coding constraint to apply in order to best answer the research question  - are we interested in whether group X (e.g., Amnesic) differed from group Y (e.g., Huntingtons), or whether group X (e.g., Amnesic) differed from the overall group mean? 


Since we are interested in comparing groups, we should reset to dummy coding, and thus should re-run our interaction model. Next, we need to perform a model comparison between the additive model and the interaction model using the `anova()` function.


\begin{aligned}
Additive Model: Score &= \beta_0 \\
      &+ \beta_1 D_\text{Amnseic} + \beta_2 D_\text{Huntingtons}   \\
      &+ \beta_3 T_\text{Grammar}  + \beta_4 D_\text{Classification} \\
      &+ \epsilon 
\end{aligned}

\begin{aligned}
Interaction Model: Score &= \beta_0 \\
      &+ \beta_1 D_\text{Amnseic} + \beta_2 D_\text{Huntingtons}   \\
      &+ \beta_3 T_\text{Grammar}  + \beta_4 D_\text{Classification} \\
      &+ \beta_5 (D_\text{Amnseic} * T_\text{Grammar}) + \beta_6 (D_\text{Huntingtons} * T_\text{Grammar})  \\
      &+ \beta_7 (D_\text{Amnseic} * T_\text{Classification}) + \beta_8 (D_\text{Huntingtons} * T_\text{Classification})  \\
      &+ \epsilon 
\end{aligned}


Interpret the result of the model comparison.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Switch back to dummy coding:
```{r}
contrasts(cog$Diagnosis) <- "contr.treatment"
contrasts(cog$Task) <- "contr.treatment"
```

Build additive model and re-run interaction model:
```{r}
mdl_add <- lm(Score ~ Diagnosis + Task, data = cog)
mdl_int <- lm(Score ~ Diagnosis * Task, data = cog)
```


The relevant function is `anova()` with the two models as inputs:
```{r}
anova(mdl_add, mdl_int)
```

:::int
We performed an F-test to compare two nested models: an additive two-factor ANOVA against a two-factor model with interaction. The test results are $F(4, 36) = 7.92, p < .001$.

At the 5\% significance level, the probability of obtaining an F-statistic as large as 7.92 or larger is <.001. 

Hence, the comparison of nested models provides evidence against the additive effects model, suggesting that we should use the interaction model as each factor has a different effect on the response depending the level of the other factor.
:::
`r solend()`


`r qbegin(3)`
Using `plot_model()` (note that this function is from `sjPlot` package - make sure that you load this), generate a plot showing the predicted mean scores for each combination of levels of the diagnosis and task factors.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

To visualize the interaction between Diagnosis and Task, we will use `plot_model()`, and ask for a plot of predicted values by specifying `type = "pred"`. Because the interaction involves the two variables, we need to specify a list of the `terms` (or names) from the model that should be plotted.

```{r}
library(sjPlot)
plot_model(mdl_int, type = "pred", terms = c("Diagnosis", "Task"))
```
`r solend()`

# Contrast analysis

We will begin by looking at each factor separately. 

In terms of the diagnostic groups, recall that we want to compare the amnesiacs to the Huntington individuals. This corresponds to a contrast with coefficients of 0, 1, and −1, for control, amnesic, and Huntingtons, respectively. 

Similarly, in terms of the tasks, we want to compare the average of the two implicit memory tasks with the explicit memory task. This corresponds to a contrast with coefficients of 0.5, 0.5, and −1 for the three tasks. 

When we are in presence of a significant interaction, the coefficients for a contrast between the means are found by multiplying each row coefficient with all column coefficients as shown below:

```{r echo=FALSE, out.width = '100%'}
knitr::include_graphics('images/contr_interaction.png')
```

This can be done in R using:

```{r}
diag_coef  <- c('control' = 0, 'amnesic' = 1, 'huntingtons' = -1)
task_coef  <- c('grammar' = 0.5, 'classification' = 0.5, 'recognition' = -1)
contr_coef <- outer(diag_coef, task_coef)   # or: diag_coef %o% task_coef
contr_coef
```

The above coefficients correspond to testing the null hypothesis

$$
H_0 : \frac{\mu_{2,1} + \mu_{2,2}}{2} - \mu_{2,3} - \left( \frac{\mu_{3,1} + \mu_{3,2}}{2} - \mu_{3,3} \right) = 0
$$

or, equivalently,

$$
H_0 : \frac{\mu_{2,1} + \mu_{2,2}}{2} - \mu_{2,3} = \frac{\mu_{3,1} + \mu_{3,2}}{2} - \mu_{3,3}
$$

which says that, in the population, the difference between the mean implicit memory and the explicit memory score is the same for amnesic patients and Huntingtons individuals. Note that the scores for the grammar and classification tasks have been averaged to obtain a single measure of 'implicit memory' score.


Now that we have the coefficients, let's call the `emmeans` function:
```{r}
emm <- emmeans(mdl_int, ~ Diagnosis*Task)
emm
```

Next, insert the coefficients following the order specified by the rows of `emm` above. That is, the first one should be for `control` `grammar`, the second for `amnesic` `grammar`, and so on...
We also give a name to this contrast, such as 'Research Hyp'.

```{r}
comp_res <- contrast(emm, method = list('Research Hyp' = c(0, 0.5, -0.5, 0, 0.5, -0.5, 0, -1, 1)))
comp_res
confint(comp_res)

```

or:

```{r}
summary(comp_res, infer = TRUE)
```


`r qbegin(4)`

Interpret the results of the contrast analysis.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

The contrast analysis yielded a t-value of 5.4 and a corresponding p value less than .001. Thus, there is strong evidence that the contrast is not zero in the population. In other words, amnesiacs and Huntingtons differ in the difference between implicit and explicit recognition memory tasks. 

Examining the cell means or, even better, the interaction plot showing the cell means (shown again below) shows that amnesiacs perform relatively better on implicit memory tasks, whereas Huntington individuals perform relatively better on explicit memory tasks, just as expected from our theory.

```{r echo = FALSE}
#ggplot(data = cog_stats, aes(x = Task, y = Avg_Score, color = Diagnosis)) +
    #geom_point(size = 3) +
    #geom_linerange(aes(ymin = Avg_Score - 2 * SE, ymax = Avg_Score + 2 * SE)) +
    #geom_line(aes(x = as.numeric(Task)))
```


It may be useful to supplement this hypothesis test with some indication of effect size. The recommendation is to report a confidence interval and interpret it in the context of the study, playing attention to the sign of the interval.

The contrast analysis shows that the 95\% confidence interval for our contrast stretches from 32.77 to 72.23. This interval does not contain zero. Thus, we can be 95\% confident that the task difference is not the same for amnesiacs as for Huntingtons, which is why we can reject the null hypothesis that the difference in differences is zero. 

`r solend()`

# Simple Effects

By considering the simple effects, we can identify at which levels of the interacting condition we see different effects. 

`r qbegin(5)`

Using `emmip()`, visualise the interaction, reversing the order of Diagnosis and Task in each. 

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r,  out.width="90%"}
library(gridExtra) #loading this package so that I can display plots using the grid.arrange function

plt_1 <- emmip(mdl_int, Diagnosis ~ Task, CIs = TRUE)
plt_2 <- emmip(mdl_int, Task ~ Diagnosis, CIs = TRUE)

#arrange plots
grid.arrange(plt_1, plt_2)
```
`r solend()`

`r qbegin(6)`
Since we have a significant interaction, we should also look at the simple main effects. Simple effects are the effect of one factor (e.g., Task) at each level of another factor (e.g., Diagnosis - Control, Huntingtons, and Amnesic).

Examine the simple effects for Task at each level of Diagnosis; and then the simple effects for Diagnosis at each level of Task. 


`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
mdl_int_simple1 <- pairs(emm, simple = "Task")
mdl_int_simple1

mdl_int_simple2 <- pairs(emm, simple = "Diagnosis")
mdl_int_simple2
```

`r solend()`

# Pairwise Comparisons

`r qbegin(7)`

Conduct exploratory pairwise comparisons to compare all levels of Diagnosis with all levels of Task.

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
pairs_res <- pairs(emm)
pairs_res

#can also plot if you'd like:
plot(pairs_res)
```




`r solend()`

:::red
### Missing / Next Steps
You may have noted that we are yet to conduct our assumption checks on our model - and you'd be right to flag this! We will recap assumption checks next week using this dataset and interaction model, and we will also explore how we can adjust our pairwise comparisons (*think* about how many comparisons were conducted above - without adjusting our $\alpha$ (or $p$-value), why might any inferences drawn be problematic?)
:::

# References



<!-- Formatting -->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
