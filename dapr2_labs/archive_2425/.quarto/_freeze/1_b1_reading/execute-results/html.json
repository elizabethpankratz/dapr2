{
  "hash": "8cebe9196a7f11bb65971ad9af3f534f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Block 1 Flash Cards\"\nlink-citations: true\ncode-annotations: hover\nparams: \n    SHOW_SOLS: TRUE\n    TOGGLE: TRUE\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n:::lo\n### <i class=\"fa fa-graduation-cap\"></i> Flash Card Aims\n\nThe purpose of these flashcards is to complement your Semester 1 Weeks 1 - 5 core learning materials i.e., your lecture and lab materials, by offering additional guidance and examples on key concepts/topics. It’s designed to deepen your understanding, clarify complex concepts, and help you make connections between different areas of study. Think of it as an extra resource that supports what you’re learning in the classroom. \n\nYou may want to consider using the below as a supporting document whilst your work through lab exercises, and/or refer to in order to aid revision. \n\n### <i class=\"fab fa-r-project\"></i> R Packages\n\nWithin this reading, the following packages are used:\n\n* **tidyverse** \n* **sjPlot**\n* **kableExtra**\n* **psych**\n\n### <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\"></i> Presenting Results\nNote that you must **not** copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more [here](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct).\n\n:::\n\n# Back to Basics\n\n\nFor an overview of basic statistical tests and core concepts (e.g., $p$-values), please revisit the [DAPR1 materials](https://uoepsy.github.io/dapr1/2324/) for a refresher (also accessible via the DAPR1 Learn page).\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-1' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-1', 'opt-start-1')\"> <span class=\"olab\">Terminology</span></span></div><div class=\"optional-body\" id = \"opt-body-1\" style=\"display: none;\">\n\n\n\nLet's spend some time to remind ourselves of some key terminology, specifically related to types of variables and study designs:\n\n\n| Term           | Definition       |\n|----------------|------------------|\n| (Observational) unit | The individual entities on which data are collected |  \n| Variable | Any characteristic recorded on the observational units |  \n| Numeric variable | A variable that records a numerical quantity for each case. For such variables standard arithmetic operations make sense. For example: height, IQ, and weight |  \n| Categorical variable | A categorical variable places units into one of several groups. For example: country of birth, dominant hand, and eye colour |  \n| Binary variable | A special case of categorical variable with only 2 possible levels. For example: handedness (left or right), smoking status (smoker or non-smoker), pass test (yes or no) |  \n| Response variable (also more commonly called a dependent variable, or outcome variable) | Measures the outcome of interest in a study |  \n| Explanatory/independent variable (also called predictors) | Are used to explain differences/changes in the response variable |  \n| Observational study | An observational study is a study in which the researcher does not manipulate any of the variables involved in the study, but merely records the values as they naturally exist | \n| Experimental study | An experiment is a study in which the researcher imposes the values of the explanatory variable on the units before measuring the response variable |  \n\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Data Exploration\n\nThe common first port of call for almost any statistical analysis is to explore the data, and we can do this visually and/or numerically.\n\n<table cellspacing=\"3\">\n<tr style=\"border-bottom: 1px solid\"><th></th><th style=\"border-right: 1px solid\">Marginal Distributions</th><th>Bivariate Associations</th></tr>\n<tr style=\"border-bottom: 1px solid\">\n<td style=\"vertical-align:top\">**Description**</td>\n<td style=\"vertical-align:top;border-right: 1px solid\">The distribution of each variable individually (i.e., *without* reference to the values of the other variables).</td>\n<td style=\"vertical-align:top\">Describing the association between two numeric variables.</td>\n</tr>\n<tr style=\"border-bottom: 1px solid\">\n<td style=\"vertical-align:top\">**Visually**</td>\n<td style=\"vertical-align:top;border-right: 1px solid\">Plot each variable individually.<br><br>You could use, for example, `geom_density()` for a density plot or `geom_histogram()` for a histogram to comment on and/or examine:<br><ul><li> The *shape* of the distribution. Look at the shape, centre and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal? </li><li> Identify any *unusual observations*. Do you notice any extreme observations (i.e., outliers)? </li></ul></td>\n<td style=\"vertical-align:top\">Plot associations among two variables.<br><br>You could use, for example, `geom_point()` for a scatterplot  to comment on and/or examine:<br><ul><li>The *direction* of the association indicates whether there is a positive or negative association</li><li>The *form* of association refers to whether the relationship between the variables can be summarized well with a straight line or some more complicated pattern </li><li>The *strength* of association entails how closely the points fall to a recognizable pattern such as a line  </li><li>*Unusual observations* that do not fit the pattern of the rest of the observations and which are worth examining in more detail </li></ul></td>\n</tr>\n<tr style=\"border-bottom: 1px solid\">\n<td style=\"vertical-align:top\">**Numerically**&nbsp;&nbsp;&nbsp;</td>\n<td style=\"vertical-align:top;border-right: 1px solid\">Compute and report summary statistics e.g., mean, standard deviation, median, min, max, etc.<br><br>You could, for example, calculate summary statistics such as the mean (`mean()`) and standard deviation (`sd()`), etc. within `summarize()`</td>\n<td style=\"vertical-align:top\">Compute and report the correlation coefficient.<br><br>You can use the `cor()` function to calculate this</td>\n</tr>\n</table>\n\n\n\n## Numeric Exploration\n\nNumeric exploration of data involves examining key statistics like mean, median, and standard deviation via descriptives tables; and assessing the associations among variables through correlation coefficients. Exploring our data numerically helps us to identify patterns and associations in the data.\n\n### Descriptives \n\n\n\n<div class=\"optional-begin\"><span id='opt-start-2' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-2', 'opt-start-2')\"> <span class=\"olab\">Descriptives Tables</span></span></div><div class=\"optional-body\" id = \"opt-body-2\" style=\"display: none;\">\n\n\n\nThere are numerous packages available that allow us to pull out descriptive statistics from our dataset such as **tidyverse** and **psych**.\n\nWhen we pull out descriptive statistics, it is useful to present these in a well formatted table for your reader. There are lots of different ways of doing this, but one of the most common (and straightforward!) is to use the `kable()` function from the package **kableExtra**. \n\nThis allows us to give our table a clear caption (via `caption = \"insert caption here\"`, align values within columns e.g., center aligned via `align = \"??\"`), and we can also round to however many decimal places we desire (standard for APA is 2 dp; via `digits = ??`). \n\nWe can also add in the function `kable_styling()`. This is really helpful for customsing your table e.g., the font size, position, and whether or not you want the table full width (as well as lots of other things - check out the helper function!). \n\nFor an overview of how to make tables in RMarkdown, see [Lesson 4 of the RMD bootcamp](https://uoepsy.github.io/scs/rmd-bootcamp/04-prettytab.html)\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-3' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-3', 'opt-start-3')\"> <span class=\"olab\">Descriptives Tables - Examples</span></span></div><div class=\"optional-body\" id = \"opt-body-3\" style=\"display: none;\">\n\n\n\n::: {.panel-tabset}\n\n##### The **tidyverse** way\n\nWe can use the `summarise()` function to numerically summarise/describe our data. Some key values we may want to consider extracting are (though not limited to): the mean (via `mean()`, standard deviation (via `sd()`), minimum value (via `min()`), maximum value (via `max()`), standard error (via `se()`), and skewness (via `skew()`). \n\nFor example:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(kableExtra)\n\n# using the pre-loaded iris dataset\n# taking the mean and standard deviation of sepal length via the summarize function\n# returning a table with a caption, where numbers are rounded to 2 dp\n# asking for a table that is not the full width of the window display\niris %>%\n    summarize(\n        M_Length = mean(Sepal.Length),\n        SD_Length = sd(Sepal.Length)\n    ) %>%\n    kable(caption = \"Sepal Length Descriptives (in cm)\", digits = 2) %>%\n    kable_styling(full_width = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Sepal Length Descriptives (in cm)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> M_Length </th>\n   <th style=\"text-align:right;\"> SD_Length </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 5.84 </td>\n   <td style=\"text-align:right;\"> 0.83 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n##### The **psych** way\n\nThe `describe()` function will produce a table of descriptive statistics. If you would like only a subset of this output (e.g., mean, sd), you can use `select()` after calling `describe()` e.g., `describe() %>% select(mean, sd)`.\n\nFor example:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(psych)\nlibrary(kableExtra)\n\n# using the pre-loaded iris dataset\n# we want to get descriptive statistics of the iris dataset, specifically the sepal length column\n# we specifically want to select the mean and standard deviation from the descriptive statistics available (try this without including this argument to see what values you all get out)\n# returning a table with a caption, where numbers are rounded to 2 dp\n# asking for a table that is not the full width of the window display\ndescribe(iris$Sepal.Length) %>%\n    select(mean, sd) %>%\n    kable(caption = \"Sepal Length Descriptives (in cm)\", digits = 2) %>%\n    kable_styling(full_width = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Sepal Length Descriptives (in cm)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> mean </th>\n   <th style=\"text-align:right;\"> sd </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> X1 </td>\n   <td style=\"text-align:right;\"> 5.84 </td>\n   <td style=\"text-align:right;\"> 0.83 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n#### Correlation\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-4' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-4', 'opt-start-4')\"> <span class=\"olab\">Correlation Coefficient</span></span></div><div class=\"optional-body\" id = \"opt-body-4\" style=\"display: none;\">\n\n\n\nThe correlation coefficient is a standardised number which quantifies the strength and direction of the linear association between two variables. In a population it is denoted by $\\rho$, and in a sample it is denoted by $r$. \n\nValues of $r$ fall between $-1$ and $1$. How to interpret:\n\n+ Size: More extreme values (i.e., the The closer $r$ is to $+/- 1$) the stronger the linear association, and the closer to $0$ a weak/no association. Commonly used cut-offs are:\n    + Weak = $.1 < |r| < .3$\n    + Moderate = $.3 < |r| < .5$\n    + Strong = $|r| > .5$\n    \n+ Direction: The sign of $r$ says nothing about the strength of the association, but its nature and direction:\n    + Positive association means that values of one variable tend to be higher when values of the other variable are higher\n    + Negative association means that values of one variable tend to be lower when values of the other variable are higher\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-5' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-5', 'opt-start-5')\"> <span class=\"olab\">Correlation Matrix</span></span></div><div class=\"optional-body\" id = \"opt-body-5\" style=\"display: none;\">\n\n\n\nA correlation matrix is a table showing the correlation coefficients - $r_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}$ - between variables. Each cell in the table shows the association between two variables. The diagonals show the correlation of a variable with itself (and are therefore always equal to 1).  \n\n:::blue\n**In R**\n\nWe can create a correlation matrix by giving the `cor()` function a dataframe. It is important to remember that all variables **must** be numeric.\n:::\n\nLet's check the structure of the iris dataset to ensure that all variables are numeric:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstr(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n\n\nWe can see that the variable *Species* in column 5 is a factor - this means that we cannot include this in our correlation matrix. Therefore, we need to *subset*, or, in other words, select specific columns. We can do this either giving the column numbers inside `[]`, or using `select()`. In our case, we want the variables in columns 1 - 4, just not 5.\n\nIf you had `NA` values within your dataset, you could choose to remove these `NA`s using `na.rm = TRUE` inside the `cor()` function.\n\n::: {.panel-tabset}\n\n## Index dataframe (`[]`)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nround(cor(iris[,c(1:4)]), digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n```\n\n\n:::\n:::\n\n\n\n## Variable selection (`select()`)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# select only the columns we want by variable name, and pass this to cor()\niris %>% \n  select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) %>%\n  cor() %>%\n  round(digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length         1.00       -0.12         0.87        0.82\nSepal.Width         -0.12        1.00        -0.43       -0.37\nPetal.Length         0.87       -0.43         1.00        0.96\nPetal.Width          0.82       -0.37         0.96        1.00\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-6' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-6', 'opt-start-6')\"> <span class=\"olab\">Correlation - Hypothesis Testing</span></span></div><div class=\"optional-body\" id = \"opt-body-6\" style=\"display: none;\">\n\n\n\nThe hypotheses of the correlation test are, as always, statements about the _population_ parameter (in this case the correlation between the two variables in the population - i.e., $\\rho$).  \n\nIf we are conducting a two tailed test, then... \n\n- $H_0: \\rho = 0$. There is _no_ linear association between $x$ and $y$ in the population.  \n- $H_1: \\rho \\neq 0$ There is a linear association between $x$ and $y$.  \n  \nIf we instead conduct a one-tailed test, then we are testing either...\n\n- $H_0: \\rho \\leq 0$ There is a negative or no linear association between $x$ and $y$   \n- $H_1: \\rho > 0$ There is a positive linear association between $x$ and $y$.\n\n**OR**\n\n- $H_0: \\rho \\geq 0$ There is a positive or no linear association between $x$ and $y$   \n- $H_1: \\rho < 0$ There is a negative linear association between $x$ and $y$.  \n\n__Test Statistic__  \n\nThe  test statistic for this test is the $t$ statistic, the formula for which depends on both the observed correlation ($r$) and the sample size ($n$):\n\n$$t = r \\sqrt{\\frac{n-2}{1-r^2}}$$\n\n\n__p-value__  \n\nWe calculate the $p$-value for our $t$-statistic as the long-run probability of a $t$-statistic with $n-2$ degrees of freedom being less than, greater than, or more extreme in either direction (depending on the direction of our alternative hypothesis) than our observed $t$-statistic.  \n\n__Assumptions__  \n\nFor a test of Pearson's correlation coefficient $r$, we need to make sure a few conditions are met:  \n\n+ Both variables are quantitative (i.e., numeric)\n+ Both variables are drawn from normally distributed populations\n+ The association between the two variables is linear  \n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-7' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-7', 'opt-start-7')\"> <span class=\"olab\">Correlation - Hypothesis Testing in R</span></span></div><div class=\"optional-body\" id = \"opt-body-7\" style=\"display: none;\">\n\n\n\n:::blue\n**In R**\n\nWe can test the significance of the correlation coefficient really easily with the function `cor.test()`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor.test(iris$Sepal.Length, iris$Petal.Length)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Petal.Length\nt = 21.646, df = 148, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8270363 0.9055080\nsample estimates:\n      cor \n0.8717538 \n```\n\n\n:::\n:::\n\n\n\nNote, by default, `cor.test()` will include only observations that have no missing data on either variable. \n\nWe can specify whether we want to conduct a one- or two-tailed test by adding the argument `alternative = ` and specifying `alternative = \"less\"`, `alternative = \"greater\"`, or `alternative = \"two.sided\"` (the latter being the default).  \n\n:::\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\n**Example Interpretation**\n\nThere was a strong positive association between sepal length and petal length $(r = .87, t(148) = 21.65, p < .001)$. These results suggested that a greater sepal length was positively associated with a greater petal length. \n\n:::\n\n:::{.callout-note}\n\nFor a detailed recap of all things correlation (including further details and examples), revisit the [Correlation lecture from DAPR1](https://uoepsy.github.io/dapr1/2324/lectures/dapR1_lec20_Correlation.pdf).\n\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n## Visual Exploration\n\nVisual exploration of our data allows us to visualize the distributions of our data, and to identify potential associations between variables.\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-8' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-8', 'opt-start-8')\"> <span class=\"olab\">How to Visualise Data</span></span></div><div class=\"optional-body\" id = \"opt-body-8\" style=\"display: none;\">\n\n\n\nTo visualise (i.e., plot) our data, we can use `ggplot()` from the **tidyverse** package. Note the key components of the `ggplot()` code:\n\n+ `data =` where we provide the name of the dataframe.\n+ `aes =` where we provide the aesthetics. These are things which we map from the data to the graph. For instance, the $x$-axis, or if we wanted to colour the columns/bars according to some aspect of the data.\n+ `+ geom_...` = where we add (using +) some geometry. These are the shapes (e.g., bars, points, etc.), which will be put in the correct place according to what we specified in `aes()`.\n+ `labs()` = where we provide labels for our plot (e.g., the $x$- and $y$-axis)\n\n\n:::{.callout-note}\n\nThere are lots of arguments that you can further customise, some of which are specified in the examples below e.g., `bins = `, `alpha = `, `fill = `, `linewidth = `. `linetype = `, `size = ` etc. For these, you can look up the helper function to see the range of arguments they can take using `?` - e.g., `?fill`\n\nIf you'd like to read more about `ggplot()`, there is a handy [cheatsheet](https://rstudio.github.io/cheatsheets/data-visualization.pdf).\n\n:::\n\nOne other thing to consider when visualising your data is how you are going to arrange your plots. Some handy tips on this:  \n  \n- Use \\n to wrap text in your titles and or axis labels  \n- The **patchwork** package allows us to arrange multiple plots in two ways - `|` arranges the plots adjacent to one another, and `/` arranges the plots on top of one another  \n\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-9' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-9', 'opt-start-9')\"> <span class=\"olab\">Data Visualisation - Marginal Examples</span></span></div><div class=\"optional-body\" id = \"opt-body-9\" style=\"display: none;\">\n\n\n\n::: {.panel-tabset}\n\n#### Histogram\n\nA histogram shows the frequency of values which fall within bins of an equal width. \n\n**Basic:**\n\n- x-axis: possible values of some variable, grouped into bins\n- y-axis: frequency of a given value or values within bins\n- *What are bins?*: A bin represents a range of scores\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram() +\n    labs(x = \"Sepal Length (in cm)\")\n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n**Updating Bins:**\n\nWithin `geom_histogram()`, we can specify `bins = ` to specify the number of columns we want (for this example, lets say we want 10):\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram(bins = 10) +\n    labs(x = \"Sepal Length (in cm)\")\n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\nAlternatively, we can specify `binwidth = ` to specify the width of each bin (it is very helpful to be aware of the scale of your variable here!):\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram(binwidth = 0.1) +\n    labs(x = \"Sepal Length (in cm)\")\n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n**Outline columns with color:**\n\nWithin `geom_histogram()`, we can specify `color = ` to set a colored outline of the columns:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram(color = \"red\") +\n    labs(x = \"Sepal Length (in cm)\")\n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n**Fill columns with color:**\n\nWithin `geom_histogram()`, we can specify `fill = ` to fill the columns with a color:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_histogram(fill = \"purple\") +\n    labs(x = \"Sepal Length (in cm)\")\n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n#### Density\n\nA visualization of the distribution of a numeric variable. \n\n**Basic:**\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_density() +\n    labs(x = \"Sepal Length (in cm)\")\n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n**Filled:**\n\nWe can fill our plot with colour by specifying `fill = ` within `geom_density()`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_density(fill = \"lightblue\") +\n    labs(x = \"Sepal Length (in cm)\")\n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n**Line Type & Width:**\n\nWe can change the type and width of the line by specifying `linetype = ` and `linewidth = ` within `geom_density()`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = iris, aes(x = Sepal.Length)) +\n    geom_density(linetype = 6, linewidth = 3) +\n    labs(x = \"Sepal Length (in cm)\")\n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-10' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-10', 'opt-start-10')\"> <span class=\"olab\">Data Visualisation - Bivariate Examples</span></span></div><div class=\"optional-body\" id = \"opt-body-10\" style=\"display: none;\">\n\n\n\nUnlike in our marginal plots where we specified our x-axis variable within `aes()`, to visualise bivariate associations, we need to specify what variables we want on both our x- and y-axis. \n\n::: {.panel-tabset}\n\n#### Scatterplot\n\nWe can use a scatterplot (since the variables are numeric and continuous) to visualise the association between the two numeric variables - these will be our x- and y-axis values. \n\nWe plot these values for each row of our dataset, and we should end up with a *cloud* of scattered points. \n\nHere we will want to comment on any key observations that we notice, including if we detect outliers or points that do not fit with the pattern in the rest of the data. Outliers are extreme observations that are not possible values of a variable or that do not seem to fit with the rest of the data. This could either be:\n\n+ *marginally* along one axis: points that have an unusual (too high or too low) x-coordinate or y-coordinate;\n+ *jointly*: observations that do not fit with the rest of the point cloud \n\n**Basic:**\n\nWe need to specify `+ geom_point()` to get a scatterplot: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point() +\n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n**Fill points with color:**\n\nWithin `geom_point()`, we can specify `color = ` to fill the points with a color:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point(color = \"orange\") +\n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n**Change size and opacity:**\n\nWe can change the size (using `size =`) and the opacity (using `alpha =`) of our geom elements on the plot. Let’s include this below:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point(size = 3, alpha = 0.5) +\n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n**Add a line of best fit:**\n\nWe can superimpose (i.e., add) a line of best fit by including the argument `+ geom_smooth()`. Since we want to fit a straight line, we want to use `method = \"lm\"`. We can also specify whether we want to display confidence intervals around our line by specifying `se = TRUE / FALSE`. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = iris, aes(x = Petal.Length, y = Sepal.Length)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(x = \"Petal Length (in cm)\", y = \"Sepal Length (in cm)\")\n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Functions and Mathematical Models\n\nBasic functions and mathematical models are foundational tools used to describe and predict associations between variables.\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-11' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-11', 'opt-start-11')\"> <span class=\"olab\">Identification & Specification</span></span></div><div class=\"optional-body\" id = \"opt-body-11\" style=\"display: none;\">\n\n\n\nConsider the function $y = 2 + 5 \\ x$. From this, we can do the following:\n\n- Identify the dependent variable (DV)\n- Identify the independent variable (IV)\n- Describe in words what the function does, and compute the output for the following input:\n\n$$\nx = \\begin{bmatrix}\n2 \\\\\n6\n\\end{bmatrix}\n$$\n\nThe function says that the $y$ value is obtained as a transformation of the $x$ value.\n\n- The dependent variable is $y$\n- The independent variable is $x$\n- The $y$ value is obtained as five times $x$, plus two.\n\n<br />\nExample (1): If $x$ equals 2, the corresponding value of $y$ will be $2 + 5 * 2 = 12$.\n<br />\nExample (2): If $x$ equals 6, the corresponding value of $y$ will be $2 + 5 * 6 = 32$.\n\n$$\ny = \\begin{bmatrix}\n2 + 5 * 2 \\\\\n2 + 5 * 6\n\\end{bmatrix}\n= \\begin{bmatrix}\n12 \\\\\n32\n\\end{bmatrix}\n$$\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-12' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-12', 'opt-start-12')\"> <span class=\"olab\">Deterministic Models - Description & Specification</span></span></div><div class=\"optional-body\" id = \"opt-body-12\" style=\"display: none;\">\n\n\n\nWe come across functions *a lot* in daily life, and probably don't think much about it. In a slightly more mathematical setting, we can write down in words and in symbols the function describing the association between the side of a square and its perimeter (e.g., to capture *how* the perimeter *varies* as a function of its side). In this case, the perimeter is the dependent variable, and the side is the independent variable.\n\nThis is what we would refer to as a *deterministic model*, as it is a model of an *exact relationship* - there can be no deviation.\n\n**Model Specification**\n\n::: {.panel-tabset}\n\n## In words\n\nThe perimeter of a square is four times the length of its side.\n\n\n## In symbols\n\nThe relationship between side and perimeter of squares is given by:\n\n$$\nPerimeter = 4 * Side\n$$\n\nIf you denote $y$ as the dependent variable _Perimeter_, and $x$ as the independent variable _Side_ we can rewrite as: \n\n$$\ny = 4 * x\n$$\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-13' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-13', 'opt-start-13')\"> <span class=\"olab\">Deterministic Models - Visualisation</span></span></div><div class=\"optional-body\" id = \"opt-body-13\" style=\"display: none;\">\n\n\n\nLet's create a dataset called `squares`, containing the perimeter of four squares having sides of length $0, 2, 5, 9$ metres, and then plot the `squares` data as points on a scatterplot. \n\nFirst, let's make our squares data. Here we will use two important functions - `tibble()` and `c()`. The `tibble()` function allows us to construct a data frame. To store a sequence of numbers into R, we can **c**ombine the values using `c()`. A sequence of elements all of the same type is called a *vector*.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#create data frame named squares\nsquares <- tibble(\n  side = c(0, 2, 5, 9), \n  perimeter = 4 * side\n)\n\n#check that our values are contained within squares\nsquares\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n   side perimeter\n  <dbl>     <dbl>\n1     0         0\n2     2         8\n3     5        20\n4     9        36\n```\n\n\n:::\n:::\n\n\n\nNow we know how `ggplot()` works, we can start to build our plot. First we specify our data (we want to use the *squares* data frame), and then our aesthetics. Since the perimeter varies as a function of side, we want side on the $x$-axis, and perimeter on the $y$-axis. We want to create a scatterplot, so we need to specify our `geom_...` argument as `geom_point()`. Lastly, we will provide clearer axis labels, and include the units of measurement. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = squares, aes(x = side, y = perimeter)) +\n  geom_point() +  \n  labs(x = 'Side (m)', y = 'Perimeter (m)',  title = 'Perimeter = 4*Side')  \n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/fig-squares1-1.png){#fig-squares1 fig-align='center' width=80%}\n:::\n:::\n\n\n\nWe could also visualise the functional relationship by connecting the individual points with a line. To do so, we need to add a new argument - `geom_line()`. If you would like to change the colour of the line from the default, you can specify `geom_line(colour = \"insert colour name\")`. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = squares, aes(x = side, y = perimeter)) +\n  geom_point() +\n  geom_line(colour = \"blue\") + \n  labs(x = 'Side (m)', y = 'Perimeter (m)',  title = 'Perimeter = 4*Side')  \n```\n\n::: {.cell-output-display}\n![](1_b1_reading_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-14' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-14', 'opt-start-14')\"> <span class=\"olab\">Deterministic Models - Predicted Values</span></span></div><div class=\"optional-body\" id = \"opt-body-14\" style=\"display: none;\">\n\n\n\nSometimes we can directly read a predicted value from the graph of the functional relationship.\n\nConsider the plot created above. For example, first we need to check where x = 2.5. Then, we draw a vertical dashed line until it meets the blue line. The y value corresponding to x = 2.5 can be read off the y-axis. In our case, we would say a side of 2.5m corresponds to a perimeter of 10m. \n\nHowever, in this case it is not that easy to read it from the drawing... This leads us to the algebraic approach:\n\nWe can substitute the x value in the formula and calculate the corresponding y value where we would conclude that the predicted perimeter of squared paintings having a 2.5m side is 10m:\n\n$$\ny = 4 \\cdot x  \\\\    \n$$\n\n$$\ny = 4 \\cdot 2.5 \\\\  \n$$\n\n$$\ny = 10  \\\\\n$$\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Statistical Models\n\nStatistical models are used to understand the associations among variables.\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-15' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-15', 'opt-start-15')\"> <span class=\"olab\">Specifying Hypotheses</span></span></div><div class=\"optional-body\" id = \"opt-body-15\" style=\"display: none;\">\n\n\n\nWe need to specify our hypotheses when testing a model as this not only defines what we are testing, but also sets the direction for statistical inference. By specifying a null hypothesis (typically stating no effect or no association) and an alternative hypothesis (indicating the presence of an association), we create a structured approach for determining the statistical significance of model parameters. Without specifying hypotheses, the interpretation of results would lack focus, making it difficult to assess the validity and relevance of the model's findings.\n\nIn regression analysis, hypothesis testing for beta coefficients is used to assess whether (each) predictor variable significantly contributes to the model. \n\nThe way we specify hypotheses is similar across simple and multiple regression models.\n\nFor each regression coefficient $\\beta_j$ (for predictor $X_j$):\n\n+ Null hypothesis ($H_0$) = $\\beta_j = 0$: The predictor variable ($X_j$) is not associated with the DV  \n+ Alternative hypothesis ($H_1$) = $\\beta_j \\neq 0$: The predictor variable ($X_j$) is associated with the DV  \n  \n  \nBased on the $p$-value or comparison of the $t$-statistic with the critical value, you can conclude whether the predictor variable is significant or not (see the simple & multiple regression Models - extracting information > model coefficients flashcard below):\n\n+ Reject $H_0$ if $|t_j|$ > critical value or $p$-value $< \\alpha$ \n+ Fail to reject $H_0$ if $|t_j|$ $\\leq$ critical value or $p$-value $\\geq \\alpha$ \n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-16' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-16', 'opt-start-16')\"> <span class=\"olab\">Simple Linear Regression Models - Description & Specification</span></span></div><div class=\"optional-body\" id = \"opt-body-16\" style=\"display: none;\">\n\n\n\nThe association between two variables (e.g., recall accuracy and age) will show deviations from the 'average pattern'. Hence, we need to create a model that allows for deviations from the linear relationship - we need a _statistical model_.\n\nA statistical model includes *both* a deterministic function and a random error term. We typically refer to the outcome (‘dependent’) variable with the letter $y$ and to our predictor (‘explanatory’/‘independent’) variables with the letter $x$. A simple (i.e., one x variable only) linear regression model thus takes the following form (where the terms $\\beta_0$ and $\\beta_1$ are numbers specifying where the line going through the data meets the y-axis (i.e., the intercept - where $x$ = 0; $\\beta_0$) and its slope (direction and gradient of line; $\\beta_1$):\n\n**Model Specification**\n\n$$\ny_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i    \n$$\n\n**Model Specification: Annotated**\n\n$$\ny_i = \\underbrace{\\beta_0 + \\beta_1 \\cdot x_i}_{\\text{function of }x} + \\underbrace{\\epsilon_i}_{\\text{random error}}  \n\\\\\n$$\n\n$$\n\\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\n$$\n\n\n**Model Specification: Explained**\n\nLet's break down what $y_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i \\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}$ actually means by considering the statement in smaller parts:\n\n+ *$y_i = \\beta_0 + \\beta_1 \\cdot x_i$*  \n  \n    + $y_i$ is our measured outcome variable (our DV)  \n    + $x_i$ is our measured predictor variable (our IV)  \n    + $\\beta_0$ is the model intercept  \n    + $\\beta_1$ is the model slope  \n\n+ *$\\epsilon \\sim N(0, \\sigma) \\text{ independently}$*   \n  \n    + $\\epsilon$ is the residual error   \n    + $\\sim$ means 'distributed according to'    \n    + $N(0, \\sigma) \\text{ independently}$ means 'normal distribution with a mean of 0 and a variance of $\\sigma$'   \n    + Together, we can say that the errors around the line have a mean of zero and constant spread as x varies    \n\n\n:::blue\n**In R**\n\nThere are basically two pieces of information that we need to pass to the `lm()` function:\n\n1. The formula: The regression formula should be specified in the form `y ~ x` where $y$ is the dependent variable (DV) and $x$ the independent variable (IV).\n2. The data: Specify which dataframe contains the variables specified in the formula.\n\nIn `R`, the syntax of the lm() function can be specified as follows (where DV = dependent variable, IV = independent variable, and dataframe_name = the name of your dataset):\n\n::: {.panel-tabset}\n\n## Option A\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_name <- lm(DV ~ IV, data = data_name) \n```\n:::\n\n\n\n## Option B\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_name <- lm(data_name$DV ~ data_name$IV)\n```\n:::\n\n\n\n\n:::\n\nyou can also specify as: \n\n::: {.panel-tabset}\n\n## Option A\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_name <- lm(DV ~ 1 + IV, data = data_name) \n```\n:::\n\n\n\n## Option B\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_name <- lm(data_name$DV ~ 1 + data_name$IV)\n```\n:::\n\n\n\n:::\n\n::: {.callout-important icon=false collapse=true}\n## Why is there a 1 in the two bottom options? \n\nWhen we specify the linear model in `R`, we include after the tilde sign ($\\sim$), the variables that appear to the right of the $\\hat \\beta$s. The intercept, or $\\beta_0$, is a constant. That is, we could write it as multiplied by 1.\n\nIncluding the 1 explicitly is not necessary because it is included by default (you can check this by comparing the outputs of A & B above with and without the 1 included - the estimates are the same!). After a while, you will find you just want to drop the 1 when calling `lm()` because you know that it’s going to be there, but in these early weeks we tried to keep it explicit to make it clear that you want the intercept to be estimated. \n:::\n\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-17' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-17', 'opt-start-17')\"> <span class=\"olab\">Simple Linear Regression Models - Example</span></span></div><div class=\"optional-body\" id = \"opt-body-17\" style=\"display: none;\">\n\n\n\nImagine that you were tasked to investigate whether there was an association between recall accuracy and age. You have been provided with data from twenty participants who studied passages of text (c500 words long), and were tested a week later. The testing phase presented participants with 100 statements about the text. They had to answer whether each statement was true or false, as well as rate their confidence in each answer (on a sliding scale from 0 to 100). The dataset contains, for each participant, the percentage of items correctly answered, their age (in years), and their average confidence rating. \n\nThe data are available at [https://uoepsy.github.io/data/recalldata.csv](https://uoepsy.github.io/data/recalldata.csv)\n\nWe could specify our model as:\n\n$$\n\\text{Recall Accuracy}_i = \\beta_0 + \\beta_1 \\cdot \\text{Age}_i + \\epsilon_i    \n$$\n\nand fit our model in `R` as\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#specify model\nrecall_simp <- lm(recall_accuracy ~ age, data = recalldata)\n\n#look at model coefficients\nrecall_simp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = recall_accuracy ~ age, data = recalldata)\n\nCoefficients:\n(Intercept)          age  \n    84.0153      -0.3026  \n```\n\n\n:::\n:::\n\n\n\n\n\nWhen we call the name of the fitted model, `recall_simp`, you can see the estimated regression coefficients $\\hat \\beta_0$ and $\\hat \\beta_1$. The line of best-fit is thus given by:^[Yes, the error term is gone. This is because the line of best-fit gives you the prediction of the average recall accuracy for a given age, and not the individual recall accuracy of an individual person, which will almost surely be different from the prediction of the line.] \n\n$$\n\\widehat{Recall~Accuracy} = 84.02 - 0.31 \\cdot \\text{Age}\n$$\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-18' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-18', 'opt-start-18')\"> <span class=\"olab\">Multiple Linear Regression Models - Description & Specification</span></span></div><div class=\"optional-body\" id = \"opt-body-18\" style=\"display: none;\">\n\n\n\nMultiple linear regression involves looking at one continuous outcome (i.e., DV), with two or more independent variables (i.e., IVs). \n\nA multiple linear regression model takes the following form:\n\n$$\ny_i = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\epsilon_i\n$$\n$$\n\\quad \\text{where} \\quad \\epsilon_i \\sim N(0, \\sigma) \\text{ independently}\n$$\nSo, for example, we could extend our recall accuracy model to include recall confidence as a predictor:\n\n$$\n\\text{Recall Accuracy}_i = \\beta_0 + \\beta_1 \\cdot \\text{Recall Confidence}_i + \\beta_2 \\cdot \\text{Age}_i + \\epsilon_i    \n$$\n\n:::blue\n\nIn **R**:\n\nMultiple and simple linear regression follow the same structure within the `lm()` function - the logic scales up to however many predictor variables we want to include in our model. You simply add (using the `+` sign) more independent variables. For example, if we wanted to build a multiple linear regression that included three independent variables, we could fit one of the following via the `lm()` function:\n\n::: {.panel-tabset}\n\n## Option A\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_name <- lm(DV ~ IV1 + IV2 + IV3, data = data_name)\n```\n:::\n\n\n\n## Option B\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_name <- lm(data_name$DV ~ data_name$IV1 + data_name$IV2 + data_name$IV3)\n```\n:::\n\n\n\n:::\n\n:::\n\n**Interpretation of Multiple Regression Coefficients**\n\nYou'll hear a lot of different ways that people explain multiple regression coefficients.  \n\nFor the model $y = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\epsilon$, the estimate $\\hat \\beta_1$ will often be reported as:  \n  \n\"the increase in $y$ for a one unit increase in $x_1$ when...\"\n\n- \"holding the effect of $x_2$ constant.\"\n- \"controlling for differences in $x_2$.\"\n- \"partialling out the effects of $x_2$.\"\n- \"holding $x_2$ equal.\"\n- \"accounting for effects of $x_2$.\" \n\nFor models with 3+ predictors, just like building the model in `R`, the logic of the above simply extends. \n\nFor example “the increase in [outcome] for a one unit increase in [predictor] when...”\n\n- “holding [other predictors] constant.”  \n- “accounting for [other predictors].”  \n- “controlling for differences in [other predictors].”  \n- “partialling out the effects of [other predictors].”  \n- “holding [other predictors] equal.”  \n- “accounting for effects of [other predictors].”  \n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-19' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-19', 'opt-start-19')\"> <span class=\"olab\">Simple & Multiple Regression Models - Extracting Information</span></span></div><div class=\"optional-body\" id = \"opt-body-19\" style=\"display: none;\">\n\n\n\n::: {.panel-tabset}\n\n## Model Call\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![Multiple regression output in R, model formula highlighted](images/recall_mdl_call.PNG){fig-align='left' width=80%}\n:::\n:::\n\n\n\nThe call section at the very top of the `summary()` output shows us the formula that was specified in `R` to fit the regression model.\n\nIn the above, we can see that recall accuracy is our DV, recall confidence and age were our two IVs, and our dataset was named recalldata.\n\n## Residuals\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![Multiple regression output in R, residuals highlighted](images/recall_mdl_residuals.PNG){fig-align='left' width=80%}\n:::\n:::\n\n\n\nResiduals are the difference between the observed values and model predicted values of the DV. \n\nIdeally, for the model to be unbiased, we want our median value to be around 0, as this would show that the errors are random fluctuations around the true line. When this is the case, we know that our model is doing a good job predicting values at the high and low ends of our dataset, and that our residuals were somewhat symmetrical. \n\n## Model Coefficients\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![Multiple regression output in R, model coefficients highlighted](images/recall_mdl_coefficients.PNG){fig-align='left' width=80%}\n:::\n:::\n\n\n\n\n::: {.panel-tabset}\n\n### Estimates\n\nOur model estimates help us to build our best fitting equation of the line that represents the association between our DV and our IV(s). \n\nIn the above example, we can build our equation for our model from this information:\n\n$$\n\\text{Recall Accuracy}_i = \\beta_0 + \\beta_1 \\cdot \\text{Recall Confidence}_i + \\beta_2 \\cdot \\text{Age}_i + \\epsilon_i\n$$\n$$\n\\widehat{\\text{Recall Accuracy}} = 36.16 + 0.90 \\cdot \\text{Recall Confidence} - 0.34 \\cdot \\text{Age} \n$$\n\n:::blue\n**In R**\n\nThere are numerous equivalent ways to obtain the estimated regression coefficients --- that is, $\\hat \\beta_0$, $\\hat \\beta_1$, ...., $\\hat \\beta_k$ --- from the fitted model (for this below example, our fitted model has been named `mdl`):\n\n- `mdl`\n- `mdl$coefficients`\n- `coef(mdl)`\n- `coefficients(mdl)`\n\n:::\n\n### Std. Error\n\nThe standard error of the coefficient is an estimate of the standard deviation of the coefficient (i.e., how much uncertainty there is in our estimated coefficient).\n\n:::blue\n**In R**\n\nIf you wanted to obtain just the standard error for each estimated regression coefficient, you could do the following (for this below example, our fitted model has been named `mdl`):\n\n- `summary(mdl)$coefficients[,2]` \n\n:::\n\nUsing the standard error, we can create confidence intervals to estimate a plausible range of values for the true population parameter. Recall the formula for obtaining a confidence interval for the population slope is:\n\n$$\n\\hat \\beta_j \\pm t^* \\cdot SE(\\hat \\beta_j)\n$$\nwhere $t^*$ denotes the critical value chosen from t-distribution with $n-k-1$ degrees of freedom (where $k$ = number of predictors and $n$ = sample size) for a desired $\\alpha$ level of confidence. \n\n:::blue\n**In R**\n\nWe can obtain the confidence intervals for the regression coefficients using the command `confint()`\n:::\n\n\n### t value\n\nThe t-statistic is the $\\beta$ coefficient divided by the standard error: \n\n$$\nt = \\frac{\\hat \\beta_j - 0}{SE(\\hat \\beta_j)}\n$$\n\nwhich follows a $t$-distribution with $n-k-1$ degrees of freedom (where $k$ = number of predictors and $n$ = sample size).\n\nWith this, we can test the the null hypothesis $H_0: \\beta_j = 0$. \n\nGenerally speaking, you want your model coefficients to have large $t$-statistics as this would indicate that the standard error was small in comparison to the coefficient. The larger our $t$-statistic, the more confident we can be that the coefficient is not 0. \n\n:::blue\n**In R**\n\nIf you wanted to obtain just the $t$-values for each estimated regression coefficient, you could do the following (for this below example, our fitted model has been named `mdl`):\n\n- `coef(summary(mdl))[, \"t value\"]`\n- `summary(mdl)$coefficients[,3]` \n\n:::\n\n### Pr(>|t|)\n\nFrom our $t$-value, we can compute our $p$-value. The $p$-value help us to understand whether our coefficient(s) are statistically significant (i.e., that the coefficient is statistically different from 0). The $p$-value of each estimate indicates the probability of observing a $t$-value at least as extreme as, or more extreme than, the one calculated from the sample data when assuming the null hypothesis to be true.\n\nIn Psychology, a $p$-value < .05 is usually used to make statements regarding statistical significance (it is important that you always state your $\\alpha$ level to help your reader understand any statements regarding statistical significance).\n\nThe number of asterisks marks corresponds with the significance of the coefficient (see the 'Signif. codes' legend just under the coefficients section). \n\n:::blue\n**In R**\n\nIf you wanted to obtain just the $p$-values for each estimated regression coefficient, you could do the following (for this below example, our fitted model has been named `mdl`):\n\n- `summary(mdl)$coefficients[,4]` \n\n:::\n\n:::\n\n## $\\sigma$\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![Multiple regression output in R, model standard deviation of the errors highlighted](images/recall_mdl_residerror.PNG){fig-align='left' width=80%}\n:::\n:::\n\n\n\nThe standard deviation of the errors, denoted by $\\sigma$, is an important quantity that our model estimates. It represents how much individual data points tend to deviate above and below the regression line - in other words, it tells us how well the model fits the data. \n\nA small $\\sigma$ indicates that the points hug the line closely and we should expect fairly accurate predictions, while a large $\\sigma$ suggests that, even if we estimate the line perfectly, we can expect individual values to deviate from it by substantial amounts.\n\nThe *estimated* standard deviation of the errors is denoted $\\hat \\sigma$, and is estimated by essentially averaging squared residuals (giving the variance) and taking the square-root: \n\n$$\n\\begin{align}\n& \\hat \\sigma = \\sqrt{\\frac{SS_{Residual}}{n - k - 1}} \\\\\n\\qquad \\\\\n& \\text{where} \\\\\n& SS_{Residual} = \\textrm{Sum of Squared Residuals} = \\sum_{i=1}^n{(\\epsilon_i)^2}\n\\end{align}\n$$\n\n:::blue\n**In R**\n\nThere are a couple of equivalent ways to obtain the estimated standard deviation of the errors --- that is, $\\hat \\sigma$ --- from the fitted model (for this example, our fitted model has been named `mdl`):\n\n- `sigma(mdl)`\n- `summary(mdl)`\n\n::: \n\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-20' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-20', 'opt-start-20')\"> <span class=\"olab\">Simple Linear Regression Models - Visualisation</span></span></div><div class=\"optional-body\" id = \"opt-body-20\" style=\"display: none;\">\n\n\n\nAs we saw above, the line representing the relationship between side and perimeter of squares is able to predict the actual perimeter value from the measurement of the side of a square.  \n\nThis is possible because the association between side and perimeter is an **exact** one. That is, any squares having the same side will have the same perimeter, and there will be no variation in those values.\n\nThe line that best fits the association between recall accuracy and age (see @fig-recallage-fitted-model), instead, is only able to predict the **average** accuracy for a given value of age.\n\nThis is because there will be a distribution of recall accuracy at each value of age. The line will fit the trend/pattern in the values, but there will be individual-to-individual variability that we must accept around that average pattern.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(recalldata, aes(x = age, y = recall_accuracy)) + \n    geom_point(size = 3, alpha = 0.5) +\n    geom_smooth(method = lm, se = FALSE) + \n    labs(x = \"Age (in years)\", y = \"Recall Accuracy (%)\")\n```\n\n::: {.cell-output-display}\n![Association between Recall Accuracy and Age](1_b1_reading_files/figure-html/fig-recallage-fitted-model-1.png){#fig-recallage-fitted-model fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-21' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-21', 'opt-start-21')\"> <span class=\"olab\">Multiple Linear Regression Models - Visualisation</span></span></div><div class=\"optional-body\" id = \"opt-body-21\" style=\"display: none;\">\n\n\n\nWhen we have 2+ predictors, we can’t just plot our data an add `geom_smooth(method=lm)`, because that would give a visualisation of a linear model with just one predictor (whichever one is on the x-axis).  \n\nInstead, we can use the function `plot_model()` from **sjPlot**. Here's an example with our recall accuracy model, with two predictors (recall confidence and age):\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_model(recall_mdl,\n           type = \"eff\",\n           terms = \"recall_confidence\",\n           show.data = TRUE)\n\nplot_model(recall_mdl,\n           type = \"eff\",\n           terms = \"age\",\n           show.data = TRUE)\n```\n\n::: {.cell-output-display}\n![Association between Recall Accuracy, Recall Confidence and Age](1_b1_reading_files/figure-html/fig-recalmod-1.png){#fig-recalmod-1 fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![Association between Recall Accuracy, Recall Confidence and Age](1_b1_reading_files/figure-html/fig-recalmod-2.png){#fig-recalmod-2 fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Model Predicted Values & Residuals\n\nModel predicted values are the estimates generated by a regression model for the dependent variable based on the independent variable(s), whilst residuals are the differences between these predicted values and the actual observed values (in turn indicating the accuracy of the model's predictions).\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-22' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-22', 'opt-start-22')\"> <span class=\"olab\">Predicted Values</span></span></div><div class=\"optional-body\" id = \"opt-body-22\" style=\"display: none;\">\n\n\n\n### Model predicted values ($\\hat y_i$) for sample data\n\nWe can get out the model predicted values for $y$, the \"y hats\" ($\\hat y$), for the data in the sample using various functions:\n\n- `predict(<fitted model>)`\n- `fitted(<fitted model>)`\n- `fitted.values(<fitted model>)`\n- `mdl$fitted.values`\n\nFor example, this will give us the estimated recall accuracy (point on our regression line) for each observed value of age for each of our 20 participants.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(recall_simp)\n```\n:::\n\n\n\nFor space saving purposes (i.e., we don't need to see all 20 values for this demonstration!), we can return the first six predicted values via `head()`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(predict(recall_simp))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4        5        6 \n62.23045 73.42542 69.49205 68.28179 70.09719 71.61002 \n```\n\n\n:::\n:::\n\n\n\n### Model predicted values for other (unobserved) data\n\nTo compute the model-predicted values for unobserved data (i.e., data not contained in the sample), we can use the following function:\n\n- `predict(<fitted model>, newdata = <dataframe>)`\n\nFor this example, we first need to remember that the model predicts `recall_accuracy` using the independent variable `age`. Hence, if we want predictions for new (unobserved) data, we first need to create a tibble with a column called `age` containing the age of individuals for which we want the prediction, and store this as a dataframe.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#Create dataframe 'newdata' containing the age values of 19, 32, and 99\nnewdata <- tibble(age = c(19,32,99))\nnewdata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 1\n    age\n  <dbl>\n1    19\n2    32\n3    99\n```\n\n\n:::\n:::\n\n\n\nThen we take `newdata` and add a new column called `accuracy_hat`, computed as the prediction from the fitted `recall_simp` using the `newdata` above:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnewdata <- newdata %>%\n  mutate(\n    accuracy_hat = predict(recall_simp, newdata = newdata)\n  )\nnewdata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n    age accuracy_hat\n  <dbl>        <dbl>\n1    19         78.3\n2    32         74.3\n3    99         54.1\n```\n\n\n:::\n:::\n\n\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-23' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-23', 'opt-start-23')\"> <span class=\"olab\">Residuals</span></span></div><div class=\"optional-body\" id = \"opt-body-23\" style=\"display: none;\">\n\n\n\nThe residuals ($\\hat \\epsilon_i$) represent the deviations between the actual responses and the predicted responses and can be obtained either as\n\n- `mdl$residuals`\n- `resid(mdl)`\n- `residuals(mdl)`\n- computing them as the difference between the response ($y_i$) and the predicted response ($\\hat y_i$)\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-24' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-24', 'opt-start-24')\"> <span class=\"olab\">Predicted Values - Example</span></span></div><div class=\"optional-body\" id = \"opt-body-24\" style=\"display: none;\">\n\n\n\nLets estimate (or predict) recall accuracy of two individuals with the following ages (a) 18, and (b) 118. There are a few ways we can do this, but first, let's recall our fitted model:\n\n$$\n\\widehat{Recall~Accuracy} = 84.02 - 0.31 \\cdot \\text{Age}\n$$\n\n\nFrom here we can:\n\n::: {.panel-tabset}\n\n## Substitute in Values\n\n- The predicted average recall accuracy for individuals who are aged 18 is:\n<br />\n$84.02 - (0.31 * 18) = 78.44$\n<br />\n<br />\n- The predicted average recall accuracy for individuals who are aged 118 is:\n<br />\n$84.02 - (0.31 * 118) = 47.44$\n\n## Use the `predict()` Function\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnewdata <- tibble(age = c(18, 118))\n\naccuracy_hat <- predict(recall_simp, newdata = newdata)\naccuracy_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2 \n78.56906 48.31237 \n```\n\n\n:::\n:::\n\n\n\n:::\n\nWe can see that both approaches (manually substituting values into the regression equation or using the `predict()` function) both give us the same values (slightly different due to rounding). \n\nBut, be careful to not go too far off the range of the available data (I don't know many 118 year olds, do you?). If you do, you will extrapolate. This is very dangerous...\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: Randall Munroe, xkcd.com](https://imgs.xkcd.com/comics/extrapolating.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n\n# Data Transformations\n\nThere are many transformations we can do to a continuous variable, but the most common ones are centering and scaling. These transformations can help to aid interpretability of our statistical models.  \n\n\n\n<div class=\"optional-begin\"><span id='opt-start-25' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-25', 'opt-start-25')\"> <span class=\"olab\">Centering</span></span></div><div class=\"optional-body\" id = \"opt-body-25\" style=\"display: none;\">\n\n\n\nCentering simply means moving the entire distribution to be centered on some new value. We achieve this by subtracting our desired center from each value of a variable. \n\nA common option is to mean center (i.e. to subtract the mean from each value). This makes our new values all relative to the mean. We can center a variable on other things, such as the minimum or maximum value of the scale we are using, or some judiciously chosen value of interest.\n\n:::blue\n\nIn **R**\n\n::: {.panel-tabset}\n\n## Manually mean center variable\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata_name <- data_name %>%\n  mutate(\n   mc_variable = variable - mean(variable)\n    )\n```\n:::\n\n\n\n## Add mean centered variable to data set via `scale()` function\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata_name <- data_name %>%\n    mutate(\n   mc_variable = scale(variable, scale = FALSE)\n   )\n```\n:::\n\n\n\n## Use `scale()` function within `lm` argument\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel <- lm(scale(DV, scale = FALSE) ~ scale(IV, scale = FALSE), data = data_name)\n```\n:::\n\n\n\n:::\n\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-26' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-26', 'opt-start-26')\"> <span class=\"olab\">Scaling</span></span></div><div class=\"optional-body\" id = \"opt-body-26\" style=\"display: none;\">\n\n\n\nScaling changes the units of the variable, and we do this by dividing the observations by some value. E.g., moving from “36 months” to “3 years” involves multiplying (scaling) the value by 1/12.\n\nThe most common transformation that involves scaling is called **standardisation**. \n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-27' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-27', 'opt-start-27')\"> <span class=\"olab\">Standardisation</span></span></div><div class=\"optional-body\" id = \"opt-body-27\" style=\"display: none;\">\n\n\n\nThis involves subtracting the mean and then dividing by the standard deviation. So standardisation centers on the sample mean and scales by the sample standard deviation. Recall that a standardized variable has mean of 0 and standard deviation of 1.\n\n**$z$-score Formula:**\n\n$$\nz_x = \\frac{x - \\bar{x}}{s_x}, \\qquad z_y = \\frac{y - \\bar{y}}{s_y}\n$$\n\n:::blue\n\nIn **R**\n\n::: {.panel-tabset}\n\n## Manually create z-scored variables and then use these in model\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata_name <- data_name %>%\n  mutate(\n   z_variable = (variable - mean(variable)) / sd(variable)\n    )\n```\n:::\n\n\n\n\n## Add scaled variable to data set via `scale()` function\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata_name <- data_name %>% \n    mutate(\n   z_variable = scale(variable)\n   )\n```\n:::\n\n\n\n\n## Use `scale()` function within `lm` argument\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel <- lm(scale(DV) ~ scale(IV), data = data_name)\n```\n:::\n\n\n\n:::\n\n:::\n\nWhen we standardise variables in a regression model, it means we can talk about all our coefficients in terms of “standard deviation units”. To the extent that it is possible to do so, this puts our coefficients on scales of the similar magnitude, making qualitative comparisons between the sizes of effects a little more easy.\n\nWe tend to refer to coefficients using standardised variables as (unsurprisingly), “standardised coefficients”\n\nThere are two main ways that people construct standardised coefficients. One of which standardises just the predictor, and the other of which standardises both predictor and outcome:\n\n\n| predictor    | outcome      | in lm                     | coefficient                       | interpretation                                   |\n| ------------ | ------------ | ------------------------- | --------------------------------- | ------------------------------------------------ |\n| standardised | raw          | `y ~ scale(x)`            | $\\beta = b \\cdot s_x$             | \"difference in Y for a 1 SD increase in X\"       |\n| standardised | standardised | `scale(y) ~ scale(x)`     | $\\beta = b \\cdot \\frac{s_x}{s_y}$ | \"difference in SD of Y for a 1 SD increase in X\" |\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Model Fit\n\nAssessing model fit involves examining metrics like the sum of squares to measure variability explained by the model, the $F$-ratio to evaluate the overall significance of the model by comparing explained variance to unexplained variance, and $R$-squared / Adjusted $R$-squared to quantify the proportion of variance in the dependent variable explained by the independent variable(s).\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-28' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-28', 'opt-start-28')\"> <span class=\"olab\">Sums of Squares</span></span></div><div class=\"optional-body\" id = \"opt-body-28\" style=\"display: none;\">\n\n\n\nTo quantify and assess a model’s utility in explaining variance in an outcome variable, we can split the total variability of that outcome variable into two terms: the variability explained by the model plus the variability left unexplained in the residuals.\n\nThe sum of squares measures the deviation or variation of data points away from the mean (i.e., how spread out are the numbers in a given dataset). We are trying to find the equation/function that best fits our data by varying the least from our data points. \n\n##### Total Sum of Squares\n\n**Formula**: \n\n$$\nSS_{Total} = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\n$$\nCan also be derived from:\n\n$$\nSS_{Total} = SS_{Model} + SS_{Residual}\n$$\n\n**In words**: \n\nSquared distance of each data point from the mean of $y$.\n\n**Description**: \n\nHow much variation there is in the DV.\n\n##### Residual Sum of Squares\n\n**Formula**: \n\n$$\nSS_{Residual} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n$$\n\n**In words**: \n\nSquared distance of each point from the predicted value.\n\n**Description**: \n\nHow much of the variation in the DV the model did not explain - a measure that captures the unexplained variation in your regression model. Lower residual sum of squares suggests that your model fits the data well, and higher suggests that the model poorly explains the data (in other words, the lower the value, the better the regression model). If the value was zero here, it would suggest the model fits perfectly with no error.\n\n##### Model Sum of Squares\n\n**Formula**: \n\n$$\nSS_{Model} = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\n$$\n\nCan also be derived from:\n\n$$\nSS_{Model} = SS_{Total} - SS_{Residual}\n$$\n**In words**: \n\nThe deviance of the predicted scores from the mean of $y$.\n\n**Description**: \n\nHow much of the variation in the DV your model explained - like a measure that captures how well the regression line fits your data.\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-29' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-29', 'opt-start-29')\"> <span class=\"olab\">F-ratio</span></span></div><div class=\"optional-body\" id = \"opt-body-29\" style=\"display: none;\">\n\n\n\n**Overview:**\n\nWe can perform a test to investigate if a model is ‘useful’ — that is, a test to see if our explanatory variable explains more variance in our outcome than we would expect by just some random chance variable.  \n\nWith one predictor, the $F$-statistic is used to test the null hypothesis that the regression slope for that predictor is zero:\n\n$$\nH_0: \\text{the model is ineffective, }b_1 = 0 \\\\  \n$$\n$$\nH_1 : \\text{the model is effective, }b_1  \\neq 0 \\\\  \n$$\n\nIn multiple regression, the logic is the same, but we are now testing against the null hypothesis that **all** regression slopes are zero. Our test is framed in terms of the following hypotheses:\n\n$$ \nH_0: \\text{the model is ineffective, }b_1,...., b_k = 0 \\\\    \n$$\n\n$$\nH_1 : \\text{the model is effective, }b_1,...., b_k  \\neq 0 \\\\  \n$$\n\nThe relevant test-statistic is the $F$-statistic, which uses “Mean Squares” (these are Sums of Squares divided by the relevant degrees of freedom). We then compare that against (you guessed it) an $F$-distribution! $F$-distributions vary according to two parameters, which are both degrees of freedom.\n\n**Formula:** \n\n$$\nF_{(df_{model},~df_{residual})} = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\\\\n\\quad \\\\\n$$\n\n$$\n\\begin{align}\n& \\text{Where:} \\\\\n& df_{model} = k \\\\\n& df_{residual} = n-k-1 \\\\\n& n = \\text{sample size} \\\\\n& k  = \\text{number of explanatory variables} \\\\\n\\end{align}\n$$\n\n\n**Description:**\n\nTo test the significance of an overall model, we can conduct an $F$-test. The $F$-test compares your model to a model containing zero predictor variables (i.e., the intercept only model), and tests whether your added predictor variables significantly improved the model.\n\nIt is called the $F$-ratio because it is the ratio of the how much of the variation is explained by the model (per parameter) versus how much of the variation is unexplained (per remaining degrees of freedom). \n\nThe $F$-test involves testing the statistical significance of the $F$-ratio.   \n\n**Q:** What does the $F$-ratio test?  \n**A:** The null hypothesis that all regression slopes in a model are zero (i.e., explain no variance in your outcome/DV). The alternative hypothesis is that **at least one of the slopes is not zero**. \n\n\nThe $F$-ratio you see at the bottom of `summary(model)` is actually a comparison between two models: your model (with some explanatory variables in predicting $y$) and the *null model*. \n\nIn regression, the null model can be thought of as the model in which all explanatory variables have zero regression coefficients. It is also referred to as the __intercept-only model__, because if all predictor variable coefficients are zero, then we are only estimating $y$ via an intercept (which will be the mean - $\\bar y$). \n\n**Interpretation:** \n\nAlongside viewing the $F$-ratio, you can see the results from testing the null hypothesis that all of the coefficients are $0$ (the alternative hypothesis is that at least one coefficient is $\\neq 0$. Under the null hypothesis that all coefficients = 0, the ratio of explained:unexplained variance should be approximately 1)\n\nIf your model predictors do explain some variance, the $F$-ratio will be significant, and you would reject the null, as this would suggest that your predictor variables included in your model improved the model fit (in comparison to the intercept only model).\n\n\n*Points to note:*\n\n- The larger your $F$-ratio, the better your model\n- The $F$-ratio will be close to 1 when the null is true (i.e., that all slopes are zero)\n\n:::blue\n\nIn **R**\n\nWe can see the $F$-statistic and associated $p$-value at the bottom of the output of `summary(<modelname>)`:\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![Multiple regression output in R, F statistic highlighted](images/recall_mdl_output_f.PNG){fig-align='left' width=80%}\n:::\n:::\n\n\n\n:::\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\n**Example Interpretation**\n\nThe linear model with recall confidence and age explained a significant amount of variance in recall accuracy beyond what we would expect by chance $F(2, 17) = 12.92, p < .001$. \n\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-30' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-30', 'opt-start-30')\"> <span class=\"olab\">R-squared and Adjusted R-squared</span></span></div><div class=\"optional-body\" id = \"opt-body-30\" style=\"display: none;\">\n\n\n\n**Overview:**\n\n$R^2$ represents the proportion of variance in $Y$ that is explained by the model predictor variables. \n\n**Formula:**\n\nThe $R^2$ coefficient is defined as the proportion of the total variability in the outcome variable which is explained by our model:\n$$\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n$$\n\nThe Adjusted $R^2$ coefficient is defined as:\n$$\n\\hat R^2 = 1 - \\frac{(1 - R^2)(n-1)}{n-k-1}\n\\quad \\\\\n$$\n\n$$\n\\begin{align}\n& \\text{Where:} \\\\\n& n = \\text{sample size} \\\\\n& k = \\text{number of explanatory variables} \\\\\n\\end{align}\n$$\n\n<br>\n\n**When to report Multiple $R^2$ vs. Adjusted $R^2$:**\n\nThe Multiple $R^2$ value should be reported for a simple linear regression model (i.e., one predictor).   \n\nUnlike $R^2$, Adjusted-$R^2$ does not necessarily increase with the addition of more explanatory variables, by the inclusion of a penalty according to the number of explanatory variables in the model. Since Adjusted-$R^2$ is adjusted for the number of predictors in the model, this should be used when there are 2 or more predictors in the model. As a side note, the Adjusted-$R^2$ should always be less than or equal to $R^2$.\n\n\n:::blue\n\nIn **R**\n\nWe can see both $R^2$ and Adjusted-$R^2$ in the second bottom row of the `summary(<modelname>)`:\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![Multiple regression output in R, R^2 statistic highlighted](images/recall_mdl_output_r.PNG){fig-align='left' width=80%}\n:::\n:::\n\n\n\n:::\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\n**Example Interpretation**\n\nTogether, recall confidence and age explained approximately 55.66% of the variance in recall accuracy. \n\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Model Comparisons\n\nOne useful thing we might want to do is compare our models with and without some predictor(s).There are numerous ways we can do this, but the method chosen depends on the models and underlying data:\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![](images/comparisons_chart.png){fig-align='left' width=100%}\n:::\n:::\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-31' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-31', 'opt-start-31')\"> <span class=\"olab\">Nested vs Non-Nested Models</span></span></div><div class=\"optional-body\" id = \"opt-body-31\" style=\"display: none;\">\n\n\n\n**Nested Models**\n\nConsider that you have two regression models where Model 1 contains a subset of the predictors contained in the other Model 2 and is fitted to the same data. More simply, Model 2 contains all of the predictors included in Model 1, **plus** additional predictor(s). This means that Model 1 is *nested* within Model 2, or that Model 1 is a *submodel* of Model 2. These two terms, at least in this setting, are interchangeable - it might be easier to think of Model 1 as your null and Model 2 as your alternative.\n\n**Non-Nested Models**\n\nConsider that you have two regression models where Model 1 contains different variables to those contained in Model 2, where both models are fitted to the same data. More simply, Model 1 and Model 2 contain unique variables that are not shared. This means that Model 1 and Model 2 are **not** nested.\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-32' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-32', 'opt-start-32')\"> <span class=\"olab\">Incremental F-test</span></span></div><div class=\"optional-body\" id = \"opt-body-32\" style=\"display: none;\">\n\n\n\nIf (*and only if*) two models are __nested__, can we compare them using an __incremental F-test__.  \n\nThis is a formal test of whether the additional predictors provide a better fitting model.  \nFormally this is the test of:  \n\n+ $H_0:$ coefficients for the added/omitted variables are all zero.\n\n+ $H_1:$ at least one of the added/omitted variables has a coefficient that is not zero. \n\nThe $F$-ratio for comparing the residual sums of squares between two models can be calculated as:\n\n$$\nF_{(df_R-df_F),~df_F} = \\frac{(SSR_R-SSR_F)/(df_R-df_F)}{SSR_F / df_F} \\\\\n\\quad \\\\\n$$\n$$\n\\begin{align}\n& \\text{Where:} \\\\\n\\\\\n& SSR_R = \\text{residual sums of squares for the restricted model} \\\\\n& SSR_F = \\text{residual sums of squares for the full model} \\\\\n& df_R = \\text{residual degrees of freedom from the restricted model} \\\\\n& df_F = \\text{residual degrees of freedom from the full model} \\\\\n\\end{align}\n$$\n\n:::blue\n**In R**\n\nWe can conduct an incremental $F$-test to compare two models by fitting both models using `lm()`, and passing them to the `anova()` function:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel1 <- lm( ... )\nmodel2 <- lm( ... )\nanova(model1, model2)\n```\n:::\n\n\n\n\nIf we wanted to, for example, compare a model with just one predictor, $x1$, to a model with 2 predictors: $x1$, and $x2$, we can assess the extent to which the variable $x2$ improves model fit:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel1 <- lm(y ~ x1, data = data_name)\nmodel2 <- lm(y ~ x1 + x2, data = data_name)\nanova(model1, model2)\n```\n:::\n\n\n\n:::\n\nFor example:\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![Model Comparisons using Incremental F-test](images/recall_mdl_anova_comp.PNG){fig-align='left' width=80%}\n:::\n:::\n\n\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\n**Example Interpretation**\n\nRecall confidence explained a significant amount of variance in recall accuracy beyond age $(F(1, 17) = 21.95, p < .001)$. \n\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-33' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-33', 'opt-start-33')\"> <span class=\"olab\">AIC & BIC</span></span></div><div class=\"optional-body\" id = \"opt-body-33\" style=\"display: none;\">\n\n\n\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) combine information about the sample size, the number of model parameters, and the residual sums of squares ($SS_{residual}$). Models do not *need* to be nested to be compared via AIC and BIC, __but__ they need to have been fit to the same dataset.  \n\nFor both of these fit indices, lower values are better, and both include a penalty for the number of predictors in the model (although BIC's penalty is harsher):\n\n$$\n\\begin{align}\nAIC = n\\,\\text{ln}\\left( \\frac{SS_{residual}}{n} \\right) + 2k \\\\\n\\end{align}\n\\quad \\\\\n$$\n\n$$\n\\begin{align}\nBIC = n\\,\\text{ln}\\left( \\frac{SS_{residual}}{n} \\right) + k\\,\\text{ln}(n) \\\\\n\\end{align}\n\\quad \\\\\n$$\n\n$$\n\\begin{align}\n& \\text{Where:} \\\\\n& SS_{residual} = \\text{sum of squares residuals} \\\\\n& n = \\text{sample size} \\\\\n& k = \\text{number of explanatory variables} \\\\\n& \\text{ln} = \\text{natural log function} \n\\end{align}\n$$\n\n:::blue\n**In R**\n\nWe can calculate AIC and BIC by using the `AIC()` and `BIC()` functions respectively:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#AIC\nAIC(<modelname>)\n\n#BIC\nBIC(<modelname>)\n```\n:::\n\n\n\n\n:::\n\nFor example:\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![Model Comparisons using AIC and BIC](images/recall_mdl_aic_bic.PNG){fig-align='left' width=80%}\n:::\n:::\n\n\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\n**Example Interpretation**\n\nBased on both AIC and BIC, the model predicting recall accuracy that included both recall confidence and age was better fitting ($AIC = 152.28; BIC = 156.27$) than the model with age alone ($AIC = 166.86; BIC = 169.85$). \n\n:::\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# General Formatting & Presenting of Results\n\n## LaTeX Symbols & Equations\n\nBy embedding LaTeX into RMarkdown, you can accurately and precisely format mathematical expressions, ensuring that they are not only technically correct but also visually appealing and easy to interpret.\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-34' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-34', 'opt-start-34')\"> <span class=\"olab\">LaTeX Guide</span></span></div><div class=\"optional-body\" id = \"opt-body-34\" style=\"display: none;\">\n\n\n\nFor an overview of how to integrate LaTeX symbols and equations, review [Lesson 9 of the RMD bootcamp](https://uoepsy.github.io/scs/rmd-bootcamp/09-latex.html). \n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n  \n## APA Formatting\n\nAPA format is a writing/presentation style that is often used in psychology to ensure consistency in communication. APA formatting applies to all aspects of writing - from formatting of papers (including tables and figures), citation of sources, and reference lists. This means that it also applies to how you present results in your Psychology courses, including DAPR2.\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-35' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-35', 'opt-start-35')\"> <span class=\"olab\">APA Formatting Guides</span></span></div><div class=\"optional-body\" id = \"opt-body-35\" style=\"display: none;\">\n\n\n\nAll results should be presented following [APA guidelines](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf). \n\nYou also need to follow APA style rules for [tables and figures](https://apastyle.apa.org/style-grammar-guidelines/tables-figures). \n\nMake sure to familiarise yourself with the above guides, and practice presenting your results following these rules.\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n## Tables\n\nWe want to ensure that we are presenting results in a well formatted table. To do so, there are lots of different packages available (see [Lesson 4 of the RMD bootcamp](https://uoepsy.github.io/scs/rmd-bootcamp/04-prettytab.html)). \n\nOne of the most convenient ways to present results from regression models is to use the `tab_model()` function from **sjPlot**\n\n\n\n<div class=\"optional-begin\"><span id='opt-start-36' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-36', 'opt-start-36')\"> <span class=\"olab\">Creating tables via tab_model</span></span></div><div class=\"optional-body\" id = \"opt-body-36\" style=\"display: none;\">\n\n\n\nWithin `tab_model()`, there are lots of different ways that you can customise your table. The most common arguments that you should use are `dv.labels`, `pred.labels`, and `title`. \n\nYou can rename your DV and IV labels by specifying `dv.labels` and `pred.labels`. To do so, specify your variable name on the left, and what you would like this to be named in the table on the right. For `title`, you can simply specify in \"\"'s what you want your title to be e.g., `title = \"This is my title\"`. \n\nHere's an example if I had fitted a model with the following information:\n\n- Model name = `mdl_test`\n- Model DV = `cognitive_score`\n- Model IVs = `SES` and `age`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmdl_test <- lm(cognitive_score ~ SES + age, data = data_name)\n```\n:::\n\n\n\nI want to change the names of `SES` and `age` to be `socio-economic status` and `age - in years` respectively. What we need to pay attention to here is the ordering of the IVs - the ordering in our `lm()` must match that in `tab_model()`. I also want to name my table *Regression Table for Cognitive Scores Model*. Here is how we would do this in **R**:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(sjPlot)\ntab_model(mdl_test,\n          pred.labels = c('Intercept', 'socio-economic status', 'age - in years'),\n          title = \"Regression Table for Cognitive Scores Model\")\n```\n:::\n\n\n\nSee [here](https://uoepsy.github.io/scs/rmd-bootcamp/zz-packs.html) for another short example. \n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n## Cross Referencing\n\nCross-referencing is a very helpful way to direct your reader through your document, and the good news is that this can be done automatically in RMarkdown. \n\n\n\n<div class=\"optional-begin\"><span id='opt-start-37' class=\"fa-solid fa-circle-right optional-icon clickable\" onclick=\"toggle_visibility('opt-body-37', 'opt-start-37')\"> <span class=\"olab\">Cross Referencing</span></span></div><div class=\"optional-body\" id = \"opt-body-37\" style=\"display: none;\">\n\n\n\nThere are three key components to allow you to successfully cross-reference within your RMarkdown document:\n\n+ A bookdown output format\n+ A caption to your figure or table\n+ A named/labeled code chunk\n\nOnce you have the above, you will be able to cross-reference using the syntax \\@ref(type:label), where label is the chunk name/label,  and type is the environment being referenced (e.g. tab for table, fig for figure, etc.).\n\nFor an in-depth overview and example of how to cross-reference, see [Lesson 7 of the RMD bootcamp](https://uoepsy.github.io/scs/rmd-bootcamp/07-refs.html). \n\n\n\n\n</div><p class=\"optional-end\"></p>\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "1_b1_reading_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}