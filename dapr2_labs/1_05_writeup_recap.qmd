---
title: "Write Up & Block 1 Recap"
link-citations: TRUE
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source('assets/setup.R')

set.seed(953)

library(DT)
library(pander)
library(sjPlot)
library(tidyverse)
```

:::lo

### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand how to write-up and provide interpretation of a linear model with multiple predictors.

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1. Be up to date with lectures
2. Have completed Labs 1 - 4

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse** 
* **patchwork**
* **sjPlot**
* **kableExtra**

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/riverview.csv) or read it in via this link https://uoepsy.github.io/data/riverview.csv. 
:::

# Section A: Write-Up

In this lab you will be presented with the output from a statistical analysis, and your job will be to write-up and present the results. We're going to use an example analysis using one of the datasets we have worked with on a number of exercises in previous labs concerning personality traits, social comparison, depression, and anxiety.  

The aim in writing should be that a reader is able to more or less replicate your analyses **without** referring to your R code. This requires detailing all of the steps you took in conducting the analysis.  
The point of using RMarkdown is that you can pull your results **directly** from the code. If your analysis changes, so does your report!  

Make sure that your final report doesn't show any R functions or code. Remember you are interpreting and reporting your results in text, tables, or plots, targeting a generic reader who may use different software or may not know R at all. If you need a reminder on how to hide code, format tables, etc., make sure to review the [rmd bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

:::{.callout-note}

## Important - Write-Up Examples & Plagiarism

The example write-up sections included below are not **perfect** - they instead should give you a good example of what information you should include within each section, and how to structure this. For example, some information is missing (e.g., interpretation of descriptive statistics, what type of interaction is present), some information could be presented more clearly (e.g., variable names in tables, table/figure titles/captions, and rationales for choices), and writing could be more concise in places (e.g., discussion section is quite long).  

Further, **you must not copy any of the write-up included below for future reports** - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more [here](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct).

:::

## Study Overview 

> **Research Question**
> 
> Does education level explain a significant amount of variance in income over seniority?

Let’s imagine a study into income disparity for workers in the city of Riverview, a a hypothetical midwestern city in the US.. We might carry out interviews and find that there is an association between the level of education and an employee’s income. However, we are unsure whether this association explains any more variance than that already accounted for by seniority level.

`r optbegin('Riverview data codebook.', FALSE, show = TRUE, toggle = params$TOGGLE)`

__Description__

The riverview data come from @Lewis-Beck2015 and contain five attributes collected from a random sample of employees working in the hypothetical city of Riverview ($n=32$). The attributes include:

- `education`: Years of formal education
- `income`: Annual income (in thousands of U.S. dollars)
- `seniority`: Years of seniority
- `gender`: Employee's gender
- `male`: Dummy coded gender variable (0 = Female, 1 = Male)
- `party`: Political party affiliation

__Preview__

The first six rows of the data are:

```{r echo=FALSE, message=FALSE}
read_csv('https://uoepsy.github.io/data/riverview.csv') %>% head %>% gt::gt()
```

`r optend()`

<div class="divider div-transparent div-dot"></div>

### Setup
`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read the riverview dataset into R, assigning it to an object named `riverview`

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r, warning=FALSE, message=FALSE}
#Loading the required package(s)
library(tidyverse)
library(patchwork)
library(sjPlot)
library(kableExtra)

#Reading in riverview data and storing in object named 'riverview'
riverview <- read_csv("https://uoepsy.github.io/data/riverview.csv")

#check first six rows
head(riverview)
```

`r solend()`

`r optbegin("Provided Analysis Code", olabel=FALSE,toggle=params$TOGGLE)`  

```{r message=FALSE, warning=FALSE, eval = FALSE}
library(tidyverse) # for all things!
library(psych) # good for descriptive stats
library(kableExtra) # useful for creating nice tables
library(sjPlot) #regression tables

riverview <- read_csv("https://uoepsy.github.io/data/riverview.csv")

# standardise scs score
scs_study <- 
  scs_study %>% 
    mutate(
      zscs = (scs-mean(scs))/sd(scs)
    )
#alternatively, you could do zscs = scale(scs, center = TRUE, scale = TRUE)

# the describe() function is from the psych package, and kable() from kableExtra which is used to make a nice table where the values are rounded to 2 decimal places using digits = 2. 
describe(scs_study %>% 
        select(dass, scs, zn))[,c(2:4,8:9)] %>% 
        kable(., caption = "DASS-21, SCS, and Neuroticism Descriptive Statistics", digits = 2) %>%
        kable_styling()

```

```{r eval = FALSE}
# scatterplot matrix of dataset without the zscs variable
pairs.panels(scs_study %>% select(-zscs))
```


```{r}
#build models
income_mdl1 <- lm(income ~ seniority, data = riverview)
income_mdl2 <- lm(income ~ seniority + education, data = riverview)

#check summary table of results for each model
summary(income_mdl1)
summary(income_mdl2)
```

```{r}
#compare models
anova(income_mdl1, income_mdl2)
```

```{r}
#create table for results
tab_model(income_mdl1, income_mdl2,
          dv.labels = c("Income", "Income"),
          pred.labels = c("seniority" = "Seniority (in years)", 
                          "education"="Formal Education (in years)"), 
          title = "Regression Table for Income Model")
```

`r optend()`

### The 3-Act Structure

We need to present our report in three clear sections - think of your sections like the 3 key parts of a play or story - we need to (1) provide some background and scene setting for the reader, (2) present our results in the context of the research question, and (3) present a resolution to our story - relate our findings back to the question we were asked and provide our answer. 

#### Act I: Analysis Strategy

`r qbegin(1)`

Attempt to draft a discussion section based on the above research question and analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Analysis Strategy - What to Include***

Your analysis strategy will contain a number of different elements detailing plans and changes to your plan. Remember, your analysis strategy should **not** contain any results. You may wish to include the following sections:  

-  Very brief data and design description:
     - Give the reader some background on the context of your write-up. For example, you may wish to describe the data source, data collection strategy, study design, number of observational units.
     - Specify the variables of interest in relation to the research question, including their unit of measurement, the allowed range (for Likert scales), and how they are scored.

-  Data management:  
     - Describe any data cleaning and/or recoding.
     - Are there any observations that have been excluded based on pre-defined criteria? How/why, and how many? 
     - \* Describe any transformations performed to aid your interpretation (i.e., mean centering, standardisation, etc.)

-  Model specification:  
     -  Clearly state your hypotheses and specify your chosen significance level.
     -  What type of statistical analysis do you plan to use to answer the research question? (e.g., simple linear regression, multiple linear regression, binary logistic regression, etc.)
     - In some cases, you may wish to include some visualisations and descriptive tables to motivate your model specification. 
     -  Specify the model(s) to be fitted to answer your given research question and analysis structure. Clearly specify the response and explanatory variables included in your model(s).

:::frame
*** This is not yet a completed list -- we will add more to this as we progress through the course (e.g., coding of categorical variables, checking assumptions, diagnostics, etc.)
:::

As noted and encouraged throughout the course, one of the main benefits of using RMarkdown is the ability to include inline R code in your document. Try to incorporate this in your write up so you can automatically pull the specified values from your code. If you need a reminder on how to do this, see [Lesson 4 of the Rmd Bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

:::

`r optbegin("Example Write-Up of Analysis Strategy Section", olabel=FALSE, toggle = params$TOGGLE)`

`r optend()`

#### Act II: Results

`r qbegin(2)`

Attempt to draft a results section based on your detailed analysis strategy and the analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Results - What To Include***

The results section should follow from your analysis strategy. This is where you would present the evidence and results that will be used to answer the research questions and can support your conclusions. Make sure that you address all aspects of the approach you outlined in the analysis strategy. 

In this section, it is useful to include tables and plots to clearly present your findings to your reader. It is important, however, to carefully select what is the key information that should be presented. You don't want to overload the reader with unnecessary information, and you also want to save space in case there is a page limit. Make use of figures with multiple panels where you can.

As a broad guideline, you want to start with the results of an exploratory data analysis, presenting tables of summary statistics and exploratory plots. You may also want to visualise associations between/among variables and report covariances or correlations. Then, you should move on to the results from your model.

:::frame
*** This is not yet a completed list -- we will add more to this as we progress through the course (e.g., evaluation of diagnostic plots, etc.)
:::

:::

`r optbegin("Example Write-Up of Results Section", olabel=FALSE, toggle = params$TOGGLE)`

`r optend()`


#### Act III: Discussion

`r qbegin(3)`

Attempt to draft a discussion section based on your results and the analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Discussion - What To Include

In the discussion section, you should summarise the key findings from the results section and provide the reader with a few take-home sentences drawing the analysis together and relating it back to the original question. 

The discussion should be relatively brief, and should not include any statistical analysis - instead think of the discussion as a conclusion, providing an answer to the research question(s).

:::

`r optbegin("Example Write-Up of Discussion Section", olabel=FALSE, toggle=params$TOGGLE)`

`r optend()`

# Section B: Weeks 1 - 4 Recap

In the second part of the lab, there is no new content - the purpose of the recap section is for you to revisit and revise the concepts you have learned over the last 4 weeks. 

:::red

Before you expand each of the boxes below, think about how comfortable you feel with each concept.  

:::

`r optbegin("Types of Models: Deterministic vs Statistical", olabel=FALSE,toggle=params$TOGGLE)`

#### __Deterministic (*Example: Perimeter & Side*)__

The mathematical model 

$$
Perimeter = 4 * Side
$$ 

or, equivalently, 
$$
y = 4 * x
$$

represents the relationship between side and perimeter of squares. This is an example of a _deterministic model_ as it is a model of an *exact relationship* - there can be no deviation.

#### __Statistical (*Example: Height & Handspan*)__

The relationship between height and handspan shows deviations from the 'average pattern'. Hence, we need to create a model that allows for deviations from the linear relationship - we need a _statistical model_.

A statistical model includes *both* a deterministic function and a random error term:
$$
Handspan = \beta_0 + \beta_1 * Height + \epsilon
$$
or, in short,
$$
y = \underbrace{\beta_0 + \beta_1 * x}_{\text{function of }x} + \underbrace{\epsilon}_{\text{random error}}
$$

The deterministic function need not be linear if the scatterplot displays signs of nonlinearity.

In the equation above, the terms $\beta_0$ and $\beta_1$ are numbers specifying where the line going through the data meets the y-axis and its slope (direction and gradient of line).

:::{.callout-note}

See Week 1 [lab](https://uoepsy.github.io/dapr2/2223/labs/1_01_function.html) and both [lecture 1](https://uoepsy.github.io/dapr2/2223/lectures/dapR2_00_Intro.html) and [lecture 2](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_01_introlm.html) for further details and to revise these concepts further.

:::


`r optend()`

`r optbegin("Null & Alternative Hypotheses", olabel=FALSE,toggle=params$TOGGLE)`

Recall that statistical hypotheses are testable mathematical statements.

We need to define a null ($H_0$) and alternative ($H_1$) hypothesis.

Points to note:

- We can only ever test the null ($H_0$), so all statements must be made in reference to this
- We can only ever reject or fail to reject the null (we can **never** accept a hypothesis)

`r optend()`

`r optbegin("Simple Linear Regression", olabel=FALSE,toggle=params$TOGGLE)`

**Formula:**  

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$
In **R**:

:::blue

There are basically two pieces of information that we need to pass to the `lm()` function:

1. The formula: The regression formula should be specified in the form `y ~ x` where $y$ is the dependent variable (DV) and $x$ the independent variable (IV).
2. The data: Specify which dataframe contains the variables specified in the formula.

+ run simple linear regression via `lm()` function

```{r eval=FALSE}
model_name <- lm(DV ~ IV, data = data_name)
```

**OR**

```{r eval=FALSE}
model_name <- lm(data_name$DV ~ data_name$IV)
```

:::

:::{.callout-note}

See Week 2 [lab](https://uoepsy.github.io/dapr2/2223/labs/1_02_slr.html) and [lectures](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_02_LM2.html) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Multiple Linear Regression", olabel=FALSE,toggle=params$TOGGLE)`

**Formula:**  


$$
y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon_i
$$
In **R**:

:::blue

Multiple and simple linear regression follow the same structure within the `lm()` function. You simply add (using the `+` sign) more independent variables.

+ run multiple linear regression (example includes three independent variables) via `lm()` function

```{r eval=FALSE}
model_name <- lm(DV ~ IV1 + IV2 + IV3, data = data_name)
```

**OR**

```{r eval=FALSE}
model_name <- lm(data_name$DV ~ data_name$IV1 + data_name$IV2 + data_name$IV3
```

:::

**Interpretation of Multiple Regression Coefficients**

You'll hear a lot of different ways that people explain multiple regression coefficients.  

For the model $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$, the estimate $\hat \beta_1$ will often be reported as:  
  
the increase in $y$ for a one unit increase in $x_1$ when...

- holding the effect of $x_2$ constant.
- controlling for differences in $x_2$.
- partialling out the effects of $x_2$.
- holding $x_2$ equal. 
- accounting for effects of $x_2$. 

:::{.callout-note}

See Week 3 [lab](https://uoepsy.github.io/dapr2/2223/labs/1_03_mlr.html) and [lectures](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_03_testinglm.html) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Partitioning Variation: Sum of Squares", olabel=FALSE,toggle=params$TOGGLE)`

### __Sum of Squares__

The sum of squares measures the deviation or variation of data points away from the mean (i.e., how spread out are the numbers in a given dataset). We are trying to find the equation/function that best fits our data by varying the least from our data points. 

#### __Total Sum of Squares__

**Formula**: 

$$SS_{Total} = \sum_{i=1}^{n}(y_i - \bar{y})^2$$

**In words**: 

Squared distance of each data point from the mean of $y$.

**Description**: 

How much variation there is in the DV.

#### __Residual Sum of Squares__

**Formula**: 

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

**In words**: 

Squared distance of each point from the predicted value.

**Description**: 

How much of the variation in the DV the model did not explain - a measure that captures the unexplained variation in your regression model. Lower residual sum of squares suggests that your model fits the data well, and higher suggests that the model poorly explains the data (in other words, the lower the value, the better the regression model). If the value was zero here, it would suggest the model fits perfectly with no error.

#### __Model Sum of Squares__ 

**Formula**: 

$$SS_{Model} = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2$$

Can also be derived from:

$$SS_{Model} = SS_{Total} - SS_{Residual}$$
**In words**: 

The deviance of the predicted scores from the mean of $y$.

**Description**: 

How much of the variation in the DV your model explained - like a measure that captures how well the regression line fits your data.

:::{.callout-note}

See Week 3 [lab](https://uoepsy.github.io/dapr2/2223/labs/1_03_mlr.html) and [lectures](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_03_testinglm.html), as well as Week 4 [lab](https://uoepsy.github.io/dapr2/2223/labs/1_04_model_fit.html) and [lectures](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_04_testinglm2.html) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("F-test & F-ratio", olabel=FALSE,toggle=params$TOGGLE)`

**Formula**: 

$$
F_{df_{model},df_{residual}} = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\
\quad \\
\begin{align}
& \text{Where:} \\
& df_{model} = k \\
& df_{residual} = n-k-1 \\
& n = \text{sample size} \\
& k  = \text{number of explanatory variables} \\
\end{align}
$$
**Description**: 

To test the significance of an overall model, we can conduct an $F$-test. The $F$-test compares your model to a model containing zero predictor variables (i.e., the intercept only model), and tests whether your added predictor variables significantly improved the model.

The $F$-test involves testing the statistical significance of the $F$-ratio. 
Q: What does the $F$-ratio test?
A: The null hypothesis that all regression slopes in a model are zero (i.e., explain no variance in your outcome/DV).

*Points to note*: 

- The larger your $F$-ratio, the better your model
- The $F$-ratio will be close to 1 when the null is true (i.e., that all slopes are zero)


**Interpretation**: 

If your model predictors do explain some variance, the $F$-ratio will be significant, and you would reject the null, as this would suggest that your predictor variables included in your model improved the model fit (in comparison to the intercept only model).

:::{.callout-note}

See Week 4 [lab](https://uoepsy.github.io/dapr2/2223/labs/1_04_model_fit.html) and [lectures](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_04_testinglm2.html) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("R-squared and Adjusted R-squared", olabel=FALSE,toggle=params$TOGGLE)`

$R^2$ represents the proportion of variance in $Y$ that is explained by the model.

The $R$-squared coefficient is defined as:
$$
R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}
$$

The Adjusted $R$-squared coefficient is defined as:
$$
\hat R^2 = 1 - \frac{(1 - R^2)(n-1)}{n-k-1}
\quad \\
\begin{align}
& \text{Where:} \\
& n = \text{sample size} \\
& k = \text{number of explanatory variables} \\
\end{align}
$$
We can see the Multiple and Adjusted $R$-squared in the `summary()` output of a model.

Points to note:

- Adjusted-$R^2$ adjusts for the number of terms in a model, and should be used when there are 2 or more predictors in the model
- Adjusted-$R^2$ should always be less than or equal to $R^2$

:::{.callout-note}

See Week 4 [lab](https://uoepsy.github.io/dapr2/2223/labs/1_04_model_fit.html) and [lectures](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_04_testinglm2.html) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Standardisation", olabel=FALSE,toggle=params$TOGGLE)`

**$z$-score Formula:**

$$
z_x = \frac{x - \bar{x}}{s_x}, \qquad z_y = \frac{y - \bar{y}}{s_y}
$$

Recall that a standardized variable has mean of 0 and standard deviation of 1.

In **R**:

:::blue

```{r eval = FALSE}
#create z-scored variables
dataframe <- dataframe %>%
  mutate(
   z_variable = (variable - mean(variable)) / sd(variable)
    )
```

**OR**

```{r eval = FALSE}
#use scale function
model <- lm(scale(DV) ~ scale(IV), data = dataset)
```

:::

:::{.callout-note}

See Week 4 [lab](https://uoepsy.github.io/dapr2/2223/labs/1_04_model_fit.html) and [lectures](https://uoepsy.github.io/dapr2/2223/lectures/dapr2_04_testinglm2.html) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Steps Involved in Modelling", olabel=FALSE,toggle=params$TOGGLE)`

You can think of the sequence of steps involved in statistical modeling as:  
$$
\text{Choose} \rightarrow \text{Fit} \rightarrow \text{Assess} \rightarrow \text{Use}
$$

:::frame
**A general rule**  
<br>
<center> Do not use (draw inferences or predictions from) a model before you have **assessed** whether the model satisfies the underlying assumptions</center>
:::

<br>
Throughout this block, we have completed the first three steps (**Choose**, **Fit**, and **Use**) in that we have:  

1. Explored/visualised our data and specified our model
2. Fitted the model in `R`  
3. Interpreted our parameter estimates

Please note that when conducting real analyses, it would be inappropriate to complete these steps without also **assessing** whether a regression model meets the assumptions. You will learn how to do this later in the course.   

`r optend()`
