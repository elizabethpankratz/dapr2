---
title: "Write Up & Block 1 Recap"
link-citations: TRUE
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source('assets/setup.R')

set.seed(953)

library(DT)
library(pander)
library(sjPlot)
library(tidyverse)
```

:::lo

### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand how to write-up and provide interpretation of linear models with single and multiple predictors.

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1. Be up to date with lectures
2. Have completed Labs 1 - 4

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse** 
* **psych**
* **patchwork**
* **sjPlot**
* **kableExtra**

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/FOMOdataset.csv) or read it in via this link https://uoepsy.github.io/data/FOMOdataset.csv. 

:::

# Section A: Write-Up

In this lab you will be presented with the output from a statistical analysis, and your job will be to write-up and present the results. We're going to use a simulated dataset based on a paper (the same one you have worked on in lectures this week) concerning Fear of Missing Out (FoMO), socio-demographic factors, and the Big Five personality traits.

The aim in writing should be that a reader is able to more or less replicate your analyses **without** referring to your R code. This requires detailing all of the steps you took in conducting the analysis.  
The point of using RMarkdown is that you can pull your results **directly** from the code. If your analysis changes, so does your report!  

Make sure that your final report doesn't show any R functions or code. Remember you are interpreting and reporting your results in text, tables, or plots, targeting a generic reader who may use different software or may not know R at all. If you need a reminder on how to hide code, format tables, etc., make sure to review the [rmd bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

:::{.callout-note}

## Important - Write-Up Examples & Plagiarism

The example write-up sections included below are not **perfect** - they instead should give you a good example of what information you should include within each section, and how to structure this. For example, some information is missing (e.g., description of data checks, interpretation of descriptive statistics), some information could be presented more clearly (e.g., variable names in tables, table/figure titles/captions, and rationales for choices), and writing could be more concise in places (e.g., discussion section is quite long).  

Further, **you must not copy any of the write-up included below for future reports** - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more [here](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct).

:::

## Study Overview 

> **Research Aim**
> 
> Explore the associations among Fear of Missing Out (FoMO), age, social media networks, and the Big Five personality traits. 
>
> **Research Questions**
> 
> + RQ1: Does age predict FoMO?
> + RQ2: Does the number of Instagram followers explain a significant amount of variance in FoMO over and above age?
> + RQ3: Does personality predict FoMO?

`r optbegin('FoMO data codebook.', FALSE, show = TRUE, toggle = params$TOGGLE)`

__Description__

The data used for this write-up exercise are simulated, drawing on recent work on Fear of Missing Out (FoMO), socio-demographic factors, and the Big Five personality traits. The simulated data are based on the findings of this work, and acted to expand upon the methods and results reported in the following paper:

Rozgonjuk, D., Sindermann, C., Elhai, J. D., & Montag, C. (2021). Individual differences in Fear of Missing Out (FoMO): Age, gender, and the Big Five personality trait domains, facets, and items. *Personality and Individual Differences, 171*, 110546.  [https://doi.org/10.1016/j.paid.2020.110546](https://doi.org/10.1016/j.paid.2020.110546)

In the current study, participants were invited to an online study investigating the associations among FoMO, socio-demographic factors, and personality. The final sample comprised 3370 people. Participants completed a FOMO scale and a personality inventory. The 10-item FOMO scale measured the extent of experiencing apprehension regarding missing out on interesting events of others on a 5-point scale (1 = “not at all true of me” to 5 = “extremely true of me”), producing a possible range of scores between 10 and 50. The Big Five Inventory (BFI) is a 45-item personality assessment questionnaire (note that only 43 items were used to match the study above) that uses a five-point response scale (1 = “very inapplicable” to 5 = “very applicable”). The BFI consists of five domains: Neuroticism (8 items; possible range of scores 8-40), Extraversion (8 items; possible range of scores 8-40), Openness to Experience (10 items; possible range of scores 10-50), Agreeableness (8 items; possible range of scores 8-40), and Conscientiousness (9 items; possible range of scores 9-45). We extended the aforementioned study to include an extra socio-demographic variable - a measure of popularity on social media based on the number of followers. Unlike the original study, we do not have measures of gender, education level, or specific country of residence.

__Data Dictionary__

The data in `FOMOdataset.csv` contain eight attributes collected from a simulated sample of $n=3370$ hypothetical individuals across the UK, and include: 

```{r echo=FALSE, message=FALSE, warning=FALSE}
fomo <- read_csv("https://uoepsy.github.io/data/FOMOdataset.csv")
tibble(
Variable = names(fomo),
Description = c("FoMO Score (as measured by the 10-item FoMO scale)", "Age (in years)", "Score on personality items assessing Neuroticism from the Big Five Inventory (BFI)", "Score on personality items assessing Extraversion from the Big Five Inventory (BFI)", "Score on personality items assessing Openness from the Big Five Inventory (BFI)", "Score on personality items assessing Agreeableness from the Big Five Inventory (BFI)", "Score on personality items assessing Conscientiousness from the Big Five Inventory (BFI)", "Total Number of Instagram Followers")
) %>% gt::gt()
```
 

__Preview__

The first six rows of the data are:

```{r echo=FALSE, message=FALSE}
read_csv('https://uoepsy.github.io/data/FOMOdataset.csv') %>% head %>% gt::gt()
```

`r optend()`

<div class="divider div-transparent div-dot"></div>

### Setup
`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read the FOMO dataset into R, assigning it to an object named `fomo`

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r, warning=FALSE, message=FALSE}
#Loading the required package(s)
library(tidyverse)
library(patchwork)
library(sjPlot)
library(kableExtra)
library(psych)

#Reading in fomo data and storing in object named 'fomo'
fomo <- read_csv("https://uoepsy.github.io/data/FOMOdataset.csv")

#check first six rows
head(fomo)
```

`r solend()`

<br>

### Provided Analysis Code

Below you will find the code required to conduct the analysis to address the research questions. This should look similar (in most areas) to what you worked through in lecture.

`r optbegin("Provided Analysis Code", olabel=FALSE,toggle=params$TOGGLE)`  

## Data Management

```{r message=FALSE, warning=FALSE}
library(tidyverse) # for all things!
library(psych) # good for descriptive stats
library(patchwork) # grouping plots together
library(kableExtra) # useful for creating nice tables
library(sjPlot) #regression tables & plots

fomo <- read_csv("https://uoepsy.github.io/data/FOMOdataset.csv")

# standardise FoMO & personality scores for RQ3
fomo <- 
  fomo %>% 
    mutate(
      FOMOz = (FOMO-mean(FOMO))/sd(FOMO),
      Oz = (O-mean(O))/sd(O),
      Cz = (C-mean(C))/sd(C),
      Ez = (E-mean(E))/sd(E),
      Az = (A-mean(A))/sd(A),      
      Nz = (N-mean(N))/sd(N))
#alternatively, you could do FOMOz = scale(FOMO, center = TRUE, scale = TRUE)
```


## Overall

```{r}
#######
#Descriptive Stats
#######


# the describe() function is from the psych package, and kable() from kableExtra which is used to make a nice table where the values are rounded to 2 decimal places using digits = 2. 
# We are first renaming our variables to give more appropriate / informative names.
#Next we are selecting columns 2, 3, 4, 8, and 9 from the describe output (n, mean, sd, min, max)

fomo %>% 
    select(FOMO, Age, TotalFollowers, N, E, O, C, A) %>%
    rename("Fear of Missing Out" = FOMO, "Age (in years)" = Age, "Number of Instagram Followers" = TotalFollowers, "Neuroticism" = N, "Extraversion" = E, "Openness" = O, "Conscientiousness" = C, "Agreeableness" = A) %>%
    describe() %>%
    select(2:4, 8:9) %>%
    rename("N" = n, "Mean" = mean, "SD" = sd, "Minimum" = min, "Maximum" = max) %>%
        kable(., caption = "FoMO, Socio-Demographic Factors, and Personality Traits Descriptive Statistics", digits = 2) %>%
        kable_styling()   

#from above, no missing values and scores within range (look at min and max values)


# scatterplot matrix, hist, and corr of FoMO, Socio-Demographic Factors, and Personality Traits Descriptive Statistics
pairs.panels(fomo %>%
    select(-FOMOz, -Oz, -Cz, -Ez, -Az, -Nz)) 
```

## RQ1 
```{r}
#######
#Descriptive Stats
#######

fomo %>% 
    select(FOMO, Age) %>%
    rename("Fear of Missing Out" = FOMO, "Age (in years)" = Age) %>%
    describe() %>%
    select(2:4, 8:9) %>%
    rename("N" = n, "Mean" = mean, "SD" = sd, "Minimum" = min, "Maximum" = max) %>%    
        kable(., caption = "FoMO and Age Descriptive Statistics", digits = 2) %>%
        kable_styling()    

# scatterplot
p1 <- ggplot(data = fomo, aes(x = Age, y = FOMO)) + 
    geom_point() + 
  geom_smooth(method = 'lm', se = FALSE, colour = 'red', linewidth=2) +
  labs(x = "(a) Age (in years)", y = "Fear of Missing Out")
p1

```

```{r}
#######
#Model Building
#######

fomo_mdl1 <- lm(FOMO ~ Age, data = fomo)
summary(fomo_mdl1)
confint(fomo_mdl1)

```

```{r}
#######
#Table for Results
#######

tab_model(fomo_mdl1,
          dv.labels = "FoMO",
          pred.labels = c("Age" = "Age (in years)"), 
          title = "RQ1: Regression Table for FoMO Model")
```


## RQ2
```{r}
#######
#Descriptive Stats
#######

fomo %>% 
    select(FOMO, Age, TotalFollowers) %>%
    rename("Fear of Missing Out" = FOMO, "Age (in years)" = Age, "Number of Instagram Followers" = TotalFollowers) %>%
    describe() %>%
    select(2:4, 8:9) %>%
    rename("N" = n, "Mean" = mean, "SD" = sd, "Minimum" = min, "Maximum" = max) %>%    
        kable(., caption = "FoMO and Socio-Demographic Factors Descriptive Statistics", digits = 2) %>%
        kable_styling()    


# scatterplots
p2 <- ggplot(data = fomo, aes(x = TotalFollowers, y = FOMO)) + 
    geom_point() + 
  geom_smooth(method = 'lm', se = FALSE, colour = 'purple', linewidth=2) +
  labs(x = "(b) Total Number of \nInstagram Followers", y = "Fear of Missing Out")

p2
```

```{r}
#######
#Model Building
#######

fomo_mdl2 <- lm(FOMO ~ Age + TotalFollowers, data = fomo)
summary(fomo_mdl2)
confint(fomo_mdl2)
```

```{r}
#######
#Model Comparison
#######

anova(fomo_mdl1 ,fomo_mdl2) %>%
    kable(caption = "Model Comparison - fomo_mdl1 vs fomo_mdl2", align = "c", digits = c(2,2,2,2,2,60)) %>%
    kable_styling(full_width = FALSE)

```

```{r}
#######
#Table for Results
#######

tab_model(fomo_mdl1 ,fomo_mdl2,
          dv.labels = c("FoMO","FoMO"),
          pred.labels = c("Age" = "Age (in years)",
                          "TotalFollowers" = "Number of Instagram Followers"), 
          title = "RQ2 - Regression Table for FoMO Model")
```

## RQ3 

```{r}
#######
#Descriptive Stats
#######

fomo %>% 
    select(FOMO, N, E, O, C, A) %>%
    rename("Fear of Missing Out" = FOMO, "Neuroticism" = N, "Extraversion" = E, "Openness" = O, "Conscientiousness" = C, "Agreeableness" = A) %>%
    describe() %>%
    select(2:4, 8:9) %>%
    rename("N" = n, "Mean" = mean, "SD" = sd, "Minimum" = min, "Maximum" = max) %>%    
        kable(., caption = "FoMO and Personality Descriptive Statistics", digits = 2) %>%
        kable_styling()  
```

```{r}
#######
#Model Building
#######

fomo_mdl3 <- lm(FOMOz ~ Nz + Ez + Oz + Cz + Az, data = fomo)
summary(fomo_mdl3)
confint(fomo_mdl3)
```

```{r}
#plot model examining significant personality predictors
N_plot <- plot_model(fomo_mdl3, type = "eff",
           terms = c("Nz"),
           show.data = TRUE,
           axis.title = c("Neuroticsm \n(z-scored)","FoMO Score (z-scored)"),
           title = "FoMO & N")

C_plot <- plot_model(fomo_mdl3, type = "eff",
           terms = c("Cz"),
           show.data = TRUE,
           axis.title = c("Conscientiousness \n(z-scored)","FoMO Score (z-scored)"),
           title = "FoMO & C")

A_plot <- plot_model(fomo_mdl3, type = "eff",
           terms = c("Az"),
           show.data = TRUE,
           axis.title = c("Agreeableness \n(z-scored)","FoMO Score (z-scored)"),
           title = "FoMO & A")

N_plot | C_plot | A_plot
```


```{r}
#create table for results - RQ3
tab_model(fomo_mdl3,
          dv.labels = "FoMO (Z-Scored)",
          pred.labels = c("Nz" = "Neuroticism (Z-Scored)",
                          "Ez" = "Extraversion (Z-Scored)",
                          "Oz" = "Openness (Z-Scored)",
                          "Az" = "Agreeableness (Z-Scored)",
                          "Cz" = "Conscientiousness (Z-Scored)"),
          title = "RQ3 - Regression Table for FoMO Model")
```

`r optend()`

### The 3-Act Structure

We need to present our report in three clear sections - think of your sections like the 3 key parts of a play or story - we need to (1) provide some background and scene setting for the reader, (2) present our results in the context of the research question, and (3) present a resolution to our story - relate our findings back to the question we were asked and provide our answer. 

#### Act I: Analysis Strategy

`r qbegin(1)`

Attempt to draft a discussion section based on the above research question and analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Analysis Strategy - What to Include***

Your analysis strategy will contain a number of different elements detailing plans and changes to your plan. Remember, your analysis strategy should **not** contain any results. You may wish to include the following sections:  

-  Very brief data and design description:
     - Give the reader some background on the context of your write-up. For example, you may wish to describe the data source, data collection strategy, study design, number of observational units.
     - Specify the variables of interest in relation to the research question, including their unit of measurement, the allowed range (for Likert scales), and how they are scored.

-  Data management:  
     - Describe any data cleaning and/or recoding.
     - Are there any observations that have been excluded based on pre-defined criteria? How/why, and how many? 
     - \* Describe any transformations performed to aid your interpretation (i.e., mean centering, standardisation, etc.)

-  Model specification:  
     -  Clearly state your hypotheses and specify your chosen significance level.
     -  What type of statistical analysis do you plan to use to answer the research question? (e.g., simple linear regression, multiple linear regression, binary logistic regression, etc.)
     - In some cases, you may wish to include some visualisations and descriptive tables to motivate your model specification. 
     -  Specify the model(s) to be fitted to answer your given research question and analysis structure. Clearly specify the response and explanatory variables included in your model(s).

:::frame
*** This is not yet a completed list -- we will add more to this as we progress through the course (e.g., coding of categorical variables, checking assumptions, diagnostics, etc.)
:::

As noted and encouraged throughout the course, one of the main benefits of using RMarkdown is the ability to include inline R code in your document. Try to incorporate this in your write up so you can automatically pull the specified values from your code. If you need a reminder on how to do this, see [Lesson 4 of the Rmd Bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

:::

`r optbegin("Example Write-Up of Analysis Strategy Section", olabel=FALSE, toggle = params$TOGGLE)`

The dataset contained information on 3370 participants, including scores on 5 personality traits assessed by the a reduced version of the 45 item Big Five Inventory (BFI; Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism), where 43 items were used. Participants were also assessed on the FoMO scale - a 10 item measure which assessed the extent to which individuals experienced apprehension regarding missing out on interesting events of others. Items on both the BFI and FoMO scales were measured on a 5-point scale, meaning that scores on the BFI could range 8-40 for Neuroticism, 8-40 for Extraversion, 10-50 for Openness to Experience, 8-40 for Agreeableness, and 9-45 for Conscientiousness; and scores on the FoMO scale could range 10-50. For both of these measures, only total scores were available. Two socio-demographic questions were also asked of all participants - their age (in years) and total number of Instagram followers. 

The aim of this report was to address three research questions:

1. Does age predict FOMO?
2. Does the total number of people one follows on Instagram predict FOMO over and above age?
3. Does personality predict FOMO?

To examine the associations among the variables of interest, a scatter plot of matrices (SPLOM) was used. This allowed for visual examination of marginal distributions via histograms, and bivariate associations between variables via scatter plots. The SPLOM also provided bivariate correlations between each of the variables. 

To address RQ1, the following simple linear regression model was used:

$$
\text{FoMO} = \beta_0 + \beta_1 \cdot Age + \epsilon
$$

where we tested whether there was a significant association between FoMO and age. Formally, this corresponded to testing whether the age coefficient was equal to zero:

$$
H_0: \beta_1 = 0
$$

$$
H_1: \beta_1 \neq 0
$$

To address RQ2, the two following models were used:
$$
\text{FoMO} = \beta_0 + \beta_1 \cdot Age + \epsilon
$$

$$
\text{FoMO} = \beta_0 + \beta_1 \cdot Age + \beta_2 \cdot Number~of~Instagram~Followers + \epsilon
$$

To determine whether the model with the number of Instagram followers and age was a significantly better fitting than the model with age alone, we used an incremental $F$-test to formally test the following hypothesis:

$$
H_0: \beta_2 = 0
$$

$$
H_1: \beta_2 \neq 0
$$

To address RQ3, the following multiple linear regression model was used, where all measures were $z$-scored:

$$
\text{FoMO} = \beta_0 + \beta_1 \cdot N + \beta_2 \cdot E + \beta_3 \cdot O + \beta_4 \cdot C + \beta_5 \cdot A + \epsilon
\quad \\  
$$
$$
\begin{align}
& \text{Where:} \\
& FoMO = \text{FoMO, z-scored} \\
& N = \text{Neuroticism, z-scored} \\
& E = \text{Extraversion, z-scored} \\
& O = \text{Openness, z-scored} \\
& C = \text{Conscientiousness, z-scored} \\
& A = \text{Agreeableness, z-scored} \\
\end{align}
$$

To investigate whether personality predicted FoMO, this corresponded to testing whether each of the Big 5 personality traits were equal to zero:

$$
H_0: \text{All}~~ \beta_j = 0 ~\text{(for j = 1, 2, 3, 4, 5)}
$$
$$ 
H_1: \text{At least one}~ \beta_j \neq \text{(for j = 1, 2, 3, 4, 5)}
$$

All participant data was complete (no missing values), with scores on the FoMO and BFI scales within possible ranges. Throughout the report, effects were considered statistically significant at $\alpha = .05$. 

`r optend()`

#### Act II: Results

`r qbegin(2)`

Attempt to draft a results section based on your detailed analysis strategy and the analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Results - What To Include***

The results section should follow from your analysis strategy. This is where you would present the evidence and results that will be used to answer the research questions and can support your conclusions. Make sure that you address all aspects of the approach you outlined in the analysis strategy. 

In this section, it is useful to include tables and plots to clearly present your findings to your reader. It is important, however, to carefully select what is the key information that should be presented. You don't want to overload the reader with unnecessary or duplicate information, and you also want to save space in case there is a page limit. Make use of figures with multiple panels where you can.

As a broad guideline, you want to start with the results of any exploratory data analysis, presenting tables of summary statistics and exploratory plots. You may also want to visualise associations between/among variables and report covariances or correlations. Then, you should move on to the results from your model.

:::frame
*** This is not yet a completed list -- we will add more to this as we progress through the course (e.g., evaluation of diagnostic plots, etc.)
:::

:::

`r optbegin("Example Write-Up of Results Section", olabel=FALSE, toggle = params$TOGGLE)`

Descriptive statistics are displayed in @tbl-desctab. 

```{r desctab, echo = FALSE}
#| label: tbl-desctab
#| tbl-cap: FoMO, Socio-Demographic Factors, and Personality Trait Descriptive Statistics
fomo %>% 
    select(FOMO, Age, TotalFollowers, N, E, O, C, A) %>%
    rename("Fear of Missing Out" = FOMO, "Age (in years)" = Age, "Number of Instagram Followers" = TotalFollowers, "Neuroticism" = N, "Extraversion" = E, "Openness" = O, "Conscientiousness" = C, "Agreeableness" = A) %>%
    describe() %>%
    select(2:4, 8:9) %>%
    rename("N" = n, "Mean" = mean, "SD" = sd, "Minimum" = min, "Maximum" = max) %>%    
        kable(., caption = "FoMO, Socio-Demographic Factors, and Personality Traits Descriptive Statistics", digits = 2) %>%
        kable_styling() 
```

Bivariate correlations showed a moderate negative association between FoMO and age; moderate positive associations between FoMO and Neuroticism and between FoMO and number of Instagram followers; and weak associations between FoMO and the remaining Big 5 personality variables (see @fig-sphcor). 

```{r sphcor, echo=FALSE, fig.cap="Bivariate scatter plots (below diagonal), histograms (diagonal), and Pearson correlation coefficient (above diagonal) for FoMO, Socio-Demographic Factors, and Personality Traits"}
#| label: fig-sphcor
#| fig-cap: Bivariate scatter plots (below diagonal), histograms (diagonal), and Pearson correlation coefficient (above diagonal) for FoMO, Socio-Demographic Factors, and Personality Traits
pairs.panels(fomo %>%
    select(- FOMOz, - Oz, - Cz, - Ez, - Az, - Nz)) 
```

```{r include=FALSE}
mdl1_sum <- summary(fomo_mdl1)
mdl2_sum <- summary(fomo_mdl2)
mdl3_sum <- summary(fomo_mdl3)
```

In relation to RQ1, full regression results, including 95\% Confidence Intervals, are shown in @tbl-rq1-results. This model was significant ($F(`r round(mdl1_sum$fstatistic['numdf'])`, `r round(mdl1_sum$fstatistic['dendf'])`)=`r round(mdl1_sum$fstatistic['value'], 2)`, p <.001)$, and results indicated that age was a significant predictor of FoMO $(\beta = `r round(mdl1_sum$coefficients['Age', 'Estimate'],2)`, CI_{95} = [`r round(confint(fomo_mdl1)['Age',], 2)`], SE = `r round(mdl1_sum$coefficients['Age','Std. Error'], 2)`, p <.001)$. Age explained approximately `r round(mdl1_sum$r.squared*100, 0)`% of the variance in FoMO scores. For every one-year increase in age, participants' FoMO scores decreased by 0.20 points. This suggested that older participants experienced less FoMO than younger participants (see @fig-rq1(a)).

```{r echo = FALSE}
#| label: tbl-rq1-results
#| tbl-cap: RQ1 - Regression Table for FoMO Model
tab_model(fomo_mdl1,
          dv.labels = "FoMO",
          pred.labels = c("Age" = "Age (in years)"), 
          title = "RQ1 - Regression Table for FoMO Model")
```

```{r echo = FALSE}
#| label: fig-rq1
#| fig-cap: "(a) Association between FoMO and Age (b) Association between FoMO and Instagram Followers"
p1 | p2
```

```{r include=FALSE}
RQ2test <- anova(fomo_mdl1, fomo_mdl2)
names(RQ2test)[6]<-"p"
```

As presented in @tbl-rq2-results, we found evidence that an individual's total number of followers on Instagram and age provided a significantly better fitting model predicting FoMO over age alone $(F(`r paste(RQ2test$Df[2])` ,`r paste(RQ2test$Res.Df[2])`) = `r round(RQ2test$F[2],2)`, p`r map_chr(RQ2test$p[2], ~ifelse(.<001,"<.001",paste0("=",round(.,2))))`)$. Together, age and the number of Instagram followers explained approximately `r round(mdl2_sum$adj.r.squared*100, 0)`% of the variance in FoMO scores.

```{r echo = FALSE}
#| label: tbl-rq2-results
#| tbl-cap: RQ2 - Model Comparison - fomo_mdl1 vs fomo_mdl2
anova(fomo_mdl1 ,fomo_mdl2) %>%
    kable(caption = "Model Comparison - fomo_mdl1 vs fomo_mdl2", align = "c", digits = c(2,2,2,2,2,60)) %>%
    kable_styling(full_width = FALSE)
```

In order to address the third research question, all scores were converted to $z$-scores. The Big 5 personality traits significantly predicted FoMO scores ($F(`r round(mdl3_sum$fstatistic['numdf'])`, `r round(mdl3_sum$fstatistic['dendf'])`) = `r round(mdl3_sum$fstatistic['value'], 2)`, p < .001$). The personality traits collectively explained approximately `r round(mdl3_sum$adj.r.squared*100, 0)`% of variance in FoMO scores. Full model results are presented in @tbl-rq3-results. For every 1 standard deviation unit increase in Neuroticism scores, FoMO scores increased by `r round(mdl3_sum$coefficients['Nz','Estimate'], 2)` standard deviations. For every 1 standard deviation unit increase in Conscientiousness scores, FoMO scores decreased by 0.31 standard deviations. For every 1 standard deviation unit increase in Agreeableness scores, FoMO scores decreased by 0.09 standard deviations. Neither Extraversion $(p = .27)$ nor Openness $(p = .47)$ were significant predictors of FoMO. 

```{r echo = FALSE}
#| label: tbl-rq3-results
#| tbl-cap: RQ3 - Regression Table for FoMO Model with Personality
tab_model(fomo_mdl3,
          dv.labels = "FoMO (Z-Scored)",
          pred.labels = c("Nz" = "Neuroticism (Z-Scored)",
                          "Ez" = "Extraversion (Z-Scored)",
                          "Oz" = "Openness (Z-Scored)",
                          "Az" = "Agreeableness (Z-Scored)",
                          "Cz" = "Conscientiousness (Z-Scored)"),
          title = "RQ3 - Regression Table for FoMO Model with Personality")
```

Associations between FoMO and the three significant personality traits (whilst holding the others constant) are shown in @fig-rq3. 

```{r echo = FALSE}
#| label: fig-rq3
#| fig-cap: "Predicted FoMO Scores by Neuroticsm, Conscientiousness, and Agreeableness after accounting for other Big 5 Traits"
N_plot | C_plot | A_plot
```

`r optend()`

#### Act III: Discussion  

`r qbegin(3)`

Attempt to draft a discussion section based on your results and the analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Discussion - What To Include

In the discussion section, you should summarise the key findings from the results section and provide the reader with a few take-home sentences drawing the analysis together and relating it back to the original question. 

The discussion should be relatively brief, and should not include any statistical analysis - instead think of the discussion as a conclusion, providing an answer to the research question(s).

:::

`r optbegin("Example Write-Up of Discussion Section", olabel=FALSE, toggle = params$TOGGLE)`

Previous research had identified an association between an individual’s fear of missing out, various socio-demographic factors, and personality. In this analysis, we examined the associations of FoMO with age, total number of Instagram followers, and personality.

Our results suggested that: (1) age was a significant predictor of FoMO, where older age was associated with lower levels of FoMO; (2) including the total number of Instagram followers as well as age resulted in a significantly better fitting model for explaining FoMO scores (where more followers were associated with higher levels of FoMO); and (3) three of the Big five personality traits - Neuroticism, Agreeableness, and Conscientiousness - were significant predictors of FoMO (specifically high Neuroticism, low Agreeableness, and low Conscientiousness). Therefore, we can reject the null hypotheses for each of our research questions. 

However, it is important to note that we can make no causal claims on the directions of these associations from these data - we cannot conclude that being a young person who is highly Neurotic and low on Agreeableness and Conscientiousness with a large number of Instagram followers will __cause__ feelings of FoMO.

`r optend()`

<br>

# Section B: Block 1 (Weeks 1 - 4) Recap

In the second part of the lab, there is no new content - the purpose of the recap section is for you to revisit and revise the concepts you have learned over the last 4 weeks. 

:::red

Before you expand each of the boxes below, think about how comfortable you feel with each concept.  

:::

`r optbegin("Types of Models: Deterministic vs Statistical", olabel=FALSE,toggle=params$TOGGLE)`

#### Deterministic (*Example: Perimeter & Side*)

The mathematical model 

$$
Perimeter = 4 \cdot Side
$$ 

or, equivalently, 
$$
y = 4 \cdot x
$$

represents the relationship between side and perimeter of squares. This is an example of a _deterministic model_ as it is a model of an *exact relationship* - there can be no deviation.

#### Statistical (*Example: Height & Handspan*)

The relationship between height and handspan shows deviations from the 'average pattern'. Hence, we need to create a model that allows for deviations from the linear relationship - we need a _statistical model_.

A statistical model includes *both* a deterministic function and a random error term:
$$
Handspan = \beta_0 + \beta_1 \cdot Height + \epsilon
$$

or, in short,

$$
y = \underbrace{\beta_0 + \beta_1 \cdot x}_{\text{function of }x} + \underbrace{\epsilon}_{\text{random error}}
$$

The deterministic function need not be linear if the scatterplot displays signs of non-linearity.

In the equation above, the terms $\beta_0$ and $\beta_1$ are numbers specifying where the line going through the data meets the y-axis and its slope (direction and gradient of line).

:::{.callout-note}

See [S1 Week 1 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_01_slr.html) and  [S1 Week 1 Lecture](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_01_introlm_lecture.html#1) for further details and to revise these concepts further.

:::

`r optend()`

`r optbegin("Null & Alternative Hypotheses", olabel=FALSE,toggle=params$TOGGLE)`

Recall that statistical hypotheses are testable mathematical statements.

We need to define a null ($H_0$) and alternative ($H_1$) hypothesis.

Points to note:

- We can only ever test the null ($H_0$), so all statements must be made in reference to this
- We can only ever reject or fail to reject the null (we can **never** accept a hypothesis)

:::{.callout-note}

See [S1 Week 2 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_02_mlr.html) and [S1 Week 3 Lecture](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_03_testingbeta.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Simple Linear Regression", olabel=FALSE,toggle=params$TOGGLE)`

**Formula:**  

$$
y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i
$$

In **R**:

:::blue

There are basically two pieces of information that we need to pass to the `lm()` function:

1. The formula: The regression formula should be specified in the form `y ~ x` where $y$ is the dependent variable (DV) and $x$ the independent variable (IV).
2. The data: Specify which dataframe contains the variables specified in the formula.

+ run simple linear regression via `lm()` function

```{r eval=FALSE}
model_name <- lm(DV ~ IV, data = data_name)
```

**OR**

```{r eval=FALSE}
model_name <- lm(data_name$DV ~ data_name$IV)
```

:::

:::{.callout-note}

See [S1 Week 1 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_01_slr.html),  [S1 Week 1 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_01_introlm_lecture.html#1), and [S1 Week 2 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_02_LM2.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Multiple Linear Regression", olabel=FALSE,toggle=params$TOGGLE)`

**Formula:**  

$$
y_i = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \epsilon_i
$$

In **R**:

:::blue

Multiple and simple linear regression follow the same structure within the `lm()` function. You simply add (using the `+` sign) more independent variables.

+ run multiple linear regression (example includes three independent variables) via `lm()` function

```{r eval=FALSE}
model_name <- lm(DV ~ IV1 + IV2 + IV3, data = data_name)
```

**OR**

```{r eval=FALSE}
model_name <- lm(data_name$DV ~ data_name$IV1 + data_name$IV2 + data_name$IV3)
```

:::

**Interpretation of Multiple Regression Coefficients**

You'll hear a lot of different ways that people explain multiple regression coefficients.  

For the model $y = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \epsilon$, the estimate $\hat \beta_1$ will often be reported as:  
  
the increase in $y$ for a one unit increase in $x_1$ when...

- holding the effect of $x_2$ constant.
- controlling for differences in $x_2$.
- partialling out the effects of $x_2$.
- holding $x_2$ equal. 
- accounting for effects of $x_2$. 

:::{.callout-note}

See [S1 Week 2 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_02_mlr.html), [S1 Week 3 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_03_mlr_stz.html), and [S1 Week 2 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_02_LM2.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Partitioning Variation: Sum of Squares", olabel=FALSE,toggle=params$TOGGLE)`

#### Sum of Squares

The sum of squares measures the deviation or variation of data points away from the mean (i.e., how spread out are the numbers in a given dataset). We are trying to find the equation/function that best fits our data by varying the least from our data points. 

##### Total Sum of Squares

**Formula**: 

$$
SS_{Total} = \sum_{i=1}^{n}(y_i - \bar{y})^2
$$

**In words**: 

Squared distance of each data point from the mean of $y$.

**Description**: 

How much variation there is in the DV.

##### Residual Sum of Squares

**Formula**: 

$$
SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
$$

**In words**: 

Squared distance of each point from the predicted value.

**Description**: 

How much of the variation in the DV the model did not explain - a measure that captures the unexplained variation in your regression model. Lower residual sum of squares suggests that your model fits the data well, and higher suggests that the model poorly explains the data (in other words, the lower the value, the better the regression model). If the value was zero here, it would suggest the model fits perfectly with no error.

##### Model Sum of Squares

**Formula**: 

$$
SS_{Model} = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2
$$

Can also be derived from:

$$
SS_{Model} = SS_{Total} - SS_{Residual}
$$
**In words**: 

The deviance of the predicted scores from the mean of $y$.

**Description**: 

How much of the variation in the DV your model explained - like a measure that captures how well the regression line fits your data.

:::{.callout-note}

See [S1 Week 1 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_01_introlm_lecture.html#1), and [S1 Week 3 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_03_testingbeta.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("F-test & F-ratio", olabel=FALSE,toggle=params$TOGGLE)`

**Formula**: 

$$
F_{(df_{model},~df_{residual})} = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\
\quad \\
$$

$$
\begin{align}
& \text{Where:} \\
& df_{model} = k \\
& df_{residual} = n-k-1 \\
& n = \text{sample size} \\
& k  = \text{number of explanatory variables} \\
\end{align}
$$

**Description**: 

To test the significance of an overall model, we can conduct an $F$-test. The $F$-test compares your model to a model containing zero predictor variables (i.e., the intercept only model), and tests whether your added predictor variables significantly improved the model.

It is called the $F$-ratio because it is the ratio of the how much of the variation is explained by the model (per paramater) versus how much of the variation is unexplained (per remaining degrees of freedom). 

The $F$-test involves testing the statistical significance of the $F$-ratio. 
Q: What does the $F$-ratio test?
A: The null hypothesis that all regression slopes in a model are zero (i.e., explain no variance in your outcome/DV).

:::blue

In **R**, at the bottom of the output of `summary(<modelname>)`,:

```{r mlroutputrf, echo=FALSE, fig.cap="Multiple regression output in R, summary.lm(). F statistic highlighted", fig.align = 'left'}
knitr::include_graphics("images/mlr_fstat.PNG")
```

:::

The $F$-ratio you see at the bottom of `summary(model)` is actually a comparison between two models: your model (with some explanatory variables in predicting $y$) and the *null model*. 

In regression, the null model can be thought of as the model in which all explanatory variables have zero regression coefficients. It is also referred to as the __intercept-only model__, because if all predictor variable coefficients are zero, then the only we are only estimating $y$ via an intercept (which will be the mean - $\bar y$). 

**Interpretation**: 

Alongside viewing the $F$-ratio, you can see the results from testing the null hypothesis that all of the coefficients are $0$ (the alternative hypothesis is that at least one coefficient is $\neq 0$. Under the null hypothesis that all coefficients = 0, the ratio of explained:unexplained variance should be approximately 1)

If your model predictors do explain some variance, the $F$-ratio will be significant, and you would reject the null, as this would suggest that your predictor variables included in your model improved the model fit (in comparison to the intercept only model).


*Points to note*: 

- The larger your $F$-ratio, the better your model
- The $F$-ratio will be close to 1 when the null is true (i.e., that all slopes are zero)

:::{.callout-note}

See [S1 Week 4 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_04_model_fit_comp.html) and [S1 Week 4 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_04_ftests#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("R-squared and Adjusted R-squared", olabel=FALSE,toggle=params$TOGGLE)`

$R^2$ represents the proportion of variance in $Y$ that is explained by the model.

The $R$-squared coefficient is defined as:
$$
R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}
$$

The Adjusted $R$-squared coefficient is defined as:
$$
\hat R^2 = 1 - \frac{(1 - R^2)(n-1)}{n-k-1}
\quad \\
$$

$$
\begin{align}
& \text{Where:} \\
& n = \text{sample size} \\
& k = \text{number of explanatory variables} \\
\end{align}
$$
We can see the Multiple and Adjusted $R$-squared in the `summary()` output of a model.

Points to note:

- Adjusted-$R^2$ adjusts for the number of terms in a model, and should be used when there are 2 or more predictors in the model
- Adjusted-$R^2$ should always be less than or equal to $R^2$

:::{.callout-note}

See [S1 Week 4 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_04_model_fit_comp.html) and [S1 Week 3  Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_03_testingbeta.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Standardisation", olabel=FALSE,toggle=params$TOGGLE)`

**$z$-score Formula:**

$$
z_x = \frac{x - \bar{x}}{s_x}, \qquad z_y = \frac{y - \bar{y}}{s_y}
$$

Recall that a standardized variable has mean of 0 and standard deviation of 1.

In **R**:

:::blue

```{r eval = FALSE}
#create z-scored variables
dataframe <- dataframe %>%
  mutate(
   z_variable = (variable - mean(variable)) / sd(variable)
    )
```

**OR**

```{r eval = FALSE}
#use scale function
model <- lm(scale(DV) ~ scale(IV), data = dataset)
```

:::

:::{.callout-note}

See [S1 Week 3 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_03_mlr_stz.html) and [S1 Week 2 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_02_LM2.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Nested vs Non-Nested Models", olabel=FALSE,toggle=params$TOGGLE)`

**Nested Models**

Consider that you have two regression models where Model 1 contains a subset of the predictors contained in the other Model 2 and is fitted to the same data. More simply, Model 2 contains all of the predictors included in Model 1, **plus** additional predictor(s). This means that Model 1 is *nested* within Model 2, or that Model 1 is a *submodel* of Model 2. These two terms, at least in this setting, are interchangeable - it might be easier to think of Model 1 as your null and Model 2 as your alternative.

**Non-Nested Models**

Consider that you have two regression models where Model 1 contains different variables to those contained in Model 2, where both models are fitted to the same data. More simply, Model 1 and Model 2 contain unique variables that are not shared. This means that Model 1 and Model 2 are **not** nested.

:::{.callout-note}

See [S1 Week 4 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_04_model_fit_comp.html) and [S1 Week 4 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_04_ftests#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Incremental F-test", olabel=FALSE,toggle=params$TOGGLE)`

If (*and only if*) two models are __nested__, can we compare them using an __incremental F-test__.  

This is a formal test of whether the additional predictors provide a better fitting model.  
Formally this is the test of:  

+ $H_0:$ coefficients for the added/omitted variables are all zero.

+ $H_1:$ at least one of the added/omitted variables has a coefficient that is not zero. 

The $F$-ratio for comparing the residual sums of squares between two models can be calculated as:

$$
F_{(df_R-df_F),~df_F} = \frac{(SSR_R-SSR_F)/(df_R-df_F)}{SSR_F / df_F} \\
\quad \\
$$
$$
\begin{align}
& \text{Where:} \\
\\
& SSR_R = \text{residual sums of squares for the restricted model} \\
& SSR_F = \text{residual sums of squares for the full model} \\
& df_R = \text{residual degrees of freedom from the restricted model} \\
& df_F = \text{residual degrees of freedom from the full model} \\
\end{align}
$$

:::{.callout-note}

See [S1 Week 4 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_04_model_fit_comp.html) and [S1 Week 4 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_04_ftests#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Comparing regression models with anova()", olabel=FALSE,toggle=params$TOGGLE)`

Remember that you want your models to be *parsimonious*, or in other words, only as complex as they need to be in order to describe the data well. This means that you need to be able to justify your model choice, and one way to do so is by comparing models via `anova()`. If your model with multiple IVs does not provide a significantly better fit to your data than a more simplistic model with less IVs, then the more simplistic model should be preferred. 

:::blue

In **R**, we can conduct an incremental $F$-test by constructing two linear regression models, and passing them to the `anova()` function: 

```{r eval = FALSE}
anova(model1, model2)
```

:::

If the $p$-value is sufficiently low (i.e., below your predetermined significance level - usually .05), then you would conclude that model 2 is significantly better fitting than model 1. If $p$ is not < .05, then you should favor the more simplistic model. 


:::{.callout-note}

See [S1 Week 4 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_04_model_fit_comp.html) and [S1 Week 4 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_04_ftests#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("AIC & BIC", olabel=FALSE,toggle=params$TOGGLE)`

AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) combine information about the sample size, the number of model parameters, and the residual sums of squares ($SS_{residual}$). Models do not *need* to be nested to be compared via AIC and BIC, __but__ they need to have been fit to the same dataset.  

For both of these fit indices, lower values are better, and both include a penalty for the number of predictors in the model (although BIC's penalty is harsher):

$$
\begin{align}
AIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + 2k \\
\end{align}
\quad \\
$$

$$
\begin{align}
BIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + k\,\text{ln}(n) \\
\end{align}
\quad \\
$$

$$
\begin{align}
& \text{Where:} \\
& SS_{residual} = \text{sum of squares residuals} \\
& n = \text{sample size} \\
& k = \text{number of explanatory variables} \\
& \text{ln} = \text{natural log function} 
\end{align}
$$

:::blue

In **R**, we can calculate AIC and BIC by using the `AIC()` and `BIC()` functions respectively.

:::

:::{.callout-note}

See [S1 Week 4 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_04_model_fit_comp.html) and [S1 Week 4 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_04_ftests#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Steps Involved in Modelling", olabel=FALSE,toggle=params$TOGGLE)`

You can think of the sequence of steps involved in statistical modeling as:  

$$
\text{Choose} \rightarrow \text{Fit} \rightarrow \text{Assess} \rightarrow \text{Use}
$$

:::frame
**A general rule**  
<br>
<center> Do not use (draw inferences or predictions from) a model before you have **assessed** whether the model satisfies the underlying assumptions</center>
:::

<br>
Throughout this block, we have completed the first three steps (**Choose**, **Fit**, and **Use**) in that we have:  

1. Explored/visualised our data and specified our model
2. Fitted the model in `R`  
3. Interpreted our parameter estimates

Please note that when conducting real analyses, it would be inappropriate to complete these steps without also **assessing** whether a regression model meets the assumptions. You will learn how to do this later in the course.   

`r optend()`
