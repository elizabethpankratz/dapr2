---
title: "Model Fit and Standardization"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---


```{r setup, include=FALSE}
source('assets/setup.R')
library(patchwork)
library(checkdown)

# knitr::opts_chunk$set(cache = TRUE)
set.seed(1)
```

:::lo
### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand the calculation and interpretation of the coefficient of determination.
1. Understand the calculation and interpretation of the F-test of model utility.
1. Understand how to standardize model coefficients and when this is appropriate to do.
1. Understand the relationship between the correlation coefficient and the regression slope.

### <i class="fa fa-check-square-o fa-2"></i> What You Need

* Attended and/or watched Week 3 lectures
* You will need to have completed [Lab 2]. 

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse**
* **patchwork**

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/riverview.csv) or read it in via this link https://uoepsy.github.io/data/riverview.csv. **Note**:  this is the same data as Lab 2.
:::

# Study Overview 

## Data recap

`r optbegin('Riverview data codebook', FALSE, show = TRUE, toggle = params$TOGGLE)`

__Description__

The riverview data come from @Lewis-Beck2015 and contain five attributes collected from a random sample of $n=32$ employees working for the city of Riverview, a hypothetical midwestern city in the US. The attributes include:

- `education`: Years of formal education
- `income`: Annual income (in thousands of U.S. dollars)
- `seniority`: Years of seniority
- `gender`: Employee's gender
- `male`: Dummy coded gender variable (0 = Female, 1 = Male)
- `party`: Political party affiliation


__Preview__

The first six rows of the data are:

```{r echo=FALSE}
library(tidyverse)
library(kableExtra)

riverview <- read_csv('https://uoepsy.github.io/data/riverview.csv')
kable(head(riverview), align='c') %>% kable_styling(full_width = FALSE)
```
`r optend()`


# Setup
`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read the riverview dataset into R assigning it to an object named `riverview`

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
#Loading the required package(s)
library(tidyverse)
library(kableExtra)

# Reading in data and storing to an object named 'riverview'
riverview <- read_csv('https://uoepsy.github.io/data/riverview.csv')
```
`r solend()`

# Exercises

`r qbegin(1)`
Fit a linear model to investigate how income varies with years of formal education.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
library(tidyverse)

riverview <- read_csv(file = "https://uoepsy.github.io/data/riverview.csv")
head(riverview)

mdl <- lm(income ~ 1 + education, data = riverview)
mdl
```

The fitted model is:
$$
\widehat{Income} = 11.32 + 2.65 \times Education
$$
`r solend()`

`r qbegin(2)`
What is the proportion of the total variability in incomes explained by the linear relationship with education level?

**Hint:** The question asks to compute the value of $R^2$.

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
The proportion of the total variability in incomes explained by the linear relationship with education level is given by R-squared.

**Option 1**

The R-squared coefficient is defined as:
$$
R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}
$$

In `R` we can write:
```{r}
riverview_fitted <- riverview %>%
  mutate(
    income_hat = predict(mdl),
    resid = income - income_hat
  )
head(riverview_fitted)

riverview_fitted %>%
  summarise(
    SSModel = sum( (income_hat - mean(income))^2 ),
    SSTotal = sum( (income - mean(income))^2 )
  ) %>%
  summarise(
    RSquared = SSModel / SSTotal
  )
```

---

**Option 2**

```{r}
summary(mdl)
```

The output of `summary()` displays the R-squared value in the following line:
```
Multiple R-squared:  0.6317
```

For the moment, ignore "Adjusted R-squared". We will come back to this later in the course.

---

**Option 3**

We can perform an **AN**alysis **O**f **VA**riance or, in short, ANOVA.
It simply means that we are examining/partitioning the total variability of a response variable.

The `anova()` function returns the sum of squares of interest in the column `Sum Sq`:
```{r}
mdl_anova <- anova(mdl)
mdl_anova
```

Consider the column `Sum Sq`. 
The entry corresponding to `education` gives $SS_{Model}$ = 4147.3, as education is the explanatory variable. The entry corresponding to `Residuals` gives $SS_{Residual}$ = 2418.2.

```{r}
# Because the column name Sum Sq has a space, we need to wrap it with backticks
SSModel <- mdl_anova$`Sum Sq`[1]
SSResidual <- mdl_anova$`Sum Sq`[2]
SSTotal <- SSModel + SSResidual

RSquared <- SSModel / SSTotal
RSquared
```

---

**Interpretation**

:::int
Approximately 63\% of the total variability in employee incomes is explained by the linear association with education level.
:::

`r solend()`


`r qbegin(3)`
Perform a model utility test at the 5\% significance level, by computing the F-statistic using its definition.

**Hint:** The relevant test-statistic is the F-statistic:

$$
\begin{split}
F = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model} / 1}{SS_{Residual} / (n-2)}
\end{split}
$$

which compares the amount of variation in the response explained by the model to the amount of variation left unexplained in the residuals.

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
df1 <- 1
df2 <- nrow(riverview) - 2
f_star <- qf(0.95, df1, df2)
f_star
```

```{r}
model_utility <- riverview_fitted %>%
  summarise(
    SSModel = sum( (income_hat - mean(income))^2 ),
    SSResid = sum( resid^2 ),
    MSModel = SSModel / 1,
    MSResid = SSResid / df2,
    FObs = MSModel / MSResid
  )
model_utility
```


:::int
We performed an F-test of model utility at the 5\% significance level, where $F(1,30) = 51.45$.

As the observed $F = 51.45$ is much larger than the critical value $F^* = 4.17$, we have strong evidence to reject the null hypothesis that the model is ineffective.
:::

<br>
Alternatively, we can compute the p-value:
```{r}
pvalue <- 1 - pf(model_utility$FObs, df1, df2)
pvalue
```
The value `5.562116e-08` simply means $5.56 \times 10^{-8}$, so it's a really small number.

:::int
We performed an F-test of model utility at the 5\% significance level, where $F(1,30) = 51.45, p<.001$.

The p-value (< .001) is much lower than the specified significance level, meaning that we have very strong evidence against the null hypothesis.
:::

`r solend()`


`r optbegin('Optional: Another formula for the F-test.', FALSE)`
With some algebra we can also show that:
$$
F = \frac{R^2 / 1}{(1 - R^2) / (n - 2) } = \frac{R^2 / df_{Model}}{(1 - R^2) / df_{Residual} }
$$

Proof:

$$
\begin{aligned}
F = \frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} 
= \frac{\frac{SS_{Model}}{SS_{Total}}}{\frac{SS_{Residual}}{SS_{Total}} \cdot \frac{1}{(n - 2)}} 
= \frac{R^2 / 1}{(1 - R^2) / (n - 2)}
\end{aligned}
$$
`r optend()`

`r qbegin(4)`
Look at the output of `summary(mdl)` and `anova(mdl)`.

For each output, identify the relevant information to conduct an F-test against the null hypothesis that the model is ineffective at predicting income using education level.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
summary(mdl)
```

The relevant row is the following:
```
F-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08
```

---

Instead, the `anova()` output is:
```{r}
anova(mdl)
```

And the relevant entries are:

- the `Df` column, which contains the degrees of freedom;
- `F value = 51.452`, which is the F-statistic;
- `Pr(>F) = 5.562e-08` = $5.562 \times 10^{-8}$, which is the p-value.


---

We might write up the test results as,

:::int
We performed an F-test for the overall significance of the regression, $F(1, 30) = 51.45, p < .001$.
The large F-statistic leads to a very small p-value ($<.001$), meaning that we have very strong evidence against the null hypothesis that the model is ineffective.

In other words, the data provide strong evidence that education is an effective predictor of income.
:::
`r solend()`


`r qbegin(5)`
Consider the `F value` output of `anova(mdl)` and the `t value` for education returned by `summary(mdl)`

```
F value = 51.452
t value = 7.173
```

Do you notice any relationship between the F-statistic for overall model utility and the t-statistic for $H_0: \beta_1 = 0$?
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
**In simple linear regression only**, the F-statistic for overall model significance is equal to the square of the t-statistic for $H_0: \beta_1 = 0$.

You can check that the squared t-statistic is equal, up to rounding error, to the F-statistic:
$$
t^2 = F \\
7.173^2 = 51.452
$$

`r optbegin('Optional: Equivalence of t-test for the slope and model utility F-test in SLR.', FALSE)`

Here we will show the equivalence of the F-test for model effectiveness and t-test for the slope.

Recall the formula of the sum of squares due to the model. We will rewrite it in an equivalent form below:
$$
\begin{aligned}
SS_{Model} &= \sum_i (\hat y_i - \bar y)^2 \\
&= \sum_i (\hat \beta_0 + \hat \beta_1 x_i - \bar y)^2 \\
&= \sum_i (\bar y - \hat \beta_1 \bar x + \hat \beta_1 x_i - \bar y)^2 \\
&= \sum_i (\hat \beta_1 (x_i - \bar x))^2 \\
&= \hat \beta_1^2 \sum_i (x_i - \bar x)^2
\end{aligned}
$$

The F-statistic is given by:
$$
\begin{aligned}
F = \frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} 
= \frac{\hat \beta_1^2 \sum_i (x_i - \bar x)^2}{\hat \sigma^2} 
= \frac{\hat \beta_1^2 }{\hat \sigma^2 / \sum_i (x_i - \bar x)^2}
\end{aligned}
$$

Now recall the formula of the t-statistic,
$$
t = \frac{\hat \beta_1}{SE(\hat \beta_1)} = \frac{\hat \beta_1}{\hat \sigma / \sqrt{\sum_i (x_i - \bar x)^2}}
$$

It is evident that the latter is obtained as the square root of the former.

`r optend()`

`r solend()`

## Back to regression coefficients

`r qbegin(6)`
Compute the average education level and the average income in the sample.

Use the `predict()` function to compute the predicted income for those with average education level.

What do you notice?
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
Sample means:
```{r}
stats <- riverview %>%
  summarise(
    mean_education = mean(education),
    mean_income = mean(income)
  )
stats
```

Prediction:
```{r}
query <- tibble(education = stats$mean_education)
query

predict(mdl, newdata = query)
```

<br>
The predicted average income for those having average education level is equal to the average income in the sample.
`r solend()`

`r qbegin(7)`
Let's formalise the previous question using symbols.
Consider the fitted model $\hat{y} = \hat \beta_0 + \hat \beta_1 x$.

What is the predicted response for an individual having an explanatory variable at the average level $\bar{x}$?

**Hint:** Substitute the formula of $\hat \beta_0$ into the equation of the fitted model.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
We require the prediction at $x = \bar{x}$, that is:
$$
\hat{y} = \hat \beta_0 + \hat \beta_1 \bar{x}
$$
Recall the formula for the fitted intercept: $\hat \beta_0 = \bar{y} - \hat \beta_1 \bar{x}$. Let's substitute it into the equation:

$$
\begin{split}
\hat{y} &= \hat \beta_0 + \hat \beta_1 \bar{x} \\
&= \bar{y} - \hat \beta_1 \bar{x} + \hat \beta_1 \bar{x} \\
&= \bar{y}
\end{split}
$$

This highlights an important property of the fitted regression line: it always passes through the point of averages $(\bar x, \bar y)$.

Intuitively, what would be your prediction $\hat y$ when $x$ equals $\bar{x}$? If you guessed $\bar{y}$ you're on track!

`r solend()`

<div class="boxy boxy-primary boxy-lightbulb">

## Binary predictors

Let's suppose that instead of having measured education in years, we had data instead on "Obtained College Degree: Yes/No". Our explanatory variable would be binary categorical (think back to our discussion of [types of data](https://uoepsy.github.io/dapr1/2122/labs/1_01_data_types.html#Types_of_data)).  

Let us pretend that everyone with >18 years of education has a college degree:
```{r}
#create new variable called 'degree', where those over age 18 are allocated to Yes condition, and those under 18 to No.
riverview <- 
  riverview %>%
    mutate(
      degree = ifelse(education > 18, "Yes", "No")
    )
```

We may then plot our relationship as a boxplot. If you want to see the individual points, you could always "jitter" them (right-hand plot below)
```{r}
ggplot(riverview, aes(x = degree, y = income)) + 
  geom_boxplot() +
ggplot(riverview, aes(x = degree, y = income)) + 
  geom_jitter(height=0, width=.05)
```



## Standardization

`r qbegin(8)`
Add to the riverview dataset two variables called `z_education` and `z_income` representing the standardized education and income variables, respectively.

**Without using R**, if you were to fit a linear regression model using the standardized response and standardized predictor, what would the intercept be?

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
riverview <- riverview %>%
  mutate(
    z_education = (education - mean(education)) / sd(education),
    z_income = (income - mean(income)) / sd(income)
  )
```

Check that they are standardized:
```{r}
riverview %>%
  summarise(
    M_z_education = mean(z_education), SD_z_education = sd(z_education), 
    M_z_income = mean(z_income), SD_z_income = sd(z_income)
  )
```

A standardized variable has mean 0 and standard deviation equal to 1.
We can substitute this in the formula of the estimated intercept:

$$
\hat \beta_0 = \bar{y} - \hat \beta_1 \bar{x} = 0 - \hat \beta_1 0 = 0
$$

The intercept of the linear model between the standardized variables is zero --- that is, the regression line passes through the origin of the graph. 

But wait... The origin is also the average of $z_x$ and the average of $z_y$ as they are standardized. In symbols $(0, 0) = (\bar{z}_x, \bar{z}_y)$. 

We expected the fitted line to pass through the point of averages --- in this case, the origin.
`r solend()`


`r qbegin(9)`
**Using R**, fit the regression model using the standardized response and explanatory variables.

What is the slope equal to?
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
Linear model for standardized variables:
```{r}
mdl_z <- lm(z_income ~ 1 + z_education, data = riverview)
summary(mdl_z)
```

<br>
Correlation coefficient of original variables:
```{r}
riverview %>%
  select(education, income) %>%
  cor()
```

<br>
The slope of the standardized variables is equal to the correlation between the original variables.
`r solend()`


`r qbegin(10)`
Interpret the slope of the standardized variables.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
:::int
For every standard deviation increase in education, income increases on average by 0.79 standard deviations.
:::
`r solend()`

# References

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
