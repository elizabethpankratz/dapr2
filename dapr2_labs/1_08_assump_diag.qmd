---
title: "Assumptions and Diagnostics"
link-citations: TRUE
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source('assets/setup.R')
library(tidyverse)
library(plotly)
library(patchwork)
library(pander)
library(car)
library(performance)
library(kableExtra)
set.seed(953)
```

:::lo

### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Be able to state the assumptions underlying a linear model
2. Specify the assumptions underlying a linear model with multiple predictors
3. Assess if a fitted model satisfies the assumptions of your model
4. Assess the effect of influential cases on linear model coefficients and overall model evaluations

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1. Be up to date with lectures
2. Have completed [Week 2](https://uoepsy.github.io/dapr2/2324/labs/1_02_mlr.html) lab exercises

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse** 
* **car**
* **see**
* **performance**
* **kableExtra**
* **sjPlot**

### <i class="fa fa-pencil-square-o" aria-hidden="true"></i> Presenting Results
All results should be presented following [APA guidelines](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf). If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the [rmd bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

The example write-up sections included as part of the solutions are **not perfect** - they instead should give you a good example of what information you should include and how to structure this. Note that you must **not** copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more [here](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct).

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/wellbeing_rural.csv) or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv 
:::

# Lab Overview

In the previous labs, we have fitted a number of regression models, including some with multiple predictors. In each case, we first specified the model, then visually explored the marginal distributions and associations among variables which would be used in the analysis. Finally, we fit the model, and began to examine the fit by studying what the various parameter estimates represented, and the spread of the residuals.

But before we draw inferences using our model estimates or use our model to make predictions, we need to be satisfied that our model meets a specific set of assumptions. If these assumptions are not satisfied, the results will not hold.

In this lab, we will check the assumptions of one of the multiple linear regression models that we have previously fitted in Block 1 using the 'mwdata' dataset (see [Week 2](https://uoepsy.github.io/dapr2/2324/labs/1_02_mlr.html)). 

# Study Overview {#sec-studyview}

> **Research Question** 
>
> Is there an association between wellbeing and time spent outdoors *after* taking into account the association between wellbeing and social interactions? 

`r optbegin("Wellbeing/Rurality data codebook.", olabel=FALSE, toggle=params$TOGGLE)`  

__Description__

From the Edinburgh & Lothians, 100 city/suburb residences and 100 rural residences were chosen at random and contacted to participate in the study. The Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), was used to measure mental health and well-being. 

Participants filled out a questionnaire including items concerning: estimated average number of hours spent outdoors each week, estimated average number of social interactions each week (whether on-line or in-person), whether a daily routine is followed (yes/no). For those respondents who had an activity tracker app or smart watch, they were asked to provide their average weekly number of steps.  
  
  
__Data Dictionary__

The data in `wellbeing_rural.csv` contain seven attributes collected from a random sample of $n=200$ hypothetical residents over Edinburgh & Lothians, and include: 

```{r echo=FALSE, message=FALSE, warning=FALSE}
mwdata  <- read_csv("https://uoepsy.github.io/data/wellbeing_rural.csv")
tibble(
variable = names(mwdata),
description = c("Age in years of respondent","Self report estimated number of hours per week spent outdoors ", "Self report estimated number of social interactions per week (both online and in-person)", "Binary 1=Yes/0=No response to the question 'Do you follow a daily routine throughout the week?'", "Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70", "Location of primary residence (City, Suburb, Rural)", "Average weekly number of steps in thousands (as given by activity tracker if available)")
) %>% gt::gt()
```
  
  
__Preview__

The first six rows of the data are:

```{r echo=FALSE, message=FALSE}
read_csv('https://uoepsy.github.io/data/wellbeing_rural.csv') %>% head %>% gt::gt()
```
  
`r optend()`

<div class="divider div-transparent div-dot"></div>

# Setup

`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read the wellbeing dataset into R, assigning it to an object named `mwdata`
4. Fit the following model:

$$
\text{Wellbeing} = \beta_0 + \beta_1 \cdot Social~Interactions + \beta_2 \cdot Outdoor~Time + \epsilon
$$

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

```{r message=FALSE}
#Loading the required package(s)
library(tidyverse)
library(car)
library(see)
library(performance)
library(kableExtra)
library(sjPlot)

# Reading in data and storing to an object named 'mwdata'
mwdata <- read_csv("https://uoepsy.github.io/data/wellbeing_rural.csv")

# wellbeing model
wb_mdl1 <- lm(wellbeing ~ outdoor_time + social_int, data = mwdata) 
```

`r solend()`

<div class="divider div-transparent div-dot"></div>

# Exercises

## Assumptions

`r qbegin(1)`

Let's start by using `check_model()` for our `wb_mdl1` model - we can refer to these plots as a guide as we work through the assumptions questions of the lab.

:::{.callout-note appearance="simple" collapse="true"}

These plots **cannot** be used in your reports - they are to be used as a guide only.

:::
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
check_model(wb_mdl1)
```

The `check_model()` function is a useful way to check the assumptions of models, as it also returns some useful notes to aid your interpretation. There does appear to be evidence that some assumptions may have been violated, **but** to be sure we need to check each assumption individually with plots that are more suitable for a statistics report. 

`r solend()`

<br>

`r qbegin(2)`

Check if the fitted model satisfies the linearity assumption for `wb_mdl1`. 

Write a sentence summarising whether or not you consider the assumption to have been met. Justify your answer with reference to the plots.

:::{.callout-tip appearance="simple" collapse="true"}

### Hint

How you check this assumption depends on the number of predictors in your model:

+ Single predictor: Use either residual vs fitted values plot (`plot(model, which = 1)`), and/or a scatterplot with loess lines
    + For interpretation - in the former, you want the residuals appear to be randomly scattered around zero, without showing any pattern with respect to the fitted values; in the latter, the loess line should closely follow the data. 

+ Multiple predictors: Use component-residual plots (also known as partial-residual plots) to check the assumption of linearity. 
    + For interpretation - you are looking for the pink line to follow a linear relationship (i.e., follow the blue line). 

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r, fig.width = 10, out.width = '85%'}
crPlots(wb_mdl1)
```

::: {.callout-important icon=false appearance="minimal"}

The smoother (the pink line) follows quite closely to a linear relationship (the dashed blue line), though there was some deviation. Overall, the evidence suggested that the linearity assumption was met.

:::

`r solend()`

<br>

`r qbegin(3)`

Check if the fitted model `wb_mdl1` satisfy the equal variance (homoscedasticity) assumption. 

Write a sentence summarising whether or not you consider the assumption to have been met. Justify your answer with reference to the plot.

:::{.callout-tip appearance="simple" collapse="true"}

### Hint

Use `residualPlots()` to plot residuals against the predictor. Since we are only interested in visually assessing our assumption checks, we can suppress the curvature test output by specifying `tests = FALSE`.

For interpretation - if the assumption is met, you should see a random scatter of $(x, y)$ points with constant mean and variance functions i.e., the vertical spread of the residuals should roughly be the same everywhere.

:::{.callout-hint collapse="true"}

**Quick Tip if plotting using `plot(model)`** 

As the residuals can be positive or negative, we can make it easier to assess equal spread by improving the 'resolution' of the points.

We can make all residuals positive by discarding the sign (take the absolute value), and then take the square root to make them closer to each other.

A plot of $\sqrt{|\text{Standardized residuals}|}$ against the fitted values can be obtained via `plot(model, which = 3)`.

:::
 
:::

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

We can visually assess by plotting the Pearson residuals against the fitted values:

```{r, fig.width = 8, out.width = '90%'}
residualPlots(wb_mdl1, tests = FALSE)
```

Or by plotting the $\sqrt{|\text{Standardized residuals}|}$ against the fitted values:

```{r}
plot(wb_mdl1, which = 3)
```

::: {.callout-important icon=false appearance="minimal"}

Partial residual plots did show non-linear trends between residuals and predictors, hence there is evidence of non-constant variance i.e., heteroscedasticity. Thus, the data did not meet the assumption of equal variance, as the spread of the standardized residuals did not appear to be constant (for the most part) as the fitted values varied.

In the second plot, all points are above 0, but the majority of the points are not very close to each other. The line does not appear to be relatively flat, and so this also suggested that the error variance does change across the fitted values.

:::

`r solend()`

<br> 

`r qbegin(4)`

Assess whether there is autocorrelation in the error terms.
  
Write a sentence summarising whether or not you consider the assumption of independence to have been met (you may have to assume certain aspects of the study design).  

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

::: {.callout-important icon=false appearance="minimal"}

Since our data were collected from a between-persons study design, we can assume (i.e., based on design, we believe) the errors to be independent.
:::

`r solend()`

<br>

`r qbegin(5)`

Check if the fitted model `wb_mdl1` satisfies the normality assumption. 
  
Write a sentence summarising whether or not you consider the assumption to have been met. Justify your answer with reference to the plots. 

:::{.callout-tip appearance="simple" collapse="true"}

### Hint

We can get the QQplot from `plot(model, which = 2)`, and you can plot the frequency distribution of the residuals via `ggplot(data = ???, aes(x = model$residuals)) + geom_histogram()`. 

For interpretation - remember that departures from a linear trend in QQ plots indicate a lack of normality.
 
:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

:::{.panel-tabset}

### Histogram

```{r}
ggplot(data = mwdata, aes(x = wb_mdl1$residuals)) +
    geom_histogram() 
```

::: {.callout-important icon=false appearance="minimal"}

The histogram indicated that the residuals (the differences between observed and predicted values) followed close to a normal distribution. 

:::

### QQ Plot

```{r}
plot(wb_mdl1, which = 2)
```

::: {.callout-important icon=false appearance="minimal"}

The QQplot indicated that the residuals followed close to a normal distribution, as the points followed a linear pattern and there was no substantial skew or departure from normality. There was some evidence of heavier tails, and we may want to examine some observations more closely (i.e., 16, 78, 109).

:::

:::

`r solend()`

<br>

## Multicollinearity 

`r qbegin(6)`

For `wb_mdl1`, calculate the variance inflation factor (VIF) for the predictors in the model.  

Write a sentence summarising whether or not you consider multicollinearity to be a problem here.  

:::{.callout-tip appearance="simple" collapse="true"}

### Hint

We can loosely interpret VIF values >5 as indicative of moderate multicollinearity, and values >10 as indicative of severe multicollinearity.

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
vif(wb_mdl1)
```

::: {.callout-important icon=false appearance="minimal"}

The VIF values for all predictors are <5, indicating that multicollinearity is not adversely affecting model estimates. 

:::

`r solend()`

## Diagnostics 

`r qbegin(7)`

Create a new tibble which contains:  

1. The original variables from the model (Hint, what does `wb_mdl1$model` give you?)
2. The fitted values from the model $\hat y$  
3. The residuals $\hat \epsilon$
4. The studentised residuals
5. The hat values
6. The Cook's Distance values

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
mdl_diagnost <- 
  tibble(
  wb_mdl1$model,
  fitted = fitted(wb_mdl1),
  resid = residuals(wb_mdl1),
  studres = rstudent(wb_mdl1),
  hats = hatvalues(wb_mdl1),
  cooksd = cooks.distance(wb_mdl1)
)
```

`r solend()`

<br>

`r qbegin(8)`

From the tibble above, comment on the following:

* Looking at the studentised residuals, are there any outliers? 
* Looking at the hat values, are there any observations with high leverage? 
* Looking at the Cook's Distance values, are there any highly influential points?  

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

::: {.panel-tabset}

## Outliers

In a standard normal distribution, 95% of the values are roughly between -2 and 2. Because of this, studentised residuals of $>2$ or $< -2$ indicate potential outlyingness. 

We can ask `R` how many of the *absolute* values (by specifying `abs()`) are $>2$:
```{r}
table(abs(mdl_diagnost$studres) > 2)
```

We have 11 `TRUE` observations, which tells us that they have |studentised residuals| $>2$. 

We can identify which of our observations have these values:

```{r}
which(abs(mdl_diagnost$studres) > 2)
```

So we know that observations (or rows) 16, 50, 53, 58, 62, 76, 78, 109, 126, 151, and 163 have absolute values that have studentised residuals of $>2$ or $< -2$.

We could also *filter* our newly created tibble to these observations to examine the values further:

```{r}
mwdata %>%
    mutate(
    studres = rstudent(wb_mdl1)) %>%
  dplyr::filter(., studres > 2 | studres < -2) %>%
  arrange(., desc(studres)) %>%
  kable(.)  %>%   
    kable_styling(., full_width = F)
```

::: {.callout-important icon=false appearance="minimal"}

There were `r sum(abs(mdl_diagnost$studres) > 2)` observations identified as potential outliers.

:::

## High Leverage

Hat values of more than $2 \bar{h}$ (2 times the average hat value) are considered high leverage. The average hat value, $\bar{h}$ is calculated as $\frac{k + 1}{n}$, where $k$ is the number of predictors, and $n$ is the sample size.

For our model:
$$
\bar h = \frac{k+1}{n} = \frac{2+1}{200} = \frac{3}{200} = 0.015
$$
We can ask whether any of observations have hat values which are greater than $2 \bar h$:

```{r}
table(mdl_diagnost$hats > (2*0.015))
```

We have 16 `TRUE` observations, which tells us that they have high leverage.

We can identify which of our observations have these values:

```{r}
which(mdl_diagnost$hats > (2*0.015))
```

So we know that observations (or rows) 25, 56, 59, 60, 72, 73, 75, 79, 127, 131, 149, 159, 165, 169, 176, and 197 have hat values which are greater than $2 \bar h$.

We could also *filter* our newly created tibble to these observations to examine the values further:

```{r}
mwdata %>%
    mutate(
    hats = hatvalues(wb_mdl1)) %>%
  dplyr::filter(., hats > (2*0.015)) %>%
  arrange(., desc(hats)) %>%
  kable(.)  %>%   
    kable_styling(., full_width = F)
```

::: {.callout-important icon=false appearance="minimal"}

There were `r sum(mdl_diagnost$hats > (2*0.015))` observations that had high leverage (> $2 \bar h$).  

:::

## Influential Points

We are using a Cook's Distance cut-off of $\frac{4}{n-k-1}$, where $k$ is the number of predictors, and $n$ is the sample size.  

For our model:
$$
D_{cutoff} = \frac{4}{n-k-1} = \frac{4}{200 - 2 - 1} = \frac{4}{197} = 0.020
$$

We can ask whether any of observations have a high influence on our model estimates:

```{r}
table(mdl_diagnost$cooksd > 0.020)
```

Yes, we have 11 `TRUE` observations, which tells us that they are above the $D_{cutoff} = 0.020$.

We can identify which of our observations have these values:

```{r}
which(mdl_diagnost$cooksd > 0.020)
```

So we know that observations (or rows) 16, 53, 58, 76, 78, 109, 125, 126, 149, 151, and 169 have $D > 0.020$. 

We could also *filter* our newly created tibble to these observations to examine the values further:

```{r}
mwdata %>%
    mutate(
    cooksd = cooks.distance(wb_mdl1)) %>%
  dplyr::filter(., cooksd > 4/(200-2-1)) %>%
  arrange(., desc(cooksd)) %>%
  kable(.)  %>%   
    kable_styling(., full_width = F)
```

You can also display the Cook's Distance values themselves using `plot(model, which = 4)`, and add a horizontal line at the $D_{cutoff} = 0.020$ using `abline(h = ???)`:

```{r}
plot(wb_mdl1, which = 4, abline(h=0.020, col="blue"))
```

::: {.callout-important icon=false appearance="minimal"}

There were `r sum(mdl_diagnost$cooksd > 0.020)` observations that had a high influence on our model estimates.

:::

:::

`r solend()`

<br>

`r qbegin(9)`

Use the function `influence.measures()` to extract these delete-1 measures^[leave-one-out deletion diagnostics] of influence.  

Choose a couple of these measures to focus on, exploring in more detail (you may want to examine values or even try plotting distributions). 

:::{.callout-tip appearance="simple" collapse="true"}

### Hint

The function `influence.measures()` returns an `infl`-type object. To plot this, we need to find a way to extract the actual numbers from it.  
What do you think `names(influence.measures(wb_mdl1))` shows you? How can we use `influence.measures(wb_mdl1)$<insert name here>` to extract the matrix of numbers?  

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

::: {.panel-tabset}

## Extracting `influence.measures()`

```{r}
#extract measures
inf_mes <- influence.measures(wb_mdl1)

#examine top ten rows, and round to 3 decimal places
round(inf_mes$infmat[1:10,], 3)

```


## Examine `DFBETA` values

DFbeta represents the difference in the beta coefficients when a case is excluded from the model versus when it's included. A large DFbeta value would suggest that a case has a substantial impact on the estimated coefficients, and thus a high influence on the model results; a small DFbeta value would suggest that the case has less influence on the estimated coefficients.

A commonly used cut-off or threshold to compare $|DFBETA|$ values (absolute values) against is $\frac{2}{\sqrt{n}}$ (see Belsley et al., (1980) p. 28 for more info)^[Belsley, D. A., Kuh, E., & Welsch, R. E. (2005). Regression diagnostics: Identifying influential data and sources of collinearity. John Wiley & Sons. DOI: 10.1002/0471725153]. 

For our model:

$$
|DFBETA_{cutoff}| \quad = \quad \frac{2}{\sqrt{n}} \quad = \quad  \frac{2}{\sqrt{200}}  = 0.141
$$

In order to extract these in order to arrange in descending order, we need to save our delete-1 measures of influence as a dataframe (via `as.data.frame()`). Then we can then arrange our DFBETA values in descending order (via `arrange(desc(???))`). To avoid returning 200 rows of output (i.e., the length of the dataframe), we can ask for the first 15 rows via (`head(., 15)`):

```{r}
#save as a dataframe
inf_mes1 <- as.data.frame(inf_mes$infmat)

#arrange dfbeta values in descending order using the absolute value, and show first 10 rows
inf_mes1 %>%
    arrange(desc(abs(dfb.1_))) %>%
    head(., 15)
```

We can see that we have `r sum(abs(inf_mes1$dfb.1_) > (2/sqrt(200)))` $|DFBETA|$ values > $\frac{2}{\sqrt{200}}$, from observations (or rows) 16, 53, 56, 75, 76, 85, 101, 109, 149, 173, and 179 that we may want to examine further:

```{r}
which(abs(inf_mes1$dfb.1_) > (2/sqrt(200)))
```

## Plotting COVRATIO statistics

Values which are $>1+\frac{3(k+1)}{n}$ or $<1-\frac{3(k+1)}{n}$ are considered as having strong influence.  

For our model, this is:
$$
1 \pm \frac{3(k+1)}{n} \quad = \quad 1 \pm\frac{3(2+1)}{200} \quad = \quad 1\pm \frac{9}{200} \quad = \quad 1\pm0.045
$$

The "infmat" bit of an `infl`-type object contains the numbers, as we can see from out output above. To use it with `ggplot()`, we will need to turn it into a dataframe (`as.data.frame()`), or a tibble (`as_tibble()`):   

```{r message=FALSE, warning=FALSE}
infdata <- inf_mes$infmat %>%
  as_tibble()
```

Now we can build our plot. It would be useful to add vertical lines at the values $\quad 1\pm0.045$. To do so, we can use the `geom_vline()` function: 

```{r message=FALSE, warning=FALSE}
ggplot(data = infdata, aes(x = cov.r)) + 
  geom_histogram() +
  geom_vline(aes(xintercept = c(1-0.045), col = "blue")) +
  geom_vline(aes(xintercept = c(1+0.045), col = "red")) + 
  theme(legend.position = "none")  #remove legend
```

It looks like a few observations may be having quite a strong influence on the standard errors here. We can check specifically how many observations are potentially having a having strong influence using the cut off $1\pm0.045$:

```{r}
table(infdata$cov.r < 1 - 0.045 | infdata$cov.r > 1 + 0.045)
```

We can identify these 15 observations to investigate further:

```{r}
which(infdata$cov.r < 1 - 0.045 | infdata$cov.r > 1 + 0.045)
```

We know that observations (or rows) 16, 25, 50, 58, 62, 72, 73, 78, 79, 109, 127, 151, 159, 165, and 176 have $\text{COVRATIO  }  1\pm0.045$. 

:::

`r solend()`

<br>

`r qbegin(10)`

What approaches would be appropriate to take given the issues highlighted above with the violations of assumptions and case diagnostic results?

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

There are lots of different options available, and there is no one right answer. Assuming that we have no issues with model specification (i.e., are not missing variables, have modeled appropriately), then we may want to consider one of the below approaches (note: this is **not** an exhaustive list!)

::: {.panel-tabset}

## Investigate Observations

The first step is to re-examine your data. It is important to be familiar with your dataset, as you need to know what values are typical, normal, and possible. Could it be the case that you have missed some impossible values (e.g., a negative value of a persons height), values outwith the possible range (e.g., a score of 55 on a survey where scores can only range 10-50), values that don't make any sense (e.g., an age of 200), or maybe there are even typos / data entry errors (e.g., forgetting to put a decimal point, so having a height of 152m instead of 1.52m)! 

If there is a simple error in the data, it could be that you can fix the typo. If that is not possible (maybe you didn't collect the data, so are unsure of what the value(s) should/could be), you will need to delete the value (i.e., set as an `NA`), because you know that it is incorrect. 

We should aim to never change a legitimate value where possible (and remember that if you have a large dataset, a small number of extreme values will be unlikely to have a strong influence on your results). 

If there is an extreme, but legitimate value that you have determined is adversely influencing your model (i.e., by examining the assumptions and diagnostics as outlined above), you may want to consider ways to reduce this influence (e.g., **winsorizing** -  which essentially truncates or caps the identified extreme values to a specified percentile, in turn reducing their influence on the model without completely eliminating the observation(s). For example, you could replace values below the 5th percentile with the 5th percentile value, and values above the 95th percentile with the 95th percentile value).

If after re-examining your data you cannot identify any atypical, non-normal, or impossible values, you may need to select a different approach as outlined below. 

## Sensitivity Analysis

This allows us to assess the **sensitivity** of our results (i.e., parameter estimates, p-values, confidence intervals) to changes in our modelling approach (i.e., the removal of observations).

We can re-fit our model after excluding our identified outliers and potentially influential observations, and compare these results to the original model.

::: {.callout-important icon=false collapse=true}

# Process of Removing Observations

The current example involves removing all identified outliers and potentially influential observations at the same time. Ideally, and to ensure a more thorough sensitivity analysis, you would remove each of these observations one at a time, assess the effects on the model by comparing to your original, reassessing the remaining pre-identified observations, and repeating the process if necessary. 

:::

::: {.panel-tabset}

### Original Model

```{r}
## wellbeing model
wb_mdl1 <- lm(wellbeing ~ outdoor_time + social_int, data = mwdata) 
summary(wb_mdl1)
```


### Model with Observations Removed

```{r}
## wellbeing model
wb_mdl2 <- lm(wellbeing ~ outdoor_time + social_int, data = mwdata[-c(16, 25, 50, 53, 56, 58, 59, 60, 62, 72, 73, 75, 76, 78, 79, 85, 101, 109, 125, 126, 127, 131, 149, 151, 159, 163, 165, 169, 173, 176, 179, 197), ])
summary(wb_mdl2)
```


### Compare `summary()` output

```{r}
tab_model(wb_mdl1, wb_mdl2,
          dv.labels = c("Wellbeing (WEMWBS Scores)", "Wellbeing (WEMWBS Scores)"),
          pred.labels = c("outdoor_time" = "Outdoor Time (hours per week)",
                          "social_int" = "Social Interactions (number per week)"),
          title = "Regression Table for Wellbeing Models wb1 and wb2")
```

:::

::: {.callout-important icon=false appearance="minimal"}

We conducted a sensitivity analysis to assess how robust our conclusions were regarding outdoor time and the weekly number of social interactions in the presence of previously identified outliers and potentially influential observations. We re-fit the model, excluding these 28 observations (14% of our original sample), and compared these model results (`wb_mdl2`) to those of our original model (`wb_mdl1`). 

There was little difference in the estimates from `wb_mdl1` and `wb_mdl2`, and so we can conclude that after conducting a sensitivity analysis, there were no meaningful differences in our results, and hence our conclusions from our original model hold. Specifically:

+ The direction of all model estimates are the same in `wb_mdl1` and `wb_mdl2` (i.e., all positive)
+ There is no difference in statistical significance, and the p-values were of a similar magnitude (i.e., all < .001)
+ The estimate and confidence intervals for `outdoor_time` are very similar
+ There are some quantitative differences in the estimate and confidence intervals for `social_int`. The estimate differs slightly in magnitude by 0.07), but given that this remains positive and significant, we do not need to be too concerned about this. 

:::

## Bootstrapping

The bootstrap method is an alternative non-parametric method of constructing a standard error. Instead of having to rely on calculating the standard error with a formula and potentially applying fancy mathematical corrections, bootstrapping involves mimicking the idea of “repeatedly sampling from the population”. It does so by repeatedly **re**sampling with **replacement** from our original sample.

What this means is that we don’t have to rely on any assumptions about our model residuals, because we actually generate an actual distribution that we can take as an approximation of our sampling distribution, meaning that we can actually look at where 95% of the distribution falls, without having to rely on any summing of squared deviations.

Note, the bootstrap may provide us with an alternative way of conducting inference, but our model may still be mis-specified. It is also very important to remember that bootstrapping is entirely reliant on utilising our original sample to pretend that it is a population (and mimic sampling from that population). If our original sample is not representative of the population that we’re interested in, bootstrapping doesn’t help us at all.

More on how bootstrapping works next week!

## OLS vs WLS Regression

The method of ordinary least squares regression (OLS: i.e., the type of regression model you have been fitting on the course) assumes that there is constant variance in the errors (*homoscedasticity*). The method of weighted least squares (WLS) can be used when the ordinary least squares assumption of constant variance in the errors is violated (i.e., you have evidence of *heteroscedasticity*, like we do in Q3 of this lab).

If we have some specific belief that your non-constant variance is due to differences in the variances of the outcome between various groups, then it might be better to use Weighted Least Squares.   

As an example, imagine we are looking at weight of different dog breeds (@fig-dogweight). The weights of chihuahuas are all quite close together (between 2 to 5kg), but the weight of, for example, spaniels is anywhere from 8 to 25kg - a much bigger variance. 

```{r}
#| echo: false
#| label: fig-dogweight
#| fig-cap: "The weights of 49 dogs, of 7 breeds"
set.seed(4)
df <- tibble(
  breed = c("chihuahua","boxer","pug","spaniel","golden retriever","beagle","lurcher"),
  m = c(8,70,18,24,70,30,45),
  sd = c(2,10,6,24,15,8,5)
  ) |>
  mutate(
    weight = map2(m,sd, ~rnorm(7,.x,.y)*0.454)
  ) |> select(breed,weight) |> unnest(cols = c(weight))
df$breed = factor(df$breed, levels=c("beagle","pug","spaniel","chihuahua","boxer","golden retriever","lurcher"))
dogdf<-df
#save(dogdf, file="../../uoepsy/data/dogweight.RData")


df_ms = df |> group_by(breed) |> summarise(m = round(mean(weight),1))
ggplot(df, aes(x=breed,y=weight))+
  geom_boxplot(fatten = NULL) + 
  stat_summary(geom="segment", 
               mapping=aes(xend=..x..-.375,yend=..y..,col=breed)) +
  stat_summary(geom="segment", 
               mapping=aes(xend=..x..+.375,yend=..y..,col=breed)) + 
  geom_label(data=df_ms,aes(label=m,y=m,col=breed))+
  guides(col="none") +
  labs(x="Dog Breed",y="Weight (kg)")

```

Recall that the default way that `lm()` deals with categorical predictors such as `dog breed`, is to compare each one to a reference level. In this case, that reference level is "beagle" (first in the alphabet). Looking at @fig-dogweight above, which comparison do you feel more confident in? 

- **A:** Beagles (14kg) vs Pugs (9.1kg). A difference of 4.9kg.  
- **B:** Beagles (14kg) vs Spaniels (19kg). A difference of 5kg.  

Hopefully, your intuition is that **A** looks like a clearer difference than **B** because there's less overlap between Beagles and Pugs than between Beagles and Spaniels. Our standard linear model, however, assumes the standard errors are identical for each comparison:  
```{r}
#| echo: false
dogmodel <- lm(weight~breed, dogdf)
.pp(summary(dogmodel),l = list(3,9:18))
```

Furthermore, we can see that we have heteroscedasticity in our residuals - the variance is not constant across the model:  
```{r}
#| fig-height: 3
plot(dogmodel, which=3)
```

Weighted least squares is a method that allows us to apply weights to each observation, where the size of the weight indicates the precision of the information contained in that observation.  

We can, in our dog-breeds example, allocate different weights to each breed. Accordingly, the Chihuahuas are given higher weights (and so Chihuahua comparisons result in a smaller SE), and Spaniels and Retrievers are given lower weights. 

```{r}
#| eval: false
library(nlme)
load(url("https://uoepsy.github.io/data/dogweight.RData"))
dogmod_wls = gls(weight ~ breed, data = dogdf, 
                 weights = varIdent(form = ~ 1 | breed))
summary(dogmod_wls)
```

```{r}
#| echo: false
library(nlme)
dogmod_wls = gls(weight ~ breed, data = dogdf, weights = varIdent(form = ~ 1 | breed))
.pp(summary(dogmod_wls),l = list(16:25))
```

We _can_ also apply weights that change according to continuous predictors (e.g. observations with a smaller value of $x$ are given more weight than observations with larger values). 

## Data Transformations

A data transformation involves the replacement of a variable (e.g., $y$) by a function of that variable in order to change the shape of a distribution or association (e.g., to help reduce skew). We can transform the outcome variable prior to fitting the model, using something such as `log(y)` or `sqrt(y)`. This will sometimes allow us to estimate a model for which our assumptions are satisfied.

Some of the most common (not an exhaustive list) transformations are:

+ **Log (`log(y)`)**: Often used for reducing right skewness. Note, this transformation cannot be applied to zero or negative values (make sure to check your data!)
+ **Square root (`sqrt(y)`)**: Also often used for reducing right skewness. This transformation can be applied to zero values (but not negative), and is commonly applied to count data

```{r}
#| echo: false
#| label: fig-trouble1
#| fig-cap: "A model of a transformed outcome variable can sometimes avoid violations of assumptions that arise when modeling the outcome variable directly. Data from https://uoepsy.github.io/data/trouble1.csv"

# set.seed(4)
# df <- tibble(
#   x = runif(100,1,100),
#   ly = x*.01 + rnorm(100),
#   y = exp(ly)
# )
# write_csv(df %>% select(x,y), file="../../uoepsy/data/trouble1.csv")
df <- read_csv("https://uoepsy.github.io/data/trouble1.csv") |> mutate(ly=log(y))
par(mfrow=c(2,2))
hist(df$y, breaks=30, main="marginal distribution of y",xlab="y")
with(df,plot(y~x, main="lm(y ~ x)\nassumptions violated",xlab="x",ylab="y"))
abline(lm(y~x,df),col="red")
hist(df$ly,  breaks=30, main="marginal distribution of log(y)",xlab="log(y)")
with(df,plot(ly~x, main="lm(log(y) ~ x)\nassumptions satisfied",xlab="x",ylab="log(y)"))
abline(lm(ly~x,df),col="red")
```

The major downside of this is that we are no longer modelling $y$, but some transformation $f(y)$ ($y$ with some function $f$ applied to it). Interpretation of the coefficients changes accordingly, such that we are no longer talking in terms of changes in y, but changes in $f(y)$. When the transformation function used is non-linear (see the Right-Hand of @fig-logtr) a change in $f(y)$ is **not the same** for every $y$. 

```{r}
#| label: fig-logtr
#| fig-cap: "The log transformation is non-linear"
#| fig-height: 3.5
#| echo: false
par(mfrow=c(1,2))
with(df,plot(y,2*y,main="y and 2*(y)",xlab="y",ylab="2(y)"))
with(df,plot(y,ly,main="y and log(y)",xlab="y",ylab="log(y)"))
```

For certain transformations, we _can_ re-express coefficients to be interpretable with respect to $y$ itself. For instance, the model using a log transform $ln(y) = b_0 + b_1(x)$ gives us a coefficient that represents statement __A__ below. We can re-express this by taking the opposite function to logarithm, the exponent, `exp()`. Similar to how this works in logistic regression, the exponentiated coefficients obtained from `exp(coef(model))` are _multiplicative_, meaning we can say something such as statement __B__

:::int

- __A:__ "a 1 unit change in $x$ is associated with a $b$ unit change in $ln(y)$".  
- __B:__ "a 1 unit change in $x$ is associated with $e^b$ __percent__ change in $y$."

:::

Finding the optimal transformation to use can be difficult, but there are methods out there to help you. One such method is the BoxCox transformation, which can be conducted using `BoxCox(variable, lambda="auto")`, from the __forecast__ package.^[This method finds an appropriate value for $\lambda$ such that the transformation $(sign(x) |x|^{\lambda}-1)/\lambda$ results in a close to normal distribution.] 

## Using Non-Linear Models

::: {.panel-tabset}

### Generalized Linear Models

Generalized Linear Models (GLMs) can appropriately deal with data that do not follow a normal distribution (which is a requirement for traditional linear models). They can accommodate various types of distributions, including the Poisson, binomial, and gamma distributions. This makes them suitable for modelling count data (e.g., number of sunny days Edinburgh has per year - yes, count data can include 0!), binary data (where there are only two possible values e.g., doesn't wear glasses vs wear glasses, smoker vs non-smoker, i.e., values that are yes/no or 0/1), and other types of non-normal data.

We will explore some GLMs later in the course (Semester 2 Block 4), where we will work with logistic regression models. 

### Higher Order Terms

Higher order regression terms refer to the inclusion of polynomial terms of degree higher than one in a regression model. In a linear regression model, the association between the dependent variable ($Y$) and the independent variable ($X$) is assumed to be linear, which means the association can be represented by a straight line. However, in many real-world scenarios, associations between variables are not strictly linear, and including higher order regression terms can help capture more complex relationships. Higher order terms that you could incorporate include quadratic, cubic, or higher degree polynomial terms. 

For example, in a quadratic regression model, the relationship between $Y$ and $X$ can be represented as:

$$
Y = \beta_0 + \beta_1 \cdot X + \beta_2 \cdot X^2 + \epsilon
$$
$$
\begin{align}
& \text{Where:} \\
& Y = \text{Dependent Variable} \\
& X = \text{Independent Variable} \\
\end{align}
$$

As in our models we've seen so far, $\beta_0$, $\beta_1$, and $\beta_2$ are the coefficients to be estimated in the above model. What is different from what we've seen in DAPR2 is the term $\beta_2 \cdot X^2$, and this represents the *quadratic term*. This allows for a curved as opposed to straight line to represent the association between $Y$ and $X$, and hence can allow us to capture more complex relationships. For example, we can model the association between height and age:

```{r}
#| echo: false
#| label: fig-nonlin
#| fig-cap: "Two linear models, one with a quadratic term (right)"  
#| out-width: "100%"
set.seed(13)
df <- tibble(
  age = sample(0:16,100,replace=T),
  height = 70 + (7*age) - (17*scale(age)^2) + rnorm(100,0,10)
)
mod1 <- lm(height ~ age, data = df)
mod2 <- lm(height ~ age + I(age^2), data = df)
df <- df %>% mutate(f1 = fitted(mod1),f2=fitted(mod2))

p1 <- ggplot(df,aes(x=age,y=height)) +
  geom_point() + 
  geom_line(aes(y=f1),lwd=1, col="blue")  +
  labs(title="Simple Linear Model") +
  theme_bw(base_size = 12)

p2 <- ggplot(df,aes(x=age,y=height)) +
  geom_point() + 
  geom_line(aes(y=f2),lwd=1,col="blue") +
  labs(title="Quadratic Model") +    
  theme_bw(base_size = 12)

p1 | p2
```

Please note that these types of models are beyond the scope of the DAPR2 course, but if you want to know more, please do read up on these in your own time. 
:::

## Removing Observations

Removing outliers and potentially influential observations should be a last resort - not all outliers are inherently 'bad' - we do expect natural variation in our population(s) of interest. Outliers can be informative about the topic under investigation, and this is why you need to be very careful about excluding outliers due only to their 'extremeness'. In doing so, you can distort your results by removing variability - i.e., by forcing the data to be more normal and less variable than it actually is, and reduce statistical power by reducing the size of your sample. 

If you do decide to remove observations, you will need to document what specific data points you excluded, and provide an explanation as to why these were excluded. 

To set specific values to `NA` in our dataset (and save this updated dataset in a new object named `mwdata2`), we could use the following code. For the purpose of this demonstration, lets say that we wanted to set any `age` values of <20 as `NA`. In the original dataset `mwdata`, we had 3 individuals aged 18, and 6 aged 19, so we should end up with 9 `NA` values in `mwdata2` column `age`:

```{r}
#specify age column in original dataset, where age is < 20, for values to be set to NA and save to new object named mwdata2 to avoid overwriting original data
mwdata2 <-  mwdata %>% 
    mutate(age = replace(age, age < 20, NA))

#check how many NA values we have - there should be 9 (so 9 TRUEs):
table(is.na(mwdata2$age))
```

If we wanted to remove a full row from the datset, we could use the following code. For the purpose of this demonstration, lets say that we wanted to remove all rows that were highlighted in the above assumption and diagnostic checks as potentially having an adverse influence on our model estimates:

```{r}
# create new dataset 'mwdata3' without (by specifying -) identified outliers and potentially influential observations
mwdata3 <- mwdata[-c(16, 25, 50, 53, 56, 58, 59, 60, 62, 72, 73, 75, 76, 78, 79, 85, 101, 109, 125, 126, 127, 131, 149, 151, 159, 163, 165, 169, 173, 176, 179, 197), ]

# check dimensions - should now have 32 rows less than original dataset 200 - 32 = 168
dim(mwdata3)
```

:::

`r solend()`

<div class="divider div-transparent div-dot"></div>

# Compile Report

`r qbegin("Compile Report", qlabel = FALSE)`  

Knit your report to PDF, and check over your work. To do so, you should make sure:

- Only the output you want your reader to see is visible (e.g., do you want to hide your code?)
- Check that the **tinytex** package is installed
- Ensure that the ‘yaml’ (bit at the very top of your document) looks something like this:

```{}
---
title: "this is my report title"
author: "B1234506"
date: "07/09/2024"
output: bookdown::pdf_document2
---
```

:::{.callout-tip appearance="simple" collapse="true"}
# What to do if you cannot knit to PDF
If you are having issues knitting directly to PDF, try the following:  

- Knit to HTML file  
- Open your HTML in a web-browser (e.g. Chrome, Firefox)  
- Print to PDF (Ctrl+P, then choose to save to PDF)  
- Open file to check formatting
:::

:::{.callout-tip appearance="simple" collapse="true"}
# Hiding Code and/or Output

:::{.panel-tabset}
## Hiding R Code

To not show the code of an R code chunk, and only show the output, write:

````
```{{r, echo=FALSE}}
# code goes here
```
````

## Hiding R Output

To show the code of an R code chunk, but hide the output, write:

````
```{{r, results='hide'}}
# code goes here
```
````

## Hiding R Code and Output

To hide both code and output of an R code chunk, write:

````
```{{r, include=FALSE}}
# code goes here
```
````
:::

:::

:::{.callout-tip appearance="simple" collapse="true"}

### Tinytex
You must make sure you have **tinytex** installed in R so that you can “Knit” your Rmd document to a PDF file:

```{r eval = FALSE}
install.packages("tinytex")
tinytex::install_tinytex()
```

:::

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

You should end up with a PDF file. If you have followed the above instructions and still have issues with knitting, speak with a Tutor. 

`r solend()`
