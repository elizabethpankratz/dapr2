{"title":"Model Fit and Comparisons","markdown":{"yaml":{"title":"Model Fit and Comparisons","link-citations":true,"params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"headingText":"<i class=\"fa fa-graduation-cap\"></i> Learning Objectives","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nsource('assets/setup.R')\nlibrary(patchwork)\nlibrary(checkdown)\n\nset.seed(1)\n```\n\n:::lo\nAt the end of this lab, you will:\n\n1. Understand how to calculate the interpret $R^2$ and adjusted-$R^2$ as a measure of model quality.\n1. Understand the calculation and interpretation of the $F$-test of model utility.\n1. Understand measures of model fit using F.  \n1. Understand the principles of model selection and how to compare models via F tests.\n1. Understand AIC and BIC.\n\n### <i class=\"fa fa-check-square-o fa-2\"></i> What You Need\n\n1. Be up to date with lectures\n2. Have completed previous lab exercises from [Week 1](https://uoepsy.github.io/dapr2/2324/labs/1_01_slr.html), [Week 2](https://uoepsy.github.io/dapr2/2324/labs/1_02_mlr.html), and [Week 3](https://uoepsy.github.io/dapr2/2324/labs/1_03_mlr_stz.html)\n\n### <i class=\"fab fa-r-project\"></i> Required R Packages\nRemember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(\" \")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). \n\nFor this lab, you will need to load the following package(s):\n\n* **tidyverse**\n* **sjPlot**\n* **kableExtra**\n\n### <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\"></i> Presenting Results\nAll results should be presented following [APA guidelines](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf). If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the [rmd bootcamp](https://uoepsy.github.io//rmd-bootcamp/).\n\nThe example write-up sections included as part of the solutions are **not perfect** - they instead should give you a good example of what information you should include and how to structure this. Note that you must **not** copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more [here](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct).\n\n### <i class=\"fa fa-file\"></i> Lab Data\nYou can download the data required for this lab [here](https://uoepsy.github.io/data/wellbeing_rural.csv) or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv. \n\n:::\n\n# Study Overview \n\n> **Research Question(s)** \n>\n> *Section I*\n>\n> + Is there an association between wellbeing and time spent outdoors after taking into account the association between wellbeing and social interactions? \n>\n> *Section II* \n>\n> + RQ1: Is the number of weekly social interactions a useful predictor of wellbeing scores?\n> + RQ2: Does weekly outdoor time explain a significant amount of variance in wellbeing scores over and above social interactions?\n\n`r optbegin(\"Wellbeing/Rurality data codebook.\", olabel=FALSE, toggle=params$TOGGLE)`  \n\n__Description__\n\nFrom the Edinburgh & Lothians, 100 city/suburb residences and 100 rural residences were chosen at random and contacted to participate in the study. The Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), was used to measure mental health and well-being. \n\nParticipants filled out a questionnaire including items concerning: estimated average number of hours spent outdoors each week, estimated average number of social interactions each week (whether on-line or in-person), whether a daily routine is followed (yes/no). For those respondents who had an activity tracker app or smart watch, they were asked to provide their average weekly number of steps.  \n  \n  \n__Data Dictionary__\n\nThe data in `wellbeing_rural.csv` contain seven attributes collected from a random sample of $n=200$ hypothetical residents over Edinburgh & Lothians, and include: \n\n```{r echo=FALSE, message=FALSE, warning=FALSE}\nmwdata  <- read_csv(\"https://uoepsy.github.io/data/wellbeing_rural.csv\")\ntibble(\nvariable = names(mwdata),\ndescription = c(\"Age in years of respondent\",\"Self report estimated number of hours per week spent outdoors \", \"Self report estimated number of social interactions per week (both online and in-person)\", \"Binary 1=Yes/0=No response to the question 'Do you follow a daily routine throughout the week?'\", \"Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70\", \"Location of primary residence (City, Suburb, Rural)\", \"Average weekly number of steps in thousands (as given by activity tracker if available)\")\n) %>% gt::gt()\n```\n  \n  \n__Preview__\n\nThe first six rows of the data are:\n\n```{r echo=FALSE, message=FALSE}\nread_csv('https://uoepsy.github.io/data/wellbeing_rural.csv') %>% head %>% gt::gt()\n```\n  \n`r optend()`\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Setup\n\n`r qbegin(\"Setup\", qlabel = FALSE)`  \n\n1. Create a new RMarkdown file\n2. Load the required package(s)\n3. Read the wellbeing dataset into R, assigning it to an object named `mwdata`\n\n`r qend()`\n\n`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`\n\n```{r message=FALSE}\n#Loading the required package(s)\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(kableExtra)\n\n# Reading in data and storing to an object named 'mwdata'\nmwdata <- read_csv(\"https://uoepsy.github.io/data/wellbeing_rural.csv\")\n```\n\n`r solend()`\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Exercises \n\n## Section I: Model Fit\n\nIn the first section of this lab, you will focus on the statistics contained within the highlighted sections of the `summary()` output below. You will be both calculating these by hand and deriving via `R` before interpreting these values in the context of the research question.\n\n```{r echo = FALSE, out.width='85%'}\nknitr::include_graphics('images/mdl_output_model.PNG')\n```\n\n<br>\n\n`r qbegin(1)`\n\nFit the following multiple linear regression model, and assign the output to an object called `mdl`, and examine the summary output.\n\n$$\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time + \\epsilon \n$$\n\n`r qend()`\n\n`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`\n\n```{r}\nmdl <- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(mdl)\n```\n\n`r solend()`\n\n<br>\n\n`r qbegin(2)`\n\nWhat is the proportion of the total variability in wellbeing scores explained by the model?\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n### Hint \n\nThe proportion of the total variability explained is given by $R^2$. Since the model includes 2 predictors, you should report the Adjusted-$R^2$.\n\nThe $R^2$ coefficient is defined as:\n\n$$\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n$$\n\nThe Adjusted-$R^2$ coefficient is defined as:\n\n$$\n\\hat R^2 = 1 - \\frac{(1 - R^2)(n-1)}{n-k-1}\n\\quad \\\\  \n$$\n\n$$\n\\begin{align}\n& \\text{Where:} \\\\\n& n = \\text{sample size} \\\\\n& k = \\text{number of explanatory variables} \\\\\n\\end{align}\n$$\n:::\n\n`r qend()`\n\n`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`\n\n::: {.panel-tabset}\n\n## Manually\n\nIn `R` we can write:\n```{r}\n#Define n & k\nn <- nrow(mwdata)\nk <- 2\n\n#Predicted scores\nwellbeing_fitted <- mwdata %>%\n  mutate(\n    wellbeing_pred = predict(mdl),\n    wellbeing_resid = wellbeing - wellbeing_pred)\n\n# Sums of Squares, and R / Adjusted R Squared\nwellbeing_fitted %>%\n  summarise(\n    SSModel = sum((wellbeing_pred - mean(wellbeing))^2),\n    SSTotal = sum((wellbeing - mean(wellbeing))^2),\n    SSResid = sum(wellbeing_resid^2)\n  ) %>% \n  summarise(\n    RSquared = SSModel / SSTotal,\n    AdjRSquared = 1-((1-(RSquared))*(n-1)/(n-k-1))\n  )\n```\n\nThe output displays the Adjusted $R$-squared value in the following column:\n\n```\nAdjRSquared\n <dbl>\n 0.118\n```\n\n## R function\n\n```{r}\n#look in second bottom row - Multiple R Squared and Adjusted R Squared both reported here\nsummary(mdl)\n```\n\nThe output of `summary()` displays the Adjusted $R$-squared value in the following line:\n\n```\nAdjusted R-squared:  0.1176 \n```\n:::\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\nApproximately 12\\% of the total variability in wellbeing scores is accounted for by social interactions and outdoor time.\n\n:::\n\n`r solend()`\n\n<br>\n\n`r qbegin(3)`\n\nWhat do you notice about the unadjusted and adjusted R-squared values?\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nAre they similar or quite different? Why might this be?\n\n:::\n\n`r qend()`\n\n`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`\n\nThe values of the unadjusted (0.1265) and adjusted R-squared (0.1176) values are quite similar. This is because the sample size is quite large $(n = 200)$, and the number of predictors $(k = 2)$ is small. \n\n`r solend()`\n\n<br>\n\n`r qbegin(4)`\n\nPerform a model utility test at the 5\\% significance level and report your results. \n\nIn other words, conduct an $F$-test against the null hypothesis that the model is ineffective at predicting wellbeing scores using social interactions and outdoor time by computing the $F$-statistic using its definition.\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nThe $F$-ratio is used to test the null hypothesis that all regression slopes are zero.  \n\nIt is called the $F$-ratio because it is the ratio of the how much of the variation is explained by the model (per parameter) versus how much of the variation is left unexplained in the residuals (per remaining degrees of freedom). \n\n$$\nF_{df_{model},df_{residual}} = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\\\\n\\quad \\\\  \n$$\n\n$$\n\\begin{align}\n& \\text{Where:} \\\\\n& df_{model} = k \\\\\n& df_{residual} = n-k-1 \\\\\n& n = \\text{sample size} \\\\\n& k  = \\text{number of explanatory variables} \\\\\n\\end{align}\n$$\n:::\n\n`r qend()`\n\n`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`\n\n::: {.panel-tabset}\n\n## Manually\n\n```{r}\n#df(model) = k \ndf1 <- 2\n\n#df(residual) = n - k - 1\ndf2 <- nrow(mwdata) - 2 - 1\n\nf_star <- qf(0.95, df1, df2)\n\n#check value\nf_star\n```\n\n```{r}\nmodel_utility <- wellbeing_fitted %>%\n  summarise(\n    SSModel = sum((wellbeing_pred - mean(wellbeing))^2),\n    SSResid = sum(wellbeing_resid^2),\n    MSModel = SSModel / df1,\n    MSResid = SSResid / df2,\n    FObs = MSModel / MSResid\n  )\nmodel_utility\n```\n\nWe can also compute the p-value:\n\n```{r}\npvalue <- 1 - pf(model_utility$FObs, df1, df2)\npvalue\n```\n\nThe value `1.643779e-06` simply means $1.6 \\times 10^{-6}$, so it's a really small number (i.e., 0.000001643779).\n\n## R function\n\n```{r}\n#look in bottom row\nsummary(mdl)\n```\n\nThe relevant row is the following:\n\n```\n\nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n```\n\n:::\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\nWe performed an $F$-test of model utility at the 5\\% significance level, where $F(2,197) = 14.26, p <.001$.\n\nThe large $F$-statistic and small $p$-value $(p <.001)$ suggested that we have very strong evidence against the null hypothesis.\n\nIn other words, the data provide strong evidence that the number of social interactions and outdoor time are predictors of wellbeing scores.\n\n:::\n\n`r solend()`\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n## Section II: Model Comparisons\n\nIn the second section of this lab, you will focus on model comparison where you will formally test a number of research questions:\n\n> + RQ1: Is the number of weekly social interactions a useful predictor of wellbeing scores?\n> + RQ2: Does weekly outdoor time explain a significant amount of variance in wellbeing scores over and above the number of weekly social interactions?\n\n<br>\n\n`r qbegin(5)`\n\nFit the below 3 models required to address the 2 research questions stated above. Note down which model(s) will be used to address each research question, and examine the results of each model. \n\nName the models as follows: \"wb_mdl0\", \"wb_mdl1\", \"wb_mdl2\"\n\n<br>\n$$\n\\text{Wellbeing} = \\beta_0  + \\epsilon\n$$\n\n<br>\n\n$$\n\\text{Wellbeing} = \\beta_0  + \\beta_1 \\cdot Social~Interactions + \\epsilon\n$$\n\n<br>\n\n$$\n\\text{Wellbeing} = \\beta_0  + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time + \\epsilon\n$$\n<br>\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nThe `summary()` function will be useful to examine the model output. \n\n:::\n\n`r qend()`\n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n::: {.panel-tabset}\n\n## wb_mdl0\n\n```{r}\n#null/intercept only model\nwb_mdl0 <- lm(wellbeing ~ 1, data = mwdata)\nsummary(wb_mdl0)\n```\n\n## wb_mdl1\n\n```{r}\n#model with social interactions\nwb_mdl1 <- lm(wellbeing ~ social_int, data = mwdata)\nsummary(wb_mdl1)\n```\n\n## wb_mdl2\n\n```{r}\n#model with social interactions and outdoor time\nwb_mdl2 <- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(wb_mdl2)\n```\n\n:::\n\nThe models required to address each research question (RQ) are:\n\n+ RQ1: Models wb_mdl0 and wb_mdl1 \n+ RQ2: Models wb_mdl1 and wb_mdl2\n\n`r solend()`\n\n<br>\n\n`r qbegin(6)`\n\nRQ1: Is the number of weekly social interactions a useful predictor of wellbeing scores? \n\nCheck that the $F$-statistic and the $p$-value are the the same from the model comparison as that which are given at the bottom of `summary(wb_mdl1)`. \n\nProvide the key model results from the two models in a single formatted table.\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nUse the `anova()` function to perform a model comparison between your model with social interactions (wb_mdl1) to the null model (wb_mdl0). Remember that the null model tests the null hypothesis that all beta coefficients are zero. By comparing *wb_mdl0* to *wb_mdl1*, we can test whether we should include the IV of 'social_int'.  \n\nYou can use **KableExtra** to present your model comparison results in a well formatted table. \n\n:::\n\n`r qend()`\n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n::: {.panel-tabset}\n\n## Model Comparison\n\nRun model comparison via `anova()`, and present results in well formatted table:\n\n```{r}\n#| label: tbl-wb0-wb1-comp-results\n#| tbl-cap: Model Comparison - wb_mdl0 vs wb_mdl1\nanova(wb_mdl0, wb_mdl1) %>%\n    kable(caption = \"Model Comparison - wb_mdl0 vs wb_mdl1\", align = \"c\", digits = c(2,2,2,2,2,4)) %>%\n    kable_styling(full_width = FALSE)\n```\n\n## Comparing `summary()` and `anova()` Outputs\n\nThe output of `anova(wb_mdl0, wb_mdl1)` displays the $F$-statistic and the $p$-value in the following line:\n\n```\n  Res.Df    RSS Df Sum of Sq     F    Pr(>F)  \n2    198 5451.1  1    334.49 12.15 0.0006045 ***\n```\n\nWe can check that the $F$-statistic and the $p$-value are the the same as that which is given at the bottom of `summary(wb_mdl1)`:\n\n```\nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n```\n\nThe $F$-statistic and the $p$-value from `anova(wb_mdl0, wb_mdl1)` and `summary(wb_mdl1)` both match! This is because the $F$-test from a model with a single predictor (i.e, 'wb_mdl1') is really just a comparison against the null model (i.e, 'wb_mdl0').\n\n```{r include=FALSE}\nmc_1 <- anova(wb_mdl0, wb_mdl1)\nnames(mc_1)[6] <- \"p\"\n```\n\n## Table of Model Results \n\n```{r}\n#| label: tbl-wb0-wb1-results\n#| tbl-cap: Regression Table for Wellbeing Models wb0 and wb1\ntab_model(wb_mdl0, wb_mdl1,\n          dv.labels = c(\"Wellbeing (WEMWBS Scores)\", \"Wellbeing (WEMWBS Scores)\"),\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\"),\n          title = \"Regression Table for Wellbeing Models wb0 and wb1\")\n```\n\n:::\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\nThe number of social interactions was found to explain a significant amount of variance in wellbeing scores ($F$(`r paste(mc_1$Df[2])` ,`r paste(mc_1$Res.Df[2])`) = `r round(mc_1$F[2],2)`, $p$`r map_chr(mc_1$p[2], ~ifelse(.<001,\"<.001\",paste0(\"=\",round(.,2))))`). The model with social interactions was significantly better fitting than the intercept-only model, and thus social interactions is a useful predictor of wellbeing scores. Full regression results are presented in @tbl-wb0-wb1-results. \n\n:::\n\n`r solend()`\n\n<br>\n\n`r qbegin(7)`\n\nLook at the amount of variation in wellbeing scores explained by models \"wb_mdl1\" and \"wb_mdl2\". \n\nFrom this, can we answer the second research question of whether weekly outdoor time explains a significant amount of variance in wellbeing scores over and above social interactions?  \n\nProvide justification/rationale for your answer. \n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nYou will need to review the R-Squared and Adjusted R-Squared values.\n\n:::\n\n`r qend()`\n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nLet's look at the amount of variance explained by each model:\n\n```{r}\nsummary(wb_mdl1)$r.squared\nsummary(wb_mdl2)$adj.r.squared\n```\n\nThe model *with* weekly outdoor time as a predictor explains 12\\% of the variance, and the model *without* explains 6\\%. But, from only looking at the proportion of variance accounted for in each model, we cannot determine which model is statistically a better fit.  \n\nTo answer the question 'Does including weekly outdoor time as a predictor provide a significantly better fit of the data?' we need to **statistically compare** wb_mdl1 to wb_mdl2. \n\n\n`r solend()`\n\n<br> \n\n`r qbegin(8)`\n\nDoes weekly outdoor time explain a significant amount of variance in wellbeing scores over and above social interactions?\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nTo address RQ2, you need to statistically compare \"wb_mdl1\" and \"wb_mdl2\".  \n\nYou can use **KableExtra** to present your model comparison results in a well formatted table. \n\n:::\n\n`r qend()`\n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n::: {.panel-tabset}\n\n## Model Comparison\n\nTo statistically compare models, we could use an incremental $F$-test to compare the models since the models are nested and from the same dataset:\n\n```{r}\n#| label: tbl-wb1-wb2-comp-results\n#| tbl-cap: Model Comparison - wb_mdl1 vs wb_mdl2\nanova(wb_mdl1, wb_mdl2) %>%\n    kable(caption = \"Model Comparison - wb_mdl1 vs wb_mdl2\", align = \"c\", digits = c(2,2,2,2,2,4)) %>%\n    kable_styling(full_width = FALSE)\n```\n\n## Table of Model Results \n\nPresent results from both models:\n\n```{r}\n#| label: tbl-wb1-wb2-results\n#| tbl-cap: Regression Table for Wellbeing Models wb1 and wb2\ntab_model(wb_mdl1, wb_mdl2,\n          dv.labels = c(\"Wellbeing (WEMWBS Scores)\", \"Wellbeing (WEMWBS Scores)\"),\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\",\n                          \"outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression Table for Wellbeing Models wb1 and wb2\")\n```\n\n:::\n\n```{r include=FALSE}\nmc_3 <- anova(wb_mdl1, wb_mdl2)\nnames(mc_3)[6]<-\"p\"\n```\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\nAs presented in @tbl-wb1-wb2-comp-results, weekly outdoor time was found to explain a significant amount of variance in wellbeing scores over and above weekly social interactions ($F$(`r paste(mc_3$Df[2])` ,`r paste(mc_3$Res.Df[2])`) = `r round(mc_3$F[2],2)`, $p$`r map_chr(mc_3$p[2], ~ifelse(.<001,\"<.001\",paste0(\"=\",round(.,2))))`).\n\n:::\n\n`r solend()`\n\n<br>\n\n`r qbegin(9)`\n\nCompare the two following models, each looking at the associations of Wellbeing scores and different predictor variables. \n\n$\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social~Interactions} + \\beta_2 \\cdot \\text{Age} + \\epsilon$  \n\n$\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Outdoor~Time} + \\epsilon$ \n\nReport which model you think best fits the data, and justify your answer. \n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nCompare using `AIC()` and `BIC()` since the models are non-nested.\n\n:::\n\n`r qend()`\n\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\n#fit models\nwb_socint_age <- lm(wellbeing ~ social_int + age, data = mwdata)\nwb_outdoor <- lm(wellbeing ~ outdoor_time, data = mwdata)\n```\n\n```{r}\n#AIC values\nAIC(wb_socint_age, wb_outdoor)\n\n#BIC values\nBIC(wb_socint_age, wb_outdoor)\n```\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\nWe used AIC and BIC model selection to distinguish between two possible models describing the association between several personal factors and wellbeing scores. The model with outdoor time included as a single predictor was better fitting (AIC = 1233.29) than the alternative model with weekly number of social interactions and age (AIC = 1236.58) included. Based on the BIC value of the former model (BIC = 1243.18), we concluded that it was better fitting than the alternative, latter model (BIC = 1249.77). \n\n:::\n\n`r solend()`\n\n<br>\n\n`r qbegin(10)`\n\nThe code below fits 6 different models based on our `mwdata`:\n\n```{r eval=FALSE}\nmodel1 <- lm(wellbeing ~ social_int, data = mwdata)\nmodel2 <- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nmodel3 <- lm(wellbeing ~ social_int + age, data = mwdata)\nmodel4 <- lm(wellbeing ~ social_int + outdoor_time + age, data = mwdata)\nmodel5 <- lm(wellbeing ~ social_int + outdoor_time + age + steps_k, data = mwdata)\nmodel6 <- lm(wellbeing ~ social_int + outdoor_time, data = wb_data)\n```\n\nFor each of the below pairs of models, what methods are/are not available for us to use for comparison and why?  \n\n+ `model1` vs `model2`\n+ `model2` vs `model3`\n+ `model1` vs `model4`\n+ `model3` vs `model5`\n+ `model2` vs `model6`\n\nThis flowchart might help you to reach your decision:\n\n```{r comparisons_chart, echo=FALSE, fig.align = 'left', out.width = \"100%\"}\nknitr::include_graphics(\"images/comparisons_chart.png\")\n```\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nYou may need to examine the dataset, and check for accuracy (e.g., are there any impossible / out of range values?) and completeness (e.g., are there any missing values?). \n\n:::\n\n`r qend()`\n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n::: {.panel-tabset}\n\n## `model1` vs `model2`\n\n+ These models are nested - `model2` contains all the variables of `model1` and they are fitted on the same dataset.  \n+ We can therefore use an $F$-test or AIC and BIC.  \n    \n## `model2` vs `model3`\n\n+ These models are __not__ nested, but they are fitted on the same dataset.  \n+ We can therefore use AIC or BIC, but we cannot use an $F$-test.  \n    \n## `model1` vs `model4`\n\n+ These models are nested - `model4` contains all the variables of `model1` and they are fitted on the same dataset.  \n+ We can therefore use an $F$-test or AIC and BIC.  \n    \n## `model3` vs `model5`\n\n+ These models are __not__ nested, and they are __not__ fitted on the same dataset. The \"steps_k\" variable contains missing values (over 30% of the data is missing for this variable), and so these whole rows are excluded from `model5` (but they are included in `model3`). \n+ We cannot compare these models.   \n  \n## `model2` vs `model6`  \n\n+ These models are nested, but they are __not__ fitted on the same dataset: `model2` uses the 'mwdata' dataset, whilst `model6` uses the 'wb_data' dataset.\n+ We cannot compare these models.   \n\n:::\n\n`r solend()`\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Compile Report\n\n`r qbegin(\"Compile Report\", qlabel = FALSE)`  \n\nKnit your report to PDF, and check over your work. To do so, you should make sure:\n\n- Only the output you want your reader to see is visible (e.g., do you want to hide your code?)\n- Check that the **tinytex** package is installed\n- Ensure that the ‘yaml’ (bit at the very top of your document) looks something like this:\n\n```{}\n---\ntitle: \"this is my report title\"\nauthor: \"B1234506\"\ndate: \"07/09/2024\"\noutput: bookdown::pdf_document2\n---\n```\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n# What to do if you cannot knit to PDF\nIf you are having issues knitting directly to PDF, try the following:  \n\n- Knit to HTML file  \n- Open your HTML in a web-browser (e.g. Chrome, Firefox)  \n- Print to PDF (Ctrl+P, then choose to save to PDF)  \n- Open file to check formatting\n:::\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n# Hiding Code and/or Output\n\n:::{.panel-tabset}\n## Hiding R Code\n\nTo not show the code of an R code chunk, and only show the output, write:\n\n````\n```{{r, echo=FALSE}}\n# code goes here\n```\n````\n\n## Hiding R Output\n\nTo show the code of an R code chunk, but hide the output, write:\n\n````\n```{{r, results='hide'}}\n# code goes here\n```\n````\n\n## Hiding R Code and Output\n\nTo hide both code and output of an R code chunk, write:\n\n````\n```{{r, include=FALSE}}\n# code goes here\n```\n````\n:::\n\n:::\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Tinytex\nYou must make sure you have **tinytex** installed in R so that you can “Knit” your Rmd document to a PDF file:\n\n```{r eval = FALSE}\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n```\n\n:::\n\n`r qend()`\n\n`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`\n\nYou should end up with a PDF file. If you have followed the above instructions and still have issues with knitting, speak with a Tutor. \n\n`r solend()`","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nsource('assets/setup.R')\nlibrary(patchwork)\nlibrary(checkdown)\n\nset.seed(1)\n```\n\n:::lo\n### <i class=\"fa fa-graduation-cap\"></i> Learning Objectives\nAt the end of this lab, you will:\n\n1. Understand how to calculate the interpret $R^2$ and adjusted-$R^2$ as a measure of model quality.\n1. Understand the calculation and interpretation of the $F$-test of model utility.\n1. Understand measures of model fit using F.  \n1. Understand the principles of model selection and how to compare models via F tests.\n1. Understand AIC and BIC.\n\n### <i class=\"fa fa-check-square-o fa-2\"></i> What You Need\n\n1. Be up to date with lectures\n2. Have completed previous lab exercises from [Week 1](https://uoepsy.github.io/dapr2/2324/labs/1_01_slr.html), [Week 2](https://uoepsy.github.io/dapr2/2324/labs/1_02_mlr.html), and [Week 3](https://uoepsy.github.io/dapr2/2324/labs/1_03_mlr_stz.html)\n\n### <i class=\"fab fa-r-project\"></i> Required R Packages\nRemember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(\" \")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). \n\nFor this lab, you will need to load the following package(s):\n\n* **tidyverse**\n* **sjPlot**\n* **kableExtra**\n\n### <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\"></i> Presenting Results\nAll results should be presented following [APA guidelines](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf). If you need a reminder on how to hide code, format tables/plots, etc., make sure to review the [rmd bootcamp](https://uoepsy.github.io//rmd-bootcamp/).\n\nThe example write-up sections included as part of the solutions are **not perfect** - they instead should give you a good example of what information you should include and how to structure this. Note that you must **not** copy any of the write-ups included below for future reports - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more [here](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct).\n\n### <i class=\"fa fa-file\"></i> Lab Data\nYou can download the data required for this lab [here](https://uoepsy.github.io/data/wellbeing_rural.csv) or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv. \n\n:::\n\n# Study Overview \n\n> **Research Question(s)** \n>\n> *Section I*\n>\n> + Is there an association between wellbeing and time spent outdoors after taking into account the association between wellbeing and social interactions? \n>\n> *Section II* \n>\n> + RQ1: Is the number of weekly social interactions a useful predictor of wellbeing scores?\n> + RQ2: Does weekly outdoor time explain a significant amount of variance in wellbeing scores over and above social interactions?\n\n`r optbegin(\"Wellbeing/Rurality data codebook.\", olabel=FALSE, toggle=params$TOGGLE)`  \n\n__Description__\n\nFrom the Edinburgh & Lothians, 100 city/suburb residences and 100 rural residences were chosen at random and contacted to participate in the study. The Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), was used to measure mental health and well-being. \n\nParticipants filled out a questionnaire including items concerning: estimated average number of hours spent outdoors each week, estimated average number of social interactions each week (whether on-line or in-person), whether a daily routine is followed (yes/no). For those respondents who had an activity tracker app or smart watch, they were asked to provide their average weekly number of steps.  \n  \n  \n__Data Dictionary__\n\nThe data in `wellbeing_rural.csv` contain seven attributes collected from a random sample of $n=200$ hypothetical residents over Edinburgh & Lothians, and include: \n\n```{r echo=FALSE, message=FALSE, warning=FALSE}\nmwdata  <- read_csv(\"https://uoepsy.github.io/data/wellbeing_rural.csv\")\ntibble(\nvariable = names(mwdata),\ndescription = c(\"Age in years of respondent\",\"Self report estimated number of hours per week spent outdoors \", \"Self report estimated number of social interactions per week (both online and in-person)\", \"Binary 1=Yes/0=No response to the question 'Do you follow a daily routine throughout the week?'\", \"Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70\", \"Location of primary residence (City, Suburb, Rural)\", \"Average weekly number of steps in thousands (as given by activity tracker if available)\")\n) %>% gt::gt()\n```\n  \n  \n__Preview__\n\nThe first six rows of the data are:\n\n```{r echo=FALSE, message=FALSE}\nread_csv('https://uoepsy.github.io/data/wellbeing_rural.csv') %>% head %>% gt::gt()\n```\n  \n`r optend()`\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Setup\n\n`r qbegin(\"Setup\", qlabel = FALSE)`  \n\n1. Create a new RMarkdown file\n2. Load the required package(s)\n3. Read the wellbeing dataset into R, assigning it to an object named `mwdata`\n\n`r qend()`\n\n`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`\n\n```{r message=FALSE}\n#Loading the required package(s)\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(kableExtra)\n\n# Reading in data and storing to an object named 'mwdata'\nmwdata <- read_csv(\"https://uoepsy.github.io/data/wellbeing_rural.csv\")\n```\n\n`r solend()`\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Exercises \n\n## Section I: Model Fit\n\nIn the first section of this lab, you will focus on the statistics contained within the highlighted sections of the `summary()` output below. You will be both calculating these by hand and deriving via `R` before interpreting these values in the context of the research question.\n\n```{r echo = FALSE, out.width='85%'}\nknitr::include_graphics('images/mdl_output_model.PNG')\n```\n\n<br>\n\n`r qbegin(1)`\n\nFit the following multiple linear regression model, and assign the output to an object called `mdl`, and examine the summary output.\n\n$$\n\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time + \\epsilon \n$$\n\n`r qend()`\n\n`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`\n\n```{r}\nmdl <- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(mdl)\n```\n\n`r solend()`\n\n<br>\n\n`r qbegin(2)`\n\nWhat is the proportion of the total variability in wellbeing scores explained by the model?\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n### Hint \n\nThe proportion of the total variability explained is given by $R^2$. Since the model includes 2 predictors, you should report the Adjusted-$R^2$.\n\nThe $R^2$ coefficient is defined as:\n\n$$\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}\n$$\n\nThe Adjusted-$R^2$ coefficient is defined as:\n\n$$\n\\hat R^2 = 1 - \\frac{(1 - R^2)(n-1)}{n-k-1}\n\\quad \\\\  \n$$\n\n$$\n\\begin{align}\n& \\text{Where:} \\\\\n& n = \\text{sample size} \\\\\n& k = \\text{number of explanatory variables} \\\\\n\\end{align}\n$$\n:::\n\n`r qend()`\n\n`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`\n\n::: {.panel-tabset}\n\n## Manually\n\nIn `R` we can write:\n```{r}\n#Define n & k\nn <- nrow(mwdata)\nk <- 2\n\n#Predicted scores\nwellbeing_fitted <- mwdata %>%\n  mutate(\n    wellbeing_pred = predict(mdl),\n    wellbeing_resid = wellbeing - wellbeing_pred)\n\n# Sums of Squares, and R / Adjusted R Squared\nwellbeing_fitted %>%\n  summarise(\n    SSModel = sum((wellbeing_pred - mean(wellbeing))^2),\n    SSTotal = sum((wellbeing - mean(wellbeing))^2),\n    SSResid = sum(wellbeing_resid^2)\n  ) %>% \n  summarise(\n    RSquared = SSModel / SSTotal,\n    AdjRSquared = 1-((1-(RSquared))*(n-1)/(n-k-1))\n  )\n```\n\nThe output displays the Adjusted $R$-squared value in the following column:\n\n```\nAdjRSquared\n <dbl>\n 0.118\n```\n\n## R function\n\n```{r}\n#look in second bottom row - Multiple R Squared and Adjusted R Squared both reported here\nsummary(mdl)\n```\n\nThe output of `summary()` displays the Adjusted $R$-squared value in the following line:\n\n```\nAdjusted R-squared:  0.1176 \n```\n:::\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\nApproximately 12\\% of the total variability in wellbeing scores is accounted for by social interactions and outdoor time.\n\n:::\n\n`r solend()`\n\n<br>\n\n`r qbegin(3)`\n\nWhat do you notice about the unadjusted and adjusted R-squared values?\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nAre they similar or quite different? Why might this be?\n\n:::\n\n`r qend()`\n\n`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`\n\nThe values of the unadjusted (0.1265) and adjusted R-squared (0.1176) values are quite similar. This is because the sample size is quite large $(n = 200)$, and the number of predictors $(k = 2)$ is small. \n\n`r solend()`\n\n<br>\n\n`r qbegin(4)`\n\nPerform a model utility test at the 5\\% significance level and report your results. \n\nIn other words, conduct an $F$-test against the null hypothesis that the model is ineffective at predicting wellbeing scores using social interactions and outdoor time by computing the $F$-statistic using its definition.\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nThe $F$-ratio is used to test the null hypothesis that all regression slopes are zero.  \n\nIt is called the $F$-ratio because it is the ratio of the how much of the variation is explained by the model (per parameter) versus how much of the variation is left unexplained in the residuals (per remaining degrees of freedom). \n\n$$\nF_{df_{model},df_{residual}} = \\frac{MS_{Model}}{MS_{Residual}} = \\frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\\\\n\\quad \\\\  \n$$\n\n$$\n\\begin{align}\n& \\text{Where:} \\\\\n& df_{model} = k \\\\\n& df_{residual} = n-k-1 \\\\\n& n = \\text{sample size} \\\\\n& k  = \\text{number of explanatory variables} \\\\\n\\end{align}\n$$\n:::\n\n`r qend()`\n\n`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`\n\n::: {.panel-tabset}\n\n## Manually\n\n```{r}\n#df(model) = k \ndf1 <- 2\n\n#df(residual) = n - k - 1\ndf2 <- nrow(mwdata) - 2 - 1\n\nf_star <- qf(0.95, df1, df2)\n\n#check value\nf_star\n```\n\n```{r}\nmodel_utility <- wellbeing_fitted %>%\n  summarise(\n    SSModel = sum((wellbeing_pred - mean(wellbeing))^2),\n    SSResid = sum(wellbeing_resid^2),\n    MSModel = SSModel / df1,\n    MSResid = SSResid / df2,\n    FObs = MSModel / MSResid\n  )\nmodel_utility\n```\n\nWe can also compute the p-value:\n\n```{r}\npvalue <- 1 - pf(model_utility$FObs, df1, df2)\npvalue\n```\n\nThe value `1.643779e-06` simply means $1.6 \\times 10^{-6}$, so it's a really small number (i.e., 0.000001643779).\n\n## R function\n\n```{r}\n#look in bottom row\nsummary(mdl)\n```\n\nThe relevant row is the following:\n\n```\n\nF-statistic: 14.26 on 2 and 197 DF,  p-value: 1.644e-06\n\n```\n\n:::\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\nWe performed an $F$-test of model utility at the 5\\% significance level, where $F(2,197) = 14.26, p <.001$.\n\nThe large $F$-statistic and small $p$-value $(p <.001)$ suggested that we have very strong evidence against the null hypothesis.\n\nIn other words, the data provide strong evidence that the number of social interactions and outdoor time are predictors of wellbeing scores.\n\n:::\n\n`r solend()`\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n## Section II: Model Comparisons\n\nIn the second section of this lab, you will focus on model comparison where you will formally test a number of research questions:\n\n> + RQ1: Is the number of weekly social interactions a useful predictor of wellbeing scores?\n> + RQ2: Does weekly outdoor time explain a significant amount of variance in wellbeing scores over and above the number of weekly social interactions?\n\n<br>\n\n`r qbegin(5)`\n\nFit the below 3 models required to address the 2 research questions stated above. Note down which model(s) will be used to address each research question, and examine the results of each model. \n\nName the models as follows: \"wb_mdl0\", \"wb_mdl1\", \"wb_mdl2\"\n\n<br>\n$$\n\\text{Wellbeing} = \\beta_0  + \\epsilon\n$$\n\n<br>\n\n$$\n\\text{Wellbeing} = \\beta_0  + \\beta_1 \\cdot Social~Interactions + \\epsilon\n$$\n\n<br>\n\n$$\n\\text{Wellbeing} = \\beta_0  + \\beta_1 \\cdot Social~Interactions + \\beta_2 \\cdot Outdoor~Time + \\epsilon\n$$\n<br>\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nThe `summary()` function will be useful to examine the model output. \n\n:::\n\n`r qend()`\n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n::: {.panel-tabset}\n\n## wb_mdl0\n\n```{r}\n#null/intercept only model\nwb_mdl0 <- lm(wellbeing ~ 1, data = mwdata)\nsummary(wb_mdl0)\n```\n\n## wb_mdl1\n\n```{r}\n#model with social interactions\nwb_mdl1 <- lm(wellbeing ~ social_int, data = mwdata)\nsummary(wb_mdl1)\n```\n\n## wb_mdl2\n\n```{r}\n#model with social interactions and outdoor time\nwb_mdl2 <- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nsummary(wb_mdl2)\n```\n\n:::\n\nThe models required to address each research question (RQ) are:\n\n+ RQ1: Models wb_mdl0 and wb_mdl1 \n+ RQ2: Models wb_mdl1 and wb_mdl2\n\n`r solend()`\n\n<br>\n\n`r qbegin(6)`\n\nRQ1: Is the number of weekly social interactions a useful predictor of wellbeing scores? \n\nCheck that the $F$-statistic and the $p$-value are the the same from the model comparison as that which are given at the bottom of `summary(wb_mdl1)`. \n\nProvide the key model results from the two models in a single formatted table.\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nUse the `anova()` function to perform a model comparison between your model with social interactions (wb_mdl1) to the null model (wb_mdl0). Remember that the null model tests the null hypothesis that all beta coefficients are zero. By comparing *wb_mdl0* to *wb_mdl1*, we can test whether we should include the IV of 'social_int'.  \n\nYou can use **KableExtra** to present your model comparison results in a well formatted table. \n\n:::\n\n`r qend()`\n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n::: {.panel-tabset}\n\n## Model Comparison\n\nRun model comparison via `anova()`, and present results in well formatted table:\n\n```{r}\n#| label: tbl-wb0-wb1-comp-results\n#| tbl-cap: Model Comparison - wb_mdl0 vs wb_mdl1\nanova(wb_mdl0, wb_mdl1) %>%\n    kable(caption = \"Model Comparison - wb_mdl0 vs wb_mdl1\", align = \"c\", digits = c(2,2,2,2,2,4)) %>%\n    kable_styling(full_width = FALSE)\n```\n\n## Comparing `summary()` and `anova()` Outputs\n\nThe output of `anova(wb_mdl0, wb_mdl1)` displays the $F$-statistic and the $p$-value in the following line:\n\n```\n  Res.Df    RSS Df Sum of Sq     F    Pr(>F)  \n2    198 5451.1  1    334.49 12.15 0.0006045 ***\n```\n\nWe can check that the $F$-statistic and the $p$-value are the the same as that which is given at the bottom of `summary(wb_mdl1)`:\n\n```\nF-statistic: 12.15 on 1 and 198 DF,  p-value: 0.0006045\n```\n\nThe $F$-statistic and the $p$-value from `anova(wb_mdl0, wb_mdl1)` and `summary(wb_mdl1)` both match! This is because the $F$-test from a model with a single predictor (i.e, 'wb_mdl1') is really just a comparison against the null model (i.e, 'wb_mdl0').\n\n```{r include=FALSE}\nmc_1 <- anova(wb_mdl0, wb_mdl1)\nnames(mc_1)[6] <- \"p\"\n```\n\n## Table of Model Results \n\n```{r}\n#| label: tbl-wb0-wb1-results\n#| tbl-cap: Regression Table for Wellbeing Models wb0 and wb1\ntab_model(wb_mdl0, wb_mdl1,\n          dv.labels = c(\"Wellbeing (WEMWBS Scores)\", \"Wellbeing (WEMWBS Scores)\"),\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\"),\n          title = \"Regression Table for Wellbeing Models wb0 and wb1\")\n```\n\n:::\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\nThe number of social interactions was found to explain a significant amount of variance in wellbeing scores ($F$(`r paste(mc_1$Df[2])` ,`r paste(mc_1$Res.Df[2])`) = `r round(mc_1$F[2],2)`, $p$`r map_chr(mc_1$p[2], ~ifelse(.<001,\"<.001\",paste0(\"=\",round(.,2))))`). The model with social interactions was significantly better fitting than the intercept-only model, and thus social interactions is a useful predictor of wellbeing scores. Full regression results are presented in @tbl-wb0-wb1-results. \n\n:::\n\n`r solend()`\n\n<br>\n\n`r qbegin(7)`\n\nLook at the amount of variation in wellbeing scores explained by models \"wb_mdl1\" and \"wb_mdl2\". \n\nFrom this, can we answer the second research question of whether weekly outdoor time explains a significant amount of variance in wellbeing scores over and above social interactions?  \n\nProvide justification/rationale for your answer. \n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nYou will need to review the R-Squared and Adjusted R-Squared values.\n\n:::\n\n`r qend()`\n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\nLet's look at the amount of variance explained by each model:\n\n```{r}\nsummary(wb_mdl1)$r.squared\nsummary(wb_mdl2)$adj.r.squared\n```\n\nThe model *with* weekly outdoor time as a predictor explains 12\\% of the variance, and the model *without* explains 6\\%. But, from only looking at the proportion of variance accounted for in each model, we cannot determine which model is statistically a better fit.  \n\nTo answer the question 'Does including weekly outdoor time as a predictor provide a significantly better fit of the data?' we need to **statistically compare** wb_mdl1 to wb_mdl2. \n\n\n`r solend()`\n\n<br> \n\n`r qbegin(8)`\n\nDoes weekly outdoor time explain a significant amount of variance in wellbeing scores over and above social interactions?\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nTo address RQ2, you need to statistically compare \"wb_mdl1\" and \"wb_mdl2\".  \n\nYou can use **KableExtra** to present your model comparison results in a well formatted table. \n\n:::\n\n`r qend()`\n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n::: {.panel-tabset}\n\n## Model Comparison\n\nTo statistically compare models, we could use an incremental $F$-test to compare the models since the models are nested and from the same dataset:\n\n```{r}\n#| label: tbl-wb1-wb2-comp-results\n#| tbl-cap: Model Comparison - wb_mdl1 vs wb_mdl2\nanova(wb_mdl1, wb_mdl2) %>%\n    kable(caption = \"Model Comparison - wb_mdl1 vs wb_mdl2\", align = \"c\", digits = c(2,2,2,2,2,4)) %>%\n    kable_styling(full_width = FALSE)\n```\n\n## Table of Model Results \n\nPresent results from both models:\n\n```{r}\n#| label: tbl-wb1-wb2-results\n#| tbl-cap: Regression Table for Wellbeing Models wb1 and wb2\ntab_model(wb_mdl1, wb_mdl2,\n          dv.labels = c(\"Wellbeing (WEMWBS Scores)\", \"Wellbeing (WEMWBS Scores)\"),\n          pred.labels = c(\"social_int\" = \"Social Interactions (number per week)\",\n                          \"outdoor_time\" = \"Outdoor Time (hours per week)\"),\n          title = \"Regression Table for Wellbeing Models wb1 and wb2\")\n```\n\n:::\n\n```{r include=FALSE}\nmc_3 <- anova(wb_mdl1, wb_mdl2)\nnames(mc_3)[6]<-\"p\"\n```\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\nAs presented in @tbl-wb1-wb2-comp-results, weekly outdoor time was found to explain a significant amount of variance in wellbeing scores over and above weekly social interactions ($F$(`r paste(mc_3$Df[2])` ,`r paste(mc_3$Res.Df[2])`) = `r round(mc_3$F[2],2)`, $p$`r map_chr(mc_3$p[2], ~ifelse(.<001,\"<.001\",paste0(\"=\",round(.,2))))`).\n\n:::\n\n`r solend()`\n\n<br>\n\n`r qbegin(9)`\n\nCompare the two following models, each looking at the associations of Wellbeing scores and different predictor variables. \n\n$\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Social~Interactions} + \\beta_2 \\cdot \\text{Age} + \\epsilon$  \n\n$\\text{Wellbeing} = \\beta_0 + \\beta_1 \\cdot \\text{Outdoor~Time} + \\epsilon$ \n\nReport which model you think best fits the data, and justify your answer. \n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nCompare using `AIC()` and `BIC()` since the models are non-nested.\n\n:::\n\n`r qend()`\n\n`r solbegin(show=TRUE, toggle=params$TOGGLE)`\n\n```{r}\n#fit models\nwb_socint_age <- lm(wellbeing ~ social_int + age, data = mwdata)\nwb_outdoor <- lm(wellbeing ~ outdoor_time, data = mwdata)\n```\n\n```{r}\n#AIC values\nAIC(wb_socint_age, wb_outdoor)\n\n#BIC values\nBIC(wb_socint_age, wb_outdoor)\n```\n\n::: {.callout-important icon=false appearance=\"minimal\"}\n\nWe used AIC and BIC model selection to distinguish between two possible models describing the association between several personal factors and wellbeing scores. The model with outdoor time included as a single predictor was better fitting (AIC = 1233.29) than the alternative model with weekly number of social interactions and age (AIC = 1236.58) included. Based on the BIC value of the former model (BIC = 1243.18), we concluded that it was better fitting than the alternative, latter model (BIC = 1249.77). \n\n:::\n\n`r solend()`\n\n<br>\n\n`r qbegin(10)`\n\nThe code below fits 6 different models based on our `mwdata`:\n\n```{r eval=FALSE}\nmodel1 <- lm(wellbeing ~ social_int, data = mwdata)\nmodel2 <- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)\nmodel3 <- lm(wellbeing ~ social_int + age, data = mwdata)\nmodel4 <- lm(wellbeing ~ social_int + outdoor_time + age, data = mwdata)\nmodel5 <- lm(wellbeing ~ social_int + outdoor_time + age + steps_k, data = mwdata)\nmodel6 <- lm(wellbeing ~ social_int + outdoor_time, data = wb_data)\n```\n\nFor each of the below pairs of models, what methods are/are not available for us to use for comparison and why?  \n\n+ `model1` vs `model2`\n+ `model2` vs `model3`\n+ `model1` vs `model4`\n+ `model3` vs `model5`\n+ `model2` vs `model6`\n\nThis flowchart might help you to reach your decision:\n\n```{r comparisons_chart, echo=FALSE, fig.align = 'left', out.width = \"100%\"}\nknitr::include_graphics(\"images/comparisons_chart.png\")\n```\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Hint \n\nYou may need to examine the dataset, and check for accuracy (e.g., are there any impossible / out of range values?) and completeness (e.g., are there any missing values?). \n\n:::\n\n`r qend()`\n\n`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`\n\n::: {.panel-tabset}\n\n## `model1` vs `model2`\n\n+ These models are nested - `model2` contains all the variables of `model1` and they are fitted on the same dataset.  \n+ We can therefore use an $F$-test or AIC and BIC.  \n    \n## `model2` vs `model3`\n\n+ These models are __not__ nested, but they are fitted on the same dataset.  \n+ We can therefore use AIC or BIC, but we cannot use an $F$-test.  \n    \n## `model1` vs `model4`\n\n+ These models are nested - `model4` contains all the variables of `model1` and they are fitted on the same dataset.  \n+ We can therefore use an $F$-test or AIC and BIC.  \n    \n## `model3` vs `model5`\n\n+ These models are __not__ nested, and they are __not__ fitted on the same dataset. The \"steps_k\" variable contains missing values (over 30% of the data is missing for this variable), and so these whole rows are excluded from `model5` (but they are included in `model3`). \n+ We cannot compare these models.   \n  \n## `model2` vs `model6`  \n\n+ These models are nested, but they are __not__ fitted on the same dataset: `model2` uses the 'mwdata' dataset, whilst `model6` uses the 'wb_data' dataset.\n+ We cannot compare these models.   \n\n:::\n\n`r solend()`\n\n<div class=\"divider div-transparent div-dot\"></div>\n\n# Compile Report\n\n`r qbegin(\"Compile Report\", qlabel = FALSE)`  \n\nKnit your report to PDF, and check over your work. To do so, you should make sure:\n\n- Only the output you want your reader to see is visible (e.g., do you want to hide your code?)\n- Check that the **tinytex** package is installed\n- Ensure that the ‘yaml’ (bit at the very top of your document) looks something like this:\n\n```{}\n---\ntitle: \"this is my report title\"\nauthor: \"B1234506\"\ndate: \"07/09/2024\"\noutput: bookdown::pdf_document2\n---\n```\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n# What to do if you cannot knit to PDF\nIf you are having issues knitting directly to PDF, try the following:  \n\n- Knit to HTML file  \n- Open your HTML in a web-browser (e.g. Chrome, Firefox)  \n- Print to PDF (Ctrl+P, then choose to save to PDF)  \n- Open file to check formatting\n:::\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n# Hiding Code and/or Output\n\n:::{.panel-tabset}\n## Hiding R Code\n\nTo not show the code of an R code chunk, and only show the output, write:\n\n````\n```{{r, echo=FALSE}}\n# code goes here\n```\n````\n\n## Hiding R Output\n\nTo show the code of an R code chunk, but hide the output, write:\n\n````\n```{{r, results='hide'}}\n# code goes here\n```\n````\n\n## Hiding R Code and Output\n\nTo hide both code and output of an R code chunk, write:\n\n````\n```{{r, include=FALSE}}\n# code goes here\n```\n````\n:::\n\n:::\n\n:::{.callout-tip appearance=\"simple\" collapse=\"true\"}\n\n### Tinytex\nYou must make sure you have **tinytex** installed in R so that you can “Knit” your Rmd document to a PDF file:\n\n```{r eval = FALSE}\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n```\n\n:::\n\n`r qend()`\n\n`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`\n\nYou should end up with a PDF file. If you have followed the above instructions and still have issues with knitting, speak with a Tutor. \n\n`r solend()`"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"include-in-header":["assets/toggling.html"],"number-sections":false,"output-file":"1_04_model_fit_comp.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","bibliography":["biblio.bib"],"toc_float":true,"code-annotations":"hover","link-citations":true,"theme":["united","assets/style-labs.scss"],"code-copy":false,"title":"Model Fit and Comparisons","params":{"SHOW_SOLS":true,"TOGGLE":true},"editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}