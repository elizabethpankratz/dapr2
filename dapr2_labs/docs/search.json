[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 2 (DAPR2) lab workbook. Using the menu above, you can find lab materials for each week."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr Umberto Noe\nDr Josiah King\nDr Emma Waterston\nDepartment of Psychology, The University of Edinburgh"
  },
  {
    "objectID": "1_02_categorical_data.html",
    "href": "1_02_categorical_data.html",
    "title": "Categorical data",
    "section": "",
    "text": "In the first five weeks of the course you should produce a PDF report using Rmarkdown for which you will receive formative feedback in week 6. Throughout those weeks, you will be working with a simulated dataset arising from a lexical decision task, adapted from Nordmann et al. (2022).\nThe report should not include any reference to R code or functions, but be written or a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\n\nLexical Decision Task. One hundred participants were asked to decide whether a presented word was a real word or a non-word. The data include 100 rows (one for each participant) and 7 variables:\n\n\n\n\nVariable Name\nDescription\n\n\n\nid\nParticipant ID\n\n\nage\nAge\n\n\nlanguage\nLanguage group (1 = monolingual, 2 = bilingual)\n\n\nrt_word\nReaction time (ms) for word trials\n\n\nrt_nonword\nReaction time (ms) for non-word trials\n\n\nacc_word\nAccuracy for word trials\n\n\nacc_nonword\nAccuracy for non-word trials\n\n\n\n\n\nYou can download the data from this link: https://uoepsy.github.io/data/ldt_data.csv.\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course:\nA1) Read the data into R and inspect it\n\n\n\n\n\n\nThis week’s task\n\n\n\nA2) Display and describe the categorical data\n\n\nA3) Display and describe the numerical data\nA4) Display and describe a relationship of interest between two or three variables of your choice\nThis week you will only focus on task A2. Below there are some guided sub-steps you may want to consider to complete task A2.\n\n\n\n\n\n\n\nNote\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as we will continue and build on last week’s work.1\nCreate a plot displaying the language distribution in the sample.2\nDescribe the language distribution in the sample.3\nCreate a new column showing the frequencies as percentages. Do you notice anything different with the actual frequencies? Why?4\nWhat is the mode of the distribution?5"
  },
  {
    "objectID": "1_02_categorical_data.html#worked-example",
    "href": "1_02_categorical_data.html#worked-example",
    "title": "Categorical data",
    "section": "Worked example",
    "text": "Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\ntips <- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   <dbl> 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    <dbl> 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit <chr> \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests <dbl> 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server <chr> \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip <dbl> 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\ntips$Day <- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit <- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server <- factor(tips$Server)\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   <dbl> 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    <dbl> 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit <fct> No, No, Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Guests <dbl> 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    <fct> Friday, Friday, Friday, Friday, Friday, Friday, Friday, Friday,…\n$ Server <fct> A, B, A, B, B, A, B, A, A, B, B, A, B, B, B, B, C, C, C, C, C, …\n$ PctTip <dbl> 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\ntips <- tips %>%\n    mutate(\n        PctTip = ifelse(PctTip > 100, NA, PctTip)\n    )\n\n\nplt_credit <- ggplot(tips, aes(x = Credit)) +\n    geom_bar() +\n    labs(x = \"Paid by credit card?\")\nplt_credit\n\n\n\n\nYou can even flip the bars if you wished to:\n\nggplot(tips, aes(x = Credit)) +\n    geom_bar() +\n    labs(x = \"Paid by credit card?\") +\n    coord_flip()\n\n\n\n\nYou can use the patchwork package to place graphs side by side. Simply create an object for each graph, and concatenate the objects with | for horizontal concatenation and / for vertical concatenation of graphs.\n\n\n\n\n\n\nRotate x-axis labels\n\n\n\nTo rotate x-axis labels, you can use this code: theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\nDon’t worry, no one remembers it, and people always google “rotate x-axis labels ggplot” to find it.\n\n\n\nlibrary(patchwork)\n\nplt1 <- ggplot(tips, aes(x = Credit)) +\n    geom_bar() +\n    labs(x = \"Paird by credit card?\")\n\nplt2 <- ggplot(tips, aes(x = Day)) +\n    geom_bar() +\n    labs(x = \"Day of week\") +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplt3 <- ggplot(tips, aes(x = Server)) +\n    geom_bar()\n\nplt1 | plt2 | plt3\n\n\n\n\n\n\nRun install.packages(\"patchwork\") first in your R console\n\ntbl_credit <- tips %>%\n    count(Credit) %>%\n    mutate(perc = round((n / sum(n)) * 100, 2))\ntbl_credit\n\n# A tibble: 2 × 3\n  Credit     n  perc\n  <fct>  <int> <dbl>\n1 No       106  67.5\n2 Yes       51  32.5\n\n\n\ntbl_day <- tips %>%\n    count(Day) %>%\n    mutate(perc = round((n / sum(n)) * 100, 2))\ntbl_day\n\n# A tibble: 5 × 3\n  Day           n  perc\n  <fct>     <int> <dbl>\n1 Monday       20 12.7 \n2 Tuesday      13  8.28\n3 Wednesday    62 39.5 \n4 Thursday     36 22.9 \n5 Friday       26 16.6 \n\n\n\ntbl_server <- tips %>%\n    count(Server) %>%\n    mutate(perc = round((n / sum(n)) * 100, 2))\ntbl_server\n\n# A tibble: 3 × 3\n  Server     n  perc\n  <fct>  <int> <dbl>\n1 A         60  38.2\n2 B         65  41.4\n3 C         32  20.4\n\n\nYou can create nice tables with the kable command or the pander command:\n\nlibrary(kableExtra)\n\nkable(tbl_credit) %>% kable_styling(full_width = FALSE)\n\n\n\n\n Credit \n    n \n    perc \n  \n\n\n No \n    106 \n    67.52 \n  \n\n Yes \n    51 \n    32.48 \n  \n\n\n\nkable(tbl_day) %>% kable_styling(full_width = FALSE)\n\n\n\n\n Day \n    n \n    perc \n  \n\n\n Monday \n    20 \n    12.74 \n  \n\n Tuesday \n    13 \n    8.28 \n  \n\n Wednesday \n    62 \n    39.49 \n  \n\n Thursday \n    36 \n    22.93 \n  \n\n Friday \n    26 \n    16.56 \n  \n\n\n\nkable(tbl_server) %>% kable_styling(full_width = FALSE)\n\n\n\n\n Server \n    n \n    perc \n  \n\n\n A \n    60 \n    38.22 \n  \n\n B \n    65 \n    41.40 \n  \n\n C \n    32 \n    20.38 \n  \n\n\n\nkable(list(tbl_credit, tbl_day, tbl_server)) %>%\n    kable_styling()\n\n\n\n \n\n\n\n Credit \n    n \n    perc \n  \n\n\n No \n    106 \n    67.52 \n  \n\n Yes \n    51 \n    32.48 \n  \n\n\n\n    \n\n\n\n Day \n    n \n    perc \n  \n\n\n Monday \n    20 \n    12.74 \n  \n\n Tuesday \n    13 \n    8.28 \n  \n\n Wednesday \n    62 \n    39.49 \n  \n\n Thursday \n    36 \n    22.93 \n  \n\n Friday \n    26 \n    16.56 \n  \n\n\n\n    \n\n\n\n Server \n    n \n    perc \n  \n\n\n A \n    60 \n    38.22 \n  \n\n B \n    65 \n    41.40 \n  \n\n C \n    32 \n    20.38 \n  \n\n\n\n  \n\n\nFrom the univariate distribution (or marginal distribution) of each categorical variable we see that the most common payment method was not a credit card, and the most common day of the week to dine at that restaurant was wednesday. Finally, the most common server among the parties was B.\n\n\nThe most common value is the mode."
  },
  {
    "objectID": "1_02_categorical_data.html#student-glossary",
    "href": "1_02_categorical_data.html#student-glossary",
    "title": "Categorical data",
    "section": "Student Glossary",
    "text": "Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse & Package\n\n\n\n%>%\n?\n\n\ngeom_bar\n?\n\n\ncount\n?\n\n\nmutate\n?\n\n\nsum\n?\n\n\nround\n?"
  },
  {
    "objectID": "1_01_design_and_data.html",
    "href": "1_01_design_and_data.html",
    "title": "Research design & data",
    "section": "",
    "text": "Did you register for RStudio Online?\n\n\n\nIf yes:\nYou can move on to the next tip.\nIf not, try this first:\n\nLog in to EASE https://www.ease.ed.ac.uk/ using your university UUN and password.\n\nIf you haven’t done it before: set your RStudio password here https://rstudio.ppls.ed.ac.uk/setpass.php, the username will be the same as your UUN (make sure you type your UUN correctly).\n\nAccess the server from https://rstudio.ppls.ed.ac.uk.\n\nIf that didn’t work:\nFill out this form to be added to the server, and wait for an email from us. This can take up to 3 working days.\n\n\n\n\n\n\n\n\nLab instructions\n\n\n\n\nPlease work through the lab exercises in groups of 4 students.\nOpen an Rmd file, and start writing your answers in there.\nYou will be given some data that you will use throughout the next 5 weeks to produce a data analysis report on those data.\n\nIn week 5, you will be asked to write up your explorations and produce a report in PDF format, which you will show to tutors in week 6 and receive formative feedback.\n\n\n\n\n\n\n\n\n\nLab help & support\n\n\n\nThe lab is structured to provide various levels of support. When attending the labs, you should directly attempt and work on the tasks. However, if you are unsure or stuck at any point, you can make use of the following help:\n\nSimply raise your hand and get help from a tutor\nHover your mouse on the superscript number to get a hint. The hints may sometimes show multiple equivalent ways of getting an answer - you just need one way\nScroll down to the Worked Example section, where you can read through a worked example.\n\n\n\n\n\n\n\n\n\nInstall tinytex\n\n\n\nBefore you begin, make sure you have tinytex installed in R so that you can “Knit” your Rmd document to a PDF file:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()"
  },
  {
    "objectID": "1_01_design_and_data.html#formative-report-a",
    "href": "1_01_design_and_data.html#formative-report-a",
    "title": "Research design & data",
    "section": "Formative report A",
    "text": "Formative report A\nIn the first five weeks of the course you should produce a PDF report using Rmarkdown for which you will receive formative feedback in week 6. Throughout those weeks, you will be working with a simulated dataset arising from a lexical decision task, adapted from Nordmann et al. (2022).\nThe report should not include any reference to R code or functions, but be written or a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\n\nData\nLexical Decision Task. One hundred participants were asked to decide whether a presented word was a real word or a non-word. The data include 100 rows (one for each participant) and 7 variables:\n\n\n\n\n\nVariable Name\nDescription\n\n\n\n\nid\nParticipant ID\n\n\nage\nAge\n\n\nlanguage\nLanguage group (1 = monolingual, 2 = bilingual)\n\n\nrt_word\nReaction time (ms) for word trials\n\n\nrt_nonword\nReaction time (ms) for non-word trials\n\n\nacc_word\nAccuracy for word trials\n\n\nacc_nonword\nAccuracy for non-word trials\n\n\n\n\n\nYou can download the data from this link: https://uoepsy.github.io/data/ldt_data.csv.\n\n\nTasks\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course:\n\n\n\n\n\n\nThis week’s task\n\n\n\nA1) Read the data into R and inspect it\n\n\nA2) Display and describe the categorical data\nA3) Display and describe the numerical data\nA4) Display and describe a relationship of interest between two or three variables of your choice\nThis week you will only focus on task A1. Below there are some guided sub-steps you may want to consider to complete task A1.\n\n\nA1 sub-tasks\n\n\n\n\n\n\nNote\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nRead the lexical decision task data into R and inspect at the data. By inspect, we literally mean to look at the data in RStudio. You don’t need to show the full data in the report.1\nHow many observations are there?2\nHow many variables are there?3\nWhat is the type of each variable?4\n\n\n\n\n\n\n\nThink about it\n\n\n\n\n\nWhat does dim(<dataset name>) return instead?\nWhat is the function of appending a [1] or [2]?\n\n\n\n\nWhat’s the minimum and maximum age in the sample? What about the average age?5\nDo you notice any issues with the ages? Look at your data and come up with a possible reason why the results are strange.6\nThe language variable appears to contain numbers. Do those numbers correspond to an amount or an attribute? Convert that variable to the appropriate type and provide more descriptive labels.7"
  },
  {
    "objectID": "1_01_design_and_data.html#worked-example",
    "href": "1_01_design_and_data.html#worked-example",
    "title": "Research design & data",
    "section": "Worked example",
    "text": "Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\n\n\ntips <- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  <dbl> <dbl> <chr>   <dbl> <chr> <chr>   <dbl>\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\n\n\nhead() shows by default the top 6 rows of the data. Use the n = ... option, e.g. head(<data>, n = 10) to change the default behaviour.\n\ndim(tips)\n\n[1] 157   7\n\n\n\n\nThis returns the number of rows and columns\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   <dbl> 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    <dbl> 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit <chr> \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests <dbl> 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server <chr> \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip <dbl> 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\n\nglimpse is part of the tidyverse package\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nAlternatives to glimpse are the data “structure” function:\n\nstr(tips)\n\nspec_tbl_df [157 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Bill  : num [1:157] 23.7 36.1 32 17.4 15.4 ...\n $ Tip   : num [1:157] 10 7 5.01 3.61 3 2.5 3.44 2.42 3 2 ...\n $ Credit: chr [1:157] \"n\" \"n\" \"y\" \"y\" ...\n $ Guests: num [1:157] 2 3 2 2 2 2 2 2 2 2 ...\n $ Day   : chr [1:157] \"f\" \"f\" \"f\" \"f\" ...\n $ Server: chr [1:157] \"A\" \"B\" \"A\" \"B\" ...\n $ PctTip: num [1:157] 42.2 19.4 15.7 20.8 19.5 13.4 16 12.4 12.7 10.7 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Bill = col_double(),\n  ..   Tip = col_double(),\n  ..   Credit = col_character(),\n  ..   Guests = col_double(),\n  ..   Day = col_character(),\n  ..   Server = col_character(),\n  ..   PctTip = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nor:\n\nsapply(tips, data.class)\n\n       Bill         Tip      Credit      Guests         Day      Server \n  \"numeric\"   \"numeric\" \"character\"   \"numeric\" \"character\" \"character\" \n     PctTip \n  \"numeric\" \n\n\n\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nThe data contains measurements for a total of 157 parties on four numeric variables: size of the bill (in dollars), size of the tip, number of guests in the group, and tip as a percentage of the bill total. The data also includes three categorical variables indicating whether or not the party paid with a credit card, the day of the week, as well as a server-specific identifier.\n\n\n\nsummary(tips)\n\n      Bill            Tip            Credit              Guests     \n Min.   : 1.66   Min.   : 0.250   Length:157         Min.   :1.000  \n 1st Qu.:15.19   1st Qu.: 2.075   Class :character   1st Qu.:2.000  \n Median :20.22   Median : 3.340   Mode  :character   Median :2.000  \n Mean   :22.73   Mean   : 3.807                      Mean   :2.096  \n 3rd Qu.:28.84   3rd Qu.: 5.000                      3rd Qu.:2.000  \n Max.   :70.51   Max.   :15.000                      Max.   :7.000  \n                 NA's   :1                                          \n     Day               Server              PctTip      \n Length:157         Length:157         Min.   :  6.70  \n Class :character   Class :character   1st Qu.: 14.30  \n Mode  :character   Mode  :character   Median : 16.20  \n                                       Mean   : 17.89  \n                                       3rd Qu.: 18.20  \n                                       Max.   :221.00  \n                                                       \n\n\n\n\nsummary returns a quick summary of the data.\nYou probably won’t understand some parts of the output above, but we will learn more in the coming weeks, so don’t worry too much about it. For the moment, you should be able to understand the minimum, maximum, and the mean.\nCurrently, it is not showing very informative output for the categorical variables.\nWe can replace each factor level with a clearer label:\n\ntips$Day <- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit <- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server <- factor(tips$Server)\n\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nUsing tidyverse:\n\ntips <- tips %>%\n    mutate(\n        Day = factor(Day,\n                     levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                     labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")),\n        Credit = factor(Credit,\n                        levels = c(\"n\", \"y\"),\n                        labels = c(\"No\", \"Yes\")),\n        Server = factor(Server)\n    )\n\nThe functions %>% and mutate are part of the tidyverse package. The former, %>%, is called pipe.\n\n\n\nLet’s check the result of the changes to the variable types:\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   <dbl> 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    <dbl> 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit <fct> No, No, Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Guests <dbl> 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    <fct> Friday, Friday, Friday, Friday, Friday, Friday, Friday, Friday,…\n$ Server <fct> A, B, A, B, B, A, B, A, A, B, B, A, B, B, B, B, C, C, C, C, C, …\n$ PctTip <dbl> 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\nsummary(tips)\n\n      Bill            Tip         Credit        Guests             Day    \n Min.   : 1.66   Min.   : 0.250   No :106   Min.   :1.000   Monday   :20  \n 1st Qu.:15.19   1st Qu.: 2.075   Yes: 51   1st Qu.:2.000   Tuesday  :13  \n Median :20.22   Median : 3.340             Median :2.000   Wednesday:62  \n Mean   :22.73   Mean   : 3.807             Mean   :2.096   Thursday :36  \n 3rd Qu.:28.84   3rd Qu.: 5.000             3rd Qu.:2.000   Friday   :26  \n Max.   :70.51   Max.   :15.000             Max.   :7.000                 \n                 NA's   :1                                                \n Server     PctTip      \n A:60   Min.   :  6.70  \n B:65   1st Qu.: 14.30  \n C:32   Median : 16.20  \n        Mean   : 17.89  \n        3rd Qu.: 18.20  \n        Max.   :221.00  \n                        \n\n\n\n\nAfter making categorical variables factors, summary shows the count of each category for the categorical variables.\nThe percentage of total bill has a maximum value of 221, which seems very strange. Someone is very unlikely to tip more than their bill total. In this case 221% of their bill value seems unlikely.\nLet’s inspect the row where PctTip is greater than 100:\n\ntips[tips$PctTip > 100, ]\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  <dbl> <dbl> <fct>   <dbl> <fct>    <fct>   <dbl>\n1  49.6    NA Yes         4 Thursday C         221\n\n\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nAlternatively, using tidyverse:\n\ntips %>% \n    filter(PctTip > 100)\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  <dbl> <dbl> <fct>   <dbl> <fct>    <fct>   <dbl>\n1  49.6    NA Yes         4 Thursday C         221\n\n\n\n\n\nWith a bill of 49.59, the tip would be 109.59 dollars:\n\n49.59 * 221 / 100\n\n[1] 109.5939\n\n\nFurthermore, we also notice that the tipping amount is not available (NA). The corresponding value in the percentage of total tip seems likely an inputting error, perhaps due to double typing the leading 2 when recording the data. We will set that value to not available (NA) with the following code:\n\ntips$PctTip[tips$PctTip > 100] <- NA\n\n\n\na > b tests whether a is greater than b. a < b tests whether a is smaller than b. a == b tests whether a is equal to b; notice the double equal sign! You can also use >= or <=\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nAlternatively you can use tidyverse:\n\ntips <- tips %>%\n    mutate(\n        PctTip = ifelse(PctTip > 100, NA, PctTip)\n    )\n\n\n\n\n\nsummary(tips)\n\n      Bill            Tip         Credit        Guests             Day    \n Min.   : 1.66   Min.   : 0.250   No :106   Min.   :1.000   Monday   :20  \n 1st Qu.:15.19   1st Qu.: 2.075   Yes: 51   1st Qu.:2.000   Tuesday  :13  \n Median :20.22   Median : 3.340             Median :2.000   Wednesday:62  \n Mean   :22.73   Mean   : 3.807             Mean   :2.096   Thursday :36  \n 3rd Qu.:28.84   3rd Qu.: 5.000             3rd Qu.:2.000   Friday   :26  \n Max.   :70.51   Max.   :15.000             Max.   :7.000                 \n                 NA's   :1                                                \n Server     PctTip     \n A:60   Min.   : 6.70  \n B:65   1st Qu.:14.30  \n C:32   Median :16.15  \n        Mean   :16.59  \n        3rd Qu.:18.05  \n        Max.   :42.20  \n        NA's   :1      \n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nThe average bill size was $22.73, and the average tip was $3.85, corresponding to roughly 17% of the total bill. Out of 157 parties, only 51 paid with a credit card. Most parties tended to be of around 2 people each, and people tended to go to that restaurant more often on Wednesday. Among the three servers, server C was the one that served the least number of parties. The data also included a missing tipping value, corresponding to a bill $49.59, and a data inputting error for the corresponding measure of the tip as a percentage of the total bill."
  },
  {
    "objectID": "1_01_design_and_data.html#student-glossary",
    "href": "1_01_design_and_data.html#student-glossary",
    "title": "Research design & data",
    "section": "Student Glossary",
    "text": "Student Glossary\nTo conclude the lab, create a glossary of R functions. You can do so by opening Microsoft Word, Excel, or OneNote and creating a table with two columns: one where you should write the name of an R function, and the other column where you should provide a brief description of what the function does.\nThis “do it yourself” glossary is an opportunity for you to revise what you have learned in today’s lab and write down a few take-home messages. You will find this glossary handy as a reference to keep next to you when you will be doing the assessed weekly quizzes.\nBelow you can find an example to get you started:\n\n\n\n\n\n\n\nFunction\nUse & Package\n\n\n\n\nread_csv\nFor reading comma separated value files. Part of tidyverse package\n\n\nView\n?\n\n\nhead\n?\n\n\nnrow\n?\n\n\nncol\n?\n\n\ndim\n?\n\n\nglimpse\n?\n\n\nstr\n?\n\n\nsummary\n?\n\n\nfactor\n?"
  },
  {
    "objectID": "1_01_func+models_practice.html",
    "href": "1_01_func+models_practice.html",
    "title": "Functions and Models",
    "section": "",
    "text": "At the end of this lab, you will:\n\nHave reviewed the main concepts from introductory statistics.\nUnderstand the concept of a function.\nBe able to discuss what a statistical model is.\nUnderstand the link between models and functions.\n\n\n\n\n\nHave attended and/or watched Week 1 lectures.\nIf new to the course, have registered for the RStudio PPLS Server\n\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nggExtra\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/handheight.csv."
  },
  {
    "objectID": "1_01_func+models_practice.html#functions-and-mathematical-models-plots",
    "href": "1_01_func+models_practice.html#functions-and-mathematical-models-plots",
    "title": "Functions and Models",
    "section": "Functions and mathematical models: Plots",
    "text": "Functions and mathematical models: Plots\n\nQuestion 4\n\n\nCreate a data set called squares containing the perimeter of four squares having sides of length \\(0, 2, 5, 9\\) metres, and then plot the squares data as points\nHint: Remember that to combine multiple numbers together we use the function c().\n\n\n\n\n Solution \n\n\n\n#Load tidyverse if not already done so\nlibrary(tidyverse)\n\n#Create 'squares' dataset using tibble()\nsquares <- tibble(\n  side = c(0, 2, 5, 9),\n  perimeter = 4 * side\n)\n\nsquares\n\n# A tibble: 4 × 2\n   side perimeter\n  <dbl>     <dbl>\n1     0         0\n2     2         8\n3     5        20\n4     9        36\n\n\n\n#Create Plot\nggplot(data = squares, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nGenerate one hundred data points, and use them to visualise the relationship between side and perimeter of squares. To do so, you need to complete four steps:\n\nCreate a sequence of one hundred side lengths (x) going from 0 to 3 metres.\nCompute the corresponding perimeters (y).\nPlot the side and perimeter data as points on a graph.\nVisualise the functional relationship between side and perimeter of squares. To do so, use the function geom_line() to connect the computed points with lines.\n\n\n\n\n\n Solution \n\n\nLet’s start by creating the side and perimeter data:\n\n#step 1 & 2\nsquares_grid <- tibble(\n  side = seq(0, 3, length.out = 100),\n  perimeter = 4 * side\n)\n\nPlot the individual points:\n\n# step 3\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4 Side')\n\n\n\n\nVisualise the functional relationship by connecting the individual points with lines:\n\n#step 4\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_line(colour = 'blue') +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4 Side')\n\n\n\n\n\n\n\nThe function \\(y = 4 \\ x\\) that you plotted above is an example of a function representing a mathematical model.\nWe typically validate a model using experimental data. However, we all know how squares work and that two squares with the same side will have the same perimeter (more on this later).\n\nQuestion 6\n\n\nThe Scottish National Gallery kindly provided us with measurements of side and perimeter (in metres) for a sample of 10 square paintings.\nThe data are provided below:\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nPlot the mathematical model of the relationship between side and perimeter for squares, and superimpose on top the experimental data from the Scottish National Gallery.\n\n\n\n\n Solution \n\n\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nggplot() +\n  geom_line(data = squares_grid, aes(x = side, y = perimeter), colour = 'blue') +\n  geom_point(data = sng, aes(x = side, y = perimeter), colour = 'black', \n             alpha = 0.5, size = 3) +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\n\nThe exact relationship between side and perimeter of squares.\n\n\n\n\nThe above plot shows perfect agreement between the observed data and the model.\n\n\n\n\nQuestion 7\n\n\nUse the mathematical model to predict the perimeter of a painting with a side of 1.5 metres.\n\n\n\n\n Solution \n\n\nWe do not have a painting with a side of 1.5 metres within the random sample of paintings from the Scottish National Gallery. However, we can predict the perimeter of an unobserved squared painting having a 1.5 metre side using the mathematical model.\nYou can obtained this prediction either using a visual approach or an algebraic one.\n Visual approach\n\n\n\n\n\nSometimes we can directly read a predicted value from the graph of the functional relationship.\nConsider the plot created in the previous question. First, we need to check where x = 1.5. Then, we draw a vertical dashed line until it meets the blue line. The y value corresponding to x = 1.5 can be read off the y-axis.\nHowever, in this case it is not that easy to read it from the drawing… Let’s try the next approach.\n Algebraic approach\nYou can substitute the x value in the formula and calculate the corresponding y value.\n\\[\ny = 4 * x = 4 * 1.5 = 6\n\\]\n We might write this up as:\n\nThe predicted perimeter of squared paintings having a 1.5m side is 6m.\n\nNOTE: Don’t forget to always include the measurement units when reporting/writing-up results!"
  },
  {
    "objectID": "1_01_func+models_practice.html#study-overview",
    "href": "1_01_func+models_practice.html#study-overview",
    "title": "Functions and Models",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Question\nHow does handspan vary as a function of height?\n\nConsider now the relationship between height (in inches) and handspan (in cm). Researchers provides data for a sample of 167 students which reported their height and handspan as part of a class survey.\nUsing the handheight data you already loaded at the start of the lab, your task is to investigate how handspan varies as a function of height for the students in the sample.\n\n Handheight codebook.\n\n\nDescription\nThe data set records the height and handspan reported by a random sample of 167 students as part of a class survey (Utts2015?).\nThe variables are:\n\nheight, measured in inches\nhandspan, measured in centimetres\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n \n  \n    height \n    handspan \n  \n \n\n  \n    68 \n    21.5 \n  \n  \n    71 \n    23.5 \n  \n  \n    73 \n    22.5 \n  \n  \n    64 \n    18.0 \n  \n  \n    68 \n    23.5 \n  \n  \n    59 \n    20.0 \n  \n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nUsing a scatterplot (since the variables are numeric and continuous) to visualise the relationship between the two numeric variables, comment on any main differences you notice with the relationship between side and perimeter of squares. Note if you detected outliers or points that do not fit with the pattern in the rest of the data.\n\n\n\n\n Solution \n\n\n\nplt <- ggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\nplt\n\n\n\n\nWe can also add marginal boxplots for each variable using the package ggExtra.\n\nggMarginal(plt, type = 'boxplot')\n\n\n\n\nThe statistical relationship between height and handspan.\n\n\n\n\nOutliers are extreme observations that are not possible values of a variable or that do not seem to fit with the rest of the data. This could either be:\n\nmarginally along one axis: points that have an unusual (too high or too low) x-coordinate or y-coordinate;\njointly: observations that do not fit with the rest of the point cloud.\n\nThe boxplots in Figure @ref(fig:handheight-scatterplot) do not highlight any outliers in the marginal distributions of height and handspan. Furthermore, from the scatterplot we do not notice any extreme observations or points that do not fit with the rest of the point cloud.\nWe notice a moderate, positive linear relationship between height and handspan.\nRecall Figure @ref(fig:squares-scatterplot), displaying the relationship between side and perimeters of squares. In the plot we notice two points on top of each other, reflecting the fact that two squares having the same side will always have the same perimeter. In fact, the data from the Scottish National Gallery include two squared paintings with a side of 1.1m, both having a measured perimeter of 4.4m.\nFigure @ref(fig:handheight-scatterplot), instead, displays the relationship between height and handspan of a sample of students. The first thing that grabs our attention is the fact that students having the same height do not necessarily have the same handspan. Rather, we clearly see a variety of handspan values for students all having a height of, for example, 70in. To be more precise, the seven students who are 70 in. tall all have differing handspans.\n\n\n\n\nQuestion 9\n\n\nUsing the following command, superimpose on top of the scatterplot a best-fit line describing how handspan varies as a function of height. For the moment, the argument se = FALSE tells R to not display uncertainty bands.\ngeom_smooth(method = lm, se = FALSE)\nComment on any differences between the lines representing the linear relationship between (a) the side and perimeter of square and (b) height and handspan.\n\n\n\n\n Solution \n\n\n\nggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_smooth(method = lm, se = FALSE) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\n\n\n\nThe best-fit line.\n\n\n\n\nThe line representing the relationship between side and perimeter of squares is able to predict the actual perimeter value from the measurement of the side of a square. This is possible because the relationship between side and perimeter is an exact one. That is, any squares having the same side will have the same perimeter, and there will be no variation in those values.\nThe line that best fits the relationship between height and handspan, instead, is only able to predict the average handspan for a given value of height. This is because there will be a distribution of handspans at each value of height. The line will fit the trend/pattern in the values, but there will be individual-to-individual variability that we must accept around that average pattern.\n\n\n\n\nQuestion 10\n\n\n\n\n\nThe line of best-fit is given by:1\n\\[\n\\widehat{Handspan} = -3 + 0.35 \\ Height\n\\]\nCalculate the predicted handspan of a student who is (a) 73in tall, and (b) 5in tall.\n\n\n\n\n Solution \n\n\n\nThe predicted average handspan for students who are 73in tall is \\(-3 + 0.35 * 73 = 22.55\\)cm.\nThe predicted average handspan for students who are 5in tall is \\(-3 + 0.35 * 5 = -1.25\\)cm.\n\nBut wait, handspan can not be negative… This does not make any sense! That’s right, we went too far off the range of the available data on heights, which were between 57in and 78in. We extrapolated. This is very dangerous…\n\n\n\n\n\nSource: Randall Munroe, xkcd.com"
  },
  {
    "objectID": "1_01.html",
    "href": "1_01.html",
    "title": "1_01",
    "section": "",
    "text": "At the end of this lab, you will:\n\nHave reviewed the main concepts from introductory statistics.\nUnderstand the concept of a function.\nBe able to discuss what a statistical model is.\nUnderstand the link between models and functions.\n\n\n\n\n\nHave attended and/or watched Week 1 lectures.\nIf new to the course, have registered for the RStudio PPLS Server\n\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nggExtra\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/handheight.csv."
  },
  {
    "objectID": "1_01.html#functions-and-mathematical-models-plots",
    "href": "1_01.html#functions-and-mathematical-models-plots",
    "title": "1_01",
    "section": "Functions and mathematical models: Plots",
    "text": "Functions and mathematical models: Plots\n\nQuestion 4\n\n\nCreate a data set called squares containing the perimeter of four squares having sides of length \\(0, 2, 5, 9\\) metres, and then plot the squares data as points\nHint: Remember that to combine multiple numbers together we use the function c().\n\n\n\n\n Solution \n\n\n\n#Load tidyverse if not already done so\nlibrary(tidyverse)\n\n#Create 'squares' dataset using tibble()\nsquares <- tibble(\n  side = c(0, 2, 5, 9),\n  perimeter = 4 * side\n)\n\nsquares\n\n# A tibble: 4 × 2\n   side perimeter\n  <dbl>     <dbl>\n1     0         0\n2     2         8\n3     5        20\n4     9        36\n\n\n\n#Create Plot\nggplot(data = squares, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nGenerate one hundred data points, and use them to visualise the relationship between side and perimeter of squares. To do so, you need to complete four steps:\n\nCreate a sequence of one hundred side lengths (x) going from 0 to 3 metres.\nCompute the corresponding perimeters (y).\nPlot the side and perimeter data as points on a graph.\nVisualise the functional relationship between side and perimeter of squares. To do so, use the function geom_line() to connect the computed points with lines.\n\n\n\n\n\n Solution \n\n\nLet’s start by creating the side and perimeter data:\n\n#step 1 & 2\nsquares_grid <- tibble(\n  side = seq(0, 3, length.out = 100),\n  perimeter = 4 * side\n)\n\nPlot the individual points:\n\n# step 3\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4 Side')\n\n\n\n\nVisualise the functional relationship by connecting the individual points with lines:\n\n#step 4\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_line(colour = 'blue') +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4 Side')\n\n\n\n\n\n\n\nThe function \\(y = 4 \\ x\\) that you plotted above is an example of a function representing a mathematical model.\nWe typically validate a model using experimental data. However, we all know how squares work and that two squares with the same side will have the same perimeter (more on this later).\n\nQuestion 6\n\n\nThe Scottish National Gallery kindly provided us with measurements of side and perimeter (in metres) for a sample of 10 square paintings.\nThe data are provided below:\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nPlot the mathematical model of the relationship between side and perimeter for squares, and superimpose on top the experimental data from the Scottish National Gallery.\n\n\n\n\n Solution \n\n\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nggplot() +\n  geom_line(data = squares_grid, aes(x = side, y = perimeter), colour = 'blue') +\n  geom_point(data = sng, aes(x = side, y = perimeter), colour = 'black', \n             alpha = 0.5, size = 3) +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\n\nThe exact relationship between side and perimeter of squares.\n\n\n\n\nThe above plot shows perfect agreement between the observed data and the model.\n\n\n\n\nQuestion 7\n\n\nUse the mathematical model to predict the perimeter of a painting with a side of 1.5 metres.\n\n\n\n\n Solution \n\n\nWe do not have a painting with a side of 1.5 metres within the random sample of paintings from the Scottish National Gallery. However, we can predict the perimeter of an unobserved squared painting having a 1.5 metre side using the mathematical model.\nYou can obtained this prediction either using a visual approach or an algebraic one.\n Visual approach\n\n\n\n\n\nSometimes we can directly read a predicted value from the graph of the functional relationship.\nConsider the plot created in the previous question. First, we need to check where x = 1.5. Then, we draw a vertical dashed line until it meets the blue line. The y value corresponding to x = 1.5 can be read off the y-axis.\nHowever, in this case it is not that easy to read it from the drawing… Let’s try the next approach.\n Algebraic approach\nYou can substitute the x value in the formula and calculate the corresponding y value.\n\\[\ny = 4 * x = 4 * 1.5 = 6\n\\]\n We might write this up as:\n\nThe predicted perimeter of squared paintings having a 1.5m side is 6m.\n\nNOTE: Don’t forget to always include the measurement units when reporting/writing-up results!"
  },
  {
    "objectID": "1_01.html#study-overview",
    "href": "1_01.html#study-overview",
    "title": "1_01",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Question\nHow does handspan vary as a function of height?\n\nConsider now the relationship between height (in inches) and handspan (in cm). Researchers provides data for a sample of 167 students which reported their height and handspan as part of a class survey.\nUsing the handheight data you already loaded at the start of the lab, your task is to investigate how handspan varies as a function of height for the students in the sample.\n\n Handheight codebook.\n\n\nDescription\nThe data set records the height and handspan reported by a random sample of 167 students as part of a class survey (Utts2015?).\nThe variables are:\n\nheight, measured in inches\nhandspan, measured in centimetres\n\nPreview\nThe first six rows of the data are:\n\n\n\n\n \n  \n    height \n    handspan \n  \n \n\n  \n    68 \n    21.5 \n  \n  \n    71 \n    23.5 \n  \n  \n    73 \n    22.5 \n  \n  \n    64 \n    18.0 \n  \n  \n    68 \n    23.5 \n  \n  \n    59 \n    20.0 \n  \n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nUsing a scatterplot (since the variables are numeric and continuous) to visualise the relationship between the two numeric variables, comment on any main differences you notice with the relationship between side and perimeter of squares. Note if you detected outliers or points that do not fit with the pattern in the rest of the data.\n\n\n\n\n Solution \n\n\n\nplt <- ggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\nplt\n\n\n\n\nWe can also add marginal boxplots for each variable using the package ggExtra.\n\nggMarginal(plt, type = 'boxplot')\n\n\n\n\nThe statistical relationship between height and handspan.\n\n\n\n\nOutliers are extreme observations that are not possible values of a variable or that do not seem to fit with the rest of the data. This could either be:\n\nmarginally along one axis: points that have an unusual (too high or too low) x-coordinate or y-coordinate;\njointly: observations that do not fit with the rest of the point cloud.\n\nThe boxplots in Figure @ref(fig:handheight-scatterplot) do not highlight any outliers in the marginal distributions of height and handspan. Furthermore, from the scatterplot we do not notice any extreme observations or points that do not fit with the rest of the point cloud.\nWe notice a moderate, positive linear relationship between height and handspan.\nRecall Figure @ref(fig:squares-scatterplot), displaying the relationship between side and perimeters of squares. In the plot we notice two points on top of each other, reflecting the fact that two squares having the same side will always have the same perimeter. In fact, the data from the Scottish National Gallery include two squared paintings with a side of 1.1m, both having a measured perimeter of 4.4m.\nFigure @ref(fig:handheight-scatterplot), instead, displays the relationship between height and handspan of a sample of students. The first thing that grabs our attention is the fact that students having the same height do not necessarily have the same handspan. Rather, we clearly see a variety of handspan values for students all having a height of, for example, 70in. To be more precise, the seven students who are 70 in. tall all have differing handspans.\n\n\n\n\nQuestion 9\n\n\nUsing the following command, superimpose on top of the scatterplot a best-fit line describing how handspan varies as a function of height. For the moment, the argument se = FALSE tells R to not display uncertainty bands.\ngeom_smooth(method = lm, se = FALSE)\nComment on any differences between the lines representing the linear relationship between (a) the side and perimeter of square and (b) height and handspan.\n\n\n\n\n Solution \n\n\n\nggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_smooth(method = lm, se = FALSE) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\n\n\n\nThe best-fit line.\n\n\n\n\nThe line representing the relationship between side and perimeter of squares is able to predict the actual perimeter value from the measurement of the side of a square. This is possible because the relationship between side and perimeter is an exact one. That is, any squares having the same side will have the same perimeter, and there will be no variation in those values.\nThe line that best fits the relationship between height and handspan, instead, is only able to predict the average handspan for a given value of height. This is because there will be a distribution of handspans at each value of height. The line will fit the trend/pattern in the values, but there will be individual-to-individual variability that we must accept around that average pattern.\n\n\n\n\nQuestion 10\n\n\n\n\n\nThe line of best-fit is given by:1\n\\[\n\\widehat{Handspan} = -3 + 0.35 \\ Height\n\\]\nCalculate the predicted handspan of a student who is (a) 73in tall, and (b) 5in tall.\n\n\n\n\n Solution \n\n\n\nThe predicted average handspan for students who are 73in tall is \\(-3 + 0.35 * 73 = 22.55\\)cm.\nThe predicted average handspan for students who are 5in tall is \\(-3 + 0.35 * 5 = -1.25\\)cm.\n\nBut wait, handspan can not be negative… This does not make any sense! That’s right, we went too far off the range of the available data on heights, which were between 57in and 78in. We extrapolated. This is very dangerous…\n\n\n\n\n\nSource: Randall Munroe, xkcd.com"
  },
  {
    "objectID": "index.html#solutions",
    "href": "index.html#solutions",
    "title": "Home",
    "section": "Solutions",
    "text": "Solutions\nSolutions will be made available immediately below each exercise. Do make sure to attempt each question before checking the solution."
  },
  {
    "objectID": "index.html#asking-questions",
    "href": "index.html#asking-questions",
    "title": "Home",
    "section": "Asking Questions",
    "text": "Asking Questions\nWe encourage you to use the various support options, details of which can be found on the Course Learn Page."
  },
  {
    "objectID": "index.html#tips-on-googling-statistics-and-r",
    "href": "index.html#tips-on-googling-statistics-and-r",
    "title": "Home",
    "section": "Tips on Googling Statistics and R",
    "text": "Tips on Googling Statistics and R\nSearching online for help with statistics and R can be both a help and a hindrance. If you have an error message in R, copy the error message into Google. The results returned can sometimes just cause more confusion, but sometimes something might jump out at you and help you solve the problem. The same applies with searching the internet for help with statistics - search for “what is a p-value”, and you’ll find many many different articles and forum discussions etc. Some of them you will find too technical, but don’t be scared - the vast majority of people work in statistics will find these too technical too. Some of them you might feel are too simple/not helpful. As a general guide, keep clicking around the search responses, and you may end up finding that someone, somewhere, has provided an explanation at the right level. If you find something during your search which you don’t quite understand, feel free to link it in a post on the discussion forum!"
  },
  {
    "objectID": "index.html#feedback-on-labs",
    "href": "index.html#feedback-on-labs",
    "title": "Home",
    "section": "Feedback on Labs",
    "text": "Feedback on Labs\nIf you wish to make suggestions for improvements to these workbooks, please email ppls.psych.stats@ed.ac.uk making sure to include the course name in the subject."
  },
  {
    "objectID": "1_01_function.html",
    "href": "1_01_function.html",
    "title": "Functions and Models",
    "section": "",
    "text": "At the end of this lab, you will:\n\nHave reviewed the main concepts from introductory statistics.\nUnderstand the concept of a function.\nBe able to discuss what a statistical model is.\nUnderstand the link between models and functions.\n\n\n\n\n\nHave attended and/or watched Week 1 lectures.\nIf new to the course, have registered for the RStudio PPLS Server\n\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nggExtra\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/handheight.csv."
  },
  {
    "objectID": "1_01_function.html#functions-and-mathematical-models-plots",
    "href": "1_01_function.html#functions-and-mathematical-models-plots",
    "title": "Functions and Models",
    "section": "Functions and mathematical models: Plots",
    "text": "Functions and mathematical models: Plots\n\nQuestion 4\n\n\nCreate a data set called squares containing the perimeter of four squares having sides of length \\(0, 2, 5, 9\\) metres, and then plot the squares data as points\nHint: Remember that to combine multiple numbers together we use the function c().\n\n\n\n\n Solution \n\n\n\n#Load tidyverse if not already done so\nlibrary(tidyverse)\n\n#Create 'squares' dataset using tibble()\nsquares <- tibble(\n  side = c(0, 2, 5, 9),\n  perimeter = 4 * side\n)\n\nsquares\n\n# A tibble: 4 × 2\n   side perimeter\n  <dbl>     <dbl>\n1     0         0\n2     2         8\n3     5        20\n4     9        36\n\n\n\n#Create Plot\nggplot(data = squares, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nGenerate one hundred data points, and use them to visualise the relationship between side and perimeter of squares. To do so, you need to complete four steps:\n\nCreate a sequence of one hundred side lengths (x) going from 0 to 3 metres.\nCompute the corresponding perimeters (y).\nPlot the side and perimeter data as points on a graph.\nVisualise the functional relationship between side and perimeter of squares. To do so, use the function geom_line() to connect the computed points with lines.\n\n\n\n\n\n Solution \n\n\nLet’s start by creating the side and perimeter data:\n\n#step 1 & 2\nsquares_grid <- tibble(\n  side = seq(0, 3, length.out = 100),\n  perimeter = 4 * side\n)\n\nPlot the individual points:\n\n# step 3\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_point() +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4 Side')\n\n\n\n\nVisualise the functional relationship by connecting the individual points with lines:\n\n#step 4\nggplot(data = squares_grid, aes(x = side, y = perimeter)) +\n  geom_line(colour = 'blue') +\n  labs(x = 'Side (m)', y = 'Perimeter (m)', title = 'Perimeter = 4 Side')\n\n\n\n\n\n\n\nThe function \\(y = 4 \\ x\\) that you plotted above is an example of a function representing a mathematical model.\nWe typically validate a model using experimental data. However, we all know how squares work and that two squares with the same side will have the same perimeter (more on this later).\n\nQuestion 6\n\n\nThe Scottish National Gallery kindly provided us with measurements of side and perimeter (in metres) for a sample of 10 square paintings.\nThe data are provided below:\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nPlot the mathematical model of the relationship between side and perimeter for squares, and superimpose on top the experimental data from the Scottish National Gallery.\n\n\n\n\n Solution \n\n\n\nsng <- tibble(\n  side = c(1.3, 0.75, 2, 0.5, 0.3, 1.1, 2.3, 0.85, 1.1, 0.2),\n  perimeter = c(5.2, 3.0, 8.0, 2.0, 1.2, 4.4, 9.2, 3.4, 4.4, 0.8)\n)\n\nggplot() +\n  geom_line(data = squares_grid, aes(x = side, y = perimeter), colour = 'blue') +\n  geom_point(data = sng, aes(x = side, y = perimeter), colour = 'black', \n             alpha = 0.5, size = 3) +\n  labs(x = 'Side (m)', y = 'Perimeter (m)')\n\n\n\n\nThe exact relationship between side and perimeter of squares.\n\n\n\n\nThe above plot shows perfect agreement between the observed data and the model.\n\n\n\n\nQuestion 7\n\n\nUse the mathematical model to predict the perimeter of a painting with a side of 1.5 metres.\n\n\n\n\n Solution \n\n\nWe do not have a painting with a side of 1.5 metres within the random sample of paintings from the Scottish National Gallery. However, we can predict the perimeter of an unobserved squared painting having a 1.5 metre side using the mathematical model.\nYou can obtained this prediction either using a visual approach or an algebraic one.\n Visual approach\n\n\n\n\n\nSometimes we can directly read a predicted value from the graph of the functional relationship.\nConsider the plot created in the previous question. First, we need to check where x = 1.5. Then, we draw a vertical dashed line until it meets the blue line. The y value corresponding to x = 1.5 can be read off the y-axis.\nHowever, in this case it is not that easy to read it from the drawing… Let’s try the next approach.\n Algebraic approach\nYou can substitute the x value in the formula and calculate the corresponding y value.\n\\[\ny = 4 * x = 4 * 1.5 = 6\n\\]\n We might write this up as:\n\nThe predicted perimeter of squared paintings having a 1.5m side is 6m.\n\nNOTE: Don’t forget to always include the measurement units when reporting/writing-up results!"
  },
  {
    "objectID": "1_01_function.html#study-overview",
    "href": "1_01_function.html#study-overview",
    "title": "Functions and Models",
    "section": "Study Overview",
    "text": "Study Overview\n\nResearch Question\nHow does handspan vary as a function of height?\n\nConsider now the relationship between height (in inches) and handspan (in cm). Researchers provides data for a sample of 167 students which reported their height and handspan as part of a class survey.\nUsing the handheight data you already loaded at the start of the lab, your task is to investigate how handspan varies as a function of height for the students in the sample.\n\n Handheight codebook.\n\n\nDescription\nThe data set records the height and handspan reported by a random sample of 167 students as part of a class survey (Utts2015?).\nThe variables are:\n\nheight, measured in inches\nhandspan, measured in centimetres\n\nPreview\nThe first six rows of the data are:\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\nRows: 167 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): height, handspan\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n \n  \n    height \n    handspan \n  \n \n\n  \n    68 \n    21.5 \n  \n  \n    71 \n    23.5 \n  \n  \n    73 \n    22.5 \n  \n  \n    64 \n    18.0 \n  \n  \n    68 \n    23.5 \n  \n  \n    59 \n    20.0 \n  \n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\nUsing a scatterplot (since the variables are numeric and continuous) to visualise the relationship between the two numeric variables, comment on any main differences you notice with the relationship between side and perimeter of squares. Note if you detected outliers or points that do not fit with the pattern in the rest of the data.\n\n\n\n\n Solution \n\n\n\nplt <- ggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\nplt\n\n\n\n\nWe can also add marginal boxplots for each variable using the package ggExtra.\n\nggMarginal(plt, type = 'boxplot')\n\n\n\n\nThe statistical relationship between height and handspan.\n\n\n\n\nOutliers are extreme observations that are not possible values of a variable or that do not seem to fit with the rest of the data. This could either be:\n\nmarginally along one axis: points that have an unusual (too high or too low) x-coordinate or y-coordinate;\njointly: observations that do not fit with the rest of the point cloud.\n\nThe boxplots in Figure @ref(fig:handheight-scatterplot) do not highlight any outliers in the marginal distributions of height and handspan. Furthermore, from the scatterplot we do not notice any extreme observations or points that do not fit with the rest of the point cloud.\nWe notice a moderate, positive linear relationship between height and handspan.\nRecall Figure @ref(fig:squares-scatterplot), displaying the relationship between side and perimeters of squares. In the plot we notice two points on top of each other, reflecting the fact that two squares having the same side will always have the same perimeter. In fact, the data from the Scottish National Gallery include two squared paintings with a side of 1.1m, both having a measured perimeter of 4.4m.\nFigure @ref(fig:handheight-scatterplot), instead, displays the relationship between height and handspan of a sample of students. The first thing that grabs our attention is the fact that students having the same height do not necessarily have the same handspan. Rather, we clearly see a variety of handspan values for students all having a height of, for example, 70in. To be more precise, the seven students who are 70 in. tall all have differing handspans.\n\n\n\n\nQuestion 9\n\n\nUsing the following command, superimpose on top of the scatterplot a best-fit line describing how handspan varies as a function of height. For the moment, the argument se = FALSE tells R to not display uncertainty bands.\ngeom_smooth(method = lm, se = FALSE)\nComment on any differences between the lines representing the linear relationship between (a) the side and perimeter of square and (b) height and handspan.\n\n\n\n\n Solution \n\n\n\nggplot(handheight, aes(x = height, y = handspan)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_smooth(method = lm, se = FALSE) +\n  labs(x = 'Height (in.)', y = 'Handspan (cm)')\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe best-fit line.\n\n\n\n\nThe line representing the relationship between side and perimeter of squares is able to predict the actual perimeter value from the measurement of the side of a square. This is possible because the relationship between side and perimeter is an exact one. That is, any squares having the same side will have the same perimeter, and there will be no variation in those values.\nThe line that best fits the relationship between height and handspan, instead, is only able to predict the average handspan for a given value of height. This is because there will be a distribution of handspans at each value of height. The line will fit the trend/pattern in the values, but there will be individual-to-individual variability that we must accept around that average pattern.\n\n\n\n\nQuestion 10\n\n\n\n\n\nThe line of best-fit is given by:1\n\\[\n\\widehat{Handspan} = -3 + 0.35 \\ Height\n\\]\nCalculate the predicted handspan of a student who is (a) 73in tall, and (b) 5in tall.\n\n\n\n\n Solution \n\n\n\nThe predicted average handspan for students who are 73in tall is \\(-3 + 0.35 * 73 = 22.55\\)cm.\nThe predicted average handspan for students who are 5in tall is \\(-3 + 0.35 * 5 = -1.25\\)cm.\n\nBut wait, handspan can not be negative… This does not make any sense! That’s right, we went too far off the range of the available data on heights, which were between 57in and 78in. We extrapolated. This is very dangerous…\n\n\n\n\n\nSource: Randall Munroe, xkcd.com"
  },
  {
    "objectID": "1_02_slr.html",
    "href": "1_02_slr.html",
    "title": "Intro to Linear Regression",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to specify a simple linear model.\nUnderstand what fitted values and residuals are.\nBe able to interpret the coefficients of a fitted model.\nBe able to test hypotheses and construct confidence intervals for the regression coefficients.\n\n\n\n\n\nBe up to date with lectures\nHave completed previous lab exercises\n\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/riverview.csv."
  },
  {
    "objectID": "1_02_slr.html#data-exploration",
    "href": "1_02_slr.html#data-exploration",
    "title": "Intro to Linear Regression",
    "section": "Data exploration",
    "text": "Data exploration\nThe common first port of call for almost any statistical analysis is to explore the data, and we can do this visually and/or numerically.\n\nMarginal distributions\nMarginal distribution: The distribution of each variable (e.g., employee incomes and education levels) without reference to the values of the other variables. Typical steps when examining the marginal distribution of a numeric variable are:\n\nVisualise the distribution of the variable. You could use, for example, geom_density() for a density plot or geom_histogram() for a histogram.\nComment on the shape of the distribution. Look at the shape, centre and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal?\nIdentify any unusual observations. Do you notice any extreme observations (i.e., outliers)?\n\n\nQuestion 1\n\n\nVisualise and describe the marginal distribution of (a) employee incomes, and (b) education level.\n\n\n\n\n Solution \n\n\nWe can plot the marginal distributions of employee incomes and education level as density curves, and add a boxplot underneath to check for the presence of outliers.\n\nEmployee Incomes\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = income)) +\n  geom_density() +\n  geom_boxplot(width = 1/300) +\n  labs(x = \"Income (in thousands of U.S. dollars)\", \n       y = \"Probability density\")\n\n\n\n\nDensity plot and boxplot of employee incomes.\n\n\n\n\nThe plot suggested that the distribution of employee incomes was unimodal, and most of the incomes were between roughly $45,000 and $70,000. The lowest income in the sample was approximately $25,000 and the highest over $80,000. This suggested there was a fair high degree of variation in the data. Furthermore, the boxplot did not highlight any outliers in the data.\nTo further summarize the distribution, it is typical to compute and report numerical summary statistics such as the mean and standard deviation. One way to compute these values is to use the summary() function:\n\ndesc_income <- riverview %>% \n  summarize(\n    M = mean(income), \n    SD = sd(income)\n    )\ndesc_income\n\n# A tibble: 1 × 2\n      M    SD\n  <dbl> <dbl>\n1  53.7  14.6\n\n\nFollowing the exploration above, we can describe the income variable as follows:\n\nThe marginal distribution of income was unimodal with a mean of approximately $53,700. There was variation in employees’ salaries (SD = $14,553).\n\n\n\nEducation Level\nVisualisation of distribution:\n\nggplot(data = riverview, aes(x = education)) +\n  geom_density() +\n  geom_boxplot(width = 1/100) +\n  labs(x = \"Education (in years)\", \n       y = \"Probability density\")\n\n\n\n\nDensity plot and boxplot of employee education levels.\n\n\n\n\nSummary statistics for the employees’ level of education:\n\ndesc_education <- riverview %>%\n  summarize(\n    M = mean(education),\n    SD = sd(education)\n    )\ndesc_education\n\n# A tibble: 1 × 2\n      M    SD\n  <dbl> <dbl>\n1    16  4.36\n\n\nAgain, we might report this as:\n\nThe marginal distribution of education was unimodal with an average of of 16 years. There was variation in employees’ level of education (SD = 4.4 years).\n\n\n\n\n\n\n\nRelationship between variables\nAfter examining the marginal distributions of the variables of interest in the analysis, we typically move on to examining relationships between the variables.\nWhen describing the relationship between two numeric variables, we typically look at their scatterplot and comment on four characteristics of the relationship:\n\nThe direction of the association indicates whether large values of one variable tend to go with large values of the other (positive association) or with small values of the other (negative association).\nThe form of association refers to whether the relationship between the variables can be summarized well with a straight line or some more complicated pattern.\nThe strength of association entails how closely the points fall to a recognizable pattern such as a line.\nUnusual observations that do not fit the pattern of the rest of the observations and which are worth examining in more detail.\n\n\nQuestion 2\n\n\nCreate a scatterplot of income and education level before calculating the correlation between the income and education level.\nMaking reference to both the plot and correlation coefficient, describe the relationship between income and level of education among the employees in the sample.\n\n\n\n\n Solution \n\n\nWe are trying to investigate how income varies when varying years of formal education. Hence, income is the dependent variable (on the y-axis), and education is the independent variable (on the x-axis).\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Education (in years)\", \n       y = \"Income (in thousands of U.S. dollars)\")\n\n\n\n\nThe relationship between employees’ education level and income.\n\n\n\n\nTo comment on the strength of the linear association we compute the correlation coefficient:\n\ncorr <- riverview %>%\n  select(education, income) %>%\n  cor()\ncorr\n\n          education    income\neducation 1.0000000 0.7947847\nincome    0.7947847 1.0000000\n\n\nthat is,\n\\[\nr_{\\text{education, income}} = 0.79\n\\]\nWe might write:\n\nThere was a strong positive linear association between education level and income for the employees in the sample. High incomes tended to be observed, on average, with more years of formal education.\nThe scatterplot did not highlight any outliers."
  },
  {
    "objectID": "1_02_slr.html#model-specification-and-fitting",
    "href": "1_02_slr.html#model-specification-and-fitting",
    "title": "Intro to Linear Regression",
    "section": "Model specification and fitting",
    "text": "Model specification and fitting\nThe scatterplot highlighted a linear relationship, where the data points were scattered around an underlying linear pattern with a roughly-constant spread as x varied.\nHence, we will try to fit a simple (i.e., one x variable only) linear regression model:\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\\\\n\\quad \\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nwhere “\\(\\epsilon \\sim N(0, \\sigma) \\text{ independently}\\)” means that the errors around the line have mean zero and constant spread as x varies.\n\nQuestion 3\n\n\nUsing the lm() function, fit a simple linear model to predict income (DV) by Education (IV), naming the output mdl.\nWrite down the equation of the fitted line.\nHint: The syntax of the lm() function is: \n[model name] <- lm([response variable i.e., dependent variable] ~ 1 + [explanatory variable i.e., independent variable], data = [dataframe])\n\n\n\n\n Solution \n\n\nThe fitted model can be written as\n\\[\n\\widehat{Income} = \\hat \\beta_0 + \\hat \\beta_1 \\ Education\n\\]\nor\n\\[\n\\widehat{Income} = \\hat \\beta_0 \\cdot 1 + \\hat \\beta_1 \\cdot Education\n\\]\nWhen we specify the linear model in R, we include after the tilde sign, ~, the variables that appear to the right of the \\(\\hat \\beta\\)s. That’s why the 1 is included.\nAs the variables are in the riverview dataframe, we would write:\n\nmdl <- lm(income ~ 1 + education, data = riverview)\nmdl\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\n\nNote that by calling the name of the fitted model, mdl, you can see the estimated regression coefficients \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). The fitted line is:\n\\[\n\\widehat{Income} = 11.32 + 2.65 \\ Education \\\\\n\\]\n\n\n\n\nQuestion 4\n\n\nExplore the following equivalent ways to obtain the estimated regression coefficients — that is, \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\) — from the fitted model:\n\nmdl i.e. simply invoke the name of the fitted model\nmdl$coefficients\ncoef(mdl)\ncoefficients(mdl)\nsummary(mdl) and look under the “Estimate” column\n\n\n\n\n\n Solution \n\n\nThe estimated parameters returned by the above methods are all equivalent. However, summary() returns more information.\n\nmdl\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nCoefficients:\n(Intercept)    education  \n     11.321        2.651  \n\nmdl$coefficients\n\n(Intercept)   education \n  11.321379    2.651297 \n\ncoef(mdl)\n\n(Intercept)   education \n  11.321379    2.651297 \n\ncoefficients(mdl)\n\n(Intercept)   education \n  11.321379    2.651297 \n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\nThe estimated intercept is \\(\\hat \\beta_0 = 11.32\\) and the estimated slope is \\(\\hat \\beta_1 = 2.65\\).\n\n\n\n\nQuestion 5\n\n\nExplore the following equivalent ways to obtain the estimated standard deviation of the errors — that is, \\(\\hat \\sigma\\) — from the fitted model mdl:\n\nsigma(mdl)\nsummary(mdl)\n\n\n Huh? What is \\(\\sigma\\)?\n\n\nThe standard deviation of the errors, denoted by \\(\\sigma\\) is an important quantity to estimate because it measures how much individual data points tend to deviate above and below the regression line.\nA small \\(\\sigma\\) indicates that the points hug the line closely and we should expect fairly accurate predictions, while a large \\(\\sigma\\) suggests that, even if we estimate the line perfectly, we can expect individual values to deviate from it by substantial amounts.\nThe estimated standard deviation of the errors is (surprisingly) denoted \\(\\hat \\sigma\\) and is equal to\n\\[\n\\hat \\sigma = \\sqrt{\\frac{SS_{Residual}}{n - 2}}\n\\]\n\n\n\n\n\n\n\n Solution \n\n\nThe estimated standard deviation of the errors can be equivalently obtained by:\n\ntyping sigma(mdl);\nlooking at the “Residual standard error” entry of the summary(mdl) output.\n\nNote: The term “Residual standard error” is a misnomer, as the help page for sigma says (check ?sigma). However, it’s hard to get rid of this bad name as it has been used in too many books showing R output.\n\nsigma(mdl)\n\n[1] 8.978116\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma = 8.98\\).\n\n\n\n\nQuestion 6\n\n\nInterpret the estimated intercept and slope, as well as the estimated standard deviation of the errors in the context of the research question.\n\n\n\n\n Solution \n\n\nWe can interpret the estimated intercept as follows,\n\nThe estimated average income associated to zero years of formal education is $11,321.\n\nFor the estimated slope we might write,\n\nThe estimated increase in average income associated to a one year increase in education is $2,651.\n\nTo interpret the estimated standard deviation of the errors we can use the fact that about 95% of values from a normal distribution fall within two standard deviations of the centre.\n\nFor any particular level of education, employee incomes should be distributed above and below the regression line with standard deviation estimated to be \\(\\hat \\sigma = 8.98\\). Since \\(2 \\hat \\sigma = 2 (8.98) = 17.96\\), we expect most (about 95%) of the employee incomes to be within about $18,000 from the regression line.\n\n\n\n\n\nQuestion 7\n\n\nPlot the data and the fitted regression line. To do so:\n\nExtract the estimated regression coefficients e.g., via betas <- coef(mdl)\nExtract the first entry of betas (i.e., the intercept) via betas[1]\nExtract the second entry of betas (i.e., the slope) via betas[2]\nProvide the intercept and slope to the function\n\ngeom_abline(intercept = <intercept>, slope = <slope>)\n\n\n\n\n Solution \n\n\nThe function coef(mdl) returns a vector: that is, a sequence of numbers all of the same type. To get the first element of the sequence you append [1], and [2] for the second.\nWe can plot the model as follows:\n\nbetas <- coef(mdl)\nintercept <- betas[1]\nslope <- betas[2]\n\nggplot(data = riverview, aes(x = education, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = intercept, slope = slope, \n              color = 'blue', size = 1) + \n  labs(x = \"Education (in years)\", \n       y = \"Income (in thousands of U.S. dollars)\")"
  },
  {
    "objectID": "1_02_slr.html#fitted-and-predicted-values",
    "href": "1_02_slr.html#fitted-and-predicted-values",
    "title": "Intro to Linear Regression",
    "section": "Fitted and predicted values",
    "text": "Fitted and predicted values\nTo compute the model-predicted values for the data in the sample:\n\npredict(<fitted model>)\nfitted(<fitted model>)\nfitted.values(<fitted model>)\nmdl$fitted.values\n\n\npredict(mdl)\n\n       1        2        3        4        5        6        7        8 \n32.53175 32.53175 37.83435 37.83435 37.83435 43.13694 43.13694 43.13694 \n       9       10       11       12       13       14       15       16 \n43.13694 48.43953 48.43953 48.43953 51.09083 53.74212 53.74212 53.74212 \n      17       18       19       20       21       22       23       24 \n53.74212 53.74212 56.39342 59.04472 59.04472 61.69601 61.69601 64.34731 \n      25       26       27       28       29       30       31       32 \n64.34731 64.34731 64.34731 66.99861 66.99861 69.64990 69.64990 74.95250 \n\n\nTo compute model-predicted values for other data:\n\npredict(<fitted model>, newdata = <dataframe>)\n\nWe first need to remember that the model predicts income using the independent variable education. Hence, if we want predictions for new data, we first need to create a tibble with a column called education containing the years of education for which we want the prediction.\n\n# make a tibble/dataframe with values for the predictor:\nnewdata <- tibble(education = c(11, 23,  49))\nnewdata\n\n# A tibble: 3 × 1\n  education\n      <dbl>\n1        11\n2        23\n3        49\n\n\nThen we take newdata and add a new column called income_hat, computed as the prediction from the fitted mdl using the newdata above:\n\nnewdata <- newdata %>%\n  mutate(\n    income_hat = predict(mdl, newdata = newdata)\n  )\nnewdata\n\n# A tibble: 3 × 2\n  education income_hat\n      <dbl>      <dbl>\n1        11       40.5\n2        23       72.3\n3        49      141."
  },
  {
    "objectID": "1_02_slr.html#residuals",
    "href": "1_02_slr.html#residuals",
    "title": "Intro to Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nThe residuals represent the deviations between the actual responses and the predicted responses and can be obtained either as\n\nmdl$residuals;\nresid(mdl);\nresiduals(mdl);\ncomputing them as the difference between the response and the predicted response.\n\n\nQuestion 8\n\n\nUse predict(mdl) to compute the fitted values and residuals. Mutate the riverview dataframe to include the fitted values and residuals as extra columns.\nAssign to the following symbols the corresponding numerical values:\n\n\\(y_{3}\\) = response variable for unit \\(i = 3\\) in the sample data\n\\(\\hat y_{3}\\) = fitted value for the third unit\n\\(\\hat \\epsilon_{5} = y_{5} - \\hat y_{5}\\) = the residual corresponding to the 5th unit.\n\n\n\n\n\n Solution \n\n\n\nriverview_fitted <- riverview %>%\n  mutate(\n    income_hat = predict(mdl),\n    resid = income - income_hat\n  )\n\nhead(riverview_fitted)\n\n# A tibble: 6 × 8\n  education income seniority gender  male party       income_hat  resid\n      <dbl>  <dbl>     <dbl> <chr>  <dbl> <chr>            <dbl>  <dbl>\n1         8   37.4         7 male       1 Democrat          32.5   4.92\n2         8   26.4         9 female     0 Independent       32.5  -6.10\n3        10   47.0        14 male       1 Democrat          37.8   9.20\n4        10   34.2        16 female     0 Independent       37.8  -3.65\n5        10   25.5         1 female     0 Republican        37.8 -12.4 \n6        12   46.5        11 female     0 Democrat          43.1   3.35\n\n\n\n\\(y_{3}\\) = 47.03\n\\(\\hat y_{3}\\) = 37.83\n\\(\\hat \\epsilon_{5} = y_{5} - \\hat y_{5}\\) = -12.36"
  },
  {
    "objectID": "1_02_slr.html#inference-for-regression-coefficients",
    "href": "1_02_slr.html#inference-for-regression-coefficients",
    "title": "Intro to Linear Regression",
    "section": "Inference for regression coefficients",
    "text": "Inference for regression coefficients\nConsider again the output of the summary() function:\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\nTo quantify the amount of uncertainty in each estimated coefficient that is due to sampling variability, we use the standard error (SE) of the coefficient. Recall that a standard error gives a numerical answer to the question of how variable a statistic will be because of random sampling.\nThe standard errors are found in the column “Std. Error”. That is, the SE of the intercept is 6.1232, and the SE of the slope corresponding to the education variable is 0.3696.\nIn this example the slope, 2.651, has a standard error of 0.37. One way to envision this is as a distribution. Our best guess (mean) for the slope parameter is 2.651. The standard deviation of this distribution is 0.37, which indicates the precision (uncertainty) of our estimate.\n\n\n\n\n\nSampling distribution of the slope coefficient. The distribution is approximately bell-shaped with a mean of 2.651 and a standard error of 0.37.\n\n\n\n\nIt shouldn’t surprise you that the reference distribution in this case is a t-distribution with \\(n-2\\) degrees of freedom, where \\(n\\) is the sample size. Recall the main formulas for obtaining a confidence interval and a test-statistic:\n\nTest statistic\nA test statistic for the null hypothesis \\(H_0: \\beta_1 = 0\\) is\n\\[\nt = \\frac{\\hat \\beta_1 - 0}{SE(\\hat \\beta_1)}\n\\]\nwhich follows a t-distribution with \\(n-2\\) degrees of freedom.\nConfidence interval\nA confidence interval for the population slope is\n\\[\n\\hat \\beta_1 \\pm t^* \\cdot SE(\\hat \\beta_1)\n\\]\nwhere \\(t^*\\) denotes the critical value chosen from t-distribution with \\(n-2\\) degrees of freedom for a desired \\(\\alpha\\) level of confidence.\n\n\nQuestion 9\n\n\nTest the hypothesis that the population slope is zero — that is, that there is no linear association between income and education level in the population.\nHint: You can find all of the necessary information in the summary() output of your model, or calculate manually.\n\n\n\n\n Solution \n\n\nWe calculate the test statistic\n\\[\nt = \\frac{\\hat \\beta_1 - 0}{SE(\\hat \\beta_1)} = \\frac{ 2.6513 - 0 }{0.3696} = 7.173\n\\]\nand compare it with the 5% critical value from a t-distribution with \\(n-2\\) degrees of freedom, which is:\n\nn <- nrow(riverview)\ntstar <- qt(0.975, df = n - 2)\ntstar\n\n[1] 2.042272\n\n\nAs \\(|t|\\) is much larger than \\(t^*\\), we reject then null hypothesis as have strong evidence against it.\nThe p-value, shown below, also confirms the conclusion.\n\n2 * (1 - pt(7.173, n - 2))\n\n[1] 5.561692e-08\n\n\nPlease note that the same information was already contained in the row corresponding to the variable “education” in the output of summary(mdl), which reported the t-statistic under t value and the p-value under Pr(>|t|):\n\nsummary(mdl)\n\n\nCall:\nlm(formula = income ~ 1 + education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.809  -5.783   2.088   5.127  18.379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3214     6.1232   1.849   0.0743 .  \neducation     2.6513     0.3696   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.978 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\nBefore we interpret the results, recall that the p-value 5.56e-08 in the Pr(>|t|) column simply means \\(5.56 \\times 10^{-8}\\). This is a very small value (i.e., 0.0000000556), hence we will report it as <.001 following the APA guidelines.\n\nWe performed a t-test against the null hypothesis that education is not a significant predictor of income: \\(t(30) = 7.173,\\ p < .001\\), two-sided. The large t-statistic led to a small \\(p\\)-value, meaning that we have evidence against the null hypothesis.\n\n\n\n\n\nQuestion 10\n\n\nCompute a confidence interval for the regression slope\n\n\n\n\n Solution \n\n\nIn the riverview example, for 95% confidence we have \\(t^* = 2.04\\):\n\nn <- nrow(riverview)\ntstar <- qt(0.975, df = n - 2)\ntstar\n\n[1] 2.042272\n\n\nThe confidence interval is:\n\nbeta1_ci <- tibble(\n  lower = 2.6513 - tstar * 0.3696,\n  upper = 2.6513 + tstar * 0.3696,\n)\nbeta1_ci\n\n# A tibble: 1 × 2\n  lower upper\n  <dbl> <dbl>\n1  1.90  3.41\n\n\nIn R it is easy to obtain the confidence intervals for the regression coefficients using the command confint():\n\nconfint(mdl, level = 0.95)\n\n                2.5 %    97.5 %\n(Intercept) -1.183935 23.826693\neducation    1.896425  3.406168\n\n\nThe result is exactly the same (up to rounding errors) as the previous one.\nWe typically report our uncertainty in a statistic by providing \\(\\text{estimate} \\pm t^* \\cdot \\text{SE}\\). Here we would say that because of sampling variation, we are 95% confident that the slope is between 1.896 and 3.406. Interpreting this, we might say,\n\nFor all Riverview city employees, each one-year difference in formal education is associated with a difference in income between $1,896 and $3,406, on average.\n\nSimilarly, we could express the uncertainty in the intercept \\(\\hat \\beta_0\\) as:\n\nThe average income for all Riverview city employees with zero years of education is between $-1,184 and $23,827."
  },
  {
    "objectID": "1_03_mlr.html",
    "href": "1_03_mlr.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Setup\n\nSetup\n\n\n\nCreate a new RMarkdown file\nLoad the required package(s)\nRead the wellbeing dataset into R, assigning it to an object named mwdata\n\n\n\n\n\n Solution \n\n\n\n#Loading the required package(s)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(sjPlot)\n\nLearn more about sjPlot with 'browseVignettes(\"sjPlot\")'.\n\n# Reading in data and storing to an object named 'mwdata'\nmwdata <- read_csv(file = 'https://uoepsy.github.io/data/wellbeing.csv')\n\nRows: 32 Columns: 5\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): location, routine\ndbl (3): wellbeing, outdoor_time, social_int\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\nStudy Overview\n\nResearch Question\nIs there an association between well-being and time spent outdoors after taking into account the relationship between well-being and social interactions.\n\nThe researchers know that other aspects of peoples’ lifestyles such as how much social interaction they have can influence their mental well-being. They are intersted in furthering this understanding, and want to account for the relationship between psychological wellbeing and time spent outdoors.\n\n\n Wellbeing data codebook.\n\n\nDescription\nResearchers interviewed 32 participants, selected at random from the population of residents of Edinburgh & Lothians. They used the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\nThe researchers also asked participants to estimate the average number of hours they spend outdoors each week, the average number of social interactions they have each week (whether on-line or in-person), and whether they believe that they stick to a routine throughout the week (Yes/No).\nThe data in wellbeing.csv contain five attributes collected from a random sample of \\(n=32\\) hypothetical residents over Edinburgh & Lothians, and include:\n\nwellbeing: Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.\n\noutdoor_time: Self report estimated number of hours per week spent outdoors\n\nsocial_int: Self report estimated number of social interactions per week (both online and in-person)\nroutine: Binary Yes/No response to the question “Do you follow a daily routine throughout the week?”\nlocation: Location of primary residence (City, Suburb, Rural)\n\nPreview\nThe first six rows of the data are:\n\n\nRows: 32 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): location, routine\ndbl (3): wellbeing, outdoor_time, social_int\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n  \n  \n    \n      wellbeing\n      outdoor_time\n      social_int\n      location\n      routine\n    \n  \n  \n    30\n7\n8\nSuburb\nRoutine\n    21\n9\n8\nCity\nNo Routine\n    38\n14\n10\nSuburb\nRoutine\n    27\n16\n10\nCity\nNo Routine\n    20\n1\n10\nRural\nNo Routine\n    37\n11\n12\nSuburb\nNo Routine\n  \n  \n  \n\n\n\n\n\n\n\n\nQuestion 1\n\n\nProduce plots of the marginal distributions (the distributions of each variable in the analysis without reference to the other variables) of the wellbeing, outdoor_time, and social_int variables.\n\n\n\n\n Solution \n\n\nWe should be familiar now with how to visualise a marginal distribution. You might choose histograms, density curves, or boxplots, or a combination:\n\nwellbeing_plot <- \n  ggplot(data = mwdata, aes(x = wellbeing)) +\n  geom_density() +\n  geom_boxplot(width = 1/250) +\n  labs(x = \"Score on WEMWBS (range 14-70)\", y = \"Probability\\ndensity\")\n\noutdoortime_plot <- \n  ggplot(data = mwdata, aes(x = outdoor_time)) +\n  geom_density() +\n  geom_boxplot(width = 1/200) +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Probability\\ndensity\")\n\nsocial_plot <- \n  ggplot(data = mwdata, aes(x = social_int)) +\n  geom_density() +\n  geom_boxplot(width = 1/150) +\n  labs(x = \"Number of social interactions per week\", y = \"Probability\\ndensity\")\n\n# the \"patchwork\" library allows us to arrange multiple plots\nwellbeing_plot / outdoortime_plot / social_plot\n\n\n\n\nMarginal distribution plots of wellbeing sores, weekly hours spent outdoors, and social interactions\n\n\n\n\n\n\nThe marginal distribution of scores on the WEMWBS is unimodal with a mean of approximately 43. There is variation in WEMWBS scores (SD = 11.7).\n\nThe marginal distribution of weekly hours spent outdoors is unimodal with a mean of approximately 14.8. There is variation in weekly hours spent outdoors (SD = 6.9).\n\nThe marginal distribution of numbers of social interactions per week is unimodal with a mean of approximately 16. There is variation in numbers of social interactions (SD = 4.4).\n\n\n\n\n\n\nQuestion 2\n\n\nProduce plots of the marginal relationships between the outcome variable (wellbeing) and each of the explanatory variables.\n\n\n\n\n Solution \n\n\n\nwellbeing_outdoor <- \n  ggplot(data = mwdata, aes(x = outdoor_time, y = wellbeing)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Time spent outdoors per week (hours)\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_social <- \n  ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Number of social interactions per week\", y = \"Wellbeing score (WEMWBS)\")\n\nwellbeing_outdoor | wellbeing_social\n\n\n\n\nScatterplots displaying the relationships between scores on the WEMWBS and a) weekly outdoor time (hours), and b) weekly number of social interactions\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nProduce a correlation matrix of the variables which are to be used in the analysis, and write a short paragraph describing the relationships.\n\nCorrelation matrix\nA table showing the correlation coefficients - \\(r_{(x,y)}=\\frac{\\mathrm{cov}(x,y)}{s_xs_y}\\) - between variables. Each cell in the table shows the relationship between two variables. The diagonals show the correlation of a variable with itself (and are therefore always equal to 1).\n\nWe can create a correlation matrix easily by giving the cor() function a dataframe. However, we only want to give it 3 columns here. Think about how we select specific columns, either using select(), or giving the column numbers inside [].\n\n\n\n\n\n\n Solution \n\n\nWe can either use:\n\n# correlation matrix of the first 3 columns\ncor(mwdata[,1:3])\n\nor:\n\n# select only the columns we want by name, and pass this to cor()\nmwdata %>% \n  select(wellbeing, outdoor_time, social_int) %>%\n  cor()\n\n             wellbeing outdoor_time social_int\nwellbeing    1.0000000    0.5815613  0.7939003\noutdoor_time 0.5815613    1.0000000  0.3394469\nsocial_int   0.7939003    0.3394469  1.0000000\n\n\n\nThere is a moderate, positive, linear relationship between weekly outdoor time and WEMWBS scores for the participants in the sample. Participants’ wellbeing scores tend to increase, on average, with the number of hours spent outdoors each week.\nThere is a moderate, positive, linear relationship between the weekly number of social interactions and WEMWBS scores for the participants in the sample. Participants’ wellbeing scores tend to increase, on average, with the weekly number of social interactions. There is also a weak positive correlation between weekly outdoor time and the weekly number of social interactions.\n\n Note that there is a weak correlation between our two explanatory variables (outdoor_time and social_int). We will return to how this might affect our model when later on we look at the assumptions of multiple regression.\n\n\n\n\nQuestion 4\n\n\nSpecify the model equation, where \\(x_1\\) = weekly outdoor time, \\(x_2\\) = weekly numbers of social interactions and \\(y\\) = scores on the WEMWBS. What are the parameters of the model. How do we denote parameter estimates?\nNext, fit the linear model in using lm(), assigning the output to an object called mdl1.\nHint: As we did for simple linear regression, we can fit our multiple regression model using the lm() function. We can add as many explanatory variables as we like, separating them with a +.\n\n\nlm( <response variable> ~ 1 + <explanatory variable 1> + <explanatory variable 2> + ... , data = <dataframe> )\n\n\n\n\n\n\n Solution \n\n\nA model for the relationship between \\(x_1\\) = weekly outdoor time, \\(x_2\\) = weekly numbers of social interactions and \\(y\\) = scores on the WEMWBS is given by:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\\\ \\quad \\\\ \\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\text{ independently}\n\\]\nIn the model specified above,\n\n\\(\\mu_{y|x_1, x_2} = \\beta_0 + \\beta_1 x + \\beta_2 x_2\\) represents the systematic part of the model giving the mean of \\(y\\) at each combination of values of \\(x_1\\) and \\(x_2\\);\n\\(\\epsilon\\) represents the error (deviation) from that mean, and the errors are independent from one another.\n\nThe parameters of our model are:\n\n\\(\\beta_0\\) (The intercept);\n\\(\\beta_1\\) (The slope across values of \\(x_1\\));\n\\(\\beta_2\\) (The slope across values of \\(x_2\\));\n\n\\(\\sigma\\) (The standard deviation of the errors).\n\nWhen we estimate these parameters from the available data, we have a fitted model (recall that the h\\(\\hat{\\textrm{a}}\\)ts are used to distinguish our estimates from the true unknown parameters):\n\\[\n\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot Outdoor Time + \\hat \\beta_2 \\cdot Social Interactions\n\\]\nAnd we have residuals \\(\\hat \\epsilon = y - \\hat y\\) which are the deviations from the observed values and our model-predicted responses.\n\nVisual\nNote that for simple linear regression we talked about our model as a line in 2 dimensions: the systematic part \\(\\beta_0 + \\beta_1 x\\) defined a line for \\(\\mu_y\\) across the possible values of \\(x\\), with \\(\\epsilon\\) as the random deviations from that line. But in multiple regression we have more than two variables making up our model.\nIn this particular case of three variables (one outcome + two explanatory), we can think of our model as a regression surface (See Figure @ref(fig:regsurf)). The systematic part of our model defines the surface across a range of possible values of both \\(x_1\\) and \\(x_2\\). Deviations from the surface are determined by the random error component, \\(\\hat \\epsilon\\).\n\n\n\n\n\nRegression surface for wellbeing ~ outdoor_time + social_int, from two different angles\n\n\n\n\nDon’t worry about trying to figure out how to visualise it if we had any more explanatory variables! We can only concieve of 3 spatial dimensions. One could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it’s not worth trying!.\n\nFitting the model in R:\n\nmdl1 <- lm(wellbeing ~ 1 + outdoor_time + social_int, data = mwdata)\n\n\n\n\n\nQuestion 5\n\n\nUsing any of:\n\nmdl1\nmdl1$coefficients\ncoef(mdl1)\ncoefficients(mdl1)\nsummary(mdl1)\n\nWrite out the estimated parameter values of:\n\n\\(\\hat \\beta_0\\), the estimated average wellbeing score associated with zero hours of outdoor time and zero social interactions per week.\n\n\\(\\hat \\beta_1\\), the estimated increase in average wellbeing score associated with one hour increase in weekly outdoor time, holding the number of social interactions constant (i.e., when the remaining explanatory variables are held at the same value or are fixed).\n\\(\\hat \\beta_2\\), the estimated increase in average wellbeing score associated with an additional social interaction per week (an increase of one), holding weekly outdoor time constant.\n\n\n\n\n\n Solution \n\n\n\ncoef(mdl1)\n\n (Intercept) outdoor_time   social_int \n   5.3703775    0.5923673    1.8034489 \n\n\n\n\\(\\hat \\beta_0\\) = 5.37\n\n\\(\\hat \\beta_1\\) = 0.59\n\n\\(\\hat \\beta_2\\) = 1.8\n\n\n\n\n\nQuestion 6\n\n\nWithin what distance from the model predicted values (the regression surface) would we expect 95% of wEMWBS wellbeing scores to be?\nHint: either sigma(mdl1) or part of the output from summary(mdl1) will help you here.\n\n\n\n\n Solution \n\n\nThe estimated standard deviation of the errors is \\(\\hat \\sigma\\) = 6.15. We would expect 95% of wellbeing scores to be within about 12.3 (\\(2 \\hat \\sigma\\)) from the model fit.\n\n\n\n\nQuestion 7\n\n\nObtain 95% confidence intervals for the regression coefficients, and write a sentence about each one.\n\n\n\n\n Solution \n\n\n\nconfint(mdl1, level = 0.95)\n\n                  2.5 %     97.5 %\n(Intercept)  -3.4660660 14.2068209\noutdoor_time  0.2468371  0.9378975\nsocial_int    1.2530813  2.3538164\n\n\n\n\nThe average wellbeing score for all those with zero hours of outdoor time and zero social interactions per week is between -3.47 and 14.21.\n\nWhen holding the number of social interactions per week constant, each one hour increase in weekly outdoor time is associated with a difference in wellbeing scores between 0.25 and 0.94, on average.\n\nWhen holding weekly outdoor time constant, each increase of one social interaction per week is associated with a difference in wellbeing scores between 1.25 and 2.35, on average.\n\n\n\n\n\n\n\nNumeric + Categorical\nLet’s do that again, but paying careful attention to where and how the process differs when we have a categorical (or “qualitative”) predictor.\n\nSuppose that the group of researchers were instead wanting to study the relationship between well-being and time spent outdoors after taking into account the relationship between well-being and having a routine.\n\n\nQuestion 8\n\n\nWe have already visualised the marginal distribution of weekly outdoor time in an earlier question, as well as its relationship with wellbeing scores.\nProduce visualisations of:\n\nthe distribution of the routine variable\nthe relationship between routine and wellbeing.\n\nNote: We cannot visualise the distribution of routine as a density curve or boxplot, because it is a categorical variable (observations can only take one of a set of discrete response values).\n\n\n\n\n Solution \n\n\ngeom_bar() will count the number of observations falling into each unique level of the routine variable:\n\nggplot(data = mwdata, aes(x = routine)) +\n  geom_bar()+\n  labs(x = \"Routine\", y = \"Frequency\")\n\n\n\n\nMarginal distribution plots of Routine (y/n)\n\n\n\n\nWe might plot the relationship between routine and wellbeing as two boxplots:\n\nggplot(data = mwdata, aes(x = routine, y = wellbeing)) +\n  geom_boxplot()+\n  labs(x = \"Routine\", y = \"Wellbeing score (WEMWBS)\")\n\n\n\n\nRelationship between wellbeing and presence of routine\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nFit the multiple regression model below using lm(), and assign it to an object named mdl2.\n\\[\nWellbeing = \\beta_0 + \\beta_1 \\cdot OutdoorTime + \\beta_2 \\cdot Routine + \\epsilon\n\\]\nExamine the summary output of the model.\n\\(\\hat \\beta_0\\) (the intercept) is the estimated average wellbeing score associated with zero hours of weekly outdoor time and zero in the routine variable. What group is the intercept the estimated wellbeing score for when they have zero hours of outdoor time? Why (think about what zero in the routine variable means)?\n\n\n\n\n Solution \n\n\n\nmdl2 <- lm(wellbeing ~ outdoor_time + routine, data = mwdata)\nsummary(mdl2)\n\n\nCall:\nlm(formula = wellbeing ~ outdoor_time + routine, data = mwdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.3597  -5.7983   0.1047   7.2899  12.5957 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     26.2525     3.9536   6.640  2.8e-07 ***\noutdoor_time     0.9152     0.2358   3.881 0.000552 ***\nroutineRoutine   7.2947     3.2507   2.244 0.032633 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.06 on 29 degrees of freedom\nMultiple R-squared:  0.4361,    Adjusted R-squared:  0.3972 \nF-statistic: 11.21 on 2 and 29 DF,  p-value: 0.0002467\n\n\nAs you can see in the output of the model, we have a coefficient called routineRoutine. This is the parameter estimate for a dummy variable which has been inputted into the model. The lm() function will automatically name the dummy variables (and therefore the coefficients) according to what level is identified by the 1. It names them <variable><Level>, so we can tell that routineRoutine is 1 for “Routine” and 0 for “No Routine”.\nThe intercept is therefore the estimated wellbeing score for those with No Routine and zero hours of outdoor time.\n\n\n\n\nQuestion 10\n\n\nWe can visualise the model \\(\\widehat{Wellbeing} = \\hat \\beta_0 + \\hat \\beta_1 \\cdot OutdoorTime + \\hat \\beta_2 \\cdot Routine\\) as two lines.\nEach line represents the model predicted values for wellbeing scores across the range of weekly outdoor time, with one line for those who report having “Routine” and one for those with “No Routine”.\nGet a pen and paper, and sketch out the plot shown in Figure @ref(fig:plot-annotate).\n\n\nScale for 'colour' is already present. Adding another scale for 'colour',\nwhich will replace the existing scale.\n\n\n\n\n\nMultiple regression model: Wellbeing ~ Outdoor Time + Routine\n\n\n\n\nAnnotate your plot with labels for each of parameter estimates from your model:\n\n\n\nParameter Estimate\nModel Coefficient\nEstimate\n\n\n\n\n\\(\\hat \\beta_0\\)\n(Intercept)\n26.25\n\n\n\\(\\hat \\beta_1\\)\noutdoor_time\n0.92\n\n\n\\(\\hat \\beta_2\\)\nroutineRoutine\n7.29\n\n\n\n\n Hint.\n\n\nBelow you can see where to add the labels, but we have not said which is which.\n\n\nScale for 'colour' is already present. Adding another scale for 'colour',\nwhich will replace the existing scale.\n\n\n\n\n\n\nA is the vertical distance between the red and blue lines (the lines are parallel, so this distance is the same wherever you cut it on the x-axis).\n\nB is the point at which the blue line cuts the y-axis.\n\nC is the vertical increase (increase on the y-axis) for the blue line associated with a 1 unit increase on the x-axis (the lines are parallel, so this is the same for the red line).\n\n\n\n\n\n\n\n\n Solution \n\n\n\nA = \\(\\hat \\beta_2\\) = routineRoutine coefficient = 7.29\nB = \\(\\hat \\beta_0\\) = (Intercept) coefficient = 26.25\nC = \\(\\hat \\beta_1\\) = outdoor_time coefficient = 0.92\n\n\n\n\n\n\n\n\n\n\nQuestion 11\n\n\nUsing the sjPlot package, run the code below. What do you think each plot is showing?\n\n#Plot 1\nplot_model(mdl2)\n\n#Plot 2\nplot_model(mdl2, type = \"pred\")\n\n#Plot 3\nplot_model(mdl2, type = \"pred\",  terms=c(\"outdoor_time\",\"routine\"), show.data=TRUE)\n\n\nThe plot_model function (and the sjPlot package) can do a lot of different things. Most packages in R come with tutorials (or “vignettes”), for instance: https://strengejacke.github.io/sjPlot/articles/plot_model_estimates.html\n\n\n\n\n\n Solution \n\n\n\n#Plot 1\nplot_model(mdl2)\n\n\n\n\nThese are the parameter estimates (the \\(\\hat \\beta\\)’s), and the confidence intervals.\n\nconfint(mdl2)\n\n                    2.5 %    97.5 %\n(Intercept)    18.1665376 34.338493\noutdoor_time    0.4329433  1.397409\nroutineRoutine  0.6462090 13.943226\n\n\nWhen we add type=\"pred\" we are asking for the predicted values. It will provide a separate plot for each explanatory variable, showing the predicted values at each level of that variable:\n\n#Plot 2\nplot_model(mdl2, type = \"pred\")\n\n$outdoor_time\n\n\n\n\n\n\n$routine\n\n\n\n\n\nWe can combine these into one plot, and ask it to show the raw data as well:\n\n#Plot 3\nplot_model(mdl2, type = \"pred\",  terms=c(\"outdoor_time\",\"routine\"), show.data=TRUE)"
  },
  {
    "objectID": "1_04_model_fit.html",
    "href": "1_04_model_fit.html",
    "title": "Model Fit and Standardization",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand the calculation and interpretation of the coefficient of determination.\nUnderstand the calculation and interpretation of the F-test of model utility.\nUnderstand how to standardize model coefficients and when this is appropriate to do.\nUnderstand the relationship between the correlation coefficient and the regression slope.\n\n\n\n\n\nBe up to date with lectures\nHave completed previous lab exercises\n\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npatchwork\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/riverview.csv. Note: this is the same data as Lab 2."
  },
  {
    "objectID": "1_04_model_fit.html#data-recap",
    "href": "1_04_model_fit.html#data-recap",
    "title": "Model Fit and Standardization",
    "section": "Data recap",
    "text": "Data recap\n\n Riverview data codebook\n\n\nDescription\nThe riverview data come from (Lewis-Beck2015?) and contain five attributes collected from a random sample of \\(n=32\\) employees working for the city of Riverview, a hypothetical midwestern city in the US. The attributes include:\n\neducation: Years of formal education\nincome: Annual income (in thousands of U.S. dollars)\nseniority: Years of seniority\ngender: Employee’s gender\nmale: Dummy coded gender variable (0 = Female, 1 = Male)\nparty: Political party affiliation\n\nPreview\nThe first six rows of the data are:\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\nRows: 32 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): gender, party\ndbl (4): education, income, seniority, male\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n \n  \n    education \n    income \n    seniority \n    gender \n    male \n    party \n  \n \n\n  \n    8 \n    37.449 \n    7 \n    male \n    1 \n    Democrat \n  \n  \n    8 \n    26.430 \n    9 \n    female \n    0 \n    Independent \n  \n  \n    10 \n    47.034 \n    14 \n    male \n    1 \n    Democrat \n  \n  \n    10 \n    34.182 \n    16 \n    female \n    0 \n    Independent \n  \n  \n    10 \n    25.479 \n    1 \n    female \n    0 \n    Republican \n  \n  \n    12 \n    46.488 \n    11 \n    female \n    0 \n    Democrat"
  },
  {
    "objectID": "1_04_model_fit.html#back-to-regression-coefficients",
    "href": "1_04_model_fit.html#back-to-regression-coefficients",
    "title": "Model Fit and Standardization",
    "section": "Back to regression coefficients",
    "text": "Back to regression coefficients\n\nQuestion 6\n\n\nCompute the average education level and the average income in the sample.\nUse the predict() function to compute the predicted income for those with average education level.\nWhat do you notice?\n\n\n\n\n Solution \n\n\nSample means:\n\nstats <- riverview %>%\n  summarise(\n    mean_education = mean(education),\n    mean_income = mean(income)\n  )\nstats\n\n# A tibble: 1 × 2\n  mean_education mean_income\n           <dbl>       <dbl>\n1             16        53.7\n\n\nPrediction:\n\nquery <- tibble(education = stats$mean_education)\nquery\n\n# A tibble: 1 × 1\n  education\n      <dbl>\n1        16\n\npredict(mdl, newdata = query)\n\n       1 \n53.74212 \n\n\n The predicted average income for those having average education level is equal to the average income in the sample.\n\n\n\n\nQuestion 7\n\n\nLet’s formalise the previous question using symbols. Consider the fitted model \\(\\hat{y} = \\hat \\beta_0 + \\hat \\beta_1 x\\).\nWhat is the predicted response for an individual having an explanatory variable at the average level \\(\\bar{x}\\)?\nHint: Substitute the formula of \\(\\hat \\beta_0\\) into the equation of the fitted model.\n\n\n\n\n Solution \n\n\nWe require the prediction at \\(x = \\bar{x}\\), that is:\n\\[\n\\hat{y} = \\hat \\beta_0 + \\hat \\beta_1 \\bar{x}\n\\]\nRecall the formula for the fitted intercept: \\(\\hat \\beta_0 = \\bar{y} - \\hat \\beta_1 \\bar{x}\\). Let’s substitute it into the equation:\n\\[\n\\begin{split}\n\\hat{y} &= \\hat \\beta_0 + \\hat \\beta_1 \\bar{x} \\\\\n&= \\bar{y} - \\hat \\beta_1 \\bar{x} + \\hat \\beta_1 \\bar{x} \\\\\n&= \\bar{y}\n\\end{split}\n\\]\nThis highlights an important property of the fitted regression line: it always passes through the point of averages \\((\\bar x, \\bar y)\\).\nIntuitively, what would be your prediction \\(\\hat y\\) when \\(x\\) equals \\(\\bar{x}\\)? If you guessed \\(\\bar{y}\\) you’re on track!"
  },
  {
    "objectID": "1_04_model_fit.html#binary-predictors",
    "href": "1_04_model_fit.html#binary-predictors",
    "title": "Model Fit and Standardization",
    "section": "Binary predictors",
    "text": "Binary predictors\nLet’s suppose that instead of having measured education in years, we had data instead on “Obtained College Degree: Yes/No”. Our explanatory variable would be binary categorical (think back to our discussion of types of data).\nLet us pretend that everyone with >18 years of education has a college degree:\n\n#create new variable called 'degree', where those over age 18 are allocated to Yes condition, and those under 18 to No.\nriverview <- \n  riverview %>%\n    mutate(\n      degree = ifelse(education > 18, \"Yes\", \"No\")\n    )\n\nWe may then plot our relationship as a boxplot. If you want to see the individual points, you could always “jitter” them (right-hand plot below)\n\nggplot(riverview, aes(x = degree, y = income)) + \n  geom_boxplot() +\nggplot(riverview, aes(x = degree, y = income)) + \n  geom_jitter(height=0, width=.05)"
  },
  {
    "objectID": "1_04_model_fit.html#standardization",
    "href": "1_04_model_fit.html#standardization",
    "title": "Model Fit and Standardization",
    "section": "Standardization",
    "text": "Standardization\n\nQuestion 8\n\n\nAdd to the riverview dataset two variables called z_education and z_income representing the standardized education and income variables, respectively.\nWithout using R, if you were to fit a linear regression model using the standardized response and standardized predictor, what would the intercept be?\n\n\n\n\n Solution \n\n\n\nriverview <- riverview %>%\n  mutate(\n    z_education = (education - mean(education)) / sd(education),\n    z_income = (income - mean(income)) / sd(income)\n  )\n\nCheck that they are standardized:\n\nriverview %>%\n  summarise(\n    M_z_education = mean(z_education), SD_z_education = sd(z_education), \n    M_z_income = mean(z_income), SD_z_income = sd(z_income)\n  )\n\n# A tibble: 1 × 4\n  M_z_education SD_z_education M_z_income SD_z_income\n          <dbl>          <dbl>      <dbl>       <dbl>\n1      1.73e-18              1  -1.28e-16           1\n\n\nA standardized variable has mean 0 and standard deviation equal to 1. We can substitute this in the formula of the estimated intercept:\n\\[\n\\hat \\beta_0 = \\bar{y} - \\hat \\beta_1 \\bar{x} = 0 - \\hat \\beta_1 0 = 0\n\\]\nThe intercept of the linear model between the standardized variables is zero — that is, the regression line passes through the origin of the graph.\nBut wait… The origin is also the average of \\(z_x\\) and the average of \\(z_y\\) as they are standardized. In symbols \\((0, 0) = (\\bar{z}_x, \\bar{z}_y)\\).\nWe expected the fitted line to pass through the point of averages — in this case, the origin.\n\n\n\n\nQuestion 9\n\n\nUsing R, fit the regression model using the standardized response and explanatory variables.\nWhat is the slope equal to?\n\n\n\n\n Solution \n\n\nLinear model for standardized variables:\n\nmdl_z <- lm(z_income ~ 1 + z_education, data = riverview)\nsummary(mdl_z)\n\n\nCall:\nlm(formula = z_income ~ 1 + z_education, data = riverview)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0863 -0.3974  0.1435  0.3523  1.2629 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.055e-18  1.091e-01   0.000        1    \nz_education  7.948e-01  1.108e-01   7.173 5.56e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6169 on 30 degrees of freedom\nMultiple R-squared:  0.6317,    Adjusted R-squared:  0.6194 \nF-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08\n\n\n Correlation coefficient of original variables:\n\nriverview %>%\n  select(education, income) %>%\n  cor()\n\n          education    income\neducation 1.0000000 0.7947847\nincome    0.7947847 1.0000000\n\n\n The slope of the standardized variables is equal to the correlation between the original variables.\n\n\n\n\nQuestion 10\n\n\nInterpret the slope of the standardized variables.\n\n\n\n\n Solution \n\n\n\nFor every standard deviation increase in education, income increases on average by 0.79 standard deviations."
  },
  {
    "objectID": "1_05_recap_writeup.html",
    "href": "1_05_recap_writeup.html",
    "title": "Block 1 Recap",
    "section": "",
    "text": "At the end of this lab, you will:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBe up to date with lectures\nHave completed labs 1 - 4\n\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/cogapoe4.csv"
  },
  {
    "objectID": "1_05_recap_writeup.html#in-line-coding",
    "href": "1_05_recap_writeup.html#in-line-coding",
    "title": "Block 1 Recap",
    "section": "In-line Coding",
    "text": "In-line Coding\nWe have seen above how to interpret and write up sections of our results, but now lets use R to help us do this using inline coding (for a refresher, see Lesson 3 of the RMD Bootcamp)."
  },
  {
    "objectID": "1_07_int1_nc+nn.html",
    "href": "1_07_int1_nc+nn.html",
    "title": "Interactions I: Num x Cat & Num x Num",
    "section": "",
    "text": "At the end of this lab, you will:\n\nUnderstand the concept of an interaction.\nBe able to interpret the meaning of a numeric \\(\\times\\) categorical interaction.\nBe able to interpret the meaning of a numeric \\(\\times\\) numeric interaction.\nUnderstand the principle of marginality and why this impacts modelling choices with interactions.\nVisualize and probe interactions.\n\n\n\n\nYou will need to have completed [Lab X].\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\npsych\nsjPlot\npatchwork\nkableExtra\nplotly\npander\n\n\n\n\nYou can download the data required for Part A of this lab here or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv.\nYou can download the data required for Part B of this lab here or read it in via this link https://uoepsy.github.io/data/scs_study.csv."
  },
  {
    "objectID": "1_07_int1_nc+nn.html#exercises",
    "href": "1_07_int1_nc+nn.html#exercises",
    "title": "Interactions I: Num x Cat & Num x Num",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion A1\n\n\n\nSpecify a multiple regression model to address the research question.\nVisually explore the associations among the variables included in your analysis.\n\n\n“Except in special circumstances, a model including a product term for interaction between two explanatory variables should also include terms with each of the explanatory variables individually, even though their coefficients may not be significantly different from zero. Following this rule avoids the logical inconsistency of saying that the effect of \\(X_1\\) depends on the level of \\(X_2\\) but that there is no effect of \\(X_1\\).”\n— (Ramsey2012?)\n\n\nHint 1: Check the “location” variable. It currently has three levels (Rural/Suburb/City), but we only want two (Rural/Not Rural). You’ll need to fix this. One way to do this would be to use ifelse() to define a variable which takes one value (“Rural”) if the observation meets from some condition, or another value (“Not Rural”) if it does not. Type ?ifelse in the console if you want to see the help function. You can use it to add a new variable either inside mutate(), or using data$new_variable_name <- ifelse(test, x, y) syntax.\nHint 2: The pairs.panels() function from the psych package will plot all variables in a dataset against one another. This will save you the time you would have spent creating individual plots.\n\n\n\n\n\n Solution \n\n\nTo address the research question, we are going to fit the following model, where \\(y\\) = wellbeing; \\(x_1\\) = weekly outdoor time; and \\(x_2\\) = whether or not the respondent lives in a rural location or not.\n\\[\ny = \\beta_0 + \\beta_1  x_1 + \\beta_2  x_2 + \\beta_3 (x_1 \\cdot x_2) + \\epsilon \\\\\n\\quad \\\\ \\text{where} \\quad \\epsilon \\sim N(0, \\sigma) \\quad \\text{independently}\n\\]\nFirst let’s create a new variable for Rural/Not Rural\n\nwrdata <- wrdata %>% \n  mutate(\n    isRural = ifelse(location == \"rural\", \"rural\", \"not rural\")\n  )\n\nNow let’s use the pairs.panels() function from the psych package. We could use it on the whole dataset, but for now we’ll just do it on the variables we’re interested in:\n\nwrdata %>% \n  select(wellbeing, social_int, isRural) %>%\n  pairs.panels()\n\n\n\n\n\n\n\n\nQuestion A2\n\n\nProduce a visualisation of the association between weekly number of social interactions and well-being, with separate facets for rural vs non-rural respondents.\n\n\n\n\n Solution \n\n\n\nggplot(data = wrdata, aes(x = social_int, y = wellbeing))\n\n\n\n  geom_point() \n\ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\n  facet_wrap(~isRural) \n\n<ggproto object: Class FacetWrap, Facet, gg>\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  <ggproto object: Class FacetWrap, Facet, gg>\n\n\n\n\n\n\nQuestion A3\n\n\nFit your model using lm(), and assign it as an object with the name “rural_mod”.\nHint: When fitting a regression model in R with two explanatory variables A and B, and their interaction, these two are equivalent:\n\ny ~ A + B + A:B\ny ~ A*B\n\n\n\n\n\n Solution \n\n\n\n#fit model including interaction between social_int and isRural\nrural_mod <- lm(wellbeing ~ 1 + social_int * isRural, data = wrdata)\n\n#check model output\nsummary(rural_mod)\n\n\nCall:\nlm(formula = wellbeing ~ 1 + social_int * isRural, data = wrdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.4845  -2.7975   0.0155   2.4539  15.6743 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              30.9986     1.4284  21.702  < 2e-16 ***\nsocial_int                0.6488     0.1160   5.593 7.42e-08 ***\nisRuralrural              1.3866     2.0510   0.676  0.49981    \nsocial_int:isRuralrural  -0.5176     0.1615  -3.206  0.00157 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.558 on 196 degrees of freedom\nMultiple R-squared:  0.2962,    Adjusted R-squared:  0.2854 \nF-statistic: 27.49 on 3 and 196 DF,  p-value: 6.97e-15\n\n\n\n\n\n\nInterpreting coefficients for A and B in the presence of an interaction A:B\nWhen you include an interaction between \\(x_1\\) and \\(x_2\\) in a regression model, you are estimating the extent to which the effect of \\(x_1\\) on \\(y\\) is different across the values of \\(x_2\\).\nWhat this means is that the effect of \\(x_1\\) on \\(y\\) depends on/is conditional upon the value of \\(x_2\\).\n(and vice versa, the effect of \\(x_2\\) on \\(y\\) is different across the values of \\(x_1\\)).\nThis means that we can no longer talk about the “effect of \\(x_1\\) holding \\(x_2\\) constant”. Instead we can talk about a marginal effect of \\(x_1\\) on \\(y\\) at a specific value of \\(x_2\\).\n\nWhen we fit the model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 \\cdot x_2) + \\epsilon\\) using lm():\n\nthe parameter estimate \\(\\hat \\beta_1\\) is the marginal effect of \\(x_1\\) on \\(y\\) where \\(x_2 = 0\\)\n\nthe parameter estimate \\(\\hat \\beta_2\\) is the marginal effect of \\(x_2\\) on \\(y\\) where \\(x_1 = 0\\)\n\n\n\n\n N.B. Regardless of whether or not there is an interaction term in our model, all parameter estimates in multiple regression are “conditional” in the sense that they are dependent upon the inclusion of other variables in the model. For instance, in \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\) the coefficient \\(\\hat \\beta_1\\) is conditional upon holding \\(x_2\\) constant. \n\n\n\nInterpreting the interaction term A:B\nThe coefficient for an interaction term can be thought of as providing an adjustment to the slope.\nIn the model below, we have a numeric*categorical interaction:\n\\[\n\\begin{align}\n\\text{wellbeing} \\ = \\ &\\beta_0 + \\beta_1 \\text{social_interactions} + \\beta_2 \\text{isRural} + \\\\\n&\\beta_3 (\\text{social_interactions} \\cdot \\text{isRural}) + \\epsilon\n\\end{align}\n\\]\nThe estimate \\(\\hat \\beta_3\\) is the adjustment to the slope \\(\\hat \\beta_1\\) to be made for the individuals in the \\(\\text{isRural}=1\\) group.\n\n\nQuestion A4\n\n\nLook at the parameter estimates from your model, and write a description of what each one corresponds to on the plot shown in Figure @ref(fig:plot-annotate-int) (it may help to sketch out the plot yourself and annotate it).\n\n“The best method of communicating findings about the presence of significant interaction may be to present a table of graph of the estimated means at various combinations of the interacting variables.”\n— (Ramsey2012?)\n\n\n\nScale for 'colour' is already present. Adding another scale for 'colour',\nwhich will replace the existing scale.\n\n\nScale for 'x' is already present. Adding another scale for 'x', which will\nreplace the existing scale.\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\nMultiple regression model: Wellbeing ~ Social Interactions * is RuralNote that the dashed lines represent predicted values below the minimum observed number of social interactions, to ensure that zero on the x-axis is visible\n\n\n\n\n\n Hints.\n\n\nHere are some options to choose from:\n\nThe point at which the blue line cuts the y-axis (where social_int = 0)\nThe point at which the red line cuts the y-axis (where social_int = 0)\nThe average vertical distance between the red and blue lines.\nThe vertical distance from the blue to the red line at the y-axis (where social_int = 0)\nThe vertical distance from the red to the blue line at the y-axis (where social_int = 0)\nThe vertical distance from the blue to the red line at the center of the plot\nThe vertical distance from the red to the blue line at the center of the plot\nThe slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line\nThe slope of the red line\nHow the slope of the line changes when you move from the blue to the red line\nHow the slope of the line changes when you move from the red to the blue line\n\n\n\n\n\n\n\n\n Solution \n\n\nWe can obtain our parameter estimates using various functions such as summary(rural_mod),coef(rural_mod), coefficients(rural_mod) etc.\n\ncoefficients(rural_mod)\n\n            (Intercept)              social_int            isRuralrural \n             30.9985688               0.6487945               1.3865688 \nsocial_int:isRuralrural \n             -0.5175856 \n\n\n\n\\(\\hat \\beta_0\\) = (Intercept) = 31: The point at which the blue line cuts the y-axis (where social_int = 0).\n\n\\(\\hat \\beta_1\\) = social_int = 0.65: The slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line.\n\\(\\hat \\beta_2\\) = isRuralrural = 1.39: The vertical distance from the blue to the red line at the y-axis (where social_int = 0).\n\n\\(\\hat \\beta_3\\) = social_int:isRuralrural = -0.52: How the slope of the line changes when you move from the blue to the red line.\n\n\n\n\n\nQuestion A5\n\n\nLoad the sjPlot package and try using the function plot_model().\nThe default behaviour of plot_model() is to plot the parameter estimates and their confidence intervals. This is where type = \"est\". Try to create a plot like Figure @ref(fig:plot-annotate-int), which shows the two lines (Hint: what are this weeks’ exercises all about? type = ???.)\n\n\n\n\n Solution \n\n\n\nplot_model(rural_mod, type=\"int\")"
  },
  {
    "objectID": "1_07_int1_nc+nn.html#exercises-1",
    "href": "1_07_int1_nc+nn.html#exercises-1",
    "title": "Interactions I: Num x Cat & Num x Num",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion B1\n\n\nProduce plots of the relevant distributions and associations involved in the research question.\n\n\n\n\n Solution \n\n\n\nggplot(data = scs_study, aes(x=dass)) + \n  geom_density() + \n  geom_boxplot(width = 1/50) +\n  labs(title=\"Marginal distribution of DASS-21 Scores\", \n       x = \"Depression Anxiety and Stress Scale\", y = \"Probability density\")\n\n\n\n\n\nThe marginal distribution of scores on the Depression, Anxiety and Stress Scale (DASS-21) is unimodal with a mean of approximately 45 and a standard deviation of 7.\n\n\nggplot(data = scs_study, aes(x=scs)) + \n  geom_density() + \n  geom_boxplot(width = 1/50) +\n  labs(title=\"Marginal distribution of Social Comparison Scale (SCS) scores\", \n       x = \"Social Comparison Scale Score\", y = \"Probability density\")\n\n\n\n\n\nThe marginal distribution of score on the Social Comparison Scale (SCS) is unimodal with a mean of approximately 36 and a standard deviation of 4. There look to be a number of outliers at the upper end of the scale.\n\n\nggplot(data = scs_study, aes(x=zn)) + \n  geom_density() + \n  geom_boxplot(width = 1/50) +\n  labs(title=\"Marginal distribution of Neuroticism (Z-Scored)\", \n       x = \"Neuroticism (Z-Scored)\", y = \"Probability density\")\n\n\n\n\n\nThe marginal distribution of Neuroticism (Z-scored) is positively skewed, with the 25% of scores falling below -0.8, 75% of scores falling below 0.59.\n\n\np1 <- ggplot(data = scs_study, aes(x=scs, y=dass)) + \n  geom_point()+\n  labs(x = \"SCS\", y = \"DASS-21\")\n\np2 <- ggplot(data = scs_study, aes(x=zn, y=dass)) + \n  geom_point()+\n  labs(x = \"Neuroticism\", y = \"DASS-21\")\n\np1 | p2\n\n\n\n# the kable() function from the kableExtra package can make table outputs print nicely into html.\nscs_study %>%\n  select(dass, scs, zn) %>%\n  cor() %>% \n  kable(digits = 2) %>%\n  kable_styling(full_width = FALSE)\n\n\n\n \n  \n      \n    dass \n    scs \n    zn \n  \n \n\n  \n    dass \n    1.00 \n    -0.23 \n    0.20 \n  \n  \n    scs \n    -0.23 \n    1.00 \n    0.11 \n  \n  \n    zn \n    0.20 \n    0.11 \n    1.00 \n  \n\n\n\n\n\n\nThere is a weak, negative, linear relationship between scores on the Social Comparison Scale and scores on the Depression Anxiety and Stress Scale for the participants in the sample. Severity of symptoms measured on the DASS-21 tend to decrease, on average, the more favourably participants view their social rank.\nThere is a weak, positive, linear relationship between the levels of Neuroticism and scores on the DASS-21. Participants who are more neurotic tend to, on average, display a higher severity of symptoms of depression, anxiety and stress.\n\n\n\n\n\nQuestion B2\n\n\nRun the code below. It takes the dataset, and uses the cut() function to add a new variable called “zn_group”, which is the “zn” variable split into 4 groups.\nRemember: we have to re-assign this output as the name of the dataset (the scs_study <- bit at the beginning) to make these changes occur in our environment (the top-right window of Rstudio). If we didn’t have the first line, then it would simply print the output.\n\nscs_study <-\n  scs_study %>%\n  mutate(\n    zn_group = cut(zn, 4)\n  )\n\nWe can see how it has split the “zn” variable by plotting the two against one another:\n(Note that the levels of the new variable are named according to the cut-points).\n\nggplot(data = scs_study, aes(x = zn_group, y = zn)) + \n  geom_point()\n\n\n\n\nPlot the association between scores on the SCS and scores on the DASS-21, for each group of the variable we just created.\nHow does the pattern differ? Does it suggest an interaction?\nTip: Rather than creating four separate plots, you might want to map some feature of the plot to the variable we created in the data, or make use of facet_wrap()/facet_grid().\n\n\n\n\n Solution \n\n\n\nggplot(data = scs_study, aes(x = scs, y = dass, col = zn_group)) + \n  geom_point() + \n  facet_grid(~zn_group) +\n  theme(legend.position = \"none\") # remove the legend\n\n\n\n\nThe associaiton between SCS scores and DASS-21 scores appears to be different between these groups. For those with a relatively high neuroticism score, the association seems stronger, while for those with a low neuroticism score there is almost no discernable association. This suggests an interaction - the association of DASS-21 ~ SCS differs across the values of neuroticism!\n\n\n\n\nCutting one of the explanatory variables up into groups essentially turns a numeric variable into a categorical one. We did this just to make it easier to visualise how a an association differs across the values of another variable, because we can imagine a separate line for the association between SCS and DASS-21 scores for each of the groups of neuroticism. However, in grouping a numeric variable like this we lose information. Neuroticism is measured on a continuous scale, and we want to capture how the association between SCS and DASS-21 differs across that continuum (rather than cutting it into chunks).\nWe could imagine cutting it into more and more chunks (see Figure @ref(fig:reglinescut)), until what we end up with is a an infinite number of lines - i.e., a three-dimensional plane/surface (recall that in for a multiple regression model with 2 explanatory variables, we can think of the model as having three-dimensions). The inclusion of the interaction term simply results in this surface no longer being necessarily flat. You can see this in Figure @ref(fig:3dint).\n\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nSeparate regression lines DASS ~ SCS for neuroticism when cut into 4 (left) or 6 (center) or 12 (right) groups\n\n\n\n\n\n\n\n\n3D plot of regression surface with interaction. You can explore the plot in the figure below from different angles by moving it around with your mouse.\n\n\n\n\nQuestion B3\n\n\nSpecify the model required in order to answer the research question (e.g., \\(\\text{??} = \\beta_0 + \\beta_1 \\cdot \\text{??} + .... + \\epsilon\\)), and then fit your model using lm().\n\n\n\n\n Solution \n\n\n\n\n\nModel to answer research question:\n\\[\n\\text{DASS-21 Score} = \\beta_0 + \\beta_1 \\cdot \\text{SCS Score} + \\beta_2 \\cdot \\text{Neuroticism} + \\beta_3 \\cdot (\\text{SCS score} \\cdot \\text{Neuroticism}) + \\epsilon\n\\]\n\n#fit interaction model\ndass_mdl <- lm(dass ~ 1 + scs*zn, data = scs_study)\n\nsummary(dass_mdl)\n\n\nCall:\nlm(formula = dass ~ 1 + scs * zn, data = scs_study)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.301  -3.825  -0.173   3.733  45.777 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 60.80887    2.45399  24.780  < 2e-16 ***\nscs         -0.44391    0.06834  -6.495 1.64e-10 ***\nzn          20.12813    2.35951   8.531  < 2e-16 ***\nscs:zn      -0.51861    0.06552  -7.915 1.06e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.123 on 652 degrees of freedom\nMultiple R-squared:  0.1825,    Adjusted R-squared:  0.1787 \nF-statistic:  48.5 on 3 and 652 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "1_08_int2_cc.html",
    "href": "1_08_int2_cc.html",
    "title": "Interactions II: Cat x Cat",
    "section": "",
    "text": "At the end of this lab, you will:\n\nBe able to interpret a categorical \\(\\times\\) categorical interaction.\nVisualize and probe interactions.\nBe able to read interaction plots.\n\n\n\n\nYou will need to have completed [Lab X].\n\n\n\nRemember to load all packages within a code chunk at the start of your RMarkdown file using library(). If you do not have a package and need to install, do so within the console using install.packages(\" \"). For further guidance on installing/updating packages, see Section C here.\nFor this lab, you will need to load the following package(s):\n\ntidyverse\nsjPlot\nemmeans\n\n\n\n\nYou can download the data required for this lab here or read it in via this link https://uoepsy.github.io/data/cognitive_experiment_3_by_2.csv"
  },
  {
    "objectID": "1_08_int2_cc.html#visualising-the-interactions",
    "href": "1_08_int2_cc.html#visualising-the-interactions",
    "title": "Interactions II: Cat x Cat",
    "section": "Visualising the interactions",
    "text": "Visualising the interactions\n\n\n\n\n\nIn the interaction plot above you can see three highlighted differences, where the differences are denoted with the Greek letter \\(\\Delta\\) (“delta”) with a hat on top, \\(\\hat \\Delta\\), to denote that those are estimates for the unknown population differences based on the available sample data. The corresponding population differences are unknown as we don’t have the data for the entire population, and they are denoted with a \\(\\Delta\\) without a hat on top.\nYou can see highlighted:\n\nThe difference in the mean score between Recognition and Grammar for Control patients, \\(\\hat \\Delta_{\\text{Control}}\\)\nThe difference in the mean score between Recognition and Grammar for Amnesic patients, \\(\\hat \\Delta_{\\text{Amnesic}}\\)\nThe difference in the mean score between Recognition and Grammar for Huntingtons patients, \\(\\hat \\Delta_{\\text{Huntingtons}}\\)\n\nAn interaction is present if the effect of Task (i.e. the difference in mean score between Recognition and Grammar tasks) substantially varies across the possible values for Diagnosis. That is, if the difference for Amnesic is not the same as that for Control, or if the difference for Huntingtons is not the same as that for Control, or both.\nThe model summary returns two rows for the interactions. Let’s focus on this row:\n## DiagnosisAmnesic:TaskRecognition      -10.000     11.719  -0.853  0.40192    \nIt returns an estimate for the difference\n\\[\n\\hat \\beta_4 = \\hat \\Delta_{\\text{Amnesic}} - \\hat \\Delta_{\\text{Control}}\n\\]\nand also performs a test for the hypothesis that the differences are equal in the population:\n\\[\nH_0: \\Delta_{\\text{Amnesic}} = \\Delta_{\\text{Control}} \\\\\nH_1: \\Delta_{\\text{Amnesic}} \\neq \\Delta_{\\text{Control}}\n\\]\nor, equivalently, that:\n\\[\nH_0: \\Delta_{\\text{Amnesic}} - \\Delta_{\\text{Control}} = 0 \\\\\nH_1: \\Delta_{\\text{Amnesic}} - \\Delta_{\\text{Control}} \\neq 0\n\\]\nLet’s now focus on the last row:\n## DiagnosisHuntingtons:TaskRecognition   40.000     11.719   3.413  0.00228 ** \nIt returns an estimate for the difference\n\\[\n\\hat \\beta_5 = \\hat \\Delta_{\\text{Huntingtons}} - \\hat \\Delta_{\\text{Control}}\n\\]\nand also performs a test for the hypothesis that the differences are equal in the population:\n\\[\nH_0: \\Delta_{\\text{Huntingtons}} = \\Delta_{\\text{Control}} \\\\\nH_1: \\Delta_{\\text{Huntingtons}} \\neq \\Delta_{\\text{Control}}\n\\]\nor, equivalently, that:\n\\[\nH_0: \\Delta_{\\text{Huntingtons}} - \\Delta_{\\text{Control}} = 0 \\\\\nH_1: \\Delta_{\\text{Huntingtons}} - \\Delta_{\\text{Control}} \\neq 0\n\\]\n\nQuestion 8\n\n\nInterpret the model output in the context of the research hypothesis.\n\n\n\n\n Solution \n\n\nLet’s recall the researchers’ hypothesis:\n\nA group of researchers wants to test an hypothesised theory according to which the difference in performance between explicit and implicit memory tasks will be greatest for Huntington patients in comparison to controls.\nOn the other hand, the difference in performance between explicit and implicit memory tasks will not significantly differ between patients with amnesia in comparison to controls.\n\nWe can get a nice printout of the model summary as follows:\n\n\ntab_model(mdl_int, show.stat = TRUE)\n\n\n\n\n \nScore\n\n\nPredictors\nEstimates\nCI\nStatistic\np\n\n\n(Intercept)\n80.00\n67.91 – 92.09\n13.65\n<0.001\n\n\nDiagnosis [Amnesic]\n-20.00\n-37.10 – -2.90\n-2.41\n0.024\n\n\nDiagnosis [Huntingtons]\n-40.00\n-57.10 – -22.90\n-4.83\n<0.001\n\n\nTask [Recognition]\n15.00\n-2.10 – 32.10\n1.81\n0.083\n\n\nDiagnosis [Amnesic] *Task [Recognition]\n-10.00\n-34.19 – 14.19\n-0.85\n0.402\n\n\nDiagnosis [Huntingtons] *Task [Recognition]\n40.00\n15.81 – 64.19\n3.41\n0.002\n\n\nObservations\n30\n\n\nR2 / R2 adjusted\n0.739 / 0.685\n\n\n\n\n\n\n\n\nWe could interpret it as follows:\n\nThe difference in scores between the recognition and grammar tasks, respectively measuring explicit and implicit memory, for Huntingtons patients in comparison to controls was significant and indicated a difference of \\(\\hat \\beta_5 = 40\\) points in explicit vs implicit memory performance: \\(t(24) = 3.41, p = 0.002\\).\nThe difference in scores between the recognition and grammar tasks, respectively measuring explicit and implicit memory, for amnesiac patients in comparison to controls was estimated to be \\(\\hat \\beta_4 = -10\\) points but it is not found to be significantly different from 0: \\(t(24) = -0.85, p = 0.40\\).\nThis indicates that the researchers’ hypothesis that the difference in performance between explicit and implicit memory tasks does not differ significantly between amnesic and control patients, while it does differ significantly between Huntington and control patients.\n\nWe can also provide an interpretation of the interaction plot:\n\nplot_model(mdl_int, type = \"int\")\n\n\n\n\nCompared to controls, amnesiac patients will have a significant deficit in explicit memory (as measured by the recognition task), but not on implicit memory (as measured by the grammar task).\nCompared to controls, Huntingtons patients will have a significant deficit in implicit memory (as measured by the grammar task) but not in explicit memory (as measured by the recognition task)."
  }
]