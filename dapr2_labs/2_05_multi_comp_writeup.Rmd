---
title: "Multiple Comparisons, Corrections, & Write-Up"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
set.seed(3)
```

:::lo
**LEARNING OBJECTIVES**

1. Understand how to apply corrections available for multiple comparisons.  
2. Understand how to write-up and provide interpretation of a 2x2 factorial ANOVA.

:::

```{r echo=FALSE}
library(emmeans)
library(tidyverse)
cog <- read_csv('https://uoepsy.github.io/data/cognitive_experiment.csv')
# head(df)

cog$Diagnosis <- factor(cog$Diagnosis, 
                       labels = c("amnesic", "huntingtons", "control"),
                       ordered = FALSE)

cog$Task <- factor(cog$Task, 
                  labels = c("grammar", "classification", "recognition"), 
                  ordered = FALSE)

cog$Diagnosis <- fct_relevel(cog$Diagnosis, "control")
cog$Task <- fct_relevel(cog$Task, "recognition")

cog <- cog %>%
    rename(Score = Y)

mdl_int <- lm(Score ~ Diagnosis * Task, data = cog)

# ggplot(cog, aes(x = Task, y = Score, color = Task)) +
#     geom_boxplot() 
# ggplot(cog, aes(x = Diagnosis, y = Score, color = Diagnosis)) +
#     geom_boxplot()
```  

# Multiple Comparisons  

In last week's exercises we began to look at how we compare different groups, by using contrast analysis to conduct tests of specific comparisons between groups. We also saw how we might conduct *pairwise comparisons*, where we test all possible pairs of group means within a given set.  

For instance, we can compare the means of the different diagnosis groups for each task: 
```{r}
emm_task <- emmeans(mdl_int, ~ Diagnosis | Task)
contr_task <- contrast(emm_task, method = 'pairwise')
contr_task
```

or we can test all different combinations of task and diagnosis group (if that was something we were theoretically interested in, which is unlikely!) which would equate to conducting 36 comparisons!  

```{r}
emm_task <- emmeans(mdl_int, ~ Diagnosis * Task)
contr_task <- contrast(emm_task, method = 'pairwise')
contr_task
```

`r optbegin("36? how do we know there are 36?", olabel=FALSE, toggle=params$TOGGLE)`
There are 3 diagnosis groups, and 3 tasks, meaning there are 9 different group means. All possible pairwise comparisons would is all different possible combinations of 2 from a set of 9.  We can work this out using the rule: 

$$
_nC_r = \frac{n!}{r!(n-r)!} \\
\begin{align} \\
& \text{Where:} \\
& n = \text{total number in the set} \\
& r = \text{number chosen} \\
& _nC_r = \text{number of combinations of r from n} \\
\end{align}


$$
In R:
```{r}
factorial(9)/(factorial(2)*(factorial(9-2)))
```
Or, easier still:
```{r}
dim(combn(9, 2))
```
`r optend()`

## Why does the number of tests matter?  

`r optbegin("refresher on making errors in hypothesis tests", olabel=FALSE,toggle=params$TOGGLE)`  
Think back to ["Type 1 errors" from DAPR1](https://uoe-psychology.github.io/uoe_psystats/dapr1/book/chap-typeerror.html) - when we conduct an hypothesis test, and we set $\alpha=0.05$, we will reject the null hypothesis $H_0$ when we find a $p < .05$. Now remember what a $p$-value represents - it is the chance of observing a statistic at least as extreme as the one we do have, assuming the null hypothesis to be true. This means that *if* $H_0$ **is** true, then we will still observe a $p < .05$ 5\% of the time. So our chance of making this error = the threshold ($\alpha$) at which below a p-value results in us rejecting $H_0$. 
`r optend()` 

But this error-rate applies to each statistical hypothesis we test. So if we conduct an experiment in which we plan on conducting lots of tests of different comparisons, the chance of an error being made increases substantially. Across the family of tests performed that chance will be much higher than 5\%.^[what defines a 'family' of tests is debateable.]

Each test conducted at $\alpha = 0.05$ has a 0.05 (or 5%) probability of Type I error (wrongly rejecting the null hypothesis). If we do 9 tests, that experimentwise error rate is $\alpha_{ew} \leq 9 \times 0.05$, where 9 is the number of comparisons made as part of the experiment.

Thus, if nine **independent** comparisons were made at the $\alpha = 0.05$ level, the experimentwise Type I error rate $\alpha_{ew}$ would be at most $9 \times 0.05 = 0.45$. That is, we could wrongly reject the null hypothesis on average 45 times out of 100. 
To make this more confusing, many of the tests in a family are not **independent** (see the lecture slides for the calculation of error rate for dependent tests).  

Here, we go through some of the different options available to us to control, or 'correct' for this problem. 

# Corrections

## Bonferroni

`r qbegin(1)`

Load the data from last week, and re-acquaint yourself with it. 

Provide a plot of the Diagnosis*Task group mean scores.  

The data is at https://uoepsy.github.io/data/cognitive_experiment.csv. 
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r eval=FALSE}
cog <- read_csv('https://uoepsy.github.io/data/cognitive_experiment.csv')
# head(df)

cog$Diagnosis <- factor(cog$Diagnosis, 
                       labels = c("amnesic", "huntingtons", "control"),
                       ordered = FALSE)

cog$Task <- factor(cog$Task, 
                  labels = c("grammar", "classification", "recognition"), 
                  ordered = FALSE)

cog$Diagnosis <- fct_relevel(cog$Diagnosis, "control")
cog$Task <- fct_relevel(cog$Task, "recognition")

cog <- cog %>%
    rename(Score = Y)
```
`r solend()`

`r qbegin(2)`

Fit the interaction model, using `lm()`. 
Pass your model to the `anova()` function, to remind yourself that there is a significant interaction present.  

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
mdl_int <- lm(Score ~ Task*Diagnosis, data = cog)
anova(mdl_int)
```
`r solend()`

`r qbegin(3)`
There are various ways to make nice tables in RMarkdown.  
Some of the most well known are:

- The **knitr** package has `kable()`  
- The **pander** package has `pander()`  

Pick one (or find go googling and find a package you like the look of), install the package (if you don't already have it), then try to create a nice pretty ANOVA table rather than the one given by `anova(model)`.  

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
library(knitr)
kable(anova(mdl_int))

library(pander)
pander(anova(mdl_int))
```

`r solend()`

`r qbegin(4)`
As in the previous week's exercises, let us suppose that we are specifically interested in comparisons of the mean score *across the different diagnosis groups for a given task*.  

Edit the code below to obtain the pairwise comparisons of diagnosis groups for each task. Use the Bonferroni method to adjust for multiple comparisons, and then obtain confidence intervals.  

```{r eval=FALSE}
library(emmeans)
emm_task <- emmeans(mdl_int, ? )
contr_task <- contrast(emm_task, method = ?, adjust = ? )
```

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
emm_task <- emmeans(mdl_int, ~ Diagnosis | Task)
contr_task <- contrast(emm_task, method = "pairwise", adjust="bonferroni")
contr_task
confint(contr_task)
```
`r solend()`

:::frame
**adjusting $\alpha$, adjusting p**

In the lecture we talked about adjusting the $\alpha$ level (i.e., instead of determining significance at $p < .05$, we might adjust and determine a result to be statistically significant if $p < .005$, depending on how many tests are in our family of tests).  

Note what the functions in R do is adjust the $p$-value, rather than the $\alpha$. The Bonferroni method simply multiplies the 'raw' p-value by the number of the tests. 

:::


`r qbegin(5)`
In question 4 above, there are 9 tests being performed, but there are 3 in each 'family' (each `Task`). 

Try changing your answer to question 4 to use `adjust = "none"`, rather than `"bonferroni"`, and confirm that the p-values are 1/3 of the size. 
  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

The first Bonferroni adjusted p-value is 0.0492. 

```{r}
0.0492/3
```

Let's check that this is the raw p-value:
```{r}
contrast(emm_task, method = "pairwise", adjust="none")
```

`r solend()`

## Šídák

`r qbegin(6)`
The Sidak approach is slightly less conservative than the Bonferroni adjustment. Doing this with the **emmeans** package is easy, can you figure out how?  

**Hint:** you just have to change the `adjust` argument in `contrast()` function. 

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
contrast(emm_task, method = "pairwise", adjust = "sidak")
```

`r solend()`


## Tukey

Like with Šídák, in R we can easily change to Tukey. For instance, if we wanted to conduct pairwise comparisons of the scores of different Diagnosis groups on different Task types (i.e., the interaction): 
```{r}
emm_task <- emmeans(mdl_int, ~ Diagnosis*Task)
contr_task <- contrast(emm_task, method = "pairwise", adjust="tukey")
contr_task
```

We can also use the following, which doesn't require the **emmeans** package. You might see this when you look online for resources. The `aov()` function is fitting an ANOVA model, and then `TukeyHSD()` compares between Diagnosis group; between Task type; and between Diagnosis*Task.  

Run the code below yourself to see the output. 

```{r eval=FALSE}
TukeyHSD(aov(Score ~ Diagnosis * Task, data = cog))
```

## Scheffe  

```{r}
emm_task <- emmeans(mdl_int, ~ Diagnosis * Task)
contr_task <- contrast(emm_task, method = "pairwise", adjust="scheffe")
contr_task
```

### When to use Which Correction

:::yellow
**Bonferroni**  

- Use Bonferroni’s method when you are interested in a small number of planned contrasts (or pairwise comparisons).
- Bonferroni's method is to divide alpha by the number of tests/confidence intervals.
- Assumes that all comparisons are independent of one another.  
- It sacrifices slightly more power than Tukey’s method (discussed below), but it can be applied to any set of contrasts or linear combinations (i.e., it is useful in more situations than Tukey).
- It is usually better than Tukey if we want to do a small number of planned comparisons.  

**Šídák**  

- (A bit) more powerful than the Bonferroni method.
- Assumes that all comparisons are independent of one another. 
- Less common than Bonferroni method, largely because it is more difficult to calculate (not a problem now we have computers).  

**Tukey**

- It specifies an exact family significance level for comparing all pairs of treatment means.  
- Use Tukey’s method when you are interested in all (or most) pairwise comparisons of means.  

**Scheffe**

- It is the most conservative (least powerful) of all tests.
- It controls the family alpha level for testing all possible contrasts.
- It should be used if you have not planned contrasts in advance.
- For testing pairs of treatment means it is too conservative (you should use Bonferroni or Šídák).

:::blue

In **R**, you can easily change which correction you are using via the `adjust = ` argument.

:::

:::

# Lie Detection Experiment - Write Up Example

In this section of the lab, you will be presented with a research question, and tasked with writing up and presenting your analyses. Try to write three complete sections - Analysis Strategy, Results, and Discussion. Make sure to familiarise yourself with the data available in the codebook below. We will use the questions to go through the analysis step by step, before writing up. Please note that the lab includes short example write ups that may not be complete for every question you are asked. It is intended to give you a sense of the style. Think about the steps you need to complete in order to answer the research question, and the order in which you should complete these. 

`r optbegin("Lie detectors: Data Codebook", olabel=FALSE, toggle=params$TOGGLE)`

120 participants took part in a study in which they were presented with 100 recordings, and were tasked with guessing whether the speaker in each recording was lying or whether they were telling the truth.  

Participants scored points every time they correctly identified a truth or a lie, and lost points whenever they mistook a lie for a truth (or vice versa). The maximum possible points to be scored was 100. 

Half of the participants (60) were shown recordings in audio and video, the other half were presented with only the audio track. 

Prior to taking part in the experiment, participants were given material to read for 10 minutes. Half of the participants in each condition (30 in the audio-only condition, and 30 in the audiovideo condition) were given instructional material used by the Police Force to train detectives to pick up on dishonesty during interrogations via various verbal and non-verbal cues. The remaining 30 participants in each condition were given a series of [cartoon strips to read](https://external-preview.redd.it/OY7xRuBBRkx8JFrjYoh3PswM2VMBDHBv4oXbpIEcB3Q.jpg?auto=webp&s=1a12316259fdfd314d4060f44f30260735741da5).  

The data is available at https://uoepsy.github.io/data/lietraining.csv  

`r optend()`

> Research Question
>
> Do Police training materials and the mode of communication influence the accuracy of veracity judgements?  


`r qbegin(1)`

Step 1 is always to read in the data, then to explore, check, describe, and visualise it.

- Load the data
- Examine the data
- Check coding of variables (ie., are categorical variables factors?)
- Produce a descriptives table for the variables of interest
- Produce a plot showing the mean points for each condition

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
#read in data
liedat <- read_csv("https://uoepsy.github.io/data/lietraining.csv")

#check coding
head(liedat)

#make variables factors & label
liedat$audiovideo <- factor(liedat$audiovideo, labels=c("audio","audio+video"))
liedat$trained <- factor(liedat$trained, labels = c("untrained","trained"))
```


```{r}
#descriptives table
liestats <- liedat %>% group_by(audiovideo, trained) %>%
  summarise(meanpoints = mean(points),
            se = sd(points)/sqrt(n())
  )
liestats
```


```{r}
#plot
ggplot(liestats, aes(x = audiovideo, y = meanpoints, color = trained)) + 
  geom_point(size = 3) +
  geom_linerange(aes(ymin = meanpoints - 2 * se, ymax = meanpoints + 2 * se)) +
  geom_path(aes(x = as.numeric(audiovideo)))
```

`r solend()`

`r qbegin(2)`

Step 2 is to run your model(s) of interest to answer your research question, and make sure that the data meet the assumptions of your chosen test. 

-  Conduct a two-way ANOVA
- Check assumptions

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
#build model
lie_mdl <- lm(points ~ audiovideo * trained, data = liedat)
```

```{r}
par(mfrow=c(2,2))
plot(lie_mdl)
par(mfrow=c(1,1))
```

```{r}
# look at model output - summary() and anova()
summary(lie_mdl)
anova(lie_mdl)
```

`r solend()`

`r qbegin(3)`
  
The third and final step(s) somewhat depends on the outcomes of step 2. Here, you may need to consider conducting further analyses before writing up / describing your results in relation to the research question. 

- Perform a pairwise comparison of the mean accuracy (as measured by points accrued) across the 2×2 factorial design, making sure to adjust for multiple comparisons by the method of your choice. 
- Interpret your results in relation to the research question (remember you can refer to your previous plotting when doing this!).

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
emms_lie <- emmeans(lie_mdl, ~ audiovideo * trained)

lie_con <- contrast(emms_lie, method = "pairwise", adjust="tukey")
lie_con

# confidence intervals
confint(lie_con)

#plot
plot(lie_con)
```

`r solend()`

### Analysis Strategy

`r optbegin("Example Write-Up of Analysis Strategy Section", olabel=FALSE, toggle = params$TOGGLE)`

Data obtained from https://uoepsy.github.io/data/lietraining.csv contained information on `nrow(liedat)` participants who took part in a study concerning lie detection. Participants were each presented with 100 recordings (half were shown recordings in audio and video, and the other half audio only), and were tasked with guessing whether the speaker in each recording was lying or whether they were telling the truth. Participants scored 1 point each time they correctly identified a truth or a lie, and lost 1 point whenever they mistook a lie for a truth (or vice versa). The maximum score was 100, where higher scores reflected higher levels of accuracy. Prior to taking part in the experiment participants were given materials to read. Half of the participants in each condition were given instructional material used by the Police Force (used to train detectives to pick up on dishonesty during interrogations via various verbal and non-verbal cues) and the remaining 30 participants in each condition were given a series of [cartoon strips to read](https://external-preview.redd.it/OY7xRuBBRkx8JFrjYoh3PswM2VMBDHBv4oXbpIEcB3Q.jpg?auto=webp&s=1a12316259fdfd314d4060f44f30260735741da5).  

All participant data was complete, and accuracy scores (points) within range i.e., 0-100. Categorical variables were coded as factors, where audio was the reference level for mode of communication, and untrained was the reference level for training materials.

To investigate whether police training materials (trained vs untrained) and the mode of communication (audio vs audiovideo) interacted to influence the accuracy of veracity judgements, a two-way ANOVA model was used. Effects were be considered statistically significant at $\alpha = 0.05$. Using dummy coding, the following model specification was used:

\begin{aligned}
\text{Points} &= \beta_0 \\
&+ \beta_1 D_\text{AudioVideo} + \beta_2 D_\text{Trained} \\
&+ \beta_3 (D_\text{AudioVideo} * T_\text{Trained}) \\
&+ \epsilon 
\end{aligned}

To address the research question of whether the interaction between training materials and mode of communication is significant, this corresponded to testing whether the interaction coefficient was equal to zero:

$$
H_0: \beta_3 = 0 \\
H_1: \beta_3 \neq 0
$$
The following assumptions were visually assessed using diagnostic plots: linearity (via plot of residuals vs fitted values), independence (with the previous plot and a plot of residuals vs index), equal variances (via a scale-location plot), and normality (via a qqplot of the residuals). We will also check if there is any evidence of multicollinearity by checking the VIF values, where values > 5 will be considered to indicate moderate multicollinearity, and values > 10 severe.

`r optend()`

### Results
`r optbegin("Example Write-Up of Results Section", olabel=FALSE, toggle = params$TOGGLE)`

Descriptive statistics are displayed in Table \@ref(tab:descript). 

```{r descript, echo = FALSE, caption = "Descriptives Table"}
descript <- liedat %>% 
    group_by(audiovideo, trained) %>%
   summarise(
       meanpoints = round(mean(points), 2),
       se = round(sd(points)/sqrt(n()), 2)
    )
knitr::kable(descript, caption = "Descriptive Statistics")
```

In the audio condition, there did not appear to be a difference between trained and non-trained scores. However, untrained scored higher than trained in the audio+video condition. There appeared to be an interaction (see Figure \@ref(fig:lieplot)). 

```{r lieplot, echo = FALSE, fig.cap = "Interaction Plot", fig.align = "center"}
#plot
ggplot(liestats, aes(x = audiovideo, y = meanpoints, color = trained)) + 
  geom_point(size = 3) +
  geom_linerange(aes(ymin = meanpoints - 2 * se, ymax = meanpoints + 2 * se)) +
  geom_path(aes(x = as.numeric(audiovideo)))
```

Accuracy of veracity judgements (measured by points scored in lie-detecting game) were analysed with a 2 (audio vs audiovideo) $\times$ 2 (untrained vs trained) between-subjects ANOVA. 

The model met assumptions of linearity and independence (see top left panel of Figure \@ref(fig:assumpt)), homoscedasticity (see bottom left panel of Figure \@ref(fig:assumpt)), and normality (see top right panel of Figure \@ref(fig:assumpt)) where QQplot showed very little deviation from the diagonal line.

```{r assumpt, echo = FALSE, fig.cap = "Assumption Checks", fig.align = "center"}
par(mfrow=c(2,2))
plot(lie_mdl)
par(mfrow=c(1,1))
```

There was a significant interaction between presentation mode and whether or not participants had received training for detecting lies $F(1, 116) = 17.37, p <. 001$ (see Table \@ref(tab:aovtable) and Figure \@ref(fig:predint)). 

```{r aovtable, echo = FALSE, caption = "ANOVA Output"}
library(pander)
pander(anova(lie_mdl))
```

Results suggested that the difference in points did not differ significantly in the audio condition regardless of training, but that there were significant differences in the audio video condition.

```{r predint, echo = FALSE, fig.cap = "Predicted Scores", fig.align = "center"}
library(sjPlot)
plot_model(lie_mdl, type = "pred",  terms = c("audiovideo", "trained"))
```

To explore the interaction further, pairwise comparisons were conducted. Tukey’s Honestly Significant Difference comparisons (see Figure \@ref(fig:contrastplot)) indicated that, contrary to what one might expect, participants who were presented with audiovisual recordings scored on average 9.5 points lower when they had read the police training materials compared to when they had received no training (95\% CI [4.87 — 14.14]). The presentation mode (audio vs audio-video) was not found to result in a significantly different average score for those who were untrained (95\% CI [-1.80 — 7.47]), and nor did training appear to have any effect on detecting lies in the audio-only condition (95\% CI [-5.61 — 3.66]).

```{r contrastplot, echo = FALSE, out.width="80%", fig.cap = "Tukey HSD Pairwise Comparisons", fig.align = "center"}
plot(lie_con)
```


`r optend()`

### Discussion
`r optbegin("Example Write-Up of Discussion Section", olabel=FALSE, toggle = params$TOGGLE)`

The findings indicated that, on the whole, people are bad at determining whether they are being presented with a lie or the truth, as the overall mean score was `r mean(liedat$points) %>% round(1)` out of 100 (SD = `r sd(liedat$points) %>% round(2)`), where a series of completely random guesses expected to score 50/100. The police training materials did not appear to improve lie detection, in fact with trained participants performing worse (compared to untrained) in the audiovisual condition. This may indicate that the training materials focus too heavily on visual cues, which perhaps are not actually associated with dishonesty in the appropriate way. 

<!-- Formatting -->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
