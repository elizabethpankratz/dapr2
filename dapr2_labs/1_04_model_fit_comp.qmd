---
title: "Model Fit and Comparison"
link-citations: TRUE
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source('assets/setup.R')
library(patchwork)
library(checkdown)

# knitr::opts_chunk$set(cache = TRUE)
set.seed(1)
```

:::lo
### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand how to calculate the interpret $R^2$ and adjusted-$R^2$ as a measure of model quality.
1. Understand the calculation and interpretation of the $F$-test of model utility.
1. Understand how to standardize model coefficients and when this is appropriate to do.

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1. Be up to date with lectures
2. Have completed previous lab exercises from [Week 1](https://uoepsy.github.io/dapr2/2223/labs/1_01_function.html), [Week 2](https://uoepsy.github.io/dapr2/2223/labs/1_02_slr.html), and [Week 3](https://uoepsy.github.io/dapr2/2223/labs/1_03_mlr.html)

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse**
* **patchwork**
* **sjPlot**

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/wellbeing.csv) or read it in via this link https://uoepsy.github.io/data/wellbeing.csv. 

**Note**:  this is the same data as Lab 3.

:::

# Study Overview 

> **Research Question** 
>
> Is there an association between well-being and time spent outdoors *after* taking into account the association between well-being and social interactions? 

`r optbegin("Wellbeing data codebook.", olabel=FALSE)`  

__Description__

Researchers interviewed 32 participants, selected at random from the population of residents of Edinburgh & Lothians. They used the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.  

The researchers also asked participants to estimate the average number of hours they spend outdoors each week, the average number of social interactions they have each week (whether on-line or in-person), and whether they believe that they stick to a routine throughout the week (Yes/No).  

The data in `wellbeing.csv` contain five attributes collected from a random sample of $n=32$ hypothetical residents over Edinburgh & Lothians, and include:

- `wellbeing`: Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.  
- `outdoor_time`: Self report estimated number of hours per week spent outdoors  
- `social_int`: Self report estimated number of social interactions per week (both online and in-person)
- `routine`: Binary Yes/No response to the question "Do you follow a daily routine throughout the week?"
- `location`: Location of primary residence (City, Suburb, Rural)

__Preview__

The first six rows of the data are:

```{r echo=FALSE, message=FALSE}
read_csv('https://uoepsy.github.io/data/wellbeing.csv') %>% head %>% gt::gt()
```
  
`r optend()`


# Setup
`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read the wellbeing dataset into R, assigning it to an object named `mwdata`

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r message=FALSE}
#Loading the required package(s)
library(tidyverse)
library(patchwork)
library(sjPlot)

# Reading in data and storing to an object named 'mwdata'
mwdata <- read_csv("https://uoepsy.github.io/data/wellbeing.csv")
```
`r solend()`

# Exercises

`r qbegin(1)`
Specify and fit a linear model to investigate how wellbeing (WEMWBS scores) are associated with time spent outdoors *after* controlling for the number of social interactions. 

Next, check the `summary()` output from the model.

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

The model is:
$$
\widehat{Wellbeing} = \hat \beta_0 + \hat \beta_1 \cdot Social Interactions + \hat \beta_2 \cdot Outdoor Time 
$$
In `R`:
```{r message=FALSE}
#fit model
mdl1 <- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)

#look at summary output from model
summary(mdl1)
```

Formally state:

+ your chosen significance level 
+ the null and alternative hypotheses

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

Effects will be considered statistically significant at $\alpha=.05$

$H_0: \beta_2 = 0$

There is no association between well-being and time spent outdoors after taking into account the relationship between well-being and social interactions

$H_1: \beta_2 \neq 0$

There is an association between well-being and time spent outdoors after taking into account the relationship between well-being and social interactions

`r solend()`

## Lab Purpose

In this lab (Lab 4), you will focus on the statistics contained within the highlighted sections of the `summary()` output below. You will be both calculating these by hand and deriving via `R` code before interpreting these values in the context of the research question following [APA guidelines](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf).

```{r echo = FALSE, out.width='85%'}
#knitr::include_graphics('images/summary_mwdata.PNG')
```

<br>

`r qbegin(2)`
What is the proportion of the total variability in wellbeing scores explained by the model?

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

The question asks to compute the value of $R^2$. Since the model includes 2 predictors, you should report the Adjusted $R^2$.

:::

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
The proportion of the total variability explained is given by R-squared.

The R-squared coefficient is defined as:
$$
R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}
$$
The Adjusted R-squared coefficient is defined as:

$$
\hat R^2 = 1 - \frac{(1 - R^2)(n-1)}{n-k-1}
\quad \\
\begin{align}
& \text{Where:} \\
& n = \text{sample size} \\
& k = \text{number of explanatory variables} \\
\end{align}
$$
::: {.panel-tabset}

## Manually

In `R` we can write:
```{r}
#R squared & adjusted R squared

wellbeing_fitted <- mwdata %>%
  mutate(
    wellbeing_hat = predict(mdl1),
    resid = wellbeing - wellbeing_hat
  )


wellbeing_fitted %>%
  summarise(
    SSModel = sum( (wellbeing_hat - mean(wellbeing))^2 ),
    SSTotal = sum( (wellbeing - mean(wellbeing))^2 )
  ) %>%
  summarise(
    RSquared = SSModel / SSTotal,
    AdjRSquared = 1-((1-(RSquared))*(32-1)/(32-2-1))
  )


```

## R function

```{r}
#look in second bottom row - Multiple R Squared and Adjusted R Squared both reported here
summary(mdl1)
```

The output of `summary()` displays the Adjusted $R$-squared value in the following line:

```
Adjusted R-squared:  0.7224 
```

:::
---

**Interpretation**

::: {.callout-important icon=false appearance="minimal"}

Approximately 72\% of the total variability in wellbeing scores is explained by associations with social interactions and outdoor time.

:::

`r solend()`

<br>

`r qbegin(3)`
Perform a model utility test at the 5\% significance level and report your results. 

In other words, conduct an $F$-test against the null hypothesis that the model is ineffective at predicting wellbeing scores using social interactions and outdoor time by computing the $F$-statistic using its definition.

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

The F-ratio is used to test the null hypothesis that all regression slopes are zero.  
It is called the F-ratio because it is the ratio of the how much of the variation is explained by the model (per paramater) versus how much of the variation is left unexplained in the residuals (per remaining degrees of freedom). 

$$
F_{df_{model},df_{residual}} = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\
\quad \\
\begin{align}
& \text{Where:} \\
& df_{model} = k \\
& df_{residual} = n-k-1 \\
& n = \text{sample size} \\
& k  = \text{number of explanatory variables} \\
\end{align}
$$
:::

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

::: {.panel-tabset}

## Manually

```{r}
df1 <- 2
df2 <- nrow(mwdata) - 2 - 1
f_star <- qf(0.95, df1, df2)
f_star
```

```{r}
model_utility <- wellbeing_fitted %>%
  summarise(
    SSModel = sum((wellbeing_hat - mean(wellbeing))^2 ),
    SSResid = sum( resid^2 ),
    MSModel = SSModel / df1,
    MSResid = SSResid / df2,
    FObs = MSModel / MSResid
  )
model_utility
```

We can also compute the p-value:

```{r}
pvalue <- 1 - pf(model_utility$FObs, df1, df2)
pvalue
```

The value `3.225548e-09` simply means $3.2 \times 10^{-9}$, so it's a really small number.

## R function

```{r}
#look in bottom row
summary(mdl1)
```

The relevant row is the following:

```

F-statistic: 41.34 on 2 and 29 DF,  p-value: 3.226e-09

```

:::

::: {.callout-important icon=false appearance="minimal"}

We performed an $F$-test of model utility at the 5\% significance level, where $F(2,29) = 41.34, p <.001$.

The large $F$-statistic and small $p$-value ($<.001$) suggested that we have very strong evidence against the null hypothesis that the model is ineffective.

In other words, the data provide strong evidence that the number of social interactions and outdoor time are effective predictors of wellbeing scores.


:::

`r solend()`

<br>

`r qbegin(4)`
Produce a visualisation of the association between wellbeing and outdoor time, after accounting for social interactions.  

:::{.callout-tip appearance="simple" collapse="true"}

### Hint
To visualise just one association, you might need to specify the `terms` argument in `plot_model()`. Don't forget you can look up the documentation by typing `?plot_model` in the console. 

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
plot_model(mdl1, type = "eff",
           terms = c("outdoor_time"), 
           show.data = TRUE)
```

`r solend()`

<br>

`r qbegin(5)`

Interpret the standardized variables presented in the above table.

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

- For every standard deviation increase in social interactions, wellbeing scores increased on average by 0.67 standard deviations.
- For every standard deviation increase in outdoor time, wellbeing scores increased on average by 0.35 standard deviations.

`r solend()`

<br>

`r qbegin(9)`

Compare the two following models, each looking at the associations of Wellbeing scores and two different predictor variables. 

$\text{Wellbeing} = \beta_0 + \beta_1 \cdot \text{Social Interactions} + \beta_2 \cdot \text{Age} + \epsilon$  

$\text{Wellbeing} = \beta_0 + \beta_1 \cdot \text{Outdoor Time} + \beta_2 \cdot \text{Routine} + \epsilon$ 

In [APA format](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf), report which model you think best fits the data. 


:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

Compare using `AIC()` and `BIC()` since the models are non-nested.

:::

`r qend()`

`r solbegin(show=TRUE, toggle=params$TOGGLE)`

```{r, eval=FALSE}
#fit models
wb_socint_age <- lm(wellbeing ~ social_int + age, data = mwdata)
wb_outdoor_routine <- lm(wellbeing ~ outdoor_time + routine, data = mwdata)
```

```{r eval=FALSE}
#AIC values
AIC(wb_socint_age, wb_outdoor_routine)

#BIC values
BIC(wb_socint_age, wb_outdoor_routine)
```

::: {.callout-important icon=false appearance="minimal"}

We used AIC and BIC model selection to distinguish between two possible models describing the association between several personal factors and wellbeing scores. The model with  outdoor time and routine included as predictors was better fitting (AIC = 1220.91) the alternative model with  weekly number of social interactions and age (AIC = 1236.58). Based on the BIC value of the former model (BIC = 1234.11) we concluded that it was substantively better fitting than the alternative, latter model (BIC = 1249.77). 

:::

`r solend()`

<br>

`r qbegin(10)`

The code below fits 5 different models based on our `wrdata`:

```{r eval=FALSE}
model1 <- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)
model2 <- lm(wellbeing ~ social_int + outdoor_time + age, data = mwdata)
model3 <- lm(wellbeing ~ social_int + outdoor_time + routine, data = mwdata)
model4 <- lm(wellbeing ~ social_int + outdoor_time + routine + age, data = mwdata)
model5 <- lm(wellbeing ~ social_int + outdoor_time + routine + steps_k, data = mwdata)
```

For each of the below pairs of models, what methods are/are not available for us to use for comparison and why?  

+ `model1` vs `model2`
+ `model2` vs `model3`
+ `model1` vs `model4`
+ `model3` vs `model5`

This flowchart might help you to reach your decision:

```{r comparisons_chart, echo=FALSE, fig.align = 'left', out.width = "100%"}
#knitr::include_graphics("images/comparisons_chart.png")
```

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

You may need to examine the dataset, and check for accuracy (e.g., are there any impossible / out of range values?) and completeness (e.g., are there any missing values?). 

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

+ `model1` vs `model2`  
  These models are nested - `model2` contains all the variables of `model1` and they are fitted on the same dataset.  
  We can therefore use an $F$-test or AIC and BIC.  
    
+ `model2` vs `model3`  
  These models are __not__ nested, but they are fitted on the same dataset.  
  We can therefore use AIC or BIC, but we cannot use an $F$-test.  
    
+ `model1` vs `model4`
  These models are nested - `model4` contains all the variables of `model1` and they are fitted on the same dataset.  
  We can therefore use an $F$-test or AIC and BIC.  
    
+ `model3` vs `model5`  
  These models are __not__ nested, and they are __not__ fitted on the same dataset. The "steps_k" variable contains missing values (over 30% of the data is missing for this variable), and so these whole rows are excluded from `model5` (but they are included in `model3`). We cannot compare these models.   

`r solend()`

<br>


