---
title: "Model Comparison via anova()"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')

set.seed(953)
```


:::lo
**LEARNING OBJECTIVES**

1. Understand measures of model fit using F.  
2. Understand the principles of model selection and how to compare models via F tests.
3. Understand AIC and BIC.
:::

# Model Fit 

:::frame  
## F-ratio  

As in simple linear regression, the F-ratio is used to test the null hypothesis that all regression slopes are zero.  

It is called the F-ratio because it is the ratio of the how much of the variation is explained by the model (per paramater) versus how much of the variation is unexplained (per remaining degrees of freedom). 

$$
F_{df_{model},df_{residual}} = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\
\quad \\
\begin{align}
& \text{Where:} \\
& df_{model} = k \\
& df_{error} = n-k-1 \\
& n = \text{sample size} \\
& k  = \text{number of explanatory variables} \\
\end{align}
$$

---

**In R,** at the bottom of the output of `summary(<modelname>)`, you can view the F ratio, along with an hypothesis test against the alternative hypothesis that the at least one of the coefficients $\neq 0$ (under the null hypothesis that all coefficients = 0, the ratio of explained:unexplained variance should be approximately 1):

```{r mlroutputrf, echo=FALSE, fig.cap="Multiple regression output in R, summary.lm(). F statistic highlighted", fig.align = 'left'}
knitr::include_graphics("images/mlroutputf.png")
```
  
:::


`r qbegin(1)`
Run the code below. It reads in the wellbeing/rurality study data, and creates a new binary variable which specifies whether or not each participant lives in a rural location.  

```{r}
library(tidyverse)
mwdata2<-read_csv("https://uoepsy.github.io/data/wellbeing_rural.csv")
mwdata2 <- 
  mwdata2 %>% mutate(
  isRural = ifelse(location=="rural","rural","notrural")
)
```


Fit the following model, and assign it the name "wb_mdl1".

$\text{Wellbeing} = \beta_0 + \beta_1 \cdot \text{Social Interactions} + \beta_2 \cdot \text{IsRural} + \epsilon$  

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
wb_mdl1 <- lm(wellbeing ~ social_int + isRural, data=mwdata2)
summary(wb_mdl1)
```


```{r echo=FALSE}
mdl1<-wb_mdl1
```

`r solend()`


# Model Comparison 


:::frame
## Incremental F-test  

:::yellow
If (*and only if*) two models are __nested__, can we compare them using an __incremental F-test.__  

### What does __nested__ mean?

Consider that you have two regression models where Model 1 contains a subset of the predictors containted in the other Model 2 and is fitted to the same data. More simply, Model 2 contains all of the predictors included in Model 1, **plus** additional predictor(s). This means that Model 1 is *nested* within Model 2, or that Model 1 is a *submodel* of Model 2. These two terms, at least in this setting, are interchangeable - it might be easier to think of Model 1 as your null and Model 2 as your alternative.

### What is an __incremental F-test__?

This is a formal test of whether the additional predictors provide a better fitting model.  
Formally this is the test of:  

+ $H_0:$ coefficients for the added/ommitted variables are all zero.
+ $H_1:$ at least one of the added/ommitted variables has a coefficient that is not zero. 
:::

The F-ratio for comparing the residual sums of squares between two models can be calculated as:

$$
F_{(df_R-df_F),df_F} = \frac{(SSR_R-SSR_F)/(df_R-df_F)}{SSR_F / df_F} \\
\quad \\
\begin{align}
& \text{Where:} \\
& SSR_R = \text{residual sums of squares for the restricted model} \\
& SSR_F = \text{residual sums of squares for the full model} \\
& df_R = \text{residual degrees of freedom from the restricted model} \\
& df_F = \text{residual degrees of freedom from the full model} \\
\end{align}
$$

:::


:::frame
## Comparing regression models with `anova()`

Remember that you want your models to be *parsimonious*, or in other words, only as complex as they need to be in order to describe the data well. This means that you need to be able to justify your model choice, and one way to do so is by comparing models via `anova()`. If your model with multiple IVs does not provide a significantly better fit to your data than a more simplistic model with less IVs, then the more simplistic model should be preferred. 

**In R,** we can conduct an incremental F-test by constructing two linear regression models, and passing them to the `anova()` function: `anova(model1, model2)`. 

If the *p*-value is sufficiently low (i.e., below your predetermined significance level - usually .05), then you would conclude that model 2 is significantly better fitting than model 1. If p is not < .05, then you should favour the more simplistic model. 

:::

`r qbegin(2)`
The F-ratio you see at the bottom of `summary(model)` is actually a comparison between two models: your model (with some explanatory variables in predicting $y$) and __the null model.__ In regression, the null model can be thought of as the model in which all explanatory variables have zero regression coefficients. It is also referred to as the __intercept-only model__, because if all predictor variable coefficients are zero, then the only we are only estimating $y$ via an intercept (which will be the mean - $\bar y$).  

Use the code below to fit the null model. 

Then, use the `anova()` function to perform a model comparison between your earlier model (wb_mdl1) and the null model. Remember that the null model tests the null hypothesis that all beta coefficients are zero. By comparing *null_model* to *wb_mdl1*, we can test whether we should include the two IVs of social_int and isRural.

Check that the F-statistic and the *p*-value are the the same as that which is given at the bottom of `summary(wb_mdl1)`. 

```{r}
null_model <- lm(wellbeing ~ 1, data = mwdata2)
```

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
# fit the null model
null_model <- lm(wellbeing ~ 1, data = mwdata2)

# model comparison null vs wb_mdl1
anova(null_model, wb_mdl1)

# extract f statistic from summary of wb_mdl1
summary(wb_mdl1)$fstatistic
# we can retrieve the p-value:
fstat = summary(wb_mdl1)$fstatistic[1]
df_1 = summary(wb_mdl1)$fstatistic[2]
df_2 = summary(wb_mdl1)$fstatistic[3]
pf(fstat, df_1, df_2, lower.tail = FALSE)
```
`r solend()`


`r qbegin(3)`
Does weekly outdoor time explain a significant amount of variance in wellbeing scores over and above weekly social interactions and location (rural vs not-rural)?  

Provide an answer to this question by fitting and comparing two models (one of them you may already have fitted in an earlier question). 

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We can compare the following models which predict wellbeing scores from weekly social interactions and location, with and without weekly outdoor time.  

+ $\text{Wellbeing} = \beta_0 + \beta_1 \cdot \text{Social Interactions} + \beta_2 \cdot \text{IsRural} + \epsilon$ 
+ $\text{Wellbeing} = \beta_0 + \beta_1 \cdot \text{Social Interactions} + \beta_2 \cdot \text{IsRural} + \beta_3 \cdot \text{Outdoor time} +  \epsilon$ 

We have already fitted the first model and assigned it the name `wb_mdl1`. 

We need to fit the second:
```{r}
wb_mdl2 <- lm(wellbeing ~ 1 + social_int + isRural + outdoor_time, data=mwdata2)
```

Let's look at the amount of variation in wellbeing scores explained by each model. Recall from semester 1 that this means looking at our R^2 value:
```{r}
summary(wb_mdl1)$adj.r.squared
summary(wb_mdl2)$adj.r.squared
```

The model *with* weekly outdoor time as a predictor explains 30\% of the variance, and the model *without* explains 25\%. But, from only looking at the proportion of variance accounted for in each model, we cannot determine which model is statistically a better fit. To answer the question 'Does including weekly outdoor time as a predictor provide a significantly better fit of the data?' we need to compare **wb_mdl2** to **wb_mdl1**.

```{r}
anova(wb_mdl1, wb_mdl2)
```
```{r include=FALSE}
mc <- anova(wb_mdl1, wb_mdl2)
names(mc)[6]<-"p"
```

:::int
Weekly outdoor time was found to explain a significant amount of variance in wellbeing scores over and above weekly social interactions and location (rural vs not-rural).  
$F$(`r paste(mc$Df[2],mc$Res.df[2],collapse=",")`)=`r round(mc$F[2],2)`, p`r map_chr(mc$p[2], ~ifelse(.<001,"<.001",paste0("=",round(.,2))))`.
:::

`r solend()`


`r qbegin(4)`
Recall the data on Big 5 Personality traits, perceptions of social ranks, and depression and anxiety scale scores: 

```{r}
scs_study <- read_csv("https://uoepsy.github.io/data/scs_study.csv")
summary(scs_study)
```


> **Research questions**   
> 
> + Part 1: Beyond neuroticism and its interaction with social comparison, does Openness predict symptoms of depression, anxiety and stress? 
>   
> + Part 2: Beyond neuroticism and its interaction with social comparison, do other personality traits predict symptoms of depression, anxiety and stress? 
> 

Construct and compare multiple regression models to answer these two question. Remember to check that your models meet assumptions (for this exercises, a quick eyeball of the diagnostic plots will suffice. Were this an actual research project, you would want to provide a more thorough check, for instance conducting formal tests of the assumptions).  
<br>
Although the solutions are available immediately for this question, we strongly advocate that you attempt it yourself before looking at them. 
`r qend()`
`r solbegin(show=TRUE, toggle=params$TOGGLE)`

First let us mean-center our social comparison scale scores, as we did in the previous labs.
```{r}
scs_study <- 
  scs_study %>%
  mutate(
    scs_mc = scs - mean(scs)
  )
```



```{r echo=FALSE}
par(mfrow=c(2,2))
```

The first question is asking whether Openness improves model fit beyond a model with just Neuoriticism, social comparison score and their interaction.  

Notice how our initial model has one very influential point, which we will remove:
```{r eval=FALSE}
dass_mod1 <- lm(dass ~ scs_mc * zn, data = scs_study)
plot(dass_mod1)
```
```{r echo=FALSE}
dass_mod1 <- lm(dass ~ scs_mc * zn, data = scs_study)
par(mfrow=c(2,2))
plot(dass_mod1)
par(mfrow=c(1,1))
```

```{r eval=FALSE}
dass_mod1 <- lm(dass ~ scs_mc * zn, data = scs_study[-35, ])
plot(dass_mod)
```
```{r echo=FALSE}
dass_mod1 <- lm(dass ~ scs_mc * zn, data = scs_study[-35, ])
par(mfrow=c(2,2))
plot(dass_mod1)
par(mfrow=c(1,1))
```

And our model with the Openness included:
```{r eval=FALSE}
dass_mod2 <- lm(dass ~ scs_mc * zn + zo, data = scs_study[-35, ])
plot(dass_mod2)
```
```{r echo=FALSE}
dass_mod2 <- lm(dass ~ scs_mc * zn + zo, data = scs_study[-35, ])
par(mfrow=c(2,2))
plot(dass_mod2)
par(mfrow=c(1,1))
```

Look at the summary of Model 2 - Openness is not a significant predictor of DASS scores (*p* = .18).
```{r}
summary(dass_mod2)
```

Lets compare the two models: 
```{r}
anova(dass_mod1, dass_mod2)
```

```{r include=FALSE}
mdass <- anova(dass_mod1, dass_mod2)
names(mdass)[6]<-"p"
```

:::int
Openness did not explain a significant amount of variance in depression, anxiety and stress scores over and above neuroticism and its interaction with social comparison.  
$F$(`r paste(mdass$Df[2],mdass$Res.df[2],collapse=",")`)=`r round(mdass$F[2],2)`, p = `r round(mdass$p[2], 3)`.
:::


The second question is asking whether including a group of predictors (the O, C, E, A personality traits) improves model fit beyond a model with just Neuoriticism, social comparison score and their interaction. We will need to compare this model (dass_mod3) to dass_mod1.

```{r}
dass_mod3 <- lm(dass ~ scs_mc * zn + zo + zc + ze + za, data = scs_study[-35, ])
par(mfrow=c(2,2))
plot(dass_mod3)
par(mfrow=c(1,1))
```

We can explore the individual coefficients of our full model, and we notice that none of the other personality variables (`zo`, `zc`, `ze`, `za`) significantly predict DASS-21 scores:

```{r}
summary(dass_mod3)
```

However, when we compare the two models, we find that including these predictors does significantly improve model fit. 
```{r}
anova(dass_mod1, dass_mod3)
```


```{r include=FALSE}
mdass2 <- anova(dass_mod1, dass_mod3)
names(mdass2)[6]<-"p"
```


:::int
Whilst the other individual personality traits did not significantly predict DASS-21 scores, the model including these traits was significantly better fitting than the model including only Neuroticism and its interaction with social comparison.
$F$(`r paste(mdass2$Df[2],mdass2$Res.df[2],collapse=",")`)=`r round(mdass2$F[2],2)`, p = `r round(mdass2$p[2], 3)`.
:::


# Stop & Think
This conclusion may be a bit confusing - are we saying that none of Openness, Conscientiousness, Agreeableness, or Extraversion significantly predict DASS-21 scores, but collectively they do? This sort of discrepancy can often be the result of multicollinearity. Note that there may be some correlation between the personality variables - you could explore this by checking `vif()`. It could also be that one variable is _masking_ the effect of the other. Check what happens when you remove one of the personality variables 
```{r}
library(car)
vif(dass_mod3)
lm(dass ~ scs_mc * zn + zo + zc + ze, data = scs_study[-35, ]) %>% summary
```

`r solend()`


:::frame  
## AIC & BIC 

:::yellow
If models are __not nested__, we __cannot__ compare them using an __incremental F-test__. Instead, for __non-nested__ models, we can use information criterion statistics, such as __AIC__ and __BIC__. 

### What does __non-nested__ mean?
Consider that you have two regression models where Model 1 contains different variables to those contained in Model 2, where both models are fitted to the same data. More simply, Model 1 and Model 2 contain unique variables that are not shared. This means that Model 1 and Model 2 are **not** nested. 

### What are __AIC__ & __BIC__?

AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) combine information about the sample size, the number of model parameters, and the residual sums of squares ($SS_{residual}$). Models do not *need* to be nested to be compared via AIC and BIC, __but__ they need to have been fit to the same dataset.  

For both of these fit indices, lower values are better, and both include a penalty for the number of predictors in the model (although BIC's penalty is harsher):

$$
AIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + 2k \\
\quad \\
BIC = n\,\text{ln}\left( \frac{SS_{residual}}{n} \right) + k\,\text{ln}(n) \\
\quad \\
\begin{align}
& \text{Where:} \\
& SS_{residual} = \text{sum of squares residuals} \\
& n = \text{sample size} \\
& k = \text{number of explanatory variables} \\
& \text{ln} = \text{natural log function} 
\end{align}
$$
:::
---

**In R,** we can calculate AIC and BIC by using the `AIC()` and `BIC()` functions. 

:::

`r qbegin(5)`

Lets compare the AIC and BIC values of two models, each looking at the associations of DASS scores and two personality traits. Fit the following models, and compare using `AIC()` and `BIC()`. Report which model you think best fits the data. 

+ $\text{DASS} = \beta_0 + \beta_1 \cdot \text{Neuroticism} + \beta_2 \cdot \text{Extraversion} +  \epsilon$

+ $\text{DASS} = \beta_0 + \beta_1 \cdot \text{Openness} + \beta_2 \cdot \text{Agreeableness} +\epsilon$

`r qend()`

`r solbegin(show=TRUE, toggle=params$TOGGLE)`


```{r}
dassNE <- lm(dass ~ zn + ze, data = scs_study[-35, ])
dassOA <- lm(dass ~ zo + za, data = scs_study[-35, ])

AIC(dassNE, dassOA)
BIC(dassNE, dassOA)
```
:::int
We used AIC and BIC model selection to distinguish between two possible models describing the relationship between several personality factors and DASS-21 scores. Our model with Neuroticism and Extroversion (AIC = 4324.77) included as predictors was better fitting than the alternative model with Openness and Agreeableness (AIC = 4348.89). Based on the BIC value of the dassNE model (BIC = 4342.71) we concluded that it was substantively better fitting than the alternative model (BIC = 4366.83). 
:::

`r solend()`

## Choosing the Right Model Comparison Approach

`r qbegin(6)`

The code below fits 5 different models:

```{r eval=FALSE}
model1 <- lm(wellbeing ~ social_int + outdoor_time, data = mwdata2)
model2 <- lm(wellbeing ~ social_int + outdoor_time + age, data = mwdata2)
model3 <- lm(wellbeing ~ social_int + outdoor_time + routine, data = mwdata2)
model4 <- lm(wellbeing ~ social_int + outdoor_time + routine + age, data = mwdata2)
model5 <- lm(wellbeing ~ social_int + outdoor_time + routine + steps_k, data = mwdata2)
```



For each of the below pairs of models, what methods are/are not available for us to use for comparison and why?  

+ `model1` vs `model2`
+ `model2` vs `model3`
+ `model1` vs `model4`
+ `model3` vs `model5`

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

+ `model1` vs `model2`  
  These models are nested - `model2` contains all the variables of `model1` and they are fitted on the same dataset.  
  We can therefore use an F-test or AIC and BIC.  
    
+ `model2` vs `model3`  
  These models are __not__ nested, but they are fitted on the same dataset.  
  We can therefore use AIC or BIC, but we cannot use an F-test.  
    
+ `model1` vs `model4`
  These models are nested - `model4` contains all the variables of `model1` and they are fitted on the same dataset.  
  We can therefore use an F-test or AIC and BIC.  
    
+ `model3` vs `model5`  
  These models are __not__ nested, and they are __not__ fitted on the same dataset. the "steps_k" variable contains missing values, and so these whole rows are excluded from `model5` (but they are included in `model3`). We cannot compare these models.   

`r solend()`


--- 

Extra reading: [Joshua Loftus' Blog: Model selection bias invalidates significance tests](http://joshualoftus.com/post/model-selection-bias-invalidates-significance-tests/)
    
<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>