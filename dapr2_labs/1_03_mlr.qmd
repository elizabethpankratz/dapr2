---
title: "Multiple Linear Regression"
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source('assets/setup.R')

set.seed(953)
```

:::lo
### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Extend the ideas of single linear regression to consider regression models with two or more predictors
2. Understand and interpret the coefficients in multiple linear regression models

### <i class="fa fa-check-square-o fa-2"></i> Requirements
1. Be up to date with lectures
2. Have completed [Week 1](https://uoepsy.github.io/dapr2/2223/labs/1_01_function.html) and [Week 2](https://uoepsy.github.io/dapr2/2223/labs/1_02_slr.html) lab exercises

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse**
* **patchwork**
* **sjPlot**

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/wellbeing.csv) or read it in via this link https://uoepsy.github.io/data/wellbeing.csv. 
:::

# Setup

`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read the wellbeing dataset into R, assigning it to an object named `mwdata`

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r message=FALSE}
#Loading the required package(s)
library(tidyverse)
library(patchwork)
library(sjPlot)

# Reading in data and storing to an object named 'mwdata'
mwdata <- read_csv("https://uoepsy.github.io/data/wellbeing.csv")
```

`r solend()`

# Study Overview 

> **Research Question** 
>
> Is there an association between well-being and time spent outdoors *after* taking into account the association between well-being and social interactions? 

`r optbegin("Wellbeing data codebook.", olabel=FALSE)`  

__Description__

Researchers interviewed 32 participants, selected at random from the population of residents of Edinburgh & Lothians. They used the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.  

The researchers also asked participants to estimate the average number of hours they spend outdoors each week, the average number of social interactions they have each week (whether on-line or in-person), and whether they believe that they stick to a routine throughout the week (Yes/No).  

The data in `wellbeing.csv` contain five attributes collected from a random sample of $n=32$ hypothetical residents over Edinburgh & Lothians, and include:

- `wellbeing`: Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.  
- `outdoor_time`: Self report estimated number of hours per week spent outdoors  
- `social_int`: Self report estimated number of social interactions per week (both online and in-person)
- `routine`: Binary Yes/No response to the question "Do you follow a daily routine throughout the week?"
- `location`: Location of primary residence (City, Suburb, Rural)

__Preview__

The first six rows of the data are:

```{r echo=FALSE, message=FALSE}
read_csv('https://uoepsy.github.io/data/wellbeing.csv') %>% head %>% gt::gt()
```
  
`r optend()`

`r qbegin(1)`

Produce plots of the _marginal distributions_ (the distributions of each variable in the analysis without reference to the other variables) of the `wellbeing`, `outdoor_time`, and `social_int` variables. 

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

- You could use, for example, `geom_density()` for a density plot or `geom_histogram()` for a histogram.
- Look at the shape, center and spread of the distribution. Is it symmetric or skewed? Is it unimodal or bimodal? 

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

We should be familiar now with how to visualise a marginal distribution. You might choose histograms, density curves, or boxplots, or a combination:   
```{r}
#| label: fig-marg_dist
#| fig-cap: "Marginal distribution plots of wellbeing sores, weekly hours spent outdoors, and social interactions"
   
wellbeing_plot <- 
  ggplot(data = mwdata, aes(x = wellbeing)) +
  geom_density() +
  geom_boxplot(width = 1/250) +
  labs(x = "Score on WEMWBS (range 14-70)", y = "Probability\ndensity")

outdoortime_plot <- 
  ggplot(data = mwdata, aes(x = outdoor_time)) +
  geom_density() +
  geom_boxplot(width = 1/200) +
  labs(x = "Time spent outdoors per week (hours)", y = "Probability\ndensity")

social_plot <- 
  ggplot(data = mwdata, aes(x = social_int)) +
  geom_density() +
  geom_boxplot(width = 1/150) +
  labs(x = "Number of social interactions per week", y = "Probability\ndensity")

# the "patchwork" library allows us to arrange multiple plots
wellbeing_plot / outdoortime_plot / social_plot
```

Summary statistics for wellbeing, outdoor time, and social interactions:

```{r}
descriptives <- mwdata %>% 
  summarize(
    M_Wellbeing = mean(wellbeing), 
    SD_Wellbeing = sd(wellbeing),
    M_OutTime = mean(outdoor_time), 
    SD_OutTime = sd(outdoor_time),
    M_SocInt = mean(social_int), 
    SD_SocInt = sd(social_int)
    )
descriptives
```


:::int  

+ The marginal distribution of scores on the WEMWBS is unimodal with a mean of approximately `r round(mean(mwdata$wellbeing),1)`. There is variation in WEMWBS scores (SD = `r round(sd(mwdata$wellbeing),1)`).   
+ The marginal distribution of weekly hours spent outdoors is unimodal with a mean of approximately `r round(mean(mwdata$outdoor_time),1)`. There is variation in weekly hours spent outdoors (SD = `r round(sd(mwdata$outdoor_time),1)`).  
+ The marginal distribution of numbers of social interactions per week is unimodal with a mean of approximately `r round(mean(mwdata$social_int),1)`. There is variation in numbers of social interactions (SD = `r round(sd(mwdata$social_int),1)`).  

:::

`r solend()`

<br>

`r qbegin(2)`
Produce plots of the _associations_ between the outcome variable (wellbeing) and each of the explanatory variables.

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

Think about:  

- *Direction* of association
- *Form* of association (can it be summarised well with a straight line?)  
- *Strength* of association (how closely do points fall to a recognizable pattern such as a line?)
- *Unusual observations* that do not fit the pattern of the rest of the observations and which are worth examining in more detail.

Plot tips: 

- use `\n` to wrap text in your titles and or axis labels
- consider using `geom_smooth()` to  superimpose the best-fitting line describing the association of interest

:::
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r message = FALSE, warning=FALSE}
#| label: fig-marg_relationship
#| fig-cap: "Scatterplots displaying the relationships between scores on the WEMWBS and a) weekly outdoor time (hours), and b) weekly number of social interactions"
wellbeing_outdoor <- 
  ggplot(data = mwdata, aes(x = outdoor_time, y = wellbeing)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) + 
  labs(x = "Time spent outdoors \nper week (hours)", y = "Wellbeing score (WEMWBS)")

wellbeing_social <- 
  ggplot(data = mwdata, aes(x = social_int, y = wellbeing)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) + 
  labs(x = "Number of social interactions \nper week", y = "Wellbeing score (WEMWBS)")

# place plots adjacent to one another
wellbeing_outdoor | wellbeing_social
```
`r solend()`

<br>
  
`r qbegin(3)`
Produce a correlation matrix of the variables which are to be used in the analysis, and write a short paragraph describing the associations. 

:::sticky
__Correlation matrix__  

A table showing the correlation coefficients - $r_{(x,y)}=\frac{\mathrm{cov}(x,y)}{s_xs_y}$ - between variables. Each cell in the table shows the relationship between two variables. The diagonals show the correlation of a variable with itself (and are therefore always equal to 1).  

:::blue
In `R`, we can create a correlation matrix by giving the `cor()` function a dataframe. However, we only want to give it 3 columns here. Think about how we select specific columns, either using `select()`, or giving the column numbers inside `[]`. 
:::

:::

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

Make sure to round your numbers in-line with APA 7th edition guidelines. The `round()` function will come in handy here, as might this [APA numbers and statistics guide](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf)! 

:::


`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We can either use:
```{r eval=FALSE}
# correlation matrix of the first 3 columns
round(cor(mwdata[,1:3]), digits = 2)
```
or:
```{r}
# select only the columns we want by name, and pass this to cor()
mwdata %>% 
  select(wellbeing, outdoor_time, social_int) %>%
  cor() %>%
    round(digits = 2)
```

::: {.callout-important icon=false appearance="minimal"}

- There was a moderate, positive, linear association between weekly outdoor time and WEMWBS scores for the participants in the sample ($r$ = .58). Higher number of hours spent outdoors each week was associated, on average, with higher wellbeing scores,   
- There was a moderate, positive, linear association between the weekly number of social interactions and WEMWBS scores for the participants in the sample ($r$ = .79). More social interactions were associated, on average,  with higher wellbeing scores.
- There was a weak positive correlation between weekly outdoor time and the weekly number of social interactions ($r$ = .34).  

::: 

:::{.callout-note}

Note that there is a weak correlation between our two explanatory variables (outdoor_time and social_int). We will return to how this might affect our model when later on we look at the assumptions of multiple regression.  

:::

`r solend()`

<br>

`r qbegin(4)`

The scatterplots we created above show moderate, positive, and linear relationships both between outdoor time and wellbeing, and between numbers of social interactions and wellbeing.

(1) Specify the form of your model, where $y$ = scores on the WEMWBS, $x_1$ = weekly number of social interactions, and $x_2$ = weekly outdoor time. 

(2) What are the _parameters_ of the model. How do we denote _parameter estimates_?  

(3) Fit the linear model in using `lm()`, assigning the output to an object called `mdl1`. 

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

As we did for simple linear regression, we can fit our multiple regression model using the `lm()` function. We can add as many explanatory variables as we like, separating them with a `+`. 

``````{r eval=FALSE, echo=TRUE}
( <response variable> ~ 1 + <explanatory variable 1> + <explanatory variable 2> + ... , data = <dataframe> )
```

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
A model for the relationship between $x_1$ = weekly outdoor time, $x_2$ = weekly numbers of social interactions and $y$ = scores on the WEMWBS is given by:
$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon \\ \quad \\ 
\text{where} \quad \epsilon \sim N(0, \sigma) \text{ independently}
$$

In the model specified above,

- $\mu_{y|x_1, x_2} = \beta_0 + \beta_1 x + \beta_2 x_2$ represents the systematic part of the model giving the mean of $y$ at each combination of values of $x_1$ and $x_2$;
- $\epsilon$ represents the error (deviation) from that mean, and the errors are independent from one another.  
  
  
The parameters of our model are:

+ $\beta_0$ (The intercept);
+ $\beta_1$ (The slope across values of $x_1$);
+ $\beta_2$ (The slope across values of $x_2$);  
+ $\sigma$ (The standard deviation of the errors).

When we estimate these parameters from the available data, we have a _fitted model_ (recall that the h$\hat{\textrm{a}}$ts are used to distinguish our _estimates_ from the _true unknown parameters_): 
$$
\widehat{Wellbeing} = \hat \beta_0 + \hat \beta_1 \cdot Social Interactions + \hat \beta_2 \cdot Outdoor Time 
$$
And we have residuals $\hat \epsilon = y - \hat y$ which are the deviations from the observed values and our model-predicted responses.  

Fitting the model in `R`:

```{r}
mdl1 <- lm(wellbeing ~ social_int + outdoor_time, data = mwdata)
```

`r solend()`

<br>
<br>

:::statbox
__Visual__

Note that for simple linear regression we talked about our model as a _line_ in 2 dimensions: the systematic part $\beta_0 + \beta_1 x$ defined a line for $\mu_y$ across the possible values of $x$, with $\epsilon$ as the random deviations from that line. But in multiple regression we have more than two variables making up our model. 

In this particular case of three variables (one outcome + two explanatory), we can think of our model as a _regression surface_ (see @fig-reg-surface). The systematic part of our model defines the surface across a range of possible values of both $x_1$ *and* $x_2$. Deviations from the surface are determined by the random error component, $\hat \epsilon$.  

```{r regsurf, echo=FALSE, out.width="100%"}
#| label: fig-reg-surface
#| fig-cap: "Regression surface for wellbeing ~  social_int + outdoor_time, from two different angles"

fit<-lm(wellbeing~outdoor_time+social_int, data=mwdata)
steps=50
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int)
wellbeing <- matrix(predict(fit, newdat), steps, steps)


par(mfrow=c(1,2))
p <- persp(outdoor_time,social_int,wellbeing, theta = 35,phi=10, col = NA)
obs <- with(mwdata, trans3d(outdoor_time,social_int, wellbeing, p))
pred <- with(mwdata, trans3d(outdoor_time, social_int, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)

p <- persp(outdoor_time,social_int,wellbeing, theta = -35,phi=10, col = NA)
obs <- with(mwdata, trans3d(outdoor_time,social_int, wellbeing, p))
pred <- with(mwdata, trans3d(outdoor_time, social_int, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)

par(mfrow=c(1,1))
```

Don't worry about trying to figure out how to visualise it if we had any more explanatory variables! We can only concieve of 3 spatial dimensions. One could imagine this surface changing over time, which would bring in a 4th dimension, but beyond that, it's not worth trying!.

:::

<br>

`r qbegin(5)`

State the research question in the form of a testable hypothesis.

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

You must define both a null ($H_0$) and alternative hypothesis ($H_1$). 

:::

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

In words:

$H_0$: There is no association between well-being and time spent outdoors after taking into account the relationship between well-being and social interactions

$H_1$: There is an association between well-being and time spent outdoors after taking into account the relationship between well-being and social interactions


In symbols:

$H_0: \beta_2 = 0$

$H_1: \beta_2 \neq 0$


`r solend()`

<br>

`r qbegin(6)`
Using any of:  

- `mdl1`
- `mdl1$coefficients`
- `coef(mdl1)`
- `coefficients(mdl1)`
- `summary(mdl1)`

Write out the estimated parameter values of: 

1. $\hat \beta_0$, the estimated average wellbeing score associated with zero hours of outdoor time and zero social interactions per week.  
2. $\hat \beta_1$, the estimated increase in average wellbeing score associated with an additional social interaction per week (an increase of one), _holding weekly outdoor time constant_.  
3. $\hat \beta_2$, the estimated increase in average wellbeing score associated with one hour increase in weekly outdoor time, _holding the number of social interactions constant_ 

:::{.callout-note}

Q: What do we mean by hold constant / controlling for / partialling out / residualizing for?

A: When the remaining explanatory variables are held at the same value or are fixed.

:::


`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

::: {.panel-tabset}

##mdl1
```{r}
mdl1
```

## mdl1$coefficients
```{r}
mdl1$coefficients
```

## coef(mdl1)
```{r}
coef(mdl1)
```

## coefficients(mdl1)
```{r}
coefficients(mdl1)
```

## summary(mdl1)
Look under the “Estimate” column:
```{r}
summary(mdl1)
```

:::

1. $\hat \beta_0$ = `r round(coef(mdl1)[1],2)`  
2. $\hat \beta_1$ = `r round(coef(mdl1)[2],2)`  
3. $\hat \beta_2$ = `r round(coef(mdl1)[3],2)`  

`r solend()`

<br>

`r qbegin(7)`
Within what distance from the model predicted values (the regression surface) would we expect 95% of wEMWBS wellbeing scores to be?

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

Either `sigma()` or part of the output from `summary()` will help you here.  

:::

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

::: {.panel-tabset}

## sigma(mdl1) 

```{r}
sigma(mdl1)
```


## summary(mdl1)

Look at the "Residual standard error" entry of the `summary(mdl)` output:

```{r}
summary(mdl1)
```

:::

The estimated standard deviation of the errors is $\hat \sigma$ = `r round(sigma(mdl1),2)`. We would expect 95% of wellbeing scores to be within about `r round(sigma(mdl1)*2,2)` ($2 \hat \sigma$) from the model fit. 

`r solend()`

`r qbegin(8)`

Based on the model, predict the wellbeing scores for the following individuals:

- Leah: Social Interactions = 24; Outdoor Time = 3
- Sean: Social Interactions = 19; Outdoor Time = 26
- Mike: Social Interactions = 15; Outdoor Time = 20
- Donna: Social Interactions = 7; Outdoor Time = 2

Who has the highest predicted wellbeing score, and who has the lowest?

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

First we need to pass the data into R:
```{r}
wellbeing_query <- tibble(social_int = c(24, 19, 15, 7),
                          outdoor_time = c(3, 26, 20, 2))
```

And next use `predict()` to get their estimated wellbeing scores:

```{r}
predict(mdl1, newdata = wellbeing_query)
```

Sean has the highest predicted wellbeing score (55.04), and Donna the lowest (19.18). 

`r solend()`

`r qbegin(9)`

Should we reject or fail to reject $H_0$? Why?

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

The research question asked whether there was an association between well-being and time spent outdoors *after* taking into account the association between well-being and social interactions. This was equivalent to testing the following null hypothesis:

$H_0: \beta_2 = 0$

Based on the model output (if we considered effects to be significant at $\alpha$ = .05), we should reject the null hypothesis since our $p$-value smaller than this ($p$ = .0015). In short, we reject the null since $p$ < .05.

`r solend()`

`r qbegin(10)`

Interpret the outdoor time coefficient in the context of the research question. 

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

A multiple regression model was used to determine if there was an association between well-being and time spent outdoors after taking into account the association between well-being and social interactions. Outdoor time was significantly associated with wellbeing scores ($\beta$ = 0.59, SE = 0.17, $p$ < .001) after controlling for the number of weekly social interactions. Results suggested that for every additional hour spent outdoors each week, wellbeing scores increased by 0.59 points. 

`r solend()`


