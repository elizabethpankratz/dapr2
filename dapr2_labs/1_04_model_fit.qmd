---
title: "Model Fit and Standardization"
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source('assets/setup.R')
library(patchwork)
library(checkdown)

# knitr::opts_chunk$set(cache = TRUE)
set.seed(1)
```

:::lo
### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand the calculation and interpretation of the coefficient of determination.
1. Understand the calculation and interpretation of the F-test of model utility.
1. Understand how to standardize model coefficients and when this is appropriate to do.
1. Understand the relationship between the correlation coefficient and the regression slope.

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1. Be up to date with lectures
2. Have completed previous lab exercises from [Week 1](https://uoepsy.github.io/dapr2/2223/labs/1_01_function.html), [Week 2](https://uoepsy.github.io/dapr2/2223/labs/1_02_slr.html), and [Week 3](https://uoepsy.github.io/dapr2/2223/labs/1_03_mlr.html)

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse**
* **patchwork**
* **kableExtra**

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/wellbeing.csv) or read it in via this link https://uoepsy.github.io/data/wellbeing.csv. **Note**:  this is the same data as Lab 3.
:::

# Study Overview 

## Data Recap

`r optbegin("Wellbeing data codebook.", olabel=FALSE)`  

__Description__

Researchers interviewed 32 participants, selected at random from the population of residents of Edinburgh & Lothians. They used the Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.  

The researchers also asked participants to estimate the average number of hours they spend outdoors each week, the average number of social interactions they have each week (whether on-line or in-person), and whether they believe that they stick to a routine throughout the week (Yes/No).  

The data in `wellbeing.csv` contain five attributes collected from a random sample of $n=32$ hypothetical residents over Edinburgh & Lothians, and include:

- `wellbeing`: Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.  
- `outdoor_time`: Self report estimated number of hours per week spent outdoors  
- `social_int`: Self report estimated number of social interactions per week (both online and in-person)
- `routine`: Binary Yes/No response to the question "Do you follow a daily routine throughout the week?"
- `location`: Location of primary residence (City, Suburb, Rural)

__Preview__

The first six rows of the data are:

```{r echo=FALSE, message=FALSE}
read_csv('https://uoepsy.github.io/data/wellbeing.csv') %>% head %>% gt::gt()
```
  
`r optend()`


# Setup
`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read the wellbeing dataset into R, assigning it to an object named `mwdata`

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r message=FALSE}
#Loading the required package(s)
library(tidyverse)
library(kableExtra)
library(patchwork)

# Reading in data and storing to an object named 'mwdata'
mwdata <- read_csv("https://uoepsy.github.io/data/wellbeing.csv")
riverview <- read_csv('https://uoepsy.github.io/data/riverview.csv')
```
`r solend()`

# Exercises

`r qbegin(1)`
Specify and fit a linear model to investigate how WEMWBS scores vary with time spent outdoors *after* controlling for the number of social interactions and whether the individual follows a daily routine. Next, check the `summary()` output from the model.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

The model is:
$$
\widehat{Wellbeing} = \hat \beta_0 + \hat \beta_1 \cdot Social Interactions + \hat \beta_2 \cdot Routine + \hat \beta_3 \cdot Outdoor Time 
$$
In `R`:
```{r message=FALSE}
#fit model
mdl <- lm(wellbeing ~ social_int + routine + outdoor_time, data = mwdata)

#look at summary output from model
summary(mdl)
```

`r solend()`

Recall the main formulas for obtaining a confidence interval and a test-statistic:

:::statbox

**Test statistic**

A test statistic for the null hypothesis $H_0: \beta_1 = 0$ is
$$
t = \frac{\hat \beta_1 - 0}{SE(\hat \beta_1)}
$$
which follows a $t$-distribution with $n-2$ degrees of freedom.

**Confidence interval**

A confidence interval for the population slope is
$$
\hat \beta_1 \pm t^* \cdot SE(\hat \beta_1)
$$
where $t^*$ denotes the critical value chosen from t-distribution with $n-2$ degrees of freedom for a desired $\alpha$ level of confidence. 

:::

<br>

`r qbegin(2)`
Test the hypothesis that the population slope is zero --- that is, that there is no linear association between wellbeing and outdoor time (*after* controlling for the number of social interactions and whether the individual follows a daily routine) in the population. 

:::{.callout-tip appearance="simple" collapse="true"}

### Hint

You can find all of the necessary information in the `summary()` output of your model, or calculate manually.

:::

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

::: {.panel-tabset}

## Manually

We calculate the test statistic
$$
t = \frac{\hat \beta_3 - 0}{SE(\hat \beta_3)} = \frac{0.5371 - 0}{0.1397} = 3.845
$$

and compare it with the 5% critical value from a $t$-distribution with $n-2$ degrees of freedom, which is:

```{r}
n <- nrow(mwdata)
tstar <- qt(0.975, df = n - 2)
tstar
#tstar = 2.042272
```

As $|t|$ ($|t|$ = 3.85) is much larger than $t^*$ ($t^*$ = 2.04), we reject then null hypothesis as we have strong evidence against it.

The $p$-value, shown below, also confirms this conclusion.
```{r}
2 * (1 - pt(3.845, n - 2))
```

## R function

Please note that the same information was already contained in the row corresponding to the variable "outdoor_time" in the output of `summary(mdl)`, which reported the $t$-statistic under `t value` and the $p$-value under `Pr(>|t|)`:
```{r}
summary(mdl)
```

:::

Before we interpret the results, note that sometimes $p$-values will be reported to $e^X$. For example, look in the `Pr(>|t|)` column for "social_int". The value $8.81e^{-09}$ simply means $8.81 \times 10^{-9}$. This is a very small value (i.e., 0.00000000881), hence we will report it as <.001 following the [APA guidelines](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf).

::: {.callout-important icon=false appearance="minimal"}

We performed a $t$-test against the null hypothesis that outdoor time was not associated with wellbeing scores. A significant association was found between level of outdoor time  (in hours per week) and wellbeing (WEMWBS scores) $t(28) = 3.84,\ p < .001$, two-sided. Thus, we have evidence to reject the null hypothesis.

:::

`r solend()`

<br>

`r qbegin(3)`

Compute a confidence interval for the regression slope for $\hat \beta_3$.

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

::: {.panel-tabset}

## Manually

For 95\% confidence we have $t^* = 2.04$:
```{r}
n <- nrow(mwdata)
tstar <- qt(0.975, df = n - 2)
tstar
```

The confidence interval is:
```{r}
beta3_ci <- tibble(
  lower = 0.5371 - tstar * 0.1397,
  upper = 0.5371 + tstar * 0.1397,
                  )
beta3_ci
```

## R function
We can easily obtain the confidence intervals for the regression coefficients using the command `confint()`:

```{r}
confint(mdl, level = 0.95)
```

The result is exactly the same (up to rounding errors) as calculating manually. 

:::

We typically report our uncertainty in a statistic by providing $\text{estimate} \pm t^* \cdot \text{SE}$. Here we would say that because of sampling variation, we are 95\% confident that the slope is between 0.252 and 0.822. Interpreting this, we might say,


::: {.callout-important icon=false appearance="minimal"}

For all Riverview city employees, each one-year difference in formal education was associated with a difference in income between \$1,896 and \$3,406, on average.

:::

Similarly, we could express the uncertainty in the intercept $\hat \beta_0$ as:

::: {.callout-important icon=false appearance="minimal"}

The average income for all Riverview city employees with zero years of education is between \$-1184 and \$23,827.

:::

`r solend()`



`r qbegin(4)`
What is the proportion of the total variability in wellbeing scores explained by the model?

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

The question asks to compute the value of $R^2$. Since the model has more than 2 predictors, you should report the Adjusted $R^2$.

:::

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
The proportion of the total variability explained is given by R-squared.

The R-squared coefficient is defined as:
$$
R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}
$$
The Adjusted R-squared coefficient is defined as:

$$
\hat R^2 = 1 - \frac{(1 - R^2)(n-1)}{n-k-1}
\quad \\
\begin{align}
& \text{Where:} \\
& n = \text{sample size} \\
& k = \text{number of explanatory variables} \\
\end{align}
$$
::: {.panel-tabset}

## Manually

In `R` we can write:
```{r}
#R squared & adjusted R squared

wellbeing_fitted <- mwdata %>%
  mutate(
    wellbeing_hat = predict(mdl),
    resid = wellbeing - wellbeing_hat
  )
head(wellbeing_fitted)

wellbeing_fitted %>%
  summarise(
    SSModel = sum( (wellbeing_hat - mean(wellbeing))^2 ),
    SSTotal = sum( (wellbeing - mean(wellbeing))^2 )
  ) %>%
  summarise(
    RSquared = SSModel / SSTotal,
    AdjRSquared = 1-((1-(RSquared))*(32-1)/(32-3-1))
  )


```


## summary()

```{r}
summary(mdl)
```

The output of `summary()` displays the Adjusted $R$-squared value in the following line:
```
Adjusted R-squared:  0.8121 
```


## Manually: ANOVA - kill

We can perform an **AN**alysis **O**f **VA**riance or, in short, ANOVA.

It simply means that we are examining/partitioning the total variability of a response variable.

The `anova()` function returns the sum of squares of interest in the column `Sum Sq`:
```{r}
mdl_anova <- anova(mdl)
mdl_anova
```

Consider the column `Sum Sq`. 
The entry corresponding to `education` gives $SS_{Model}$ = 4147.3, as education is the explanatory variable. The entry corresponding to `Residuals` gives $SS_{Residual}$ = 2418.2.

```{r}
# Because the column name Sum Sq has a space, we need to wrap it with backticks
SSModel <- (mdl_anova$`Sum Sq`[1])+(mdl_anova$`Sum Sq`[2])+(mdl_anova$`Sum Sq`[3])
SSResidual <- mdl_anova$`Sum Sq`[4]
SSTotal <- SSModel + SSResidual

RSquared <- SSModel / SSTotal
RSquared
```

Now to calculate the Adjusted R Squared, simply sub the values into the formula given above:

$$
\hat R^2 = 1 - \frac{(1 - 0.83)(32-1)}{32-3-1}
$$
In R: 
```{r}
#sub in values - pull from R as opposed to including rounded numbers to improve accuracy of estimation
AdjRSquared <- 1 - ((1-RSquared)*(nrow(mwdata)-1))/(nrow(mwdata)-3-1)
AdjRSquared 
```


:::
---

**Interpretation**

::: {.callout-important icon=false appearance="minimal"}

Approximately 81\% of the total variability in wellbeing scores is explained by associations with social interactions, routine, and outdoor time.

:::

`r solend()`


`r qbegin(5)`
Perform a model utility test at the 5\% significance level, by computing the F-statistic using its definition.

:::{.callout-tip appearance="simple" collapse="true"}

### Hint 

The F-ratio is used to test the null hypothesis that all regression slopes are zero.  
It is called the F-ratio because it is the ratio of the how much of the variation is explained by the model (per paramater) versus how much of the variation is left unexplained in the residuals (per remaining degrees of freedom). 

$$
F_{df_{model},df_{residual}} = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\
\quad \\
\begin{align}
& \text{Where:} \\
& df_{model} = k \\
& df_{residual} = n-k-1 \\
& n = \text{sample size} \\
& k  = \text{number of explanatory variables} \\
\end{align}
$$
:::

`r qend()`

`r optbegin('Optional: Another formula for the F-test.', FALSE)`

With some algebra we can also show that:
$$
F = \frac{R^2 / 1}{(1 - R^2) / (n - 2) } = \frac{R^2 / df_{Model}}{(1 - R^2) / df_{Residual} }
$$

Proof:

$$
\begin{aligned}
F = \frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} 
= \frac{\frac{SS_{Model}}{SS_{Total}}}{\frac{SS_{Residual}}{SS_{Total}} \cdot \frac{1}{(n - 2)}} 
= \frac{R^2 / 1}{(1 - R^2) / (n - 2)}
\end{aligned}
$$

`r optend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
df1 <- 1
df2 <- nrow(mwdata) - 2
f_star <- qf(0.95, df1, df2)
f_star
```

```{r}
model_utility <- wellbeing_fitted %>%
  summarise(
    SSModel = sum((wellbeing_hat - mean(wellbeing))^2 ),
    SSResid = sum( resid^2 ),
    MSModel = SSModel / 1,
    MSResid = SSResid / df2,
    FObs = MSModel / MSResid
  )
model_utility
```


::: {.callout-important icon=false appearance="minimal"}

We performed an $F$-test of model utility at the 5\% significance level, where $F(1,30) = 51.45$.

As the observed $F = 51.45$ is much larger than the critical value $F^* = 4.17$, we have strong evidence to reject the null hypothesis that the model is ineffective.
:::

<br>
Alternatively, we can compute the p-value:
```{r}
pvalue <- 1 - pf(model_utility$FObs, df1, df2)
pvalue
```
The value `5.562116e-08` simply means $5.56 \times 10^{-8}$, so it's a really small number.

::: {.callout-important icon=false appearance="minimal"}

We performed an $F$-test of model utility at the 5\% significance level, where $F(1,30) = 51.45, p<.001$.

The $p$-value (< .001) is much lower than the specified significance level, meaning that we have evidence against the null hypothesis.
:::

`r solend()`




`r qbegin(6)`
Look at the output of `summary(mdl)` and `anova(mdl)`.

For each output, identify the relevant information to conduct an F-test against the null hypothesis that the model is ineffective at predicting income using education level.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

::: {.panel-tabset}

## summary() output
```{r}
summary(mdl)
```

The relevant row is the following:

```
F-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08
```

## anova() output
```{r}
anova(mdl)
```

And the relevant entries are:

- the `Df` column, which contains the degrees of freedom;
- `F value = 51.452`, which is the F-statistic;
- `Pr(>F) = 5.562e-08` = $5.562 \times 10^{-8}$, which is the p-value.

:::
---

**Interpretation**

::: {.callout-important icon=false appearance="minimal"}

We performed an $F$-test for the overall significance of the regression, $F(1, 30) = 51.45, p < .001$.

The large $F$-statistic leads to a very small $p$-value ($<.001$), meaning that we have very strong evidence against the null hypothesis that the model is ineffective.

In other words, the data provide strong evidence that education is an effective predictor of income.

:::

`r solend()`


`r qbegin(8)`
Consider the `F value` output of `anova(mdl)` and the `t value` for education returned by `summary(mdl)`

```
F value = 51.452
t value = 7.173
```

Do you notice any relationship between the F-statistic for overall model utility and the t-statistic for $H_0: \beta_1 = 0$?
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
**In simple linear regression only**, the F-statistic for overall model significance is equal to the square of the t-statistic for $H_0: \beta_1 = 0$.

You can check that the squared t-statistic is equal, up to rounding error, to the F-statistic:
$$
t^2 = F \\
7.173^2 = 51.452
$$

`r optbegin('Optional: Equivalence of t-test for the slope and model utility F-test in SLR.', FALSE)`

Here we will show the equivalence of the F-test for model effectiveness and t-test for the slope.

Recall the formula of the sum of squares due to the model. We will rewrite it in an equivalent form below:
$$
\begin{aligned}
SS_{Model} &= \sum_i (\hat y_i - \bar y)^2 \\
&= \sum_i (\hat \beta_0 + \hat \beta_1 x_i - \bar y)^2 \\
&= \sum_i (\bar y - \hat \beta_1 \bar x + \hat \beta_1 x_i - \bar y)^2 \\
&= \sum_i (\hat \beta_1 (x_i - \bar x))^2 \\
&= \hat \beta_1^2 \sum_i (x_i - \bar x)^2
\end{aligned}
$$

The $F$-statistic is given by:
$$
\begin{aligned}
F = \frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} 
= \frac{\hat \beta_1^2 \sum_i (x_i - \bar x)^2}{\hat \sigma^2} 
= \frac{\hat \beta_1^2 }{\hat \sigma^2 / \sum_i (x_i - \bar x)^2}
\end{aligned}
$$

Now recall the formula of the $t$-statistic,
$$
t = \frac{\hat \beta_1}{SE(\hat \beta_1)} = \frac{\hat \beta_1}{\hat \sigma / \sqrt{\sum_i (x_i - \bar x)^2}}
$$

It is evident that the latter is obtained as the square root of the former.

`r optend()`

`r solend()`


## Standardization

`r qbegin(8)`
Fit the regression model using the standardized response and explanatory variables.


:::{.callout-tip appearance="simple" collapse="true"}

### Hint

You can either:

1. Add to the "mwdata" dataset three variables called `z_wellbeing`, `z_social_int`, and `z_outdoor_time` representing the standardized welllbeing, social interactions and outdoor time variables, respectively.
or
2. Use the `scale()` function when specifying your `lm()` statement.

:::

`r qend()`


`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

::: {.panel-tabset}

## Z-score

```{r}
mwdata <- mwdata %>%
  mutate(
    z_wellbeing = (wellbeing - mean(wellbeing)) / sd(wellbeing),
    z_social_int = (social_int - mean(social_int)) / sd(social_int),
    z_outdoor_time = (outdoor_time - mean(outdoor_time)) / sd(outdoor_time)
  )
```

Check that they are standardized:
```{r}
mwdata %>%
  summarise(
    M_z_wellbeing = round(mean(z_wellbeing),2), SD_z_wellbeing = sd(z_wellbeing), 
    M_z_social_int = round(mean(z_social_int),2), SD_z_social_int = sd(z_social_int),
    M_z_outdoor_time = round(mean(z_outdoor_time),2), SD_z_outdoor_time = sd(z_outdoor_time)
  )
#mean of 0, SD of 1 - all good to go
```

Run model:
```{r}
#with z scoring
mdl_z <- lm(z_wellbeing ~ z_social_int + z_outdoor_time, data = mwdata)
summary(mdl_z)
round(summary(mdl_z)$coefficients,3)
```



## scale() function

```{r}
#use scale
mdl_s <- lm(scale(wellbeing) ~ scale(social_int) + scale(outdoor_time), data = mwdata)
summary(mdl_s)
round(summary(mdl_s)$coefficients,3)
```

:::

`r solend()`


`r qbegin(10)`
Interpret the slope of the standardized variables.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

::: {.callout-important icon=false appearance="minimal"}

For every standard deviation increase in education, income increases on average by 0.79 standard deviations.

:::
`r solend()`



# References
