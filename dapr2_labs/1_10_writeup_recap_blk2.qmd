---
title: "Write Up & Block 2 Recap"
link-citations: TRUE
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source('assets/setup.R')

set.seed(953)

library(DT)
library(pander)
library(sjPlot)
library(tidyverse)
```

:::lo

### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand how to write-up and provide interpretation of XXXXXXXXXXXXX

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1. Be up to date with lectures
2. Have completed Labs 7 - 10

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse** 
* **psych**
* **patchwork**
* **sjPlot**
* **kableExtra**

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/FOMOdataset.csv) or read it in via this link https://uoepsy.github.io/data/FOMOdataset.csv. 

:::

# Section A: Write-Up

In this lab you will be presented with the output from a statistical analysis, and your job will be to write-up and present the results. We're going to use a simulated dataset based on a paper (the same one you have worked on in lectures this week) concerning Fear of Missing Out (FoMO), socio-demographic factors, and the Big Five personality traits.

The aim in writing should be that a reader is able to more or less replicate your analyses **without** referring to your R code. This requires detailing all of the steps you took in conducting the analysis.  
The point of using RMarkdown is that you can pull your results **directly** from the code. If your analysis changes, so does your report!  

Make sure that your final report doesn't show any R functions or code. Remember you are interpreting and reporting your results in text, tables, or plots, targeting a generic reader who may use different software or may not know R at all. If you need a reminder on how to hide code, format tables, etc., make sure to review the [rmd bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

:::{.callout-note}

## Important - Write-Up Examples & Plagiarism

The example write-up sections included below are not **perfect** - they instead should give you a good example of what information you should include within each section, and how to structure this. For example, some information is missing (e.g., description of data checks, interpretation of descriptive statistics), some information could be presented more clearly (e.g., variable names in tables, table/figure titles/captions, and rationales for choices), and writing could be more concise in places (e.g., discussion section is quite long).  

Further, **you must not copy any of the write-up included below for future reports** - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more [here](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct).

:::

## Study Overview 

> **Research Aim**
> 
> Explore the associations among Fear of Missing Out (FoMO), age, social media networks, and the Big Five personality traits. 
>
> **Research Questions**
> 
> + RQ1: Does age predict FoMO?
> + RQ2: Does the number of Instagram followers explain a significant amount of variance in FoMO over and above age?
> + RQ3: Does personality predict FoMO?

`r optbegin('FoMO data codebook.', FALSE, show = TRUE, toggle = params$TOGGLE)`

__Description__

The data used for this write-up exercise are simulated, drawing on recent work on Fear of Missing Out (FoMO), socio-demographic factors, and the Big Five personality traits. The simulated data are based on the findings of this work, and acted to expand upon the methods and results reported in the following paper:

Rozgonjuk, D., Sindermann, C., Elhai, J. D., & Montag, C. (2021). Individual differences in Fear of Missing Out (FoMO): Age, gender, and the Big Five personality trait domains, facets, and items. *Personality and Individual Differences, 171*, 110546.  [https://doi.org/10.1016/j.paid.2020.110546](https://doi.org/10.1016/j.paid.2020.110546)

In the current study, participants were invited to an online study investigating the associations among FoMO, socio-demographic factors, and personality. The final sample comprised 3370 people. Participants completed a FOMO scale and a personality inventory. The 10-item FOMO scale measured the extent of experiencing apprehension regarding missing out on interesting events of others on a 5-point scale (1 = “not at all true of me” to 5 = “extremely true of me”), producing a possible range of scores between 10 and 50. The Big Five Inventory (BFI) is a 45-item personality assessment questionnaire (note that only 43 items were used to match the study above) that uses a five-point response scale (1 = “very inapplicable” to 5 = “very applicable”). The BFI consists of five domains: Neuroticism (8 items; possible range of scores 8-40), Extraversion (8 items; possible range of scores 8-40), Openness to Experience (10 items; possible range of scores 10-50), Agreeableness (8 items; possible range of scores 8-40), and Conscientiousness (9 items; possible range of scores 9-45). We extended the aforementioned study to include an extra socio-demographic variable - a measure of popularity on social media based on the number of followers. Unlike the original study, we do not have measures of gender, education level, or specific country of residence.

__Data Dictionary__

The data in `FOMOdataset.csv` contain eight attributes collected from a simulated sample of $n=3370$ hypothetical individuals across the UK, and include: 

```{r echo=FALSE, message=FALSE, warning=FALSE}
fomo <- read_csv("https://uoepsy.github.io/data/FOMOdataset.csv")
tibble(
Variable = names(fomo),
Description = c("FoMO Score (as measured by the 10-item FoMO scale)", "Age (in years)", "Score on personality items assessing Neuroticism from the Big Five Inventory (BFI)", "Score on personality items assessing Extraversion from the Big Five Inventory (BFI)", "Score on personality items assessing Openness from the Big Five Inventory (BFI)", "Score on personality items assessing Agreeableness from the Big Five Inventory (BFI)", "Score on personality items assessing Conscientiousness from the Big Five Inventory (BFI)", "Total Number of Instagram Followers")
) %>% gt::gt()
```
 

__Preview__

The first six rows of the data are:

```{r echo=FALSE, message=FALSE}
read_csv('https://uoepsy.github.io/data/FOMOdataset.csv') %>% head %>% gt::gt()
```

`r optend()`

<div class="divider div-transparent div-dot"></div>

### Setup
`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read the FOMO dataset into R, assigning it to an object named `fomo`

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r, warning=FALSE, message=FALSE}
#Loading the required package(s)
library(tidyverse)
library(patchwork)
library(sjPlot)
library(kableExtra)
library(psych)

#Reading in fomo data and storing in object named 'fomo'
fomo <- read_csv("https://uoepsy.github.io/data/FOMOdataset.csv")

#check first six rows
head(fomo)
```

`r solend()`

<br>

### Provided Analysis Code

Below you will find the code required to conduct the analysis to address the research questions. This should look similar (in most areas) to what you worked through in lecture.

`r optbegin("Provided Analysis Code", olabel=FALSE,toggle=params$TOGGLE)`  

## Data Management

```{r message=FALSE, warning=FALSE}
library(tidyverse) # for all things!
library(psych) # good for descriptive stats
library(patchwork) # grouping plots together
library(kableExtra) # useful for creating nice tables
library(sjPlot) #regression tables & plots

fomo <- read_csv("https://uoepsy.github.io/data/FOMOdataset.csv")

# standardise FoMO & personality scores for RQ3
fomo <- 
  fomo %>% 
    mutate(
      FOMOz = (FOMO-mean(FOMO))/sd(FOMO),
      Oz = (O-mean(O))/sd(O),
      Cz = (C-mean(C))/sd(C),
      Ez = (E-mean(E))/sd(E),
      Az = (A-mean(A))/sd(A),      
      Nz = (N-mean(N))/sd(N))
#alternatively, you could do FOMOz = scale(FOMO, center = TRUE, scale = TRUE)
```


## Overall

```{r}
#######
#Descriptive Stats
#######


# the describe() function is from the psych package, and kable() from kableExtra which is used to make a nice table where the values are rounded to 2 decimal places using digits = 2. 
# We are first renaming our variables to give more appropriate / informative names.
#Next we are selecting columns 2, 3, 4, 8, and 9 from the describe output (n, mean, sd, min, max)

fomo %>% 
    select(FOMO, Age, TotalFollowers, N, E, O, C, A) %>%
    rename("Fear of Missing Out" = FOMO, "Age (in years)" = Age, "Number of Instagram Followers" = TotalFollowers, "Neuroticism" = N, "Extraversion" = E, "Openness" = O, "Conscientiousness" = C, "Agreeableness" = A) %>%
    describe() %>%
    select(2:4, 8:9) %>%
    rename("N" = n, "Mean" = mean, "SD" = sd, "Minimum" = min, "Maximum" = max) %>%
        kable(., caption = "FoMO, Socio-Demographic Factors, and Personality Traits Descriptive Statistics", digits = 2) %>%
        kable_styling()   

#from above, no missing values and scores within range (look at min and max values)


# scatterplot matrix, hist, and corr of FoMO, Socio-Demographic Factors, and Personality Traits Descriptive Statistics
pairs.panels(fomo %>%
    select(-FOMOz, -Oz, -Cz, -Ez, -Az, -Nz)) 
```

## RQ1 
```{r}
#######
#Descriptive Stats
#######

fomo %>% 
    select(FOMO, Age) %>%
    rename("Fear of Missing Out" = FOMO, "Age (in years)" = Age) %>%
    describe() %>%
    select(2:4, 8:9) %>%
    rename("N" = n, "Mean" = mean, "SD" = sd, "Minimum" = min, "Maximum" = max) %>%    
        kable(., caption = "FoMO and Age Descriptive Statistics", digits = 2) %>%
        kable_styling()    

# scatterplot
p1 <- ggplot(data = fomo, aes(x = Age, y = FOMO)) + 
    geom_point() + 
  geom_smooth(method = 'lm', se = FALSE, colour = 'red', linewidth=2) +
  labs(x = "(a) Age (in years)", y = "Fear of Missing Out")
p1

```

```{r}
#######
#Model Building
#######

fomo_mdl1 <- lm(FOMO ~ Age, data = fomo)
summary(fomo_mdl1)
confint(fomo_mdl1)

```

```{r}
#######
#Table for Results
#######

tab_model(fomo_mdl1,
          dv.labels = "FoMO",
          pred.labels = c("Age" = "Age (in years)"), 
          title = "RQ1: Regression Table for FoMO Model")
```


## RQ2
```{r}
#######
#Descriptive Stats
#######

fomo %>% 
    select(FOMO, Age, TotalFollowers) %>%
    rename("Fear of Missing Out" = FOMO, "Age (in years)" = Age, "Number of Instagram Followers" = TotalFollowers) %>%
    describe() %>%
    select(2:4, 8:9) %>%
    rename("N" = n, "Mean" = mean, "SD" = sd, "Minimum" = min, "Maximum" = max) %>%    
        kable(., caption = "FoMO and Socio-Demographic Factors Descriptive Statistics", digits = 2) %>%
        kable_styling()    


# scatterplots
p2 <- ggplot(data = fomo, aes(x = TotalFollowers, y = FOMO)) + 
    geom_point() + 
  geom_smooth(method = 'lm', se = FALSE, colour = 'purple', linewidth=2) +
  labs(x = "(b) Total Number of \nInstagram Followers", y = "Fear of Missing Out")

p2
```

```{r}
#######
#Model Building
#######

fomo_mdl2 <- lm(FOMO ~ Age + TotalFollowers, data = fomo)
summary(fomo_mdl2)
confint(fomo_mdl2)
```

```{r}
#######
#Model Comparison
#######

anova(fomo_mdl1 ,fomo_mdl2) %>%
    kable(caption = "Model Comparison - fomo_mdl1 vs fomo_mdl2", align = "c", digits = c(2,2,2,2,2,60)) %>%
    kable_styling(full_width = FALSE)

```

```{r}
#######
#Table for Results
#######

tab_model(fomo_mdl1 ,fomo_mdl2,
          dv.labels = c("FoMO","FoMO"),
          pred.labels = c("Age" = "Age (in years)",
                          "TotalFollowers" = "Number of Instagram Followers"), 
          title = "RQ2 - Regression Table for FoMO Model")
```

## RQ3 

```{r}
#######
#Descriptive Stats
#######

fomo %>% 
    select(FOMO, N, E, O, C, A) %>%
    rename("Fear of Missing Out" = FOMO, "Neuroticism" = N, "Extraversion" = E, "Openness" = O, "Conscientiousness" = C, "Agreeableness" = A) %>%
    describe() %>%
    select(2:4, 8:9) %>%
    rename("N" = n, "Mean" = mean, "SD" = sd, "Minimum" = min, "Maximum" = max) %>%    
        kable(., caption = "FoMO and Personality Descriptive Statistics", digits = 2) %>%
        kable_styling()  
```

```{r}
#######
#Model Building
#######

fomo_mdl3 <- lm(FOMOz ~ Nz + Ez + Oz + Cz + Az, data = fomo)
summary(fomo_mdl3)
confint(fomo_mdl3)
```

```{r}
#plot model examining significant personality predictors
N_plot <- plot_model(fomo_mdl3, type = "eff",
           terms = c("Nz"),
           show.data = TRUE,
           axis.title = c("Neuroticsm \n(z-scored)","FoMO Score (z-scored)"),
           title = "FoMO & N")

C_plot <- plot_model(fomo_mdl3, type = "eff",
           terms = c("Cz"),
           show.data = TRUE,
           axis.title = c("Conscientiousness \n(z-scored)","FoMO Score (z-scored)"),
           title = "FoMO & C")

A_plot <- plot_model(fomo_mdl3, type = "eff",
           terms = c("Az"),
           show.data = TRUE,
           axis.title = c("Agreeableness \n(z-scored)","FoMO Score (z-scored)"),
           title = "FoMO & A")

N_plot | C_plot | A_plot
```


```{r}
#create table for results - RQ3
tab_model(fomo_mdl3,
          dv.labels = "FoMO (Z-Scored)",
          pred.labels = c("Nz" = "Neuroticism (Z-Scored)",
                          "Ez" = "Extraversion (Z-Scored)",
                          "Oz" = "Openness (Z-Scored)",
                          "Az" = "Agreeableness (Z-Scored)",
                          "Cz" = "Conscientiousness (Z-Scored)"),
          title = "RQ3 - Regression Table for FoMO Model")
```

`r optend()`

### The 3-Act Structure

We need to present our report in three clear sections - think of your sections like the 3 key parts of a play or story - we need to (1) provide some background and scene setting for the reader, (2) present our results in the context of the research question, and (3) present a resolution to our story - relate our findings back to the question we were asked and provide our answer. 

#### Act I: Analysis Strategy

`r qbegin(1)`

Attempt to draft a discussion section based on the above research question and analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Analysis Strategy - What to Include***

Your analysis strategy will contain a number of different elements detailing plans and changes to your plan. Remember, your analysis strategy should **not** contain any results. You may wish to include the following sections:  

-  Very brief data and design description:
     - Give the reader some background on the context of your write-up. For example, you may wish to describe the data source, data collection strategy, study design, number of observational units.
     - Specify the variables of interest in relation to the research question, including their unit of measurement, the allowed range (for Likert scales), and how they are scored. If you have categorical data, you will need to specify the levels and coding of your variables, and what was specified as your reference level and the justification for this choice.

-  Data management:  
     - Describe any data cleaning and/or recoding.
     - Are there any observations that have been excluded based on pre-defined criteria? How/why, and how many? 
     - \* Describe any transformations performed to aid your interpretation (i.e., mean centering, standardisation, etc.)

-  Model specification:  
     -  Clearly state your hypotheses and specify your chosen significance level.
     -  What type of statistical analysis do you plan to use to answer the research question? (e.g., simple linear regression, multiple linear regression, binary logistic regression, etc.)
     - In some cases, you may wish to include some visualisations and descriptive tables to motivate your model specification. 
     -  Specify the model(s) to be fitted to answer your given research question and analysis structure. Clearly specify the response and explanatory variables included in your model(s). This includes specifying the type of coding scheme applied if using categorical data. 
     - Specify the assumption and diagnostic checks that you will conduct. Specify what plots you will use, and how you will evaluate these. 
:::

As noted and encouraged throughout the course, one of the main benefits of using RMarkdown is the ability to include inline R code in your document. Try to incorporate this in your write up so you can automatically pull the specified values from your code. If you need a reminder on how to do this, see [Lesson 4 of the Rmd Bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

:::

`r optbegin("Example Write-Up of Analysis Strategy Section", olabel=FALSE, toggle = params$TOGGLE)`

The dataset contained information on 3370 participants, including scores on 5 personality traits assessed by the a reduced version of the 45 item Big Five Inventory (BFI; Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism), where 43 items were used. Participants were also assessed on the FoMO scale - a 10 item measure which assessed the extent to which individuals experienced apprehension regarding missing out on interesting events of others. Items on both the BFI and FoMO scales were measured on a 5-point scale, meaning that scores on the BFI could range 8-40 for Neuroticism, 8-40 for Extraversion, 10-50 for Openness to Experience, 8-40 for Agreeableness, and 9-45 for Conscientiousness; and scores on the FoMO scale could range 10-50. For both of these measures, only total scores were available. Two socio-demographic questions were also asked of all participants - their age (in years) and total number of Instagram followers. 

The aim of this report was to address three research questions:

1. Does age predict FOMO?
2. Does the total number of people one follows on Instagram predict FOMO over and above age?
3. Does personality predict FOMO?

To examine the associations among the variables of interest, a scatter plot of matrices (SPLOM) was used. This allowed for visual examination of marginal distributions via histograms, and bivariate associations between variables via scatter plots. The SPLOM also provided bivariate correlations between each of the variables. 

To address RQ1, the following simple linear regression model was used:

$$
\text{FoMO} = \beta_0 + \beta_1 \cdot Age + \epsilon
$$

where we tested whether there was a significant association between FoMO and age. Formally, this corresponded to testing whether the age coefficient was equal to zero:

$$
H_0: \beta_1 = 0
$$

$$
H_1: \beta_1 \neq 0
$$

To address RQ2, the two following models were used:
$$
\text{FoMO} = \beta_0 + \beta_1 \cdot Age + \epsilon
$$

$$
\text{FoMO} = \beta_0 + \beta_1 \cdot Age + \beta_2 \cdot Number~of~Instagram~Followers + \epsilon
$$

To determine whether the model with the number of Instagram followers and age was a significantly better fitting than the model with age alone, we used an incremental $F$-test to formally test the following hypothesis:

$$
H_0: \beta_2 = 0
$$

$$
H_1: \beta_2 \neq 0
$$

To address RQ3, the following multiple linear regression model was used, where all measures were $z$-scored:

$$
\text{FoMO} = \beta_0 + \beta_1 \cdot N + \beta_2 \cdot E + \beta_3 \cdot O + \beta_4 \cdot C + \beta_5 \cdot A + \epsilon
\quad \\  
$$
$$
\begin{align}
& \text{Where:} \\
& FoMO = \text{FoMO, z-scored} \\
& N = \text{Neuroticism, z-scored} \\
& E = \text{Extraversion, z-scored} \\
& O = \text{Openness, z-scored} \\
& C = \text{Conscientiousness, z-scored} \\
& A = \text{Agreeableness, z-scored} \\
\end{align}
$$

To investigate whether personality predicted FoMO, this corresponded to testing whether each of the Big 5 personality traits were equal to zero:

$$
H_0: \text{All}~~ \beta_j = 0 ~\text{(for j = 1, 2, 3, 4, 5)}
$$
$$ 
H_1: \text{At least one}~ \beta_j \neq \text{(for j = 1, 2, 3, 4, 5)}
$$

All participant data was complete (no missing values), with scores on the FoMO and BFI scales within possible ranges. Throughout the report, effects were considered statistically significant at $\alpha = .05$. 

`r optend()`

#### Act II: Results

`r qbegin(2)`

Attempt to draft a results section based on your detailed analysis strategy and the analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Results - What To Include***

The results section should follow from your analysis strategy. This is where you would present the evidence and results that will be used to answer the research questions and can support your conclusions. Make sure that you address all aspects of the approach you outlined in the analysis strategy (including the evaluation of assumptions and diagnostics). 

In this section, it is useful to include tables and plots to clearly present your findings to your reader. It is important, however, to carefully select what is the key information that should be presented. You don't want to overload the reader with unnecessary or duplicate information, and you also want to save space in case there is a page limit. Make use of figures with multiple panels where you can.

As a broad guideline, you want to start with the results of any exploratory data analysis, presenting tables of summary statistics and exploratory plots. You may also want to visualise associations between/among variables and report covariances or correlations. Then, you should move on to the results from your model.

:::

`r optbegin("Example Write-Up of Results Section", olabel=FALSE, toggle = params$TOGGLE)`

Descriptive statistics are displayed in @tbl-desctab. 

```{r desctab, echo = FALSE}
#| label: tbl-desctab
#| tbl-cap: FoMO, Socio-Demographic Factors, and Personality Trait Descriptive Statistics
fomo %>% 
    select(FOMO, Age, TotalFollowers, N, E, O, C, A) %>%
    rename("Fear of Missing Out" = FOMO, "Age (in years)" = Age, "Number of Instagram Followers" = TotalFollowers, "Neuroticism" = N, "Extraversion" = E, "Openness" = O, "Conscientiousness" = C, "Agreeableness" = A) %>%
    describe() %>%
    select(2:4, 8:9) %>%
    rename("N" = n, "Mean" = mean, "SD" = sd, "Minimum" = min, "Maximum" = max) %>%    
        kable(., caption = "FoMO, Socio-Demographic Factors, and Personality Traits Descriptive Statistics", digits = 2) %>%
        kable_styling() 
```

Bivariate correlations showed a moderate negative association between FoMO and age; moderate positive associations between FoMO and Neuroticism and between FoMO and number of Instagram followers; and weak associations between FoMO and the remaining Big 5 personality variables (see @fig-sphcor). 

```{r sphcor, echo=FALSE, fig.cap="Bivariate scatter plots (below diagonal), histograms (diagonal), and Pearson correlation coefficient (above diagonal) for FoMO, Socio-Demographic Factors, and Personality Traits"}
#| label: fig-sphcor
#| fig-cap: Bivariate scatter plots (below diagonal), histograms (diagonal), and Pearson correlation coefficient (above diagonal) for FoMO, Socio-Demographic Factors, and Personality Traits
pairs.panels(fomo %>%
    select(- FOMOz, - Oz, - Cz, - Ez, - Az, - Nz)) 
```

```{r include=FALSE}
mdl1_sum <- summary(fomo_mdl1)
mdl2_sum <- summary(fomo_mdl2)
mdl3_sum <- summary(fomo_mdl3)
```

In relation to RQ1, full regression results, including 95\% Confidence Intervals, are shown in @tbl-rq1-results. This model was significant ($F(`r round(mdl1_sum$fstatistic['numdf'])`, `r round(mdl1_sum$fstatistic['dendf'])`)=`r round(mdl1_sum$fstatistic['value'], 2)`, p <.001)$, and results indicated that age was a significant predictor of FoMO $(\beta = `r round(mdl1_sum$coefficients['Age', 'Estimate'],2)`, CI_{95} = [`r round(confint(fomo_mdl1)['Age',], 2)`], SE = `r round(mdl1_sum$coefficients['Age','Std. Error'], 2)`, p <.001)$. Age explained approximately `r round(mdl1_sum$r.squared*100, 0)`% of the variance in FoMO scores. For every one-year increase in age, participants' FoMO scores decreased by 0.20 points. This suggested that older participants experienced less FoMO than younger participants (see @fig-rq1(a)).

```{r echo = FALSE}
#| label: tbl-rq1-results
#| tbl-cap: RQ1 - Regression Table for FoMO Model
tab_model(fomo_mdl1,
          dv.labels = "FoMO",
          pred.labels = c("Age" = "Age (in years)"), 
          title = "RQ1 - Regression Table for FoMO Model")
```

```{r echo = FALSE}
#| label: fig-rq1
#| fig-cap: "(a) Association between FoMO and Age (b) Association between FoMO and Instagram Followers"
p1 | p2
```

```{r include=FALSE}
RQ2test <- anova(fomo_mdl1, fomo_mdl2)
names(RQ2test)[6]<-"p"
```

As presented in @tbl-rq2-results, we found evidence that an individual's total number of followers on Instagram and age provided a significantly better fitting model predicting FoMO over age alone $(F(`r paste(RQ2test$Df[2])` ,`r paste(RQ2test$Res.Df[2])`) = `r round(RQ2test$F[2],2)`, p`r map_chr(RQ2test$p[2], ~ifelse(.<001,"<.001",paste0("=",round(.,2))))`)$. Together, age and the number of Instagram followers explained approximately `r round(mdl2_sum$adj.r.squared*100, 0)`% of the variance in FoMO scores.

```{r echo = FALSE}
#| label: tbl-rq2-results
#| tbl-cap: RQ2 - Model Comparison - fomo_mdl1 vs fomo_mdl2
anova(fomo_mdl1 ,fomo_mdl2) %>%
    kable(caption = "Model Comparison - fomo_mdl1 vs fomo_mdl2", align = "c", digits = c(2,2,2,2,2,60)) %>%
    kable_styling(full_width = FALSE)
```

In order to address the third research question, all scores were converted to $z$-scores. The Big 5 personality traits significantly predicted FoMO scores ($F(`r round(mdl3_sum$fstatistic['numdf'])`, `r round(mdl3_sum$fstatistic['dendf'])`) = `r round(mdl3_sum$fstatistic['value'], 2)`, p < .001$). The personality traits collectively explained approximately `r round(mdl3_sum$adj.r.squared*100, 0)`% of variance in FoMO scores. Full model results are presented in @tbl-rq3-results. For every 1 standard deviation unit increase in Neuroticism scores, FoMO scores increased by `r round(mdl3_sum$coefficients['Nz','Estimate'], 2)` standard deviations. For every 1 standard deviation unit increase in Conscientiousness scores, FoMO scores decreased by 0.31 standard deviations. For every 1 standard deviation unit increase in Agreeableness scores, FoMO scores decreased by 0.09 standard deviations. Neither Extraversion $(p = .27)$ nor Openness $(p = .47)$ were significant predictors of FoMO. 

```{r echo = FALSE}
#| label: tbl-rq3-results
#| tbl-cap: RQ3 - Regression Table for FoMO Model with Personality
tab_model(fomo_mdl3,
          dv.labels = "FoMO (Z-Scored)",
          pred.labels = c("Nz" = "Neuroticism (Z-Scored)",
                          "Ez" = "Extraversion (Z-Scored)",
                          "Oz" = "Openness (Z-Scored)",
                          "Az" = "Agreeableness (Z-Scored)",
                          "Cz" = "Conscientiousness (Z-Scored)"),
          title = "RQ3 - Regression Table for FoMO Model with Personality")
```

Associations between FoMO and the three significant personality traits (whilst holding the others constant) are shown in @fig-rq3. 

```{r echo = FALSE}
#| label: fig-rq3
#| fig-cap: "Predicted FoMO Scores by Neuroticsm, Conscientiousness, and Agreeableness after accounting for other Big 5 Traits"
N_plot | C_plot | A_plot
```

`r optend()`

#### Act III: Discussion  

`r qbegin(3)`

Attempt to draft a discussion section based on your results and the analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Discussion - What To Include

In the discussion section, you should summarise the key findings from the results section and provide the reader with a few take-home sentences drawing the analysis together and relating it back to the original question. 

The discussion should be relatively brief, and should not include any statistical analysis - instead think of the discussion as a conclusion, providing an answer to the research question(s).

:::

`r optbegin("Example Write-Up of Discussion Section", olabel=FALSE, toggle = params$TOGGLE)`

Previous research had identified an association between an individual’s fear of missing out, various socio-demographic factors, and personality. In this analysis, we examined the associations of FoMO with age, total number of Instagram followers, and personality.

Our results suggested that: (1) age was a significant predictor of FoMO, where older age was associated with lower levels of FoMO; (2) including the total number of Instagram followers as well as age resulted in a significantly better fitting model for explaining FoMO scores (where more followers were associated with higher levels of FoMO); and (3) three of the Big five personality traits - Neuroticism, Agreeableness, and Conscientiousness - were significant predictors of FoMO (specifically high Neuroticism, low Agreeableness, and low Conscientiousness). Therefore, we can reject the null hypotheses for each of our research questions. 

However, it is important to note that we can make no causal claims on the directions of these associations from these data - we cannot conclude that being a young person who is highly Neurotic and low on Agreeableness and Conscientiousness with a large number of Instagram followers will __cause__ feelings of FoMO.

`r optend()`

<br>

# Section B: Block 1 (Weeks 7 - 10) Recap

In the second part of the lab, there is no new content - the purpose of the recap section is for you to revisit and revise the concepts you have learned over the last 4 weeks. 

:::red

Before you expand each of the boxes below, think about how comfortable you feel with each concept.  

:::

`r optbegin("Binary Variables", olabel=FALSE,toggle=params$TOGGLE)`

We can include categorical predictors in a linear regression, but the interpretation of the coefficients is very specific. Whereas we talked about coefficients being interpreted as "the change in $y$ associated with a 1-unit increase in $x$", for categorical explanatory variables, coefficients can be considered to examine differences in group means. However, they are actually doing exactly the same thing - the model is simply translating the levels (like "Yes"/"No") in to 0s and 1s!  

Our coefficients are just the same as before. The intercept is where our predictor equals zero, and the slope is the change in our outcome variable associated with a 1-unit change in our predictor.  

However, "zero" for this predictor variable now corresponds to a whole level. This is known as the "reference level". Accordingly, the 1-unit change in our predictor (the move from "zero" to "one") corresponds to the difference between the two levels. 

:::{.callout-note}

See [S1 Week 7 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_06_dummy.html) and [S1 Week 7 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_06_LMcategorical1.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Categorical Predictors with k levels", olabel=FALSE,toggle=params$TOGGLE)`

We saw that a _binary categorical_ variable gets inputted into our model as a variable of 0s and 1s (these typically get called __"dummy variables"__).  

:::statbox
__Dummy variables__ are numeric variables that represent categorical data.  
:::

When we have a _categorical_ explanatory variable with __more than 2 levels__, our model gets a bit more - it needs not just one, but _a number of_ dummy variables. For a categorical variable with $k$ levels, we can express it in $k-1$ dummy variables.  

For example, the "species" column below has three levels, and can be expressed by the two variables "species_dog" and "species_parrot":  
```{r echo=FALSE, out.width="80%"}
data.frame(
  species = c("cat","cat","dog","parrot","dog","cat","..."),
  species_dog = c(0,0,1,0,1,0,"..."),
  species_parrot = c(0,0,0,1,0,0,"...")
)
```

+ The "cat" level is expressed whenever both the "species_dog" and "species_parrot" variables are 0.
+ The "dog" level is expressed whenever the "species_dog" variable is 1 and the "species_parrot" variable is 0.
+ The "parrot" level is expressed whenever the "species_dog" variable is 0 and the "species_parrot" variable is 1.  

`R` will do all of this re-expression for us. If we include in our model a categorical explanatory variable with 4 different levels, the model will estimate 3 parameters - one for each dummy variable. We can interpret the parameter estimates (the coefficients we obtain using `coefficients()`,`coef()` or `summary()`) as the estimated increase in the outcome variable associated with an increase of one in each dummy variable (holding all other variables equal).  

```{r echo=FALSE}
set.seed(348)
catplot <- tibble(
  species = rep(c("cat","dog","parrot"), each = 15),
  outcome = c(rnorm(15,60,5), rnorm(15,50,5), rnorm(15,55,5))
)
cstat = coef(lm(outcome~species,catplot))
pander::pander(summary(lm(outcome~species,catplot))$coefficients)
```

Note that in the above example, an increase in 1 of "species_dog" is the difference between a "cat" and a "dog". An increase in one of "species_parrot" is the difference between a "cat" and a "parrot". We think of the "cat" category in this example as the _reference level_ - it is the category against which other categories are compared against. 

```{r echo=FALSE, message=FALSE, warning = FALSE}
ggplot(catplot, aes(x=species, y=outcome))+
  #geom_boxplot(fatten=NULL)+
  geom_jitter(height=0,width=.05, alpha=.4)+
  geom_point(x=1,y=cstat[1], col="blue",size=3)+
  annotate("text",x=1,y=cstat[1],label=expression(paste(beta[0], " (intercept)")), col="blue", hjust=1.1)+
  geom_segment(aes(x=1,xend=2,y=cstat[1],yend=cstat[1]+cstat[2]),col="blue")+
  geom_segment(aes(x=1,xend=2,y=cstat[1],yend=cstat[1]),col="blue", lty="dashed")+
  geom_segment(aes(x=2,xend=2,y=cstat[1],yend=cstat[1]+cstat[2]),col="blue", lty="dashed")+
  annotate("text",x=2.15,y=mean(c(cstat[1],sum(cstat[1:2]))),label=expression(paste(beta[1], " (slope)")), col="blue", hjust=.35)+
  
  geom_segment(aes(x=1,xend=3,y=cstat[1],yend=cstat[1]+cstat[3]),col="blue")+
  geom_segment(aes(x=1,xend=3,y=cstat[1],yend=cstat[1]),col="blue", lty="dashed")+
  geom_segment(aes(x=3,xend=3,y=cstat[1],yend=cstat[1]+cstat[3]),col="blue", lty="dashed")+
  annotate("text",x=3.15,y=mean(c(cstat[1],sum(cstat[c(1,3)]))),label=expression(paste(beta[2], " (slope)")), col="blue", hjust=.35)
```

:::{.callout-note}

See [S1 Week 7 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_06_dummy.html) and [S1 Week 7 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_06_LMcategorical1.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Side Contraints", olabel=FALSE,toggle=params$TOGGLE)`

Possible side-constraints on the parameters are:

|       Name      |             Constraint            |             Meaning of $\beta_0$            |         R         |
|:---------------:|:---------------------------------:|:-------------------------------------------:|:-----------------:|
| Sum to zero (Effects Coding) | $\beta_1 + \beta_2 + \beta_3 = 0$ | $\beta_0 = \mu$   |    `contr.sum`    |
| Reference group (Dummy Coding) |           $\beta_1 = 0$           | $\beta_0 = \mu_1$ | `contr.treatment` |


:::blue

**IMPORTANT**

- By default `R` uses the reference group constraint. If your factor has $g$ levels, your regression model will have $g-1$ dummy variables (`R` creates them for you)

- We can switch back to the default reference group constraint by applying either of these:

```{r eval = FALSE}
# Option 1
contrasts(rest_spend$music) <- NULL
# Option 2
contrasts(rest_spend$music) <- "contr.treatment"
```
:::

:::{.callout-note}

See [S1 Week 7 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_06_dummy.html), [S1 Week 8 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_07_effects.html), [S1 Week 7 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_06_LMcategorical1.html#1), [S1 Week 8 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_07_lmcategorical2.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Contrasts: Rules for Assigning Weights", olabel=FALSE,toggle=params$TOGGLE)`  

+ **Rule 1**: Weights are -1 $\geq$ x $\leq$ 1
+ **Rule 2**: The group(s) in one chunk are given negative weights, the group(s) in the other get positive weights
+ **Rule 3**: The sum of the weights of the comparison must be 0
+ **Rule 4**: If a group is not involved in the comparison, weight is 0
+ **Rule 5**: For a given comparison, weights assigned to group(s) are equal to 1 divided by the number of groups in that chunk.
+ **Rule 6**: Restrict yourself to running $k$ - 1 comparisons (where $k$ = number of groups)
+ **Rule 7**: Each contrast can only compare 2 chunks of variance
+ **Rule 8**: Once a group singled out, it can not enter other contrasts 

:::{.callout-note}

See [S1 Week 8 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_07_effects.html) and [S1 Week 8 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_07_lmcategorical2.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Assumptions: Linearity", olabel=FALSE,toggle=params$TOGGLE)`

### Simple Linear Regression
In simple linear regression (SLR) with only one explanatory variable, we could assess linearity through a simple scatterplot of the outcome variable against the explanatory. This would allow us to check if the errors have a mean of zero. If this assumption was met, the residuals would appear to be randomly scattered around zero.  
The rationale for this is that, once you remove from the data the linear trend, what's left over in the residuals should not have any trend, i.e. have a mean of zero.

### Multiple Regression
In multiple regression, however, it becomes more necessary to rely on diagnostic plots of the model residuals. This is because we need to know whether the relations are linear between the outcome and each predictor _after accounting for the other predictors in the model._  

In order to assess this, we use **partial-residual plots** (also known as 'component-residual plots'). This is a plot with each explanatory variable $x_j$ on the x-axis, and **partial residuals** on the y-axis.

Partial residuals for a predictor $x_j$ are calculated as:
$$
\hat \epsilon + \hat \beta_j x_j
$$

:::blue

In **R**, we can easily create these plots for all predictors in the model by using the `crPlots()` function from the `car` package.  

:::

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Assumptions: Equal Variances (Homoscedasticity)", olabel=FALSE,toggle=params$TOGGLE)`

The equal variances assumption is that the error variance $\sigma^2$ is constant across values of the predictor(s) $x_1, \dots,  x_k$, and across values of the fitted values $\hat y$. This sometimes gets termed "Constant" vs "Non-constant" variance. This is presented visually in @fig-ncv-violate and @fig-ncv-noviolate. 

```{r ncv1, echo=FALSE}
library(patchwork)
n=1000
x <- runif(n, min = 0, max = 100)
y.increasing <- 3 + 0.2 * x + (1 + x / 25) * rnorm(n, sd = 3)
y.good <- 3 + 0.1 * x + rnorm(n, sd = 3)


lm.good <- lm(y.good ~ x)
lm.bad <-lm(y.increasing~x)

p1<-ggplot(NULL, aes(x=fitted(lm.bad), y=resid(lm.bad)))+
  geom_point(shape=1)+
  labs(x="fitted",y="residuals")+
  theme(axis.text = element_blank())

p2<-ggplot(NULL, aes(x=fitted(lm.good), y=resid(lm.good)))+
  geom_point(shape=1)+
  labs(x="fitted",y="residuals")+
  theme(axis.text = element_blank())

p3<-ggplot(NULL, aes(x=fitted(lm.bad)<mean(fitted(lm.bad)), y=resid(lm.bad)))+
  geom_point(shape=1)+
  labs(x="fitted",y="residuals")+
  theme(axis.text = element_blank())

p4<-ggplot(NULL, aes(x=fitted(lm.good)<mean(fitted(lm.good)), y=resid(lm.good)))+
  geom_point(shape=1)+
  labs(x="fitted",y="residuals")+
  theme(axis.text = element_blank())
```

```{r echo = FALSE, fig.width = 8, out.width = '90%'}
#| label: fig-ncv-violate
#| fig-cap: "Non-constant variance for numeric and categorical X"
(p1 | p3) + plot_annotation(title = "Non-constant variance")
```


```{r echo = FALSE, fig.width = 8, out.width = '90%'}
#| label: fig-ncv-noviolate
#| fig-cap: "Constant variance for numeric and categorical X"
(p2 | p4) + plot_annotation(title = "Constant variance")
```

:::blue

In **R**, we can create plots of the _Pearson residuals_ against the predicted values $\hat y$ and against the predictors $x_1$, ... $x_k$ by using the `residualPlots()` function from the `car` package. This function also provides the results of a lack-of-fit test for each of these relationships (note when it is the fitted values $\hat y$ it gets called "Tukey's test").  

:::

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Assumptions: Independence (of errors)", olabel=FALSE,toggle=params$TOGGLE)`

The 'independence of errors' assumption is the condition that the errors do not have some underlying relationship which is causing them to influence one another. 
<br>

There are many sources of possible dependence, and often these are issues of study design. For example, we may have groups of observations in our data which we would expect to be related (e.g., multiple trials from the same participant). Our modelling strategy would need to take this into account.
<br>

One form of dependence is **autocorrelation** - this is when observations influence those adjacent to them. It is common in data for which *time* is a variable of interest (e.g, the humidity today is dependent upon the rainfall yesterday). 

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Assumptions: Normality (of errors)", olabel=FALSE,toggle=params$TOGGLE)`

The normality assumption is the condition that the errors $\epsilon$ are normally distributed in the population.  

We can visually assess this condition through histograms, density plots, and quantile-quantile plots (QQplots) of our residuals $\hat \epsilon$.   

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Multicollinearity", olabel=FALSE,toggle=params$TOGGLE)`

For the linear model with **multiple** explanatory variables, we need to also think about **multicollinearity** - this is when two (or more) of the predictors in our regression model are moderately or highly correlated.  

We can assess multicollinearity using the **variance inflation factor (VIF)**, which for a given predictor $x_j$ is calculated as:  

$$
VIF_j = \frac{1}{1-R_j^2} \\
$$

Suggested cut-offs for VIF are varied. Some suggest 10, others 5. Define what you will consider an acceptable value _prior_ to calculating it. You could loosely interpret VIF values >5 as moderate multicollinearity and values >10 as severe multicollinearity.    

:::blue

In **R**, the `vif()` function from the `car` package will provide VIF values for each predictor in your model. 

:::

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Individual Case Diagnostics", olabel=FALSE,toggle=params$TOGGLE)`

We have seen in the case of the simple linear regression that individual cases in our data can influence our model more than others. We know about:

+ **Regression outliers:** A large residual $\hat \epsilon_i$ - i.e., a big discrepancy between their predicted y-value and their observed y-value.  
    + **Standardised residuals:** For residual $\hat \epsilon_i$, divide by the estimate of the standard deviation of the residuals. In R, the `rstandard()` function will give you these
    + **Studentised residuals:** For residual $\hat \epsilon_i$, divide by the estimate of the standard deviation of the residuals excluding case $i$. In R, the `rstudent()` function will give you these.
+ **High leverage cases:** These are cases which have considerable _potential_ to influence the regression model (e.g., cases with an unusual combination of predictor values). 
    + **Hat values:** are used to assess leverage. In R, The `hatvalues()` function will retrieve these. 
+ **High influence cases:** When a case has high leverage *and* is an outlier, it will have a large influence on the regression model. 
    + **Cook's Distance:** combines *leverage* (hatvalues) with *outlying-ness* to capture influence: $D_i = \text{Outlyingness} \times \text{Leverage}$. Cook's distance refers to the average distance the $\hat{y}$ values will move if a given case is removed. In `R`, the `cooks.distance()` function will provide these values. 
Alongside Cook's Distance, we can examine the extent to which model estimates and predictions are affected when an entire case is dropped from the dataset and the model is refitted.  
+ **DFFit:** the change in the predicted value at the $i^{th}$ observation with and without the $i^{th}$ observation is included in the regression.  
+ **DFbeta:**  the change in a specific coefficient with and without the $i^{th}$ observation is included in the regression.  
+ **DFbetas:**  the change in a specific coefficient divided by the standard error, with and without the $i^{th}$ observation is included in the regression.  
+ **COVRATIO:** measures the effect of an observation on the covariance matrix of the parameter estimates. In simpler terms, it captures an observation's influence on standard errors.

:::blue

In **R**, we can get lots of these measures with the `influence.measures()` function:


+ `influence.measures(my_model)` will give you out a dataframe of the various measures.
+ `summary(influence.measures(my_model))` will provide a nice summary of what R deems to be the influential points.

:::

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Bootstrap: Terminology", olabel=FALSE,toggle=params$TOGGLE)`

- A _parameter_ is a numerical summary for the population, e.g. the population slope $\beta_1$.
- A _statistic_ is a numerical summary calculated from the sample data, e.g. the estimated slope in the sample $\widehat \beta_1$. We use the sample statistic as a best guess, or estimate, for the unknown population parameter.
- A _bootstrap sample_ is chosen with replacement from an existing sample, using the same sample size.
- A _bootstrap statistic_ is a statistic computed for each bootstrap sample.
- A _bootstrap distribution_ collects bootstrap statistics for many bootstrap samples.

:::{.callout-note}

See [S1 Week 10 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_09_bootstrap.html) and [S1 Week 10 Lectures]() for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Bootstrap", olabel=FALSE,toggle=params$TOGGLE)`

The _bootstrap_ is a general approach to assessing whether the sample results are statistically significant or not, and allows us to draw inferences to the population from a regression model. This method is assumption-free and does not rely on conditions such as normality of the residuals.

It is based on sampling repeatedly with replacement (to avoid always getting the original sample exactly) from the data at hand, and then computing the regression coefficients from each re-sample. We will equivalently use the word "bootstrap sample" or "resample" (for **sample** with **re**placement).

:::frame

The basic principle is:

<center>
__The population is to the original sample__

__as__

__the original sample is to the bootstrap samples.__

</center>

:::

Because we only have one sample of size $n$, and we do not have access to the data for the entire population, we consider our original sample as our best approximation to the population. 

To be more precise, we assume that the population is made up of many, many copies of our original sample. Then, we take multiple samples each of size $n$ from this assumed population. This is equivalent to sampling _with replacement_ from the original sample.

```{r echo=FALSE, out.width = '90%'}
knitr::include_graphics('images/reg-boot.png')
```

:::{.callout-note}

See [S1 Week 10 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_09_bootstrap.html) and [S1 Week 10 Lectures]() for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Bootstrap: In R", olabel=FALSE,toggle=params$TOGGLE)`

Follow these steps:

* 1: Load the `car` library
* 2: Use the `Boot()` function (do not forget the uppercase B!) which takes as arguments:
    - the fitted model
    - `f`, saying which bootstrap statistics to compute on each bootstrap sample. By default `f = coef`, returning the regression coefficients.
    - `R`, saying how many bootstrap samples to compute. By default `R = 999` but this could be any number. To experiment we recommend 1000, when you want to produce results for journals, it is typical to go with 10,000 or more.
    - `ncores`, saying if to perform the calculations in parallel (and more efficiently). However, this will depend on your PC, and you need to find how many cores you have by running `parallel::detectCores()` on your PC. By default the function uses `ncores = 1`.
* 3: Run the code. However, please remember that the `Boot()` function does **not** want a model which was fitted using data with `NAs`. To remove, for example, you could use `na.omit`.
* 4: Look at the `summary()` of the bootstrap results. When doing so the output will show, for each regression coefficient, the value in the original sample in the column `original`, and in the `bootSE` column, the estimate of the variability of the coefficient from bootstrap sample to bootstrap sample. The `bootSE` provides us the bootstrap standard error, or bootstrap SE in short. We can use this to answer the key question of how accurate our estimate is.
* 5: Compute confidence intervals. Use your preferred confidence level (usually, and by default, 95%)
* 6: Provide interpretation in the context of your research question and report results in APA format. 

:::blue

In **R**, for example:
```{r eval = FALSE}
#specify model
mymodel <- lm(y ~ x1 + x2, data = mydata)

#load car package
library(car)

#bootstrap model
bootmymodel <- Boot(mymodel, R = 1000)

#confidence intervals
Confint(bootmymodel, level = 0.95, type = "perc")
```

:::

:::{.callout-note}

See [S1 Week 10 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_09_bootstrap.html) and [S1 Week 10 Lectures]() for further details, examples, and to revise these concepts further.

:::

`r optend()`
